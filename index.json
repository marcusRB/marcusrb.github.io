[{"authors":["admin"],"categories":null,"content":"  “If you torture the data long enough, it will confess.” – Ronald Coase\n Data Specialist  Data collection Data prep and processing Data mining \u0026amp; Data modeling Business Intelligence \u0026amp; Big Data Analyst AI: Machine Learning \u0026amp; Deep Learning Data Visualization: Power BI - Tableau - Qlik - Google Data Studio Big Data environment AWS • GCP • Azure • Databricks Robotics \u0026amp; IoT  Analista Digital  Gestión plan Analítica digital Implementación y configuración Tag Management Visualización y Data Analysis Consultoría y asesoría SEO \u0026amp; SEM (Search, Display, YouTube, Shopping, Mobile)  Larga experiencia en gestión de proyectos para grandes cuentas en retail, banco-finanza, seguros, energy\u0026amp;power, industria y corporate. Sectores industriales, comerciales nacionales en España, Europa y LATAM.\nDocente \u0026amp; Formador in-company: En los últimos 8 años he impartido +3500 horas de formación en:\n data science, inteligencia artificial, machine learning, deep learning, creación material y tutoría en programas de grados, posgrados y máster, además de cursos en data science y data analytics. big data, infraestructura en cloud AWS, GCP, Azure, tutoriales para machine learning y utilizo de pipeline para ETL, Storage, Base de datos, explotación y visualización. Hadoop y Spark. Herramientas de minería Knime, RapidMiner, H2O, Databricks, entre muchos. data analytics, fundamentos y Data mining en R-Studio, Weka, Python business intelligence, ETL: Talend, Pentaho, SSIS - Data Warehouse, SQL, modelado de datos: SSAS, DAX data visualization, visualización de datos e interpretación con cuadro de mando y dashboard en Google Data Studio, PowerBI, Tableau, Qlik Sense, Grafana. digital marketing, SEO, Google Ads(ex AdWords), FacebookAds, Google Analytics, Tag Manager, Optimize. ecommerce, SEO, Prestashop, Wordpress, Comercio Electrónico y Mobile  Formación presencial y a distancia en: escuelas de negocios:\n EAE Business School IIM Instituto Internacional de Marketing IEDGE Business School IMF Business School IEBS Business School CICE escuela profesional  universidades:\n Colaborador/Consultor en la UOC Universitat Oberta Catalunya MIT Massachusetts Institute Technology  otros centros:\n Neoland Fictizia AdveiSchool Digital Brain Aula Creactiva  plataformas eLearning:\n Udemy YouTube Google Scholar EdX / Coursera vTutor  Otras colaboraciones y formaciones in-company de Machine Learning, Data Analytics y Digital Analysis-Tag Management y Visualización de datos.\nFORMACIONES ACTUALES / FUTURAS:  Máster de Business Intelligence, curso presencial/remoto de Inteligencia de negocio, ETL, Power BI, SSAS, Data Integration en CICE Business School\n Bootcamp de Data Science y de Data Analytics, Machine Learning, Deep Learning y Big Data en Máster en Data Science Neoland presencial y online en Madrid.\n Bootcamp de Data Science y Deep Learning en ARTIFICIAL INTELLINGE SCHOOL\n Data Analysis, cursos presenciales/ a distancia de Google Tag Manager y Digital Analytics en KPIschool.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"es","lastmod":1609845561,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.marcusrb.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"“If you torture the data long enough, it will confess.” – Ronald Coase\n Data Specialist  Data collection Data prep and processing Data mining \u0026amp; Data modeling Business Intelligence \u0026amp; Big Data Analyst AI: Machine Learning \u0026amp; Deep Learning Data Visualization: Power BI - Tableau - Qlik - Google Data Studio Big Data environment AWS • GCP • Azure • Databricks Robotics \u0026amp; IoT  Analista Digital  Gestión plan Analítica digital Implementación y configuración Tag Management Visualización y Data Analysis Consultoría y asesoría SEO \u0026amp; SEM (Search, Display, YouTube, Shopping, Mobile)  Larga experiencia en gestión de proyectos para grandes cuentas en retail, banco-finanza, seguros, energy\u0026amp;power, industria y corporate.","tags":null,"title":"Marco Russo","type":"authors"},{"authors":null,"categories":null,"content":" Guias, recursos y tutoriales de Apache Hadoop, con ejemplos a descargar.\nEstructura recursos - Introducción Apache Hadoop  ","date":1609718400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609872091,"objectID":"33b0ccef66c483571bd5919e8e822bf3","permalink":"https://www.marcusrb.com/big-data/recursos-apache-hadoop/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/big-data/recursos-apache-hadoop/","section":"recursos","summary":"Recursos de Apache Hadoop","tags":null,"title":"Recursos y tutoriales de Apache Hadoop","type":"docs"},{"authors":null,"categories":null,"content":" Guias, recursos y tutoriales de Apache Nifi, con ejemplos a descargar.\nEstructura recursos - Introducción Apache Nifi  ","date":1609718400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609872091,"objectID":"97ba7cb4ec851d346a8a37ce0a7e5906","permalink":"https://www.marcusrb.com/big-data/recursos-apache-nifi/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/big-data/recursos-apache-nifi/","section":"recursos","summary":"Recursos de Apache Nifi","tags":null,"title":"Recursos y tutoriales de Apache Nifi","type":"docs"},{"authors":null,"categories":null,"content":" Guias, recursos y tutoriales de Microsoft Power BI, con ejemplos a descargar con Power Query, Power M, DAX y visualización de datos.\nEstructura recursos Introducción a Power BI Carga de datos  ","date":1609718400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"a14c2fa351d6e2dd79d0cb70cd063a6d","permalink":"https://www.marcusrb.com/recursos-power-bi/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/recursos-power-bi/","section":"recursos","summary":"Tutoriales de Microsoft Power BI.","tags":null,"title":"Tutoriales de Power BI, DAX, Power Query, Power M","type":"docs"},{"authors":null,"categories":null,"content":" Recursos de Cloud Computing especificamente de los entornos de los proveedores más importantes del mercado: Amazon Web Services AWS, Google Cloud Platform GCP, Microsoft Azure, Databricks.\n [Online courses]() Project or software documentation Tutorials Cheatsheets  Índice Learning Path Estructura programación de Cloud computing Introducción Cloud computing GCP - Google Cloud Platform AWS - Amazon Web Services Azure - Microsoft Azure [ ] [Databricks - ]  ","date":1607385600,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"e9cb917e4a8e4a7a7c8221453c0f4a95","permalink":"https://www.marcusrb.com/cursos/cloud-computing/","publishdate":"2020-12-08T00:00:00Z","relpermalink":"/cursos/cloud-computing/","section":"cursos","summary":"Learning Path de Cloud Computing. Material, tutoriales, documentos técnicos ingenería de datos, data pipeline, scripting e implementación de servicios en la nube.","tags":null,"title":"Aprendizaje en Cloud computing","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1607382000,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"39112b11c6d9630b39daead6bad00bd2","permalink":"https://www.marcusrb.com/aws-amazon-web-services/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/aws-amazon-web-services/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción AWS","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1607382000,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"598fb2077a5fbeeb9241a73864a46148","permalink":"https://www.marcusrb.com/fundamentos-cloud-computing/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/fundamentos-cloud-computing/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción Cloud computing","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Cheatsheets GIT  Aprender GIT  Cheatsheets R Studio  Base R Rstudio IDE Data Transformation in R Datatable in R Regex in R String in R Shiny Data Visualization in R  Cheatsheets Maths para Data Science  Matemática para Machine Learning Álgebra lineal para Machine Learning Data Science Cheatsheet example tex doc  ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"b462330858401207c46d4d316c34435c","permalink":"https://www.marcusrb.com/recursos/cheatsheets/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/recursos/cheatsheets/","section":"recursos","summary":"Se presentan diferentes cheatsheets de programación Python, Pandas, Numpy, Machine Learning.","tags":null,"title":"Recursos de Cheatsheets","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Los recursos de este curso están disponibles en:\n Cursos online fundamentos de analítica digital Cursos online Avanzado con analítica digital Video-Tutoriales Google Analytics Test examen Certificación Google Analytics Examénes Google Academy  Índice Learning Path Google Analytics ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"5cfb1958fff8f36d390fbbb2c635596a","permalink":"https://www.marcusrb.com/curso-google-analytics/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/curso-google-analytics/","section":"cursos","summary":"Learning Path de Google Analytics. Aprende a como realizar informes, analizar e implementar eventos en Google Analytics.","tags":null,"title":"Learning Path de Google Analyitcs","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Introducción a Unix Sistema de ficheros [Control de procesos] Flujos de información Procesamiento de ficheros de texto Expresiones regulares Scripts de Bash Administrador Administración de software 1 Variables de entorno Administración de software 2 Redes  ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"0b62685bd33eda9a3b1b56f5abb496e8","permalink":"https://www.marcusrb.com/tutorial-unix/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/tutorial-unix/","section":"recursos","summary":"Tutorial de Unix.","tags":null,"title":"Tutorial de Unix, Shell y Bash","type":"docs"},{"authors":null,"categories":null,"content":"  [Online courses]() Project or software documentation Tutorials Cheatsheets  Índice Learning Path Estructura programación de Business Analytics Introducción de SQL Advanced SQL [SQL Server]() SQL Server Integration Services SQL Server Analysis Services SQL Server Reporting Services  ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"ad1943255f02bcab238e271d976c3ef7","permalink":"https://www.marcusrb.com/cursos/business-analytics/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/business-analytics/","section":"cursos","summary":"Learning Path de Business Intelligence, Business y Data Analytics. Material, tutoriales, documentos técnicos sobre minería de datos, exploración estadísticos.","tags":null,"title":"Aprendizaje en Business Analytics","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"3ae8e87f7fda34d466474732ee5babf0","permalink":"https://www.marcusrb.com/gcp-google-cloud-platform/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/gcp-google-cloud-platform/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción GCP","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"cb7e22473a4c409f4350a474c17ba4f6","permalink":"https://www.marcusrb.com/cursos/business-analytics/intro-sql/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/business-analytics/intro-sql/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción SQL","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"d0c211b2e8dda821e9a21c6ac7d7f360","permalink":"https://www.marcusrb.com/microsoft-azure/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/microsoft-azure/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción Azure","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 2 millones de usuarios a nivel mundial, R se convierte rápidamente en el lenguaje de programación líder en estadística y ciencia de datos. Cada año, el número de usuarios de R crece en 40%, y cada vez más organizaciones lo están usando para sus actividades cotidianas. En esta introducción a R, vas a dominar los elementos básicos de este bello lenguaje de programación: vectores, factores, listas y data frames. Con el conocimiento obtenido en este curso serás capaz de llevar a cabo tus propios análisis de datos.\nIntroducción En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!\nVectores Vamos a ir a Las Vegas! donde aprenderemos como analizar los resultados de los juegos usando vectores en R! Después de completar este capítulo serás capaz de crear vectores en R, nombrarlos, seleccionar elementos de ellos y comparar diferentes vectores.\nMatrices En este capítulo vamos a aprender a trabajar con matrices en R. Al finalizar el capítulo serás capaz de crear matrices y hacer operaciones básicas con éstas. Vamos a analizar cuánto dinero hizo la película Star Wars para ilustrar el uso de matrices en R. Que la fuerza te acompañe!\nFactores Existen un tipo de variables llamadas variables categóricas. Por ejemplo, el género puede ser femenino o masculino. En R las variables categóricas son llamadas factores. Dada la importancia de los factores en el analisis de datos, vamos a aprender a crearlos y a todo lo relacionado con su manejo. Empecemos!\nData frames La mayoría de los datos con los que trabajarás en R van a ser guardados en data frames. Al finalizar este capítulo vas a ser capaz de crear data frames, seleccionar partes del mismo y ordenar los datos que contiene de acuerdo a cierta variable.\nListas Las listas, en contraste con los vectores, pueden tener elementos de diferentes tipos. En este capítulo aprenderemos a crear, nombrar y extraer elementos de las listas. ¿Listo? ;-)\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"28f9d7224604c4e9584f65a59a789d63","permalink":"https://www.marcusrb.com/cursos/r-studio/intro-r/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/r-studio/intro-r/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"Introducción de R","type":"docs"},{"authors":null,"categories":null,"content":"En esta sección se publicarán los diferentes formatos de cursos y tutoriales, de rutas de aprendizajes de la plataforma de Google Marketing, Data Science, Data y Business Analytics, Data Visualization. Además de las documentaciones técnicas, manuales, recursos y actualizaciones. Los repositorios serán accesibles en GitHub o Bitbucket.\n Documentación técnica de proyectos Tutoriales  - Cloud Computing - UNIX - Lenguajes de programación - Big Data - Inteligencia Artificial - Business Intelligence   ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609870427,"objectID":"50c02874674d87e7751fbf5ba97977f7","permalink":"https://www.marcusrb.com/tutoriales/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/tutoriales/","section":"recursos","summary":"En la sección de tutoriales, se vincularán los diferentes proyectos realizados, así como los tutoriales de IoT, Raspberry, Python, R, Machine Learning, además de analítica avanzada de datos.","tags":null,"title":"Tutoriales","type":"docs"},{"authors":null,"categories":null,"content":" Machine Learning con Python [Python Web Scraping Tutorials] [Linear Regression in Python] [Practical Text Classification With Python and Keras]  ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"06a43490f132c07e2cf757cd3fe5f144","permalink":"https://www.marcusrb.com/recursos/tutoriales/python/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/recursos/tutoriales/python/","section":"recursos","summary":"En la sección de tutoriales, se vincularán los diferentes proyectos realizados, así como los tutoriales de IoT, Raspberry, Python, R, Machine Learning, además de analítica avanzada de datos.","tags":null,"title":"Python","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Los recursos de este curso están disponibles en:\n Cursos online fundamentos de analítica digital Cursos online Avanzado con analítica digital Video-Tutoriales Google Analytics Test examen Certificación Google Analytics Examénes Google Academy  Índice Learning Path Estructura programación de GTM 1. Introducción de GTM 2. GTM 2  ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"54594a5e01aa8f1be132301af0147359","permalink":"https://www.marcusrb.com/curso-google-tag-manager/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/curso-google-tag-manager/","section":"cursos","summary":"Learning Path de Google Tag Manager. Aprende cómo implementar etiquetas de Google y de terceros en una interfaz muy intuitiva.","tags":null,"title":"Learning Path de Google Tag Manager","type":"docs"},{"authors":null,"categories":null,"content":" 1.1. ¿Qué es Google Tag Manager? Google Tag Manager es un sistema de administración de etiquetas que permite recoger datos relacionados con campañas de marketing, tráfico web y el comportamiento del usuario en el sitio web, actualizando de forma fácil y rápida las etiquetas y los fragmentos de código de un sitio web. Es decir, permite editar HTML y JavaScript directamente sin tener que acceder al código de seguimiento. Se pueden añadir y actualizar etiquetas personalizadas, de terceros o de Ads, Google Analytics, Firebase Analytics y Floodlight desde la interfaz de usuario de Tag Manager en lugar de cambiar el código de los sitios web. De este modo, se reduce el número de errores y se evita recurrir a un desarrollador o programador para configurar las etiquetas.\nEn definitiva, una vez conozcamos el funcionamiento de Google Tag Manager, podremos administrar las etiquetas de nuestro sitio web de forma fácil y rápida sin necesidad de depender de un departamento IT o de desarrollo. Además, este software facilita mucho la tarea en una campaña de marketing, porque supone un acceso a la web en tiempo real (recordemos que al insertar manualmente los códigos de seguimiento los datos pueden tardar hasta 24 horas en aparecer en Google Analytics) y nos permite unificar todos los códigos en uno solo, lo que significa trabajar con un solo ID de Google Tag Manager y una sola interfaz de usuario\n1.2. ¿Por qué Google Tag Manager? Cuando se trata de una campaña de publicidad digital o un E-Commerce, si no medimos los resultados y tomamos decisiones estratégicas a través de ellos, no podremos mejorar. Como hemos visto hasta ahora, para poder medir el rendimiento de todas las campañas, visitas y los funcionamientos de los procesos de una página web, necesitamos insertar píxeles o códigos de seguimiento. Éstos sirven para enviar un recuento de conversiones o de visitas y atribuírselo a las campañas adecuadas. Normalmente, quien se encarga de gestionarlo son programadores expertos.\nPongámonos en la situación de un experto y profesional de Marketing Online trabajando en una campaña. Casi todo está listo: la creatividad de los anuncios, los banners están contratados para aparecer en los medios, los enlaces están etiquetados, las redes sociales están actualizadas, etc. ¿Qué pasa cuando llega la hora de trabajar con Google Analytics, donde se tienen que insertar píxeles y códigos de seguimiento referidos a cada página de interés? No todas las empresas disponen de los medios necesarios para tener operativo un departamento de desarrollo de forma constante ni contratar programadores para esta constante tarea de inserción de códigos.\nSi desde el departamento de desarrollo se pueden ahorrar todo este tiempo y este riesgo de cometer errores al insertar códigos, y si esta tarea se convierte en una tarea fácil y rápida de hacer para cualquier profesional del sector de Marketing, los resultados se multiplican. Al fin y al cabo, tanto los departamentos de Desarrollo como de Marketing tienen un interés común; trabajan enfocados al mismo objetivo y para el mismo cliente. Si ambos departamentos cooperan entre sí, las campañas de marketing y publicidad van a ser más exitosas.\nLa publicidad online representa un alto porcentaje de la publicidad mundial a día de hoy, y estos píxeles de los que hablamos son un elemento clave en el proceso, hasta el punto de que incluso existen empresas que se dedican exclusivamente a mejorar la medición y gestión de campañas online. Esto exige nuevos perfiles de profesionales, porque mientras sube la inversión en marketing digital, también debe hacerlo en procesos de análisis y supervisión de esas tareas del rendimiento de cada una de las inversiones que han intervenido. Este perfil profesional es conocido como Tag Manager.\n5 Google Tag Manager Ante esta tendencia, hay muchas empresas que ya han visto la oportunidad y han creado sus softwares de configuración de etiquetas. Hay algunas que son de pago, pero la de Google es gratuita, llamada Google Tag Manager.\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"d33c10412facbc9fd24629cf2e3ec451","permalink":"https://www.marcusrb.com/cursos/google-marketing-platform/google-ads/gads-display/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/cursos/google-marketing-platform/google-ads/gads-display/","section":"cursos","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" 1.1. ¿Qué es Google Tag Manager? Google Tag Manager es un sistema de administración de etiquetas que permite recoger datos relacionados con campañas de marketing, tráfico web y el comportamiento del usuario en el sitio web, actualizando de forma fácil y rápida las etiquetas y los fragmentos de código de un sitio web. Es decir, permite editar HTML y JavaScript directamente sin tener que acceder al código de seguimiento. Se pueden añadir y actualizar etiquetas personalizadas, de terceros o de Ads, Google Analytics, Firebase Analytics y Floodlight desde la interfaz de usuario de Tag Manager en lugar de cambiar el código de los sitios web. De este modo, se reduce el número de errores y se evita recurrir a un desarrollador o programador para configurar las etiquetas.\nEn definitiva, una vez conozcamos el funcionamiento de Google Tag Manager, podremos administrar las etiquetas de nuestro sitio web de forma fácil y rápida sin necesidad de depender de un departamento IT o de desarrollo. Además, este software facilita mucho la tarea en una campaña de marketing, porque supone un acceso a la web en tiempo real (recordemos que al insertar manualmente los códigos de seguimiento los datos pueden tardar hasta 24 horas en aparecer en Google Analytics) y nos permite unificar todos los códigos en uno solo, lo que significa trabajar con un solo ID de Google Tag Manager y una sola interfaz de usuario\n1.2. ¿Por qué Google Tag Manager? Cuando se trata de una campaña de publicidad digital o un E-Commerce, si no medimos los resultados y tomamos decisiones estratégicas a través de ellos, no podremos mejorar. Como hemos visto hasta ahora, para poder medir el rendimiento de todas las campañas, visitas y los funcionamientos de los procesos de una página web, necesitamos insertar píxeles o códigos de seguimiento. Éstos sirven para enviar un recuento de conversiones o de visitas y atribuírselo a las campañas adecuadas. Normalmente, quien se encarga de gestionarlo son programadores expertos.\nPongámonos en la situación de un experto y profesional de Marketing Online trabajando en una campaña. Casi todo está listo: la creatividad de los anuncios, los banners están contratados para aparecer en los medios, los enlaces están etiquetados, las redes sociales están actualizadas, etc. ¿Qué pasa cuando llega la hora de trabajar con Google Analytics, donde se tienen que insertar píxeles y códigos de seguimiento referidos a cada página de interés? No todas las empresas disponen de los medios necesarios para tener operativo un departamento de desarrollo de forma constante ni contratar programadores para esta constante tarea de inserción de códigos.\nSi desde el departamento de desarrollo se pueden ahorrar todo este tiempo y este riesgo de cometer errores al insertar códigos, y si esta tarea se convierte en una tarea fácil y rápida de hacer para cualquier profesional del sector de Marketing, los resultados se multiplican. Al fin y al cabo, tanto los departamentos de Desarrollo como de Marketing tienen un interés común; trabajan enfocados al mismo objetivo y para el mismo cliente. Si ambos departamentos cooperan entre sí, las campañas de marketing y publicidad van a ser más exitosas.\nLa publicidad online representa un alto porcentaje de la publicidad mundial a día de hoy, y estos píxeles de los que hablamos son un elemento clave en el proceso, hasta el punto de que incluso existen empresas que se dedican exclusivamente a mejorar la medición y gestión de campañas online. Esto exige nuevos perfiles de profesionales, porque mientras sube la inversión en marketing digital, también debe hacerlo en procesos de análisis y supervisión de esas tareas del rendimiento de cada una de las inversiones que han intervenido. Este perfil profesional es conocido como Tag Manager.\n5 Google Tag Manager Ante esta tendencia, hay muchas empresas que ya han visto la oportunidad y han creado sus softwares de configuración de etiquetas. Hay algunas que son de pago, pero la de Google es gratuita, llamada Google Tag Manager.\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"12eb02daef3f3412bdc6b3612b44e6b4","permalink":"https://www.marcusrb.com/cursos/google-marketing-platform/google-ads/gads-remarketing/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/cursos/google-marketing-platform/google-ads/gads-remarketing/","section":"cursos","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" 1.1. ¿Qué es Google Tag Manager? Google Tag Manager es un sistema de administración de etiquetas que permite recoger datos relacionados con campañas de marketing, tráfico web y el comportamiento del usuario en el sitio web, actualizando de forma fácil y rápida las etiquetas y los fragmentos de código de un sitio web. Es decir, permite editar HTML y JavaScript directamente sin tener que acceder al código de seguimiento. Se pueden añadir y actualizar etiquetas personalizadas, de terceros o de Ads, Google Analytics, Firebase Analytics y Floodlight desde la interfaz de usuario de Tag Manager en lugar de cambiar el código de los sitios web. De este modo, se reduce el número de errores y se evita recurrir a un desarrollador o programador para configurar las etiquetas.\nEn definitiva, una vez conozcamos el funcionamiento de Google Tag Manager, podremos administrar las etiquetas de nuestro sitio web de forma fácil y rápida sin necesidad de depender de un departamento IT o de desarrollo. Además, este software facilita mucho la tarea en una campaña de marketing, porque supone un acceso a la web en tiempo real (recordemos que al insertar manualmente los códigos de seguimiento los datos pueden tardar hasta 24 horas en aparecer en Google Analytics) y nos permite unificar todos los códigos en uno solo, lo que significa trabajar con un solo ID de Google Tag Manager y una sola interfaz de usuario\n1.2. ¿Por qué Google Tag Manager? Cuando se trata de una campaña de publicidad digital o un E-Commerce, si no medimos los resultados y tomamos decisiones estratégicas a través de ellos, no podremos mejorar. Como hemos visto hasta ahora, para poder medir el rendimiento de todas las campañas, visitas y los funcionamientos de los procesos de una página web, necesitamos insertar píxeles o códigos de seguimiento. Éstos sirven para enviar un recuento de conversiones o de visitas y atribuírselo a las campañas adecuadas. Normalmente, quien se encarga de gestionarlo son programadores expertos.\nPongámonos en la situación de un experto y profesional de Marketing Online trabajando en una campaña. Casi todo está listo: la creatividad de los anuncios, los banners están contratados para aparecer en los medios, los enlaces están etiquetados, las redes sociales están actualizadas, etc. ¿Qué pasa cuando llega la hora de trabajar con Google Analytics, donde se tienen que insertar píxeles y códigos de seguimiento referidos a cada página de interés? No todas las empresas disponen de los medios necesarios para tener operativo un departamento de desarrollo de forma constante ni contratar programadores para esta constante tarea de inserción de códigos.\nSi desde el departamento de desarrollo se pueden ahorrar todo este tiempo y este riesgo de cometer errores al insertar códigos, y si esta tarea se convierte en una tarea fácil y rápida de hacer para cualquier profesional del sector de Marketing, los resultados se multiplican. Al fin y al cabo, tanto los departamentos de Desarrollo como de Marketing tienen un interés común; trabajan enfocados al mismo objetivo y para el mismo cliente. Si ambos departamentos cooperan entre sí, las campañas de marketing y publicidad van a ser más exitosas.\nLa publicidad online representa un alto porcentaje de la publicidad mundial a día de hoy, y estos píxeles de los que hablamos son un elemento clave en el proceso, hasta el punto de que incluso existen empresas que se dedican exclusivamente a mejorar la medición y gestión de campañas online. Esto exige nuevos perfiles de profesionales, porque mientras sube la inversión en marketing digital, también debe hacerlo en procesos de análisis y supervisión de esas tareas del rendimiento de cada una de las inversiones que han intervenido. Este perfil profesional es conocido como Tag Manager.\n5 Google Tag Manager Ante esta tendencia, hay muchas empresas que ya han visto la oportunidad y han creado sus softwares de configuración de etiquetas. Hay algunas que son de pago, pero la de Google es gratuita, llamada Google Tag Manager.\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"c179b391e5ce7c09260e6ac0150dad95","permalink":"https://www.marcusrb.com/cursos/google-marketing-platform/google-ads/gads-search/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/cursos/google-marketing-platform/google-ads/gads-search/","section":"cursos","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" 1.1. ¿Qué es Google Tag Manager? Google Tag Manager es un sistema de administración de etiquetas que permite recoger datos relacionados con campañas de marketing, tráfico web y el comportamiento del usuario en el sitio web, actualizando de forma fácil y rápida las etiquetas y los fragmentos de código de un sitio web. Es decir, permite editar HTML y JavaScript directamente sin tener que acceder al código de seguimiento. Se pueden añadir y actualizar etiquetas personalizadas, de terceros o de Ads, Google Analytics, Firebase Analytics y Floodlight desde la interfaz de usuario de Tag Manager en lugar de cambiar el código de los sitios web. De este modo, se reduce el número de errores y se evita recurrir a un desarrollador o programador para configurar las etiquetas.\nEn definitiva, una vez conozcamos el funcionamiento de Google Tag Manager, podremos administrar las etiquetas de nuestro sitio web de forma fácil y rápida sin necesidad de depender de un departamento IT o de desarrollo. Además, este software facilita mucho la tarea en una campaña de marketing, porque supone un acceso a la web en tiempo real (recordemos que al insertar manualmente los códigos de seguimiento los datos pueden tardar hasta 24 horas en aparecer en Google Analytics) y nos permite unificar todos los códigos en uno solo, lo que significa trabajar con un solo ID de Google Tag Manager y una sola interfaz de usuario\n1.2. ¿Por qué Google Tag Manager? Cuando se trata de una campaña de publicidad digital o un E-Commerce, si no medimos los resultados y tomamos decisiones estratégicas a través de ellos, no podremos mejorar. Como hemos visto hasta ahora, para poder medir el rendimiento de todas las campañas, visitas y los funcionamientos de los procesos de una página web, necesitamos insertar píxeles o códigos de seguimiento. Éstos sirven para enviar un recuento de conversiones o de visitas y atribuírselo a las campañas adecuadas. Normalmente, quien se encarga de gestionarlo son programadores expertos.\nPongámonos en la situación de un experto y profesional de Marketing Online trabajando en una campaña. Casi todo está listo: la creatividad de los anuncios, los banners están contratados para aparecer en los medios, los enlaces están etiquetados, las redes sociales están actualizadas, etc. ¿Qué pasa cuando llega la hora de trabajar con Google Analytics, donde se tienen que insertar píxeles y códigos de seguimiento referidos a cada página de interés? No todas las empresas disponen de los medios necesarios para tener operativo un departamento de desarrollo de forma constante ni contratar programadores para esta constante tarea de inserción de códigos.\nSi desde el departamento de desarrollo se pueden ahorrar todo este tiempo y este riesgo de cometer errores al insertar códigos, y si esta tarea se convierte en una tarea fácil y rápida de hacer para cualquier profesional del sector de Marketing, los resultados se multiplican. Al fin y al cabo, tanto los departamentos de Desarrollo como de Marketing tienen un interés común; trabajan enfocados al mismo objetivo y para el mismo cliente. Si ambos departamentos cooperan entre sí, las campañas de marketing y publicidad van a ser más exitosas.\nLa publicidad online representa un alto porcentaje de la publicidad mundial a día de hoy, y estos píxeles de los que hablamos son un elemento clave en el proceso, hasta el punto de que incluso existen empresas que se dedican exclusivamente a mejorar la medición y gestión de campañas online. Esto exige nuevos perfiles de profesionales, porque mientras sube la inversión en marketing digital, también debe hacerlo en procesos de análisis y supervisión de esas tareas del rendimiento de cada una de las inversiones que han intervenido. Este perfil profesional es conocido como Tag Manager.\n5 Google Tag Manager Ante esta tendencia, hay muchas empresas que ya han visto la oportunidad y han creado sus softwares de configuración de etiquetas. Hay algunas que son de pago, pero la de Google es gratuita, llamada Google Tag Manager.\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"cc03f64f350fa5eb48c250fd9280a84a","permalink":"https://www.marcusrb.com/cursos/google-marketing-platform/google-tag-manager/gtm01/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/cursos/google-marketing-platform/google-tag-manager/gtm01/","section":"cursos","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM from Scratch","type":"docs"},{"authors":null,"categories":null,"content":"","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"1ef39d6937d14058c1c8382d88913c1c","permalink":"https://www.marcusrb.com/cursos/google-marketing-platform/google-analytics/ga101/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/cursos/google-marketing-platform/google-analytics/ga101/","section":"cursos","summary":"Una pequeña introducción al entorno de Google Analytics, variables, activadores y etiquetas","tags":null,"title":"Google Analytics from Scratch","type":"docs"},{"authors":null,"categories":null,"content":"  Project or software documentation Tutoriales Cheatsheets  Índice Learning Path Estructura programación de Visualización de datos Google Data Studio Power BI fundamentos Power BI avanzado Tableau Visualización en Python Visualización en R Studio  ","date":1586044800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"806cbbbc1f9c214bc13750162af91deb","permalink":"https://www.marcusrb.com/cursos/data-visualization/","publishdate":"2020-04-05T00:00:00Z","relpermalink":"/cursos/data-visualization/","section":"cursos","summary":"Learning Path de Visualización de datos, se indicarán los cursos de Data Studio, Tableau y Power BI.","tags":null,"title":"Cursos y formación en Visualización de datos","type":"docs"},{"authors":null,"categories":null,"content":" Programa En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!\nTidy Vamos a ir a Las Vegas! donde aprenderemos como analizar los resultados de los juegos usando vectores en R! Después de completar este capítulo serás capaz de crear vectores en R, nombrarlos, seleccionar elementos de ellos y comparar diferentes vectores.\n","date":1568505600,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"8241c8bbf24045925b675c8e8a0354f5","permalink":"https://www.marcusrb.com/cursos/r-studio/advanced-r/","publishdate":"2019-09-15T00:00:00Z","relpermalink":"/cursos/r-studio/advanced-r/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"R Avanzado","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 2 millones de usuarios a nivel mundial, R se convierte rápidamente en el lenguaje de programación líder en estadística y ciencia de datos. Cada año, el número de usuarios de R crece en 40%, y cada vez más organizaciones lo están usando para sus actividades cotidianas. En esta introducción a R, vas a dominar los elementos básicos de este bello lenguaje de programación: vectores, factores, listas y data frames. Con el conocimiento obtenido en este curso serás capaz de llevar a cabo tus propios análisis de datos.\nCómo funciona En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!\n[Basic Data Exploration]() Vamos a ir a Las Vegas! donde aprenderemos como analizar los resultados de los juegos usando vectores en R! Después de completar este capítulo serás capaz de crear vectores en R, nombrarlos, seleccionar elementos de ellos y comparar diferentes vectores.\nEl primer modelo de ML En este capítulo vamos a aprender a trabajar con matrices en R. Al finalizar el capítulo serás capaz de crear matrices y hacer operaciones básicas con éstas. Vamos a analizar cuánto dinero hizo la película Star Wars para ilustrar el uso de matrices en R. Que la fuerza te acompañe!\nFactores Existen un tipo de variables llamadas variables categóricas. Por ejemplo, el género puede ser femenino o masculino. En R las variables categóricas son llamadas factores. Dada la importancia de los factores en el analisis de datos, vamos a aprender a crearlos y a todo lo relacionado con su manejo. Empecemos!\nData frames La mayoría de los datos con los que trabajarás en R van a ser guardados en data frames. Al finalizar este capítulo vas a ser capaz de crear data frames, seleccionar partes del mismo y ordenar los datos que contiene de acuerdo a cierta variable.\n## Listas Las listas, en contraste con los vectores, pueden tener elementos de diferentes tipos. En este capítulo aprenderemos a crear, nombrar y extraer elementos de las listas. ¿Listo? ;-)\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"a2c021e40c80338ab4108c0985363457","permalink":"https://www.marcusrb.com/cursos/data-science/intro-machine-learning/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/data-science/intro-machine-learning/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"Introducción Machine Learning","type":"docs"},{"authors":null,"categories":null,"content":" DESCRIPTION - summary TBD\nHello, Python En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!\nFunciones Vamos a ir a Las Vegas! donde aprenderemos como analizar los resultados de los juegos usando vectores en R! Después de completar este capítulo serás capaz de crear vectores en R, nombrarlos, seleccionar elementos de ellos y comparar diferentes vectores.\nMatrices En este capítulo vamos a aprender a trabajar con matrices en R. Al finalizar el capítulo serás capaz de crear matrices y hacer operaciones básicas con éstas. Vamos a analizar cuánto dinero hizo la película Star Wars para ilustrar el uso de matrices en R. Que la fuerza te acompañe!\nFactores Existen un tipo de variables llamadas variables categóricas. Por ejemplo, el género puede ser femenino o masculino. En R las variables categóricas son llamadas factores. Dada la importancia de los factores en el analisis de datos, vamos a aprender a crearlos y a todo lo relacionado con su manejo. Empecemos!\nData frames La mayoría de los datos con los que trabajarás en R van a ser guardados en data frames. Al finalizar este capítulo vas a ser capaz de crear data frames, seleccionar partes del mismo y ordenar los datos que contiene de acuerdo a cierta variable.\n## Listas Las listas, en contraste con los vectores, pueden tener elementos de diferentes tipos. En este capítulo aprenderemos a crear, nombrar y extraer elementos de las listas. ¿Listo? ;-)\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"a0f8b7148615fee060b11ea8857cca24","permalink":"https://www.marcusrb.com/cursos/python/py101/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/python/py101/","section":"cursos","summary":"Una pequeña introducción a la sintaxis de Python, variables y números","tags":null,"title":"Python from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" 2 gtm 2\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"b712a7de2fbe9557dace7c032a7a11b7","permalink":"https://www.marcusrb.com/cursos/google-marketing-platform/google-tag-manager/gtm02/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/cursos/google-marketing-platform/google-tag-manager/gtm02/","section":"cursos","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM 2","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Los recursos de este curso están disponibles en:\n Cursos online fundamentos de analítica digital Cursos online Avanzado con analítica digital Video-Tutoriales Google Analytics Test examen Certificación Google Analytics Examénes Google Academy  Índice Learning Path de Google Ads TBD\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"72ef2cdc33624e0a300d295005a88b66","permalink":"https://www.marcusrb.com/curso-google-ads/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/curso-google-ads/","section":"cursos","summary":"Learning Path de Google Ads. Aprende cómo crear y gestionar campañas de Google Ads de Búsqueda, Display, Remarketing y Ecommerce.","tags":null,"title":"Learning Path de Google Ads","type":"docs"},{"authors":null,"categories":null,"content":" 2  [ ] Google Analytics 4  ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"43253b9cb40aa0debbe19d6036460bd0","permalink":"https://www.marcusrb.com/cursos/google-marketing-platform/google-analytics/ga4/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/cursos/google-marketing-platform/google-analytics/ga4/","section":"cursos","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"Google Analytics 2","type":"docs"},{"authors":null,"categories":null,"content":"","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"0359d0dca97f4d76ad4d61d71f2cd83c","permalink":"https://www.marcusrb.com/cursos/data-science/data-mining/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/data-science/data-mining/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"Introducción Data Mining","type":"docs"},{"authors":null,"categories":null,"content":" Math for Data Science Useful resources to improve your Math skills -\nCourses 1) Khan Academy is the best online free resource to learn Math for Data Science. (https://lnkd.in/eWZFANt).\n2) Krista King has also done a great job in creating exceptionally good introductory course. She is too good in designing the course. (https://lnkd.in/eyMecjA).\n3) 3Blue1Brown (https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/playlists).\nMUST READ Books 1) The Elements of Statistical Learning(Springer Series).\n2) Introduction to Linear Algebra by Gilbert Strang.\n3) Naked Statistics by Charles Wheelan.\n4) An Introduction to Statistical Learning: with Applications in R.\n5) Pattern Recognition and Machine Learning by Christopher M. Bishop.\n6) Pattern Classification ((A Wiley-Interscience publication).\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"a70de7a107c52374bbbf76e929fee446","permalink":"https://www.marcusrb.com/cursos/data-science/math-data-science/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/data-science/math-data-science/","section":"cursos","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"Matemática para Data Science","type":"docs"},{"authors":null,"categories":null,"content":" Learning Path This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  Índice Learning Path Estructura programación de Python  [] Introducción de Python [] Python para NO Desarrolladores [] Advanced Python [] Pandas [] Data Manipulation  ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"c2fe02f887efcd0404b01de245f7ada8","permalink":"https://www.marcusrb.com/cursos/python/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/python/","section":"cursos","summary":"Learning Path de Python. De fundamentos de programación, material de Python básico hasta avaznado, web app en Flask y Django.","tags":null,"title":"Learning Path de Python","type":"docs"},{"authors":null,"categories":null,"content":" R como recursos para la análisis de datos Gracias a la aportación de una fuerte comunidad, aquí tendré la ocasión de crear una wiki para aprender a utilizar R studio para la análisis de datos.\nQué es R? R es un entorno para la gestión y análisis de datos, primera exploración estadística y visualización gráfica. Es un software de oper source (su distribución tiene licencia GNU GPL) además de multiplataformas, UNIX, Linux, OSx, Windows.\n Online courses Project or software documentation Tutorials Cheatsheets  Estructura programación en R Instalación de R Paquetes Área de trabajo Directorio de trabajo Comandos Objetos y Clases Vectores Factores Listas Índices Funciones script Fórmulas Operadores Datasets de ejemplos R para Data Science  Fuente\nIntroducción · R. avanzado Estás leyendo la primera edición de Advanced R; para la última versión ver la [segunda edición] [1].\nCon más de 10 años de experiencia en programación en R, he tenido el lujo de poder pasar mucho tiempo tratando de descubrir y entender cómo funciona el lenguaje. Este libro es mi intento de transmitir lo que he aprendido para que pueda convertirse rápidamente en un programador de R efectivo. Leerlo te ayudará a evitar los errores que he cometido y los callejones sin salida que he caído, y te enseñará herramientas, técnicas y modismos útiles que pueden ayudarte a atacar muchos tipos de problemas. En el proceso, espero demostrar que, a pesar de sus peculiaridades frustrantes, R es, en esencia, un lenguaje elegante y hermoso, bien adaptado para el análisis de datos y las estadísticas.\nSi eres nuevo en R, te preguntarás qué hace que valga la pena aprender un lenguaje tan peculiar. Para mí, algunas de las mejores características son:\n Es gratis, de código abierto y está disponible en todas las plataformas principales. Como resultado, si realiza su análisis en R, cualquiera puede replicarlo fácilmente. Un conjunto masivo de paquetes para modelado estadístico, aprendizaje automático, visualización e importación y manipulación de datos. Independientemente del modelo o gráfico que intente hacer, es probable que alguien ya haya intentado hacerlo. Como mínimo, puedes aprender de sus esfuerzos. Herramientas de vanguardia. Los investigadores en estadística y aprendizaje automático a menudo publicarán un paquete R para acompañar sus artículos. Esto significa acceso inmediato a las últimas técnicas e implementaciones estadísticas. Soporte de lenguaje profundo para el análisis de datos. Esto incluye características como valores perdidos, marcos de datos y subconjuntos. Una comunidad fantástica. Es fácil obtener ayuda de expertos en la [lista de correo R-help] [2], [stackoverflow] [3] o en listas de correo específicas de temas como [R-SIG-mixed-models] [4] o [ggplot2 ] [5]. También puede conectarse con otros alumnos R a través de [twitter] [6], [linkedin] [7], y a través de muchos [grupos de usuarios] locales [8]. Potentes herramientas para comunicar sus resultados. Los paquetes R facilitan la producción de html o pdf [informes] [9], o la creación de [sitios web interactivos] [10]. Una base sólida en la programación funcional. Las ideas de la programación funcional son adecuadas para resolver muchos de los desafíos del análisis de datos. R proporciona un kit de herramientas potente y flexible que le permite escribir código conciso pero descriptivo. Un [IDE] [11] adaptado a las necesidades de análisis de datos interactivos y programación estadística. Potentes instalaciones de metaprogramación. R no es solo un lenguaje de programación, también es un entorno para el análisis interactivo de datos. Sus capacidades de metaprogramación le permiten escribir funciones mágicamente concisas y concisas y proporcionan un entorno excelente para diseñar lenguajes específicos de dominio. Diseñado para conectarse a lenguajes de programación de alto rendimiento como C, Fortran y C ++.  Por supuesto, R no es perfecto. El mayor desafío de R es que la mayoría de los usuarios de R no son programadores. Esto significa que:\n Gran parte del código R que verá en la naturaleza está escrito a toda prisa para resolver un problema acuciante. Como resultado, el código no es muy elegante, rápido o fácil de entender. La mayoría de los usuarios no revisan su código para abordar estas deficiencias. En comparación con otros lenguajes de programación, la comunidad R tiende a centrarse más en los resultados que en los procesos. El conocimiento de las mejores prácticas de ingeniería de software es irregular: por ejemplo, no hay suficientes programadores de R que usen control de código fuente o pruebas automatizadas. La metaprogramación es una espada de doble filo. Demasiadas funciones de R usan trucos para reducir la cantidad de tipeo a costa de crear código que es difícil de entender y que puede fallar de maneras inesperadas. La inconsistencia abunda en los paquetes contribuidos, incluso dentro de la base R. Te enfrentas a más de 20 años de evolución cada vez que usas R. Aprender R puede ser difícil porque hay muchos casos especiales para recordar. R no es un lenguaje de programación particularmente rápido, y el código R mal escrito puede ser terriblemente lento. R también es un usuario despilfarrador de memoria.  Personalmente, creo que estos desafíos crean una gran oportunidad para que los programadores experimentados tengan un profundo impacto positivo en R y la comunidad R. Los usuarios de R se preocupan por escribir código de alta calidad, particularmente para la investigación reproducible, pero aún no tienen las habilidades para hacerlo. Espero que este libro no solo ayude a más usuarios de R a convertirse en programadores de R, sino que también aliente a los programadores de otros idiomas a contribuir a R.\nQuién debería leer este libro Este libro está dirigido a dos audiencias complementarias:\n Programadores de R intermedios que desean profundizar en R y aprender nuevas estrategias para resolver diversos problemas. Programadores de otros idiomas que están aprendiendo R y quieren entender por qué R funciona de la manera que lo hace.  Para aprovechar al máximo este libro, deberá haber escrito una cantidad decente de código en R u otro lenguaje de programación. Es posible que no conozca todos los detalles, pero debe estar familiarizado con el funcionamiento de las funciones en R y, aunque actualmente puede tener dificultades para usarlas de manera efectiva, debe estar familiarizado con la familia de aplicaciones (como apply () y lapply ()).\nLo que obtendrás de este libro Este libro describe las habilidades que creo que un programador avanzado de R debería tener: la capacidad de producir código de calidad que pueda usarse en una amplia variedad de circunstancias.\nDespués de leer este libro, usted:\n Familiarícese con los fundamentos de R. Comprenderá los tipos de datos complejos y las mejores formas de realizar operaciones en ellos. Tendrá una comprensión profunda de cómo funcionan las funciones y podrá reconocer y utilizar los cuatro sistemas de objetos en R. Comprenda lo que significa la programación funcional y por qué es una herramienta útil para el análisis de datos. Podrá aprender rápidamente cómo usar las herramientas existentes y tener el conocimiento para crear sus propias herramientas funcionales cuando sea necesario. Aprecia la espada de doble filo de la metaprogramación. Podrá crear funciones que utilizan evaluaciones no estándar de una manera basada en principios, ahorrando escritura y creando código elegante para expresar operaciones importantes. También comprenderá los peligros de la metaprogramación y por qué debe tener cuidado con su uso. Tener una buena intuición para las operaciones en R lentas o usar mucha memoria. Sabrás cómo usar la creación de perfiles para identificar los cuellos de botella de rendimiento, y sabrás suficiente C ++ para convertir funciones R lentas en equivalentes C ++ rápidos. Se sienta cómodo leyendo y entendiendo la mayoría del código R. Reconocerá expresiones idiomáticas comunes (incluso si no las usaría usted mismo) y podrá criticar el código de los demás.  Hay dos meta-técnicas que son tremendamente útiles para mejorar sus habilidades como programador de R: leer el código fuente y adoptar una mentalidad científica.\nLeer el código fuente es importante porque te ayudará a escribir un mejor código. Un gran lugar para comenzar a desarrollar esta habilidad es mirar el código fuente de las funciones y paquetes que usa con más frecuencia. Encontrarás cosas que vale la pena emular en tu propio código y desarrollarás un sentido del gusto por lo que hace un buen código R. También verá cosas que no le gustan, ya sea porque sus virtudes no son obvias o ofende su sensibilidad. Sin embargo, dicho código es valioso porque ayuda a concretar sus opiniones sobre el código bueno y el malo.\nUna mentalidad científica es extremadamente útil cuando se aprende R. Si no comprende cómo funciona algo, desarrolle una hipótesis, diseñe algunos experimentos, ejecútelos y registre los resultados. Este ejercicio es extremadamente útil ya que si no puede resolver algo y necesita ayuda, puede mostrar fácilmente a los demás lo que intentó. Además, cuando aprenda la respuesta correcta, estará mentalmente preparado para actualizar su visión del mundo. Cuando describo claramente un problema a otra persona (el arte de crear un [ejemplo reproducible] [12]), a menudo descubro la solución yo mismo.\nLectura recomendada R sigue siendo un idioma relativamente joven, y los recursos para ayudarlo a comprenderlo aún están madurando. En mi viaje personal para comprender R, me ha resultado especialmente útil utilizar recursos de otros lenguajes de programación. R tiene aspectos de lenguajes de programación tanto funcionales como orientados a objetos (OO). Aprender cómo se expresan estos conceptos en R lo ayudará a aprovechar su conocimiento existente de otros lenguajes de programación y lo ayudará a identificar áreas en las que puede mejorar.\nPara entender por qué los sistemas de objetos de R funcionan de la manera que lo hacen, encontré [_La estructura e interpretación de los programas de computadora _] 13 de Harold Abelson y Gerald Jay Sussman, particularmente útil. Es un libro conciso pero profundo. Después de leerlo, sentí por primera vez que realmente podía diseñar mi propio sistema orientado a objetos. El libro fue mi primera introducción al estilo de función genérico de OO común en R. Me ayudó a comprender sus fortalezas y debilidades. SICP también habla mucho sobre programación funcional y cómo crear funciones simples que se vuelven poderosas cuando se combinan.\nPara comprender las compensaciones que R ha realizado en comparación con otros lenguajes de programación, encontré [_Conceptos, técnicas y modelos de programación de computadoras _] [14] de Peter van Roy y Sef Haridi extremadamente útiles. Me ayudó a comprender que la semántica de copiar en modificar de R hace que sea mucho más fácil razonar sobre el código, y que si bien su implementación actual no es particularmente eficiente, es un problema que se puede resolver.\nSi quieres aprender a ser un mejor programador, no hay mejor lugar al que recurrir que [_The Pragmatic Programmer _] [15] de Andrew Hunt y David Thomas. Este libro es independiente del lenguaje y proporciona excelentes consejos sobre cómo ser un mejor programador.\nObteniendo ayuda Actualmente, hay dos lugares principales para obtener ayuda cuando estás atascado y no puedes descubrir qué está causando el problema: [stackoverflow] [16] y la lista de correo de R-help. Puede obtener ayuda fantástica en ambos lugares, pero tienen sus propias culturas y expectativas. Por lo general, es una buena idea pasar un poco de tiempo al acecho, aprendiendo sobre las expectativas de la comunidad, antes de publicar su primera publicación.\nAlgunos buenos consejos generales:\n Asegúrese de tener la última versión de R y del paquete (o paquetes) con los que tiene problemas. Puede ser que su problema sea el resultado de un error recientemente corregido. Pase algún tiempo creando un [ejemplo reproducible] [12]. Esto es a menudo un proceso útil por derecho propio, porque en el curso de hacer que el problema sea reproducible, a menudo descubres lo que está causando el problema. Busque problemas relacionados antes de publicar. Si alguien ya ha hecho su pregunta y ha sido respondida, es mucho más rápido para todos si usa la respuesta existente.  Agradecimientos Me gustaría agradecer a los incansables colaboradores de R-help y, más recientemente, [stackoverflow] [3]. Hay muchos para nombrar individualmente, pero me gustaría agradecer especialmente a Luke Tierney, John Chambers, Dirk Eddelbuettel, JJ Allaire y Brian Ripley por dar generosamente su tiempo y corregir mis innumerables malentendidos.\nEste libro fue [escrito al aire libre] [17], y los capítulos se anunciaron en [twitter] [18] cuando se completó. Es realmente un esfuerzo de la comunidad: muchas personas leen borradores, errores tipográficos, mejoras sugeridas y contenido contribuido. Sin esos colaboradores, el libro no sería tan bueno como es, y estoy profundamente agradecido por su ayuda. Un agradecimiento especial a Peter Li, que leyó el libro de principio a fin y proporcionó muchas soluciones. Otros colaboradores destacados fueron Aaron Schumacher, @crtahlin, Lingbing Feng, @juancentro y @johnbaums.\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"9efcb725d81e67f125690b98f84866db","permalink":"https://www.marcusrb.com/cursos/r-studio/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/r-studio/","section":"cursos","summary":"Learning Path de R. Documentos, tutoriales, cheatsheets para el aprendizaje de R Studio de básico a avanzado.","tags":null,"title":"Learning Path de R","type":"docs"},{"authors":null,"categories":null,"content":"  [Online courses]() Project or software documentation Tutorials Cheatsheets  Índice Learning Path  Data Science A-Z [R Programming]() Python A-Z Machine Learning A-Z Machine Learning Practical Statistics for Business Analytics and Marketing R Programming Advanced Deep Learning A-Z Machine Learning Classification  Estructura programación de Data Science Introducción de Machine Learning Matemática para Data Science [] Minería de datos [] Datatons \u0026amp; competitions  ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"629408205866fe5c8e34f54acdde6247","permalink":"https://www.marcusrb.com/cursos/data-science/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/cursos/data-science/","section":"cursos","summary":"Learning Path de Data Science. Material, tutoriales, documentos técnicos sobre aprendizaje automático, algoritmos, deep learning e inteligencia artificial.","tags":null,"title":"Aprendizaje en Data Science","type":"docs"},{"authors":null,"categories":null,"content":" En la sección de Google Marketing Platform, se presentarán los recursos y las herramientas para empresas pequeñas y las soluciones avanzadas para las grandes empresas.\nQué es Google Marketing platform Google Marketing Platform es una plataforma unificada de publicidad y analíticas que permite a sus equipos de marketing colaborar de forma más sólida gracias a que pueden aprovechar las integraciones existentes entre DoubleClick y Suite Google Analytics 360.\n Vamos a unificar nuestros productos para anunciantes de DoubleClick y Suite Google Analytics 360 bajo una única marca: Google Marketing Platform.\n Como parte del lanzamiento de Google Marketing Platform, la página de inicio de Suite Analytics 360 se convirtió el 24 de julio en la página principal de Marketing Platform.\nEsta nueva marca, Google Marketing Platform, junto con los nuevos nombres y logotipos de los productos, queda reflejada en las interfaces correspondientes, los Centros de Ayuda y la formación, entre otros materiales.\nEstructura Los siguientes productos están divididos en las secciones:\n Google Analytics Google Tag Manager Google Ads  ","date":1567296000,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1609845561,"objectID":"075a337808c4fafd8f7b307f3e24658f","permalink":"https://www.marcusrb.com/cursos/google-marketing-platform/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/cursos/google-marketing-platform/","section":"cursos","summary":"Recursos y documentación técnica de los cursos del ecosistema de Google: Ads, Analytics, Tag Manager, Data Studio, Optimize, Search Console","tags":null,"title":"Google Marketing Platform","type":"docs"},{"authors":null,"categories":null,"content":"El sistema de archivos controla como se almacenan los archivos en el ordenador. Sus dos tareas principales son guardar y leer archivos previamente guardados.\n","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"9e35d8c3086b8d4ad7aaf8b17f2de36d","permalink":"https://www.marcusrb.com/power-bi/02-carga-datos/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/power-bi/02-carga-datos/","section":"recursos","summary":"El sistema de archivos controla como se almacenan los archivos en el ordenador. Sus dos tareas principales son guardar y leer archivos previamente guardados.","tags":null,"title":"Carga de datos en Power BI","type":"docs"},{"authors":null,"categories":null,"content":"TBD\n","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609872091,"objectID":"a9e7594c5c98319d44c89eab940b18f4","permalink":"https://www.marcusrb.com/apache-hadoop/01-intro-apache-hadoop/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/apache-hadoop/01-intro-apache-hadoop/","section":"recursos","summary":"Recursos y materiales de Apache Hadoop","tags":null,"title":"Introducción a Apache Hadoop","type":"docs"},{"authors":null,"categories":null,"content":"TBD\n","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609872091,"objectID":"5ac141235e43ae8517640e341ad98fe0","permalink":"https://www.marcusrb.com/apache-nifi/01-intro-apache-nifi/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/apache-nifi/01-intro-apache-nifi/","section":"recursos","summary":"Recursos y materiales de Apache Nifi","tags":null,"title":"Introducción a Apache Nifi","type":"docs"},{"authors":null,"categories":null,"content":"TBD\n","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"553a090c4c87cc5132a4c2a2312778d4","permalink":"https://www.marcusrb.com/power-bi/01-intro-power-bi/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/power-bi/01-intro-power-bi/","section":"recursos","summary":"Recursos y materiales de la tool de visualización Microsoft Power BI.","tags":null,"title":"Introducción a Power BI","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.\nTus primeros comandos BigQuery Para usar BigQuery, importaremos el paquete de Python a continuación:\nfrom google.cloud import bigquery  El primer paso en el flujo de trabajo es crear un objeto Client. Como pronto verá, este objeto Client desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Trabajaremos con un conjunto de datos de publicaciones en Hacker News, un sitio web que se centra en noticias de informática y seguridad cibernética.\nEn BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos hacker_news está contenido en el proyecto bigquery-public-data. Para acceder al conjunto de datos,\n Comenzamos construyendo una referencia al conjunto de datos con el método dataset(). A continuación, utilizamos el método get_dataset(), junto con la referencia que acabamos de construir, para obtener el conjunto de datos.  # Construct a reference to the \u0026quot;hacker_news\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;hacker_news\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.\nUsamos el método list_tables() para listar las tablas en el conjunto de datos.\n# List all the tables in the \u0026quot;hacker_news\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there are four!) for table in tables: print(table.table_id)  OUTPUT\ncomments full full_201510 stories  De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla full en el conjunto de datos hacker_news.\n# Construct a reference to the \u0026quot;full\u0026quot; table table_ref = dataset_ref.table(\u0026quot;full\u0026quot;) # API request - fetch the table table = client.get_table(table_ref)  En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.\nEsquema de la tabla La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.\nEn este ejemplo, investigaremos la tabla completa full que obtuvimos anteriormente.\n# Print information on all the columns in the \u0026quot;full\u0026quot; table in the \u0026quot;hacker_news\u0026quot; dataset table.schema  OUTPUT\n[SchemaField('by', 'STRING', 'NULLABLE', \u0026quot;The username of the item's author.\u0026quot;, ()), SchemaField('score', 'INTEGER', 'NULLABLE', 'Story score', ()), SchemaField('time', 'INTEGER', 'NULLABLE', 'Unix time', ()), SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', 'Timestamp for the unix time', ()), SchemaField('title', 'STRING', 'NULLABLE', 'Story title', ()), SchemaField('type', 'STRING', 'NULLABLE', 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', ()), SchemaField('url', 'STRING', 'NULLABLE', 'Story url', ()), SchemaField('text', 'STRING', 'NULLABLE', 'Story or comment text', ()), SchemaField('parent', 'INTEGER', 'NULLABLE', 'Parent comment ID', ()), SchemaField('deleted', 'BOOLEAN', 'NULLABLE', 'Is deleted?', ()), SchemaField('dead', 'BOOLEAN', 'NULLABLE', 'Is dead?', ()), SchemaField('descendants', 'INTEGER', 'NULLABLE', 'Number of story or poll descendants', ()), SchemaField('id', 'INTEGER', 'NULLABLE', \u0026quot;The item's unique id.\u0026quot;, ()), SchemaField('ranking', 'INTEGER', 'NULLABLE', 'Comment ranking', ())]  Cada SchemaField nos informa sobre una columna específica (a la que también nos referimos como un campo field). En orden, la información es:\n El nombre de la columna. El tipo de campo (o tipo de datos) en la columna El modo de la columna (\u0026lsquo;NULLABLE\u0026rsquo; significa que una columna permite valores NULL y es el valor predeterminado) Una descripción de los datos en esa columna. El primer campo tiene el SchemaField:  SchemaField (\u0026lsquo;by\u0026rsquo;, \u0026lsquo;string\u0026rsquo;, \u0026lsquo;NULLABLE\u0026rsquo;, \u0026ldquo;El nombre de usuario del autor del elemento\u0026rdquo;, ()\nEsto nos dice:\n el campo (o columna) es llamado por los datos en este campo son cadenas, Se permiten valores NULL y Contiene los nombres de usuario correspondientes al autor de cada elemento.  Podemos usar el método list_rows() para verificar solo las primeras cinco líneas de la tabla completa full para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery RowIterator que se puede convertir rápidamente en un DataFrame de pandas con el método to_dataframe().\n# Preview the first five lines of the \u0026quot;full\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  El método list_rows() también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!\n# Preview the first five entries in the \u0026quot;by\u0026quot; column of the \u0026quot;full\u0026quot; table client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()  EXERCISE (Exercise_ Getting Started With SQL and BigQuery)\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"1c9befb8f41d6613971a8100ea10b1e8","permalink":"https://www.marcusrb.com/cursos/cloud-computing/intro-aws/aws101-intro/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/cursos/cloud-computing/intro-aws/aws101-intro/","section":"cursos","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.","tags":null,"title":"Conceptos de Amazon Web Services AWS","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web\nTBD\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"447b132ad8321b2068eb9e49c6c3c1f7","permalink":"https://www.marcusrb.com/cursos/cloud-computing/intro-azure/azure101-intro/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/cursos/cloud-computing/intro-azure/azure101-intro/","section":"cursos","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web\nTBD","tags":null,"title":"Conceptos de Microsoft Azure","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web\nTBD\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"7b7e8032aa75bfb65531ae2d6e4bafbd","permalink":"https://www.marcusrb.com/cursos/cloud-computing/intro-gcp/gcp101-intro/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/cursos/cloud-computing/intro-gcp/gcp101-intro/","section":"cursos","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web\nTBD","tags":null,"title":"Conceptos de Google Cloud Platform GCP","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.\nTus primeros comandos BigQuery Para usar BigQuery, importaremos el paquete de Python a continuación:\nfrom google.cloud import bigquery  El primer paso en el flujo de trabajo es crear un objeto Client. Como pronto verá, este objeto Client desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Trabajaremos con un conjunto de datos de publicaciones en Hacker News, un sitio web que se centra en noticias de informática y seguridad cibernética.\nEn BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos hacker_news está contenido en el proyecto bigquery-public-data. Para acceder al conjunto de datos,\n Comenzamos construyendo una referencia al conjunto de datos con el método dataset(). A continuación, utilizamos el método get_dataset(), junto con la referencia que acabamos de construir, para obtener el conjunto de datos.  # Construct a reference to the \u0026quot;hacker_news\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;hacker_news\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.\nUsamos el método list_tables() para listar las tablas en el conjunto de datos.\n# List all the tables in the \u0026quot;hacker_news\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there are four!) for table in tables: print(table.table_id)  OUTPUT\ncomments full full_201510 stories  De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla full en el conjunto de datos hacker_news.\n# Construct a reference to the \u0026quot;full\u0026quot; table table_ref = dataset_ref.table(\u0026quot;full\u0026quot;) # API request - fetch the table table = client.get_table(table_ref)  En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.\nEsquema de la tabla La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.\nEn este ejemplo, investigaremos la tabla completa full que obtuvimos anteriormente.\n# Print information on all the columns in the \u0026quot;full\u0026quot; table in the \u0026quot;hacker_news\u0026quot; dataset table.schema  OUTPUT\n[SchemaField('by', 'STRING', 'NULLABLE', \u0026quot;The username of the item's author.\u0026quot;, ()), SchemaField('score', 'INTEGER', 'NULLABLE', 'Story score', ()), SchemaField('time', 'INTEGER', 'NULLABLE', 'Unix time', ()), SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', 'Timestamp for the unix time', ()), SchemaField('title', 'STRING', 'NULLABLE', 'Story title', ()), SchemaField('type', 'STRING', 'NULLABLE', 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', ()), SchemaField('url', 'STRING', 'NULLABLE', 'Story url', ()), SchemaField('text', 'STRING', 'NULLABLE', 'Story or comment text', ()), SchemaField('parent', 'INTEGER', 'NULLABLE', 'Parent comment ID', ()), SchemaField('deleted', 'BOOLEAN', 'NULLABLE', 'Is deleted?', ()), SchemaField('dead', 'BOOLEAN', 'NULLABLE', 'Is dead?', ()), SchemaField('descendants', 'INTEGER', 'NULLABLE', 'Number of story or poll descendants', ()), SchemaField('id', 'INTEGER', 'NULLABLE', \u0026quot;The item's unique id.\u0026quot;, ()), SchemaField('ranking', 'INTEGER', 'NULLABLE', 'Comment ranking', ())]  Cada SchemaField nos informa sobre una columna específica (a la que también nos referimos como un campo field). En orden, la información es:\n El nombre de la columna. El tipo de campo (o tipo de datos) en la columna El modo de la columna (\u0026lsquo;NULLABLE\u0026rsquo; significa que una columna permite valores NULL y es el valor predeterminado) Una descripción de los datos en esa columna. El primer campo tiene el SchemaField:  SchemaField (\u0026lsquo;by\u0026rsquo;, \u0026lsquo;string\u0026rsquo;, \u0026lsquo;NULLABLE\u0026rsquo;, \u0026ldquo;El nombre de usuario del autor del elemento\u0026rdquo;, ()\nEsto nos dice:\n el campo (o columna) es llamado por los datos en este campo son cadenas, Se permiten valores NULL y Contiene los nombres de usuario correspondientes al autor de cada elemento.  Podemos usar el método list_rows() para verificar solo las primeras cinco líneas de la tabla completa full para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery RowIterator que se puede convertir rápidamente en un DataFrame de pandas con el método to_dataframe().\n# Preview the first five lines of the \u0026quot;full\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  El método list_rows() también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!\n# Preview the first five entries in the \u0026quot;by\u0026quot; column of the \u0026quot;full\u0026quot; table client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()  EXERCISE (Exercise_ Getting Started With SQL and BigQuery)\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"d99c3c0c16cc6056768653d0069f966d","permalink":"https://www.marcusrb.com/cursos/cloud-computing/intro-cloud/cloud101-intro/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/cursos/cloud-computing/intro-cloud/cloud101-intro/","section":"cursos","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.","tags":null,"title":"Conceptos de Cloud computing","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.\nTus primeros comandos BigQuery Para usar BigQuery, importaremos el paquete de Python a continuación:\nfrom google.cloud import bigquery  El primer paso en el flujo de trabajo es crear un objeto Client. Como pronto verá, este objeto Client desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Trabajaremos con un conjunto de datos de publicaciones en Hacker News, un sitio web que se centra en noticias de informática y seguridad cibernética.\nEn BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos hacker_news está contenido en el proyecto bigquery-public-data. Para acceder al conjunto de datos,\n Comenzamos construyendo una referencia al conjunto de datos con el método dataset(). A continuación, utilizamos el método get_dataset(), junto con la referencia que acabamos de construir, para obtener el conjunto de datos.  # Construct a reference to the \u0026quot;hacker_news\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;hacker_news\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.\nUsamos el método list_tables() para listar las tablas en el conjunto de datos.\n# List all the tables in the \u0026quot;hacker_news\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there are four!) for table in tables: print(table.table_id)  OUTPUT\ncomments full full_201510 stories  De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla full en el conjunto de datos hacker_news.\n# Construct a reference to the \u0026quot;full\u0026quot; table table_ref = dataset_ref.table(\u0026quot;full\u0026quot;) # API request - fetch the table table = client.get_table(table_ref)  En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.\nEsquema de la tabla La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.\nEn este ejemplo, investigaremos la tabla completa full que obtuvimos anteriormente.\n# Print information on all the columns in the \u0026quot;full\u0026quot; table in the \u0026quot;hacker_news\u0026quot; dataset table.schema  OUTPUT\n[SchemaField('by', 'STRING', 'NULLABLE', \u0026quot;The username of the item's author.\u0026quot;, ()), SchemaField('score', 'INTEGER', 'NULLABLE', 'Story score', ()), SchemaField('time', 'INTEGER', 'NULLABLE', 'Unix time', ()), SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', 'Timestamp for the unix time', ()), SchemaField('title', 'STRING', 'NULLABLE', 'Story title', ()), SchemaField('type', 'STRING', 'NULLABLE', 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', ()), SchemaField('url', 'STRING', 'NULLABLE', 'Story url', ()), SchemaField('text', 'STRING', 'NULLABLE', 'Story or comment text', ()), SchemaField('parent', 'INTEGER', 'NULLABLE', 'Parent comment ID', ()), SchemaField('deleted', 'BOOLEAN', 'NULLABLE', 'Is deleted?', ()), SchemaField('dead', 'BOOLEAN', 'NULLABLE', 'Is dead?', ()), SchemaField('descendants', 'INTEGER', 'NULLABLE', 'Number of story or poll descendants', ()), SchemaField('id', 'INTEGER', 'NULLABLE', \u0026quot;The item's unique id.\u0026quot;, ()), SchemaField('ranking', 'INTEGER', 'NULLABLE', 'Comment ranking', ())]  Cada SchemaField nos informa sobre una columna específica (a la que también nos referimos como un campo field). En orden, la información es:\n El nombre de la columna. El tipo de campo (o tipo de datos) en la columna El modo de la columna (\u0026lsquo;NULLABLE\u0026rsquo; significa que una columna permite valores NULL y es el valor predeterminado) Una descripción de los datos en esa columna. El primer campo tiene el SchemaField:  SchemaField (\u0026lsquo;by\u0026rsquo;, \u0026lsquo;string\u0026rsquo;, \u0026lsquo;NULLABLE\u0026rsquo;, \u0026ldquo;El nombre de usuario del autor del elemento\u0026rdquo;, ()\nEsto nos dice:\n el campo (o columna) es llamado por los datos en este campo son cadenas, Se permiten valores NULL y Contiene los nombres de usuario correspondientes al autor de cada elemento.  Podemos usar el método list_rows() para verificar solo las primeras cinco líneas de la tabla completa full para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery RowIterator que se puede convertir rápidamente en un DataFrame de pandas con el método to_dataframe().\n# Preview the first five lines of the \u0026quot;full\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  El método list_rows() también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!\n# Preview the first five entries in the \u0026quot;by\u0026quot; column of the \u0026quot;full\u0026quot; table client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()  EXERCISE (Exercise_ Getting Started With SQL and BigQuery)\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"0a9645d6204ad6597ae47db7b4724964","permalink":"https://www.marcusrb.com/cursos/cloud-computing/intro-aws/aws101-servicios/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/cursos/cloud-computing/intro-aws/aws101-servicios/","section":"cursos","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.","tags":null,"title":"Conceptos de Amazon Web Services AWS","type":"docs"},{"authors":null,"categories":null,"content":" Unix es una familia de sistemas operativos. La primera versión de Linux fue desarrollada a partir de 1969. Unix se caracteriza por ser portable y multitarea.\n\nHoy en día los sistemas operativos Unix son ampliamente utilizados en multitud de dispositivos que abarcan desde los supercomputadores más capaces hasta los teléfonos móviles más populares, pasando por los ordenadores que utilizamos diariamente en nuestros escritorios. La filosofía de los sistemas Unix se caracteriza por:\n un sistema de ficheros jerárquico, una gran colección de pequeños programas que pueden trabajar en serie, el uso de ficheros de texto para almacenar los datos, tratar los dispositivos como ficheros.  Linux y MacOS X son ejemplos de sistemas Unix.\nUnix es un sistema operativo portable, multitarea y multiusuario desarrollado a partir de 1969.\nLinux Linux es una familia de sistemas operativos de tipo Unix que utilizan el kernel Linux. Linux puede instalarse en prácticamente cualquier ordenador personal además en en teléfonos móviles y supercomputadores.\nEl nombre proviene del programador original, un estudiante llamado Linus Torvals, que en 1991 completando las herramientas GNU desarrolladas por el proyecto GNU de la Fundación del Software Libre, creó la primera versión de este sistema operativo. El papel fundamental jugado por estas herramientas libres del proyecto GNU hace que este sistema operativo sea denominado también como GNU/Linux, pero en este texto utilizaremos la denominación más sencilla y corta.\nEl desarrollo de Linux es uno de los ejemplos más claros de desarrollo de software libre por una comunidad dispersa de programadores. Cualquiera puede usar el sistema operativo, estudiarlo y modificarlo. Estos derechos están protegidos por la licencia GPL (GNU General Public License).\nDistribuciones Linux, como cualquier otro sistema operativo, se compone de un gran número de piezas, que, en este caso, son desarrolladas de forma independiente por miles de programadores y proyectos. Normalmente estas piezas son integradas por un distribuidor y Linux es suministrado como una distribución Linux. Las distribuciones Linux incluyen todo el software necesario para instalar un servidor o un escritorio. Algunas de las aplicaciones comúnmente incluidas incluyen: el navegador web Firefox y las aplicaciones de oficina LibreOffice.\nExisten cientos de distribuciones Linux. Estas distribuciones están adaptadas para usuarios o tareas específicas. Algunas de estas distribuciones están desarrolladas o apoyadas por empresas como Fedora (Red Hat) y Ubuntu (Canonical) mientras que otras son mantenidas por la propia comunidad de usuarios como Debian.\nSoftware libre El software libre es software que puede ser utilizado, estudiado, modificado, copiado y redistribuido sin restricciones. Habitualmente el software libre suele ser además gratuito, pero ese no tiene por que ser necesariamente el caso.\nEn la práctica el software libre se distribuye junto al código fuente que lo hace posible y junto a una nota en la que se explican cuales son los derechos y las obligaciones del usuario final. Esta nota se denomina licencia. El movimiento del software libre fue iniciado por Richard Stallman en 1983. Stallman decidió crear un sistema compatible con Unix completamente libre al que llamó GNU (GNU is Not Unix). Con el tiempo este sistema acabaría uniéndose al kernel de Linus para formar un sistema operativo completo.\nDado que las aplicaciones del software libre suelen ser gratuitas, su modelo de negocio suele basarse en el cobro de los servicios de soporte al usuario y de adaptación del software.\nIntroducción a Ubuntu Ubuntu es una distribución Linux mantenida por la empresa Canonical. Está orientada a usuarios de escritorio y sus puntos fuertes son su facilidad de uso y de instalación. Aunque el escritorio es algo distinto al de Windows o Mac OS X familiarizarse con él para un usuario acostumbrado a cualquiera de los otros sistemas operativos no debería presentar muchos problemas.\nSu instalación resulta muy sencilla. Al instalarla, una gran cantidad de software se instala de forma automática para facilitar su uso como escritorio. Ejemplos de estos programas son LibreOffice o Firefox. Además de estos programas instalados por defecto una enorme cantidad de programas se encuentra disponible para ser instalados con unos pocos clicks de ratón.\nUbuntu está basada en una distribución mantenida por la comunidad de usuarios llamada Debian. El principal objetivo de Debian es crear un sistema operativo robusto que incluya la mayor proporción posible de programas libres.\nExisten numerosos manuales de utilización de Ubuntu, pero algunas de las guías más completas son la Ubuntu Desktop Guide y el manual Getting Started with Ubuntu. Vamos a recorrer lo principales conceptos de este sistema operativo basandonos en este manual.\nLos usuarios Los sistemas Unix son multiusuario, es decir soportan que varios usuarios los utilicen simultáneamente. Todos los usuarios, excepto uno, tienen unos privilegios bastante restringidos y no pueden modificar el sistema. De este modo unos usuarios se ven protegidos de las acciones de los otros.\nExiste un usuario especial llamado root con privilegios de administración absolutos sobre el sistema. Para realizar las tareas cotidianas nunca hay que acceder al sistema como root. En Ubuntu este usuario está deshabilitado por defecto y sólo se pueden adquirir los privilegios de administrador temporalmente.\nEl escritorio Todos las distribciones basadas en entronos graficos (GUI) suelen tener varios entornos de escritorio para eleguir. Los entornos de escritorio suelen diferir por:\n El estilo y apariencia del entorno La forma en la que los diferentes elementos se disponen en la pantalla La forma en la que el ususario navega por el escritorio  En el caso de Ubuntu el entorno de escritorio por defecto se denomina Unity. Se caracteriza por tener dos barras, la denominada Menu Bar y el Launcher. El Menu Bar incorpora por una lado los menus de las aplicaciones que estań activas y, por otro, un area de indicadores que nos ofrecen informacion actualizada del sistema en todo momento. El Launcher es la barra vertical que facilita el acceso a las aplicaciones mas usadas y a su estado, además de a los discos montados y a la papelera. Además tenemos el selector de escritorios virtuales. En el launcher encontramos varias aplicaciones especiales:\n El menu El selector de escritorios virtuales La papelera  Los escritorios virtuales sirven para ampliar la zona de trabajo. Por defecto hay 4 escritorios virtuales que amplían nuestro monitor por cuatro.\nNavegando por el sistema de ficheros Al sistema de ficheros se accede a traves del menu. Tenemos la posibilidad de buscar en la barra de busqueda o navegar directamente por el menu. Una vez que seleccionemos la carpeta se abrirá una ventana del navegador de ficheros con la carpeta seleccionada. Por cierto, los términos carpeta y directorio son sinónimos, al igual que fichero y archivo.\nDesde el launcher también podemos acceder a nuestra carpeta personal. Este directorio personal y sus subdirectorios son los únicos lugares en los que podremos almacenar nuestros archivos personales. El resto del sistema de archivos estará restringido para funciones de administración del sistema.\nEjercicios Para comprobar que no tenemos problemas de manejo del sistema vamos a realizar una serie de tareas:\n Explorar el escritorio abriendo programas, moviéndote entre escritorios virtuales, minimizando y maximizando las aplicaciones, etc. Navegar al directorio “Documentos” y comprobar si tenemos algún archivo guardado. Crear un subdirectorio llamado “curso” dentro del directorio “Documentos”. Crear un fichero de texto mediante el editor de textos gedit y guardarlo en el directorio que acabamos de crear. Copiar el fichero anterior al directorio personal. Eliminar el fichero original. Reinicia el sistema operativo. Bloquea la sesión de tu usuario y vuelve a entrar en ella Añade un nuevo usuario al sistema Sal de tu usuario actual y entra como el nuevo usuario Cambiar el password del nuevo usuario Configura el protector de pantalla para que se inicie a los 10 minutos de inactividad Modifica los ajustes de la aplicación terminal para que el tipo de letra tenga un tamaño de 11 puntos Ancla la aplicación terminal a la barra de aplicaciones y desancla el editor de hojas de cálculo LibreOffice  La terminal de UNIX La Shell (o terminal) es un interprete de comandos. Es simplemente un modo alternativo de controlar un ordenador basado en una interfaz de texto. La terminal nos permite ejecutar software escribiendo el nombre del programa que queremos ejecutar en la terminal. Podemos pedirle al ordenador que ejecute un programa mediante el ratón ciclando en distintos lugares del escritorio o podemos escribir una orden para conseguir el mismo objetivo. Por ejemplo, para pedirle al ordenador que nos de una lista de los archivos presentes en un directorio podemos abrir un navegador de archivos o podemos escribir en la terminal:\n$ ls folder_name file_1.txt file_2.txt  Ninguna de las dos formas de comunicarse con el ordenador es mejor que la otra aunque en ciertas ocasiones puede resultar más conveniente utilizar una u otra Las ventajas de la línea de comandos son:\n Necesidad. Existe mucho software que está sólo disponible en la terminal. Esto es especialmente cierto en el área de la bioinformática. Flexibilidad. Los programas gráficos suelen ser muy adecuados para realizar la tarea para la que han sido creados, pero son difíciles de adaptar para otras tareas. Los programas diseñados para ser usados en la línea de comandos suelen ser muy versátiles. Reproducibilidad. Documentar y repetir el proceso seguido para realizar un análisis con un programa gráfico es muy costoso puesto que es difícil describir la secuencia de clicks y doble clicks que hemos realizado. Por el contrario, los procesos realizados mediante la línea de comandos son muy fáciles de documentar puesto que tan sólo debemos guardar el texto que hemos introducido en la pantalla. Fiabilidad. Los programas básicos de Unix fueron creados en los años 70 y han sido probados por innumerables usuarios por lo que se han convertido en piezas de código extraordinariamente confiables. Recursos. Las interfaces gráficas suelen consumir muchos recursos mientras que los programas que funcionan en línea de comandos suelen ser extraordinariamente livianos y rápidos. Este poco uso de recursos facilita, por ejemplo, que se utilice a través de la red.  El problema de la terminal es que para poder utilizarla debemos saber previamente qué queremos hacer y cómo. Es habitual descubrir como funciona un programa con una interfaz gráfica sin tener que leer un manual, esto no sucede en la terminal.\nPara usar la línea de comandos hay que abrir una terminal. Se abrirá una terminal con un mensaje similar a:\nusuario $  Este pequeño mensaje se denomina prompt y el cursor parpadeante que aparece junto al él indica que el ordenador está esperando una orden. El mensaje exacto que aparece en el prompt puede variar ligeramente, pero en Ubuntu suele ser similar a:\nusuario@ordenador:~/documentos$  En el prompt de Ubuntu se nos muestra el nombre del usuario, el nombre del ordenador y el directorio en el que nos encontramos actualmente, es decir, el directorio de trabajo actual.\nCuando el prompt se muestra podemos ejecutar cualquier cosa, por ejemplo le podemos pedir que liste los ficheros mediante el comando ls (LiSt)::\nusuario $ ls lista_libros.txt rectas_cocina/  ls, como cualquier otro comando, es en realidad un programa que el ordenador ejecuta. Cuando escribimos la orden (y pulsamos enter) el programa se ejecuta. Mientras el programa está ejecutándose el prompt desaparece y no podemos ejecutar ningún otro comando. Pasado el tiempo el programa termina su ejecución y el prompt vuelve a aparecer. En el caso del comando ls el tiempo de ejecución es tan pequeño que suele ser imperceptible.\nLos programas suelen tener unas entradas y unas salidas. Dependiendo del caso estas pueden ser ficheros o caracteres introducidos o impresos en la pantalla. Por ejemplo, el resultado de ls es simplemente una lista impresa de ficheros y directorios en la interfaz de comandos.\nNormalmente el comportamiento de los programas puede ser modificado pasándoles parámetros. Por ejemplo, podríamos pedirle al programa ls que nos imprima una lista de ficheros más detallada escribiendo:\n$ ls -l  Ayuda Cada comando tiene unos parámetros y opciones distintos. La forma estándar de pedirles que nos enseñen cuales son estos parámetros suele ser utilizar las opciones ‘–help’, ‘-h’ o ‘-help’, aunque esto puede variar en comandos no estándar.\n$ ls --help Modo de empleo: ls [OPCIÓN]... [FICHERO]... List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort.  Otro modo de acceder a una documentación más detallada es acceder al manual del programa utilizando el comando man (MANual):\n$ man ls (para terminar pulsar \u0026quot;q\u0026quot;)  man es un programa interactivo, cuando ejecutamos el comando el programa se abre y el prompt desaparece. man es en realidad un visor de ficheros de texto por lo que cuando lo ejecutamos la pantalla se rellena con la ayuda del programa que hemos solicitado. Podemos ir hacia abajo o hacia arriba y podemos buscar en el contenido de la ayuda. El prompt y la posibilidad de ejecutar otro programa no volverán a aparecer hasta que no cerremos el programa interactivo. En el caso de man para cerrar el programa hay que pulsar la tecla “q”.\nCompletado automático e historia El intérprete de comandos dispone de algunas utilidades para facilitarnos su uso. Una de las más utilizadas es el completado automático. Podemos evitarnos escribir una gran parte de los comandos haciendo uso de la tecla tabulador. Si empezamos a escribir un comando y pulsamos la tecla tabulador el sistema completará el comando por nosotros. Para probarlo creemos los ficheros datos_1.txt, datos_2.txt y tesis.txt::\n~$ touch datos_1.txt ~$ touch datos_2.txt ~$ touch experimento.txt  Si ahora empezamos a escribir cp e y pulsamos el tabulador dos veces, el intérprete de comandos completará el comando automáticamente::\n~$ cp e ~$ cp experimento.txt  Si el intérprete encuentra varias alternativas completará el comando hasta el punto en el que no haya ambigüedad. Si deseamos que imprima una lista de todas las alternativas disponibles para continuar con el comando deberemos pulsar el tabulador dos veces.\n~$ cp d $ cp datos_ datos_1.txt datos_2.txt ~$ cp datos_  Otra de las funcionalidades que más nos pueden ayudar es la historia. El intérprete recuerda todos los comandos que hemos introducido anteriormente. Si queremos podemos obtener una lista de todo lo que hemos ejecutado utilizando el comando history. Pero lo más socorrido es simplemente utilizar los cursores arriba y abajo para revisar los comandos anteriores. Otra forma de acceder a la historia es utilizar la combinación de teclas control y r. De este modo podemos buscar comandos antiguos sencillamente.\nEjercicio  Lista todos los comandos que empiezan por apt  Bibliografía Existen numerosas fuentes sobre la historia y la filosofía de Unix, de Linux y del software libre. Entre ellas se encuentran:\n Las páginas de la wikipedia sobre: Unix, Linux, Ubuntu y [software libre](https://bioinf.comav.upv.es/courses/unix/. Rebel Code, un libro de Glyn Moody dedicado a la historia del movimiento del software libre. La catedral y el bazar de Eric S. Raymond. Un ensayo sobre los beneficios del modelo de desarrollo asociados al software libre. The Art of Unix Programming (pdf de Eric S. Raymond. Dedicado a la filosofía de los sistemas Unix. El excelente [Ubuntu manual](https://bioinf.comav.upv.es/courses/unix/ [pdf](https://bioinf.comav.upv.es/courses/unix/. La documentación oficial y de la comunidad de Ubuntu.  Hay varios cursos para iniciarse en el uso de la línea de comandos de Unix, como:\n Put Yourself in Command de la Free Software Fundation, copia en pdf. Learning the shell de Linuxcommand.org. Rute User’s Tutorial and Exposition de Paul Sheer, copia en pdf. Learning the Unix Operating System.  ","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"10925b8e03529f94f082e936a6b954a5","permalink":"https://www.marcusrb.com/unix/01-unix-intro/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/unix/01-unix-intro/","section":"recursos","summary":"Unix es una familia de sistemas operativos. La primera versión de Linux fue desarrollada a partir de 1969. Unix se caracteriza por ser portable y multitarea.","tags":null,"title":"UNIX from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" El sistema de archivos controla como se almacenan los archivos en el ordenador. Sus dos tareas principales son guardar y leer archivos previamente guardados.\nSistemas jerárquicos Los sistemas de archivos suelen tener directorios en los que organizar los archivos y estos directorios suelen estar organizados jerárquicamente. La jerarquía implica que un directorio puede contener subdirectorios. El directorio más alto en la jerarquía del que cuelgan todos los demás se denomina raíz (root). En los sistemas Unix el directorio raíz se representa con una barra “*/*” y sólo existe una jerarquía, es decir, sólo existe un directorio raíz, incluso aunque haya distintos discos duros en el ordenador.\nDentro del directorio raíz podemos encontrar diversos subdirectorios, por ejemplo en Linux existe el directorio home. home es por tanto un subdictorio del directorio raíz. Esta relación se representa como:\n/home  home es el directorio dónde se encuentran los directorios de los usuarios en un sistema Linux. Imaginemos que tiene los subdirectorios alicia y juan. Se representaría como:\n/home/alicia /home/juan  Existe un estándar, denominado Filesystem Hierarchy Standard que define la estructura de directorios de los sistemas Unix. Los sistemas Unix suelen seguir este estándar, aunque a veces lo violan en algunos aspectos. Por ejemplo en MacOS X el directorio donde se encuentran los direcotorios de los usuarios se denomina Users y no home\nEn algunos sistemas operativos no UNIX la barra se escribe al revés “\u0026rdquo;, a pesar de que la convención siempre fue la contraria.\nEn el directorio raíz hay diversos directorios que, en la mayoría de los casos, sólo deberían interesarnos si estamos administrando el ordenador. Los usuarios normalmente sólo escriben dentro de un directorio de su propiedad localizado dentro de /home y denominado como su nombre de usuario.\nLos usuarios también pueden escribir en /tmp aunque normalmente son los procesos lanzados por estos lo que hacen esta escritura. Es importante revisar el espacio libre en la partición en la que se encuentra /tmp para que no se colapse el sistema. Recuerda que /tmp es borrado habitualmente por el sistema. Normalmente con cada nuevo arranque.\nRutas absolutas, relativas y directorio de trabajo Para referirnos a un archivo o a un directorio debemos indicar su ruta (path. Un ejemplo de ruta podría ser:\n/home/alicia/documentos/tesis.md  Este tipo de rutas en las que se especifican todos los subdirectorios empezando desde el directorio raíz se denominan rutas absolutas.\nPara no tener que escribir la ruta absoluta completa cada vez que queremos referirnos a un archivo o a un directorio se crearon los conceptos de directorio de trabajo y de ruta relativa.\nEl directorio de trabajo es una propiedad del terminal (del shell) en la que estamos trabajando. Siempre que estemos trabajando en una terminal tendremos asignado un directorio de trabajo. Por ejemplo, si nuestro usuario es alicia sería normal que al abrir un terminal nuestro directorio de trabajo fuese:\n/home/alicia  El directorio de trabajo se utiliza para escribir rutas a archivos relativas al mismo. De este modo nos ahorramos escribir bastante. Imaginemos que Alicia tiene en su directorio un documento llamado peliculas.txt. La ruta absoluta sería.\n/home/alicia/peliculas.txt  Mientras su directorio de trabajo sea /home/alicia la ruta relativa sería simplemetne:\npeliculas.txt  Es decir, podemos escribir rutas relativas al directorio de trabajo, rutas que en vez de partir del directorio raíz parten desde el directorio de trabajo. Las rutas relativas se diferencian de las absolutas en los sistemas Unix porque las absolutas empiezan por “y” las relativas no.\nEs común referirse al directorio de trabajo de una terminal como a un lugar en el que nos encontramos mientras estamos trabajando en la terminal. Siempre que estemos en una terminal estaremos dentro de un directorio de trabajo.\nPor ejemplo, cuando abrimos un nuevo terminal el directorio de trabajo se sitúa en /home/nombre_de_usuario. Si ejecutamos el comando ls, el programa asumirá que queremos listar los archivos presentes en ese directorio y no en otro cualquiera. Existe un comando que nos informa sobre el directorio de trabajo actual, pwd (Print Working Directory):\n$ pwd /home/alicia  Si deseamos podemos modificar el directorio de trabajo “moviéndonos” a otro directorio. Para lograrlo hay que utilizar el comando cd (Change Directory):\n$ cd documentos $ pwd /home/alicia/documentos  A partir de ese momento los comandos asumirán que si no se les indica lo contrario el directorio desde el que deben trabajar es /home/alicia/documentos.\ncd además tiene algunos parámetros especiales:\ncd Ir al directorio $HOME del usuario. cd - Ir al directorio de trabajo previo  Directorio $HOME El directorio $HOME en los sistemas Unix, que son sistemas multiusuario, es el directorio en el que el usuario debe mantener sus ficheros y directorios. Fuera de este directorio el usuario tendrá unos permisos restringidos puesto que sus acciones podrían afectar a otros usuarios.\nEn Linux los directorios $HOME de los usuarios son subdirectorios del directorio /home.\nEl directorio $HOME de un usuario es además el directorio de trabajo por defecto, es decir, el directorio de trabajo que se establece cuando se abre una terminal.\nMoviendo, renombrando y copiando ficheros En primer lugar vamos a crear un fichero de prueba:\n~$ touch data.txt ~$ ls data.txt  El comando touch, en este caso, ha creado un fichero vacío.\nLos ficheros se copian con el comando cp (CoPy):\n~$ cp data.txt data.bak.txt ~$ ls data.bak.txt data.txt  Se mueven y renombran con el mv (MoVe):\n~$ mv data.txt experimento_1.txt ~$ ls data.bak.txt experimento_1.txt  Para crear un nuevo directorio podemos utilizar la orden mkdir (MaKeDIRectory):\n~$ mkdir exp_1 ~$ ls data.bak.txt exp_1 experimento_1.txt  mv también sirve para mover ficheros entre directorios:\n~$ mv experimento_1.txt exp_1/ ~$ ls data.bak.txt exp_1 ~$ ls exp_1/ experimento_1.txt  Los ficheros se eliminan con la orden rm (ReMove):\n~$ rm data.bak.txt ~$ ls exp_1  En la línea de comandos de los sistemas Unix cuando se borra un fichero se borra definitivamente, no hay papelera. Una vez ejecutado el rm no podremos recuperar el archivo.\nLos comandos cp y rm no funcionarán bien con los directorios a no ser que modifiquemos el comportamiento que muestran por defecto:\n~$ rm exp_1/ rm: cannot remove exp_1/ Is a directory ~$ cp exp_1/ exp_1_bak/ cp: omitting directory exp_1/  Esto sucede porque para copiar o borrar un directorio hay que copiar o borrar todos sus contenidos recursivamente y esto podría alterar muchos datos con un sólo comando. Por esta razón se exige que estos dos comandos incluyan un modificador que les indique que sí deben funcionar recursivamente cuando tratan con directorios:\n~$ cp -r exp_1/ exp_1_bak/ ~$ ls exp_1 exp_1_bak ~$ rm -r exp_1_bak/ ~$ ls exp_1  Nombres de directorios y archivos En Unix los archivos pueden tener prácticamente cualquier nombre. Existe la convención de acabar los nombres con un punto y una pequeña extensión que indica el tipo de archivo. Pero esto es sólo una convención, en realidad podríamos no utilizar este tipo de nomenclatura.\nSi deseamos utilizar nombres de archivos que no vayan a causar extraños comportamientos en el futuro lo mejor sería seguir unas cuantas reglas al nombrar un archivo:\n Añadir una extensión para recordarnos el tipo de archivo, por ejemplo .txt para los archivos de texto. No utilizar en los nombres:  espacios, caracteres no alfanuméricos, ni caracteres no ingleses como letras acentuadas o eñes.   Por supuesto, podríamos crear un archivo denominado “$ñ 1.txt” para referirnos a un archivo de sonido, pero esto conllevaría una sería de problemas que aunque son solventables nos dificultarán el trabajo.\nAdemás es importante recordar que en Unix las mayúsculas y las minúsculas no son lo mismo. Los ficheros “documento.txt”, “Documento.txt” y “DOCUMENTO.TXT” son tres ficheros distintos.\nOtra convención utilizada en los sistema Unix es la de ocultar los archivos cuyos nombres comienzan por punto “.”. Por ejemplo el archivo “.oculto” no aparecerá normalmente cuando pedimos el listado de un directorio. Esto se utiliza normalmente para guardar archivos de configuración que no suelen ser utilizados directamente por los usuarios. Para listar todos los archivos (All), ya sean éstos ocultos o no se puede ejecutar:\n$ ls -a . .fontconfig .HyperTree .pki .. fsm.jpg .ICEauthority .recently-used  Esta convención de ocultar los ficheros cuyo nombre comienza por un punto se mantiene también en el navegador gráfico de ficheros. En este caso podemos pedir que se muestren estos archivos en el menú Ver -\u0026gt; Mostrar los archivos ocultos.\nPara acelerar el acceso a ciertos directorios existen algunos nombres especiales que son bastante útiles:\n* \u0026quot;..\u0026quot; indica el directorio padre del directorio actual * \u0026quot;.\u0026quot; indica el directorio actual * \u0026quot;~\u0026quot; representa la $HOME del usuario  WildCards En muchas ocasiones resulta útil tratar los ficheros de un modo conjunto. Por ejemplo, imaginemos que queremos mover todos los ficheros de texto a un directorio y la imágenes a otro. Creemos una pequeña demostración::\n~$ touch exp_1a.txt ~$ touch exp_1b.txt ~$ touch exp_1b.jpg ~$ touch exp_1a.jpg ~$ ls exp_1 exp_1a.jpg exp_1a.txt exp_1b.jpg exp_1b.txt  Podemos referirnos a todos los archivos que acaban en txt utilizando un asterisco:\n~$ mv *txt exp_1 ~$ ls exp_1 exp_1a.jpg exp_1b.jpg  El asterisco sustituye a cualquier texto, por lo que al escribir *txt incluimos a cualquier fichero que tenga un nombre cualquiera, pero que termine con las letras txt. Podríamos por ejemplo referirnos a los ficheros del experimento 1a:\n~$ ls *1a* exp_1a.jpg  Esta herramienta es muy potente y útil, pero tenemos que tener cuidado con ella, sobre todo cuando la combinamos con rm. Por ejemplo la orden:\n$ rm -r *  Borraría todos los ficheros y directorios que se encuentren bajo el directorio de trabajo actual, si lo hacemos perderemos todos los ficheros y directorios que cuelgan del actual directorio de trabajo, puede que esto sea lo que queramos, pero hemos de andar con cuidado.\nEjercicios  ¿Cuáles son los ficheros y directorios presentes en el directorio raíz? ¿Cuáles son todos los archivos presentes en nuestro directorio de usuario? Crea un directorio llamado experimento. Crea con touch los archivos datos1.txt y datos2.txt dentro del directorio experimento. Vuelve al directorio principal de tu usuario y desde allí lista los archivos presentes en el directorio experimento usando rutas absolutas y relativas Haz del directorio ~/Documentos tu directorio de trabajo y repite el ejercicio anterior Borra todos los archivos que contengan un 2 en el directorio experimento. Copia el directorio experimento a un nuevo directorio llamado exp_seguridad. Borra el directorio experimento. Renombra el directorio exp_seguridad a experimento. Copia el fichero /etc/passwd al directorio ~/Documentos Copia el fichero /etc/passwd al directorio ~/Documentos llamándolo usuarios.txt  Obteniendo información sobre archivos y directorios ls es un comando capaz de mostrarnos información extra sobre los archivos y directorios que lista. Por ejemplo podemos pedirle, usando la opción -l (Long), que nos muestre quién es el dueño del archivo y cuanto ocupa y qué permisos tiene además de otras cosas::\n~$ ls exp_1 ~$ ls -l total 4 drwxr-xr-x 2 usuario usuario 4096 Oct 13 09:48 exp_1  La información sobre la cantidad de disco ocupada la da por defecto en bytes, si la queremos en un formato más inteligible podemos utilizar la opción -h (Human):\n~$ ls -lh total 4.0K drwxr-xr-x 2 usuario usuario 4.0K Oct 13 09:48 exp_1  Podemos consultar el tipo de un archivo mediante el comando file.\n~$ file imagen.png imagen.png: PNG image data, 1920 x 1080, 8-bit/color RGB, non-interlaced  En principio, el tipo de un archivo no está determinado por la extensión, la extensión es sólo parte del nombre, aunque hay software que viola o complementa este principio. El tipo de archivo está determinado por su magic number. El magic number está compuesto por una corta serie de bytes que indican el tipo de archivo.\nPermisos Unix desde su origen ha sido un sistema multiusuario. Para conseguir que cada usuario pueda trabajar en sus archivos, pero que no pueda interferir accidental o deliberadamente con los archivos de otros usuarios se estableció desde el principio un sistema de permisos. Por defecto un usuario tiene permiso para leer y modificar sus propios archivos y directorios, pero no los de los demás. En los sistemas Unix los ficheros pertenecen a un usuario concreto y existen unos permisos diferenciados para este usuario y para el resto. Además el usuario pertenece a un grupo de trabajo. Por ejemplo, imaginemos que la usuaria alicia puede pertenecer al grupo de trabajo “diagnostico”. Si alicia crea un fichero este tendrá unos permisos diferentes para alicia, para el resto de miembros de su grupo y para el resto de usuarios del ordenador. Podemos ver los permisos asociados a los ficheros utilizando el comando ls con la opción -l (Long)::\n~$ ls -l total 7324 -rw-r--r-- 1 alicia diagnostico 1059 Oct 20 12:42 busqueda_leukemia_100.txt -rw-r--r-- 1 alicia diagnostico 0 Oct 13 10:53 datos_1.txt drwxr-xr-x 2 alicia diagnostico 4096 Oct 13 10:29 experimento  En este caso, los ficheros listados pertenecen Alicia y al grupo diagnostico. Los permisos asignados al usuario, a los miembros del grupo y al resto de usuarios están resumidos en la primeras letras de cada línea::\ndrwxr-x---  La primera letra indica el tipo de fichero listado: (d) directorio, (-) fichero u otro tipo especial. Las siguientes nueve letras muestran, en grupos de tres, los permisos para el usuario, para el grupo y para el resto de usuarios del ordenador. Cada grupo de tres letras indica los permisos de lectura (Read), escritura (Write) y ejecución (eXecute). En el caso anterior el usuario tiene permiso de lectura, escritura y ejecución (rwx), el grupo tiene permiso de lectura y ejecución (r-x), es decir no puede modificar el fichero o el directorio, y el resto de usuarios no tienen ningún permiso (—).\nEn los ficheros normales el permiso de lectura indica si el fichero puede ser leído, el de escritura si puede ser modificado y el de ejecución si puede ser ejecutado. En el caso de los directorios el de escritura indica si podemos añadir o borrar ficheros del directorio y el de ejecución si podemos listar los contenidos del directorio.\nEstos permisos pueden ser modificados con la orden chmod. En chmod cada grupo de usuarios se representa por una letra:\n u: usuario dueño del fichero g: grupo de usuarios del dueño del fichero o: todos los otros usuarios a: todos los tipos de usuario (dueño, grupo y otros)  Los tipos de permisos también están abreviados por letras:\n r: lectura w: escritura x: ejecución  Con estas abreviaturas podemos modificar los permisos existentes.\nHacer un fichero ejecutable:\n$ chmod u+x  O:\n$ chmod a+x  También podemos mediante chmod indicar los permisos para un tipo de usuario determinado.\n$ chmod a=rwx  Un modo algo menos intuitivo, pero más útil de utilizar chmod es mediante los números octales que representan los permisos.\n- lectura: 4 - escritura: 2 - ejecución: 1  Para modificar los permisos de este modo debemos indicar el número octal que queremos que represente los permisos del fichero. La primera cifra representará al dueño, la segunda al grupo y la tercera al resto de usuarios. Por ejemplo si queremos que único permiso para el dueño y su grupo sea la lectura y que no haya ningún permiso para el resto de usuarios:\n$ chmod 110 fichero.txt  También podemos combinar permisos sumando los números anteriores. Por ejemplo, permiso para leer y escribir para el dueño y ningún permiso para el resto.\n$ chmod 300 fichero.txt  Permisos de lectura, escritura y ejecución para el dueño y su grupo y ninguno para el resto.\n$ chmod 770 fichero.txt  Las restricciones para los permisos no afectan al usuario root, al administrador del sistema. root también puede modificar quien el dueño y el grupo al que pertenecen los ficheros mediante los comando chown y chgrp.\n$ chown alicia fichero.txt $ chown diagnostico fichero.txt  Obteniendo información sobre el sistema de archivos El sistema de archivos puede abarcar una o más particiones. Una partición es una región de un disco o de cualquier otro medio de almacenamiento. Las instalaciones de Windows tienen normalmente una partición por disco, pero en Linux esto no es tan habitual. Cada partición tiene un sistema de archivos propio, pero en Unix estos sistemas deben estar montados en algún lugar dentro de la jerarquía que cuelga de la raíz. En Windows cada partición tiene por defecto una jerarquía independiente.\nPodemos pedir información sobre el espacio ocupado por las distintas particiones que tenemos actualmente montadas usando el comando df (Disk Free).\n$ df -h S.ficheros Tamaño Usados Disp Uso% Montado en udev 7,8G 0 7,8G 0% /dev tmpfs 1,6G 9,8M 1,6G 1% /run /dev/nvme0n1p2 25G 8,1G 16G 35% / tmpfs 7,8G 5,3M 7,8G 1% /dev/shm tmpfs 5,0M 4,0K 5,0M 1% /run/lock tmpfs 7,8G 0 7,8G 0% /sys/fs/cgroup /dev/nvme0n1p4 206G 18G 178G 9% /home /dev/nvme0n1p1 511M 3,6M 508M 1% /boot/efi /dev/sda1 2,7T 117G 2,5T 5% /home/jose/magnet tmpfs 1,6G 64K 1,6G 1% /run/user/1000  Algunos de los sistemas de archivos montados puede que no se correspondan con particiones en un disco físico sino con espacios de la memoria RAM que son utilizados como sistemas de archivos especiales.\nEl commando du (disk usage) informa sobre el espacio que ocupa un árbol de directorios. Este comando tiene equivalentes gráficos como Baobab o xdiskusage. Podemos pedir a du que nos muestre cuanto espacio ocupan los directorios bajo el directorio analysis:\n$ du -h analyses/ 36K\tanalyses/alicia/cache 204K\tanalyses/alicia/differential_snps/differential 252K\tanalyses/alicia/differential_snps/non_differentia 919M\tanalyses/  Si sólo queremos obtener el resultado para el directorio que le hemos dado y no para sus subdirectorios podemos utilizar el parámetro -s:\n$ du -sh analyses/ 919M\tanalyses/  Si queremos información sobre todos los archivos y no sólo los directorios podemos usar -a:\n$ du -ha analyses/ 32K\tanalyses/alicia/cache/min_called_rate_samples_cache.pickle 36K\tanalyses/alicia/cache 8,0K\tanalyses/alicia/look_for_matching_accessions.py  Ejercicios  ¿Cuáles son los permisos de los directorios presentes en el directorio raíz y en nuestro directorio de usuario? ¿A quién pertenecen los ficheros y qué permisos tienen los distintos usuarios del ordenador? Crea un directorio en tu home y muestra los permisos que tiene. Cambia los permisos para que sólo tu usuario pueda acceder al nuevo directorio Crea un fichero nuevo y dale permisos de ejecución para todos los usuarios Último fichero modificado en el directorio /etc. Lista los ficheros de /etc con su tamaño y ordénalos por tamaño. Copia todos los ficheros y directorios del directorio /etc cuyo nombre comience por s. ¿Has podido copiarlos todos? ¿Cuánto espacio libre queda en las distintas particiones del sistema? ¿Cuánto espacio ocupan todos los ficheros y subdirectorios de tu $HOME?  Compresion y descompresión de ficheros Existen distintos formatos de compresión de ficheros como: gzip, bzip, zip o rar. Los formatos más utilizados en Unix son gzip y bzip.\nComprimir un fichero con gzip o bzip:\n$ gzip informacion_snps.txt $ ls informacion_snps.txt.gz $ bzip2 accs.txt $ ls accs.txt.bz2  bzip2 comprime más que gzip, pero es más lento. gzip también dispone de varios niveles de compresión, cuanto más comprime más lenta suele ser la compresión.\nPodemos descomprimir cualquier fichero utilizando la línea de comandos:\n$ gunzip informacion_snps.txt.gz $ ls informacion_snps.txt $ bunzip2 accs.txt.bz2 $ ls accs.txt  Muchos estamos acostumbrados al formato zip. Un fichero zip no se corresponde en realidad con un sólo fichero comprimido sino con varios. Un fichero zip hace dos cosas: unir varios ficheros en uno y comprimir el resultado. Los comandos que hemos visto (gzip y bzip2) son capaces de comprimir un sólo archivo, pero no pueden unir varios archivos en uno. tar es el comando capaz de unir varios archivos en uno.\n$ ls seq1.fasta seq2.fasta $ tar -cvf secuencias.tar seq* seq1.fasta seq2.fasta $ ls secuencias.tar seq1.fasta seq2.fasta  tar también es capaz de desempaquetar los archivos que habíamos unido.\n$ ls secuencias.tar $ rm seq1.fasta seq2.fasta $ tar -xvf secuencias.tar seq1.fasta seq2.fasta $ ls secuencias.tar seq1.fasta seq2.fasta  El problema es que utilizando el comando tar tal y como lo hemos hecho hemos conseguido unir y separar archivos, pero no hemos comprimido el fichero unido. Para hacerlo podríamos utilizar los comandos gzip o bzip2, pero este no es el modo habitula de hacerlo. Dado que casi siempre que unamos archivos en un archivo tar también querremos comprimir el resultado el comando tar tiene también la capacidad de comprimir y descomprimir utilizando los algoritmos gzip y bzip2. Unir y comprimir con gzip varios archivos:\n$ tar -cvzf secuencias.tar.gz seq* seq1.fasta seq2.fasta $ ls secuencias.tar.gz seq1.fasta seq2.fasta  Descomprimir un archivo tar.gz:\n$ tar -xvzf secuencias.tar.gz seq1.fasta seq2.fasta $ ls secuencias.tar.gz seq1.fasta seq2.fasta  También podemos descomprimir el contenido de un fichero de texto y enviar el resultado a la terminal con el comando zcat.\n$ zcat fichero.txt.gz\nCon bzip2.\n$ tar -cvjf secuencias.tar.bz seq* seq1.fasta seq2.fasta $ ls secuencias.tar.bz seq1.fasta seq2.fasta $ tar -xvjf secuencias.tar.bz seq1.fasta seq2.fasta  Ejercicios  Crea un fichero de texto en el directorio ~/Documentos y comprimelo con gzip Muestra el contenido del fichero anterior en pantalla sin descomprimirlo previamente Crea un archivo tar de todo el contenido del directorio ~/Documentos Comprime el fichero tar anterior Vuelve a hacer los ejercicios 2 y 3, pero en un sólo paso Descomprime el fichero tar.gz anterior en un nuevo directorio llamado Documentos2  Enlaces duros y blandos Podemos pensar en el nombre de un fichero como en una etiqueta que apunta a una posición concreta en el disco duro, en realidad es un puntero a un inodo.\nPodmeos pensar en un enlace duro como en un nombre adicional para un archivo. Si tenemos un archivo en el disco y creamos un enlace duro tendremos dos nombres para ese único archivo.\n$ ls archivo1.txt $ ln archivo1.txt nombre2.txt $ ls archivo1.txt nombre2.txt  Las dos referencias, nombres, al archivo serán indistinguibles. Si borramos un nombre quedará el otro. Si modificamos un archivo se modifica independientemente del nombre por el cual estemos accediendo a él. No es muy común utilizar enlaces duro salvo en aplicaciones muy concretas, por ejemplo en versiones de copias de seguridad.\nUn enlace blando, más comumente conocido como un enlace simbólico, es una referencia al nombre de un archivo, no al archivo en sí.\n$ ls archivo1.txt $ ln -s archivo1.txt nombre3.txt $ ls -l -rw-rw-r-- 1 jose jose 0 sep 27 15:16 archivo1.txt lrwxrwxrwx 1 jose jose 12 sep 27 15:16 nombre3.txt -\u0026gt; archivo1.txt  Si eliminamos el archivo original el enlace quedará roto.\n$ rm archivo1.txt $ cat nombre3.txt cat: nombre3.txt: No existe el archivo o el directorio  El comportamiento de ambos tipos de enlaces cambia si sobreescribimos el fichero.\nx $ echo \u0026quot;hola\u0026quot; \u0026gt; hola.txt $ cat hola.txt hola $ ln hola.txt hola2.txt $ ln -s hola.txt hola3.txt $ ls -l -rw-rw-r-- 2 jose jose 5 sep 27 15:23 hola2.txt lrwxrwxrwx 1 jose jose 8 sep 27 15:25 hola3.txt -\u0026gt; hola.txt -rw-rw-r-- 2 jose jose 5 sep 27 15:23 hola.txt $ echo \u0026quot;adios\u0026quot; \u0026gt; adios.txt $ mv adios.txt hola.txt $ cat hola.txt adios $ cat hola2.txt hola  Los enlaces blandos funcionan incluso entre distintos sistemas de archivos o particiones, los duros no.\nEjercicios  Crea un enlace simbólico a un fichero de texto dentro del direcotorio ~/Documentos Crea un enlace duro al mismo fichero. Edita el fichero de texto y observa como cambian ambos enlaces Crea un nuevo fichero de texto con otro contenido. Sustituye el primer fichero con el segundo y observa el resultado en ambos enlaces Crea dos enlaces, uno simbólico y otro duro, a un fichero. Elimina el fichero y observa el resultado en ambos enlaces  Acceso remoto Una de las grandes ventajas de utilizar la terminal es que podemos acceder a terminales en otros ordenadores muy fácilmente. El protocolo más utilizado para acceder a terminales de forma remota es ssh (Secure Shell). ssh tiene un gran número de posibilidades, pero el uso más habitual es utilizarlo para abrir terminales en ordenadores remotos que tienen un servicio ssh. ssh es seguro porque cifra las comunicaciones entre el cliente y el servidor. ssh se diseñó como una alternativa segura a telnet. No debemos usar el protocolo telnet porque las comunicaciones en telnet, incluidas las claves de acceso, no están cifradas y cualquiera puede tener acceso a ellas.\nPara acceder a una computadora que implemente el protocolo ssh podemos usar el programa ssh, pero previamente tenemos que tener una cuenta en esa computadora. Imaginemos que alicia tiene una cuenta en un ordenador que tiene un servicio ssh. Para conectarse puede hacer:\n$ ssh alicia@ordenador.upv.es  Si el nombre de la cuenta de usuario en el ordenador cliente y en el servidor es el mismo puede obviar el nombre de usuario.\n$ ssh ordenador.upv.es  A continuación el servidor le pedirá la clave correspondiente a ese usuario.\nExisten clientes ssh para windows con los que nos podemos conectar a servidores ssh. Uno muy común es putty.\nUna tarea muy habitual cuando estamos trabajando en un ordenador remoto es enviar o traer ficheros desde el mismo. Esto también lo podemos hacer utilizando el protocolo ssh por lo que podremos hacerlo de un modo seguro en cualquier ordenador que no de acceso ssh. El programa más sencillo para hacerlo desde Unix es scp (Secure CoPy). scp tiene una interfaz muy similar a cp pero acepta que los ficheros de origen y destino estén en distintos ordenadores:\n$ scp alicia@remotehost.edu:/remote/directory/seq.txt /some/local/directory $ scp /some/local/directory/seq.txt alicia@remotehost.edu:/remote/directory/  En windows también hay distintos clientes scp, uno de ellos es winscp.\nUna alternativa a scp que tiene más capacidades, como enviar fragmentos de ficheros, es rsync. rsync está diseñado para mantener varios archivos sincronizados entre dos ordenadores, pero también ser puede utilizar para copiar archivos de un ordenador a otro como scp. rsync puede establecer la conexión utilizando distintos protocolos, pero uno de ellos es ssh por lo que funcionará también con cualquier servidor ssh.\nSi lo que queremos es descargar un fichero desde un servidor en internet, por ejemplo desde una página web, al ordenador remoto en el que estamos trabajando en una sesión ssh podemos utilizar el comando wget o su alternativa curl.\n$ wget https://http://ncbi.nlm.nih.gov/una_secuencia.fasta  Ejercicios  Contectate a un servidor remoto usando ssh Transfiere un fichero desde tu ordenador al servidor Descarga el fichero https://www.gnu.org/licenses/gpl.txt directamente en el ordenador remoto Copia el fichero gpl.txt a tu ordenador  ","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"7e0893c7e3d0db333debce0ed853f3f5","permalink":"https://www.marcusrb.com/unix/02-sistema-ficheros/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/unix/02-sistema-ficheros/","section":"recursos","summary":"El sistema de archivos controla como se almacenan los archivos en el ordenador. Sus dos tareas principales son guardar y leer archivos previamente guardados.","tags":null,"title":"Sistema de ficheros","type":"docs"},{"authors":null,"categories":null,"content":" Vivimos en la era de grandes cantidades de datos, computadoras potentes e inteligencia artificial. Este es solo el comienzo. La ciencia de datos y el aprendizaje automático están impulsando el reconocimiento de imágenes, el desarrollo de vehículos autónomos, las decisiones en los sectores financiero y energético, los avances en medicina, el auge de las redes sociales y más. La regresión lineal es una parte importante de esto.\nLa regresión lineal es una de las técnicas estadísticas y de aprendizaje automático fundamentales. Ya sea que desee hacer estadísticas, aprendizaje automático o computación científica, hay buenas posibilidades de que lo necesite. Es recomendable aprenderlo primero y luego proceder hacia métodos más complejos.\nAl final de este artículo, habrás aprendido:\n¿Qué es la regresión lineal? Para qué regresión lineal se utiliza Cómo funciona la regresión lineal Cómo implementar la regresión lineal en Python, paso a paso\nRegresión El análisis de regresión es uno de los campos más importantes en estadística y aprendizaje automático. Hay muchos métodos de regresión disponibles. La regresión lineal es una de ellas.\n¿Qué es la regresión? La regresión busca relaciones entre variables.\nPor ejemplo, puede observar a varios empleados de alguna compañía e intentar comprender cómo sus salarios dependen de las características, como la experiencia, el nivel de educación, el rol, la ciudad en la que trabajan, etc.\nEste es un problema de regresión en el que los datos relacionados con cada empleado representan una observación. La presunción es que la experiencia, la educación, el rol y la ciudad son las características independientes, mientras que el salario depende de ellas.\nDel mismo modo, puede intentar establecer una dependencia matemática de los precios de las casas en sus áreas, número de dormitorios, distancias al centro de la ciudad, etc.\nEn general, en el análisis de regresión, generalmente considera algún fenómeno de interés y tiene una serie de observaciones. Cada observación tiene dos o más características. Siguiendo el supuesto de que (al menos) una de las características depende de las otras, intenta establecer una relación entre ellas.\nEn otras palabras, debe encontrar una función que asigne algunas características o variables a otras lo suficientemente bien.\nLas características dependientes se denominan variables dependientes, salidas o respuestas.\nLas características independientes se denominan variables independientes, entradas o predictores.\nLos problemas de regresión generalmente tienen una variable dependiente continua y sin límites. Sin embargo, las entradas pueden ser datos continuos, discretos o incluso categóricos, como género, nacionalidad, marca, etc.\nEs una práctica común denotar las salidas con 𝑦 y las entradas con 𝑥. Si hay dos o más variables independientes, se pueden representar como el vector 𝐱 = (𝑥₁, \u0026hellip;, 𝑥ᵣ), donde 𝑟 es el número de entradas.\n¿Cuándo necesitas regresión? Por lo general, se necesita una regresión para responder si un fenómeno influye en el otro y cómo se relacionan varias variables. Por ejemplo, puede usarlo para determinar si y en qué medida la experiencia o el género afectan los salarios.\nLa regresión también es útil cuando desea pronosticar una respuesta utilizando un nuevo conjunto de predictores. Por ejemplo, podría intentar predecir el consumo de electricidad de un hogar para la próxima hora dada la temperatura exterior, la hora del día y el número de residentes en ese hogar.\nLa regresión se usa en muchos campos diferentes: economía, ciencias de la computación, ciencias sociales, etc. Su importancia aumenta cada día con la disponibilidad de grandes cantidades de datos y una mayor conciencia del valor práctico de los datos.\nRegresión lineal La regresión lineal es probablemente una de las técnicas de regresión más importantes y ampliamente utilizadas. Es uno de los métodos de regresión más simples. Una de sus principales ventajas es la facilidad de interpretación de los resultados.\nFormulación del problema Al implementar la regresión lineal de alguna variable dependiente 𝑦 en el conjunto de variables independientes 𝐱 = (𝑥₁, \u0026hellip;, 𝑥ᵣ), donde 𝑟 es el número de predictores, se supone una relación lineal entre 𝑦 y 𝐱: 𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽ᵣ𝑥ᵣ + 𝜀. Esta ecuación es la ecuación de regresión. 𝛽₀, 𝛽₁, \u0026hellip;, 𝛽ᵣ son los coeficientes de regresión, y 𝜀 es el error aleatorio.\nLa regresión lineal calcula los estimadores de los coeficientes de regresión o simplemente los pesos predichos, denotados con 𝑏₀, 𝑏₁, \u0026hellip;, 𝑏ᵣ. Definen la función de regresión estimada 𝑓 (𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ. Esta función debe capturar las dependencias entre las entradas y salidas lo suficientemente bien.\nLa respuesta estimada o pronosticada, 𝑓 (𝐱ᵢ), para cada observación 𝑖 = 1, \u0026hellip;, 𝑛, debe estar lo más cerca posible de la respuesta real correspondiente 𝑦ᵢ. Las diferencias 𝑦ᵢ - 𝑓 (𝐱ᵢ) para todas las observaciones 𝑖 = 1, \u0026hellip;, 𝑛, se denominan residuales. La regresión se trata de determinar los mejores pesos pronosticados, es decir, los pesos correspondientes a los residuos más pequeños.\nPara obtener los mejores pesos, generalmente minimiza la suma de los residuos cuadrados (SSR) para todas las observaciones 𝑖 = 1, \u0026hellip;, 𝑛: SSR = Σᵢ (𝑦ᵢ - 𝑓 (𝐱ᵢ)) ². Este enfoque se llama el método de mínimos cuadrados ordinarios.\nRendimiento de regresión La variación de las respuestas reales 𝑦ᵢ, 𝑖 = 1, \u0026hellip;, 𝑛, se debe en parte a la dependencia de los predictores 𝐱ᵢ. Sin embargo, también hay una variación inherente adicional de la salida.\nEl coeficiente de determinación, denotado como 𝑅², le indica qué cantidad de variación en 𝑦 puede explicarse por la dependencia de 𝐱 utilizando el modelo de regresión particular. Mayor 𝑅² indica un mejor ajuste y significa que el modelo puede explicar mejor la variación de la salida con diferentes entradas.\nEl valor 𝑅² = 1 corresponde a SSR = 0, es decir, al ajuste perfecto ya que los valores de las respuestas pronosticadas y reales se ajustan completamente entre sí.\nRegresión lineal simple La regresión lineal simple o de una sola variable es el caso más simple de regresión lineal con una sola variable independiente, 𝐱 = 𝑥.\nLa siguiente figura ilustra la regresión lineal simple:\n","date":1568502000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"85d08fc1ba6084b68ab432a269b8f282","permalink":"https://www.marcusrb.com/recursos/tutoriales/python/pytut-linear/","publishdate":"2019-09-15T00:00:00+01:00","relpermalink":"/recursos/tutoriales/python/pytut-linear/","section":"recursos","summary":"Vivimos en la era de grandes cantidades de datos, computadoras potentes e inteligencia artificial. Este es solo el comienzo. La ciencia de datos y el aprendizaje automático están impulsando el reconocimiento de imágenes, el desarrollo de vehículos autónomos, las decisiones en los sectores financiero y energético, los avances en medicina, el auge de las redes sociales y más. La regresión lineal es una parte importante de esto.\nLa regresión lineal es una de las técnicas estadísticas y de aprendizaje automático fundamentales.","tags":null,"title":"Python Tutorial - Linear Regression in Python","type":"docs"},{"authors":null,"categories":null,"content":" La ciencia de datos es una disciplina emocionante que le permite convertir datos sin procesar en comprensión, conocimiento y conocimiento. El objetivo de \u0026ldquo;R for Data Science\u0026rdquo; es ayudarlo a aprender las herramientas más importantes en R que le permitirán hacer ciencia de datos. Después de leer este libro, tendrá las herramientas para abordar una amplia variedad de desafíos de la ciencia de datos, utilizando las mejores partes de R.\nLo que vas a aprender La ciencia de datos es un campo enorme, y no hay forma de que puedas dominarlo leyendo un solo libro. El objetivo de este libro es brindarle una base sólida en las herramientas más importantes. Nuestro modelo de las herramientas necesarias en un proyecto típico de ciencia de datos se parece a esto:\n`{r echo = FALSE, out.width =\u0026quot; 75% \u0026quot;} knitr :: include_graphics (\u0026quot;diagrams / data-science.png\u0026quot;)  `\nPrimero debe importar sus datos en R. Esto generalmente significa que toma los datos almacenados en un archivo, base de datos o API web, y los carga en un marco de datos en R. Si no puede ingresar sus datos en R, puede ¡No hagas ciencia de datos!\nUna vez que haya importado sus datos, es una buena idea ponerlos en orden. Poner en orden sus datos significa almacenarlos en una forma consistente que coincida con la semántica del conjunto de datos con la forma en que se almacenan. En resumen, cuando sus datos están ordenados, cada columna es una variable y cada fila es una observación. Los datos ordenados son importantes porque la estructura consistente le permite enfocar su lucha en preguntas sobre los datos, no en luchar para obtener los datos en la forma correcta para diferentes funciones.\nUna vez que tenga datos ordenados, un primer paso común es transformar. La transformación incluye reducir las observaciones de interés (como todas las personas en una ciudad, o todos los datos del año pasado), crear nuevas variables que son funciones de variables existentes (como calcular la velocidad a partir de la distancia y el tiempo) y calcular un conjunto de resumen estadísticas (como recuentos o medios). Juntos, ordenar y transformar se llaman wrangling, ¡porque obtener sus datos en una forma natural para trabajar a menudo se siente como una pelea!\nUna vez que tenga datos ordenados con las variables que necesita, hay dos motores principales de generación de conocimiento: visualización y modelado. Estos tienen fortalezas y debilidades complementarias, por lo que cualquier análisis real se repetirá entre ellos muchas veces.\nLa visualización es una actividad fundamentalmente humana. Una buena visualización le mostrará cosas que no esperaba o planteará nuevas preguntas sobre los datos. Una buena visualización también puede indicar que está haciendo una pregunta incorrecta o que necesita recopilar datos diferentes. Las visualizaciones pueden sorprenderte, pero no escales particularmente bien porque requieren que un humano las interprete.\nLos Modelos son herramientas complementarias para la visualización. Una vez que haya hecho sus preguntas lo suficientemente precisas, puede usar un modelo para responderlas. Los modelos son una herramienta fundamentalmente matemática o computacional, por lo que generalmente escalan bien. Incluso cuando no lo hacen, ¡generalmente es más barato comprar más computadoras que comprar más cerebros! Pero cada modelo hace suposiciones y, por su propia naturaleza, un modelo no puede cuestionar sus propias suposiciones. Eso significa que un modelo no puede sorprenderte fundamentalmente.\nEl último paso de la ciencia de datos es comunicación, una parte absolutamente crítica de cualquier proyecto de análisis de datos. No importa qué tan bien sus modelos y visualización lo hayan llevado a comprender los datos, a menos que también pueda comunicar sus resultados a otros.\nAlrededor de todas estas herramientas está programación. La programación es una herramienta transversal que utiliza en cada parte del proyecto. No es necesario ser un programador experto para ser un científico de datos, pero aprender más sobre programación vale la pena porque convertirse en un mejor programador le permite automatizar tareas comunes y resolver nuevos problemas con mayor facilidad.\nUtilizará estas herramientas en cada proyecto de ciencia de datos, pero para la mayoría de los proyectos no son suficientes. Hay una regla aproximada de 80-20 en juego; puede abordar aproximadamente el 80% de cada proyecto utilizando las herramientas que aprenderá en este libro, pero necesitará otras herramientas para abordar el 20% restante. A lo largo de este libro, le indicaremos los recursos donde puede obtener más información.\nCómo está organizado este libro La descripción anterior de las herramientas de la ciencia de datos está organizada de manera aproximada de acuerdo con el orden en que las usa en un análisis (aunque, por supuesto, las repetirá varias veces). En nuestra experiencia, sin embargo, esta no es la mejor manera de aprenderlos:\n Comenzar con la ingesta de datos y el orden es subóptimo porque el 80% del tiempo es rutinario y aburrido, y el otro 20% del tiempo es extraño y frustrante. ¡Ese es un mal lugar para comenzar a aprender un nuevo tema! En lugar, comenzaremos con la visualización y transformación de datos que ya han sido importado y ordenado. De esa manera, cuando ingiere y ordena sus propios datos, su la motivación se mantendrá alta porque sabes que el dolor lo vale.  Algunos temas se explican mejor con otras herramientas. Por ejemplo, creemos que Es más fácil entender cómo funcionan los modelos si ya conoce visualización, datos ordenados y programación.  Las herramientas de programación no son necesariamente interesantes por derecho propio, pero le permiten abordar problemas considerablemente más desafiantes. Bien darle una selección de herramientas de programación en el medio del libro, y entonces verás cómo se pueden combinar con las herramientas de ciencia de datos para abordar Problemas interesantes de modelado.  Dentro de cada capítulo, tratamos de seguir un patrón similar: comience con algunos ejemplos motivadores para que pueda ver la imagen más grande y luego profundice en los detalles. Cada sección del libro se combina con ejercicios para ayudarlo a practicar lo que ha aprendido. Si bien es tentador saltarse los ejercicios, no hay mejor manera de aprender que practicar en problemas reales.\nLo que no aprenderás Hay algunos temas importantes que este libro no cubre. Creemos que es importante mantenerse centrado sin piedad en lo esencial para que pueda comenzar a trabajar lo más rápido posible. Eso significa que este libro no puede cubrir todos los temas importantes.\nBig data Este libro se centra orgullosamente en pequeños conjuntos de datos en memoria. Este es el lugar correcto para comenzar porque no puede abordar big data a menos que tenga experiencia con datos pequeños. Las herramientas que aprende en este libro manejarán fácilmente cientos de megabytes de datos, y con un poco de cuidado, generalmente puede usarlas para trabajar con 1-2 Gb de datos. Si trabaja habitualmente con datos más grandes (10-100 Gb, por ejemplo), debe obtener más información sobre data.table. Este libro no enseña data.table porque tiene una interfaz muy concisa que hace que sea más difícil de aprender ya que ofrece menos claves lingüísticas. Pero si está trabajando con datos de gran tamaño, la rentabilidad del rendimiento vale la pena el esfuerzo adicional requerido para aprenderlo.\nSi sus datos son más grandes que esto, considere cuidadosamente si su problema de big data podría ser un pequeño problema de datos disfrazado. Si bien los datos completos pueden ser grandes, a menudo los datos necesarios para responder una pregunta específica son pequeños. Es posible que pueda encontrar un subconjunto, una submuestra o un resumen que se ajuste a la memoria y que aún le permita responder la pregunta que le interesa. El desafío aquí es encontrar los datos pequeños correctos, que a menudo requieren mucha iteración.\nOtra posibilidad es que su problema de big data sea en realidad una gran cantidad de problemas de data pequeña. Cada problema individual puede caber en la memoria, pero tiene millones de ellos. Por ejemplo, es posible que desee ajustar un modelo a cada persona en su conjunto de datos. Eso sería trivial si solo tuvieras 10 o 100 personas, pero en cambio tienes un millón. Afortunadamente, cada problema es independiente de los demás (una configuración que a veces se llama vergonzosamente paralela), por lo que solo necesita un sistema (como Hadoop o Spark) que le permita enviar diferentes conjuntos de datos a diferentes computadoras para su procesamiento. Una vez que haya descubierto cómo responder la pregunta para un solo subconjunto utilizando las herramientas descritas en este libro, aprenderá nuevas herramientas como sparklyr, rhipe y ddr para resolverlo para el conjunto de datos completo.\nPython, Julia y amigos En este libro, no aprenderá nada sobre Python, Julia o cualquier otro lenguaje de programación útil para la ciencia de datos. Esto no es porque pensemos que estas herramientas son malas. ¡Ellos no están! Y en la práctica, la mayoría de los equipos de ciencia de datos usan una combinación de lenguajes, a menudo al menos R y Python.\nSin embargo, creemos firmemente que es mejor dominar una herramienta a la vez. Mejorará más rápido si bucea profundamente, en lugar de extenderse poco a poco sobre muchos temas. Esto no significa que solo deba saber una cosa, solo que generalmente aprenderá más rápido si se apega a una cosa a la vez. Debes esforzarte por aprender cosas nuevas a lo largo de tu carrera, pero asegúrate de que tu comprensión sea sólida antes de pasar a la siguiente cosa interesante.\nCreemos que R es un gran lugar para comenzar su viaje de ciencia de datos porque es un entorno diseñado desde cero para apoyar la ciencia de datos. R no es solo un lenguaje de programación, sino que también es un entorno interactivo para hacer ciencia de datos. Para apoyar la interacción, R es un lenguaje mucho más flexible que muchos de sus pares. Esta flexibilidad viene con sus desventajas, pero la gran ventaja es lo fácil que es desarrollar gramáticas adaptadas para partes específicas del proceso de ciencia de datos. Estos mini idiomas lo ayudan a pensar en problemas como científico de datos, al tiempo que respaldan una interacción fluida entre su cerebro y la computadora.\nDatos no rectangulares Este libro se enfoca exclusivamente en datos rectangulares: colecciones de valores que están asociados con una variable y una observación. Hay muchos conjuntos de datos que no encajan naturalmente en este paradigma: incluyendo imágenes, sonidos, árboles y texto. Pero los marcos de datos rectangulares son extremadamente comunes en la ciencia y la industria, y creemos que son un gran lugar para comenzar su viaje de ciencia de datos.\nConfirmación de hipótesis Es posible dividir el análisis de datos en dos campos: generación de hipótesis y confirmación de hipótesis (a veces llamado análisis confirmatorio). El objetivo de este libro es descaradamente la generación de hipótesis o la exploración de datos. Aquí observará profundamente los datos y, en combinación con el conocimiento de su materia, generará muchas hipótesis interesantes para ayudar a explicar por qué los datos se comportan de la manera en que lo hacen. Evalúa las hipótesis de manera informal, utilizando su escepticismo para desafiar los datos de múltiples maneras.\nEl complemento de la generación de hipótesis es la confirmación de hipótesis. La confirmación de la hipótesis es difícil por dos razones:\n Necesita un modelo matemático preciso para generar datos falsificables predicciones Esto a menudo requiere una considerable sofisticación estadística.\n Solo puede usar una observación una vez para confirmar una hipótesis. Tan pronto como lo usa más de una vez que vuelve a hacer análisis exploratorios. Esto significa hacer una confirmación de hipótesis que necesita \u0026ldquo;preregistrarse\u0026rdquo; (escriba de antemano) su plan de análisis, y no se desvíe de él incluso cuando has visto los datos. Hablaremos un poco sobre algunos estrategias que puede usar para facilitar esto en modelado.\n  Es común pensar en el modelado como una herramienta para la confirmación de hipótesis y la visualización como una herramienta para la generación de hipótesis. Pero esa es una falsa dicotomía: los modelos a menudo se usan para exploración, y con un poco de cuidado puede usar la visualización para confirmar. La diferencia clave es con qué frecuencia mira cada observación: si mira solo una vez, es una confirmación; si miras más de una vez, es exploración.\nPrerrequisitos Hemos hecho algunas suposiciones sobre lo que ya sabes para aprovechar al máximo este libro. En general, debe tener conocimientos numéricos y es útil si ya tiene experiencia en programación. Si nunca ha programado antes, es posible que Garrett Hands on Programming with R de Garrett sea un complemento útil de este libro.\nHay cuatro cosas que necesita para ejecutar el código en este libro: R, RStudio, una colección de paquetes R llamada tidyverse, y un puñado de otros paquetes. Los paquetes son las unidades fundamentales del código R reproducible. Incluyen funciones reutilizables, la documentación que describe cómo usarlas y datos de muestra.\nR Para descargar R, vaya a CRAN, el ** c ** omprehensive ** R ** ** a ** rchive ** n ** etwork. CRAN se compone de un conjunto de servidores espejo distribuidos en todo el mundo y se utiliza para distribuir paquetes R y R. No intente elegir un espejo que esté cerca de usted: en su lugar, use el espejo en la nube, https://cloud.r-project.org, que automáticamente lo resuelve por usted.\nUna nueva versión principal de R sale una vez al año, y hay 2-3 lanzamientos menores cada año. Es una buena idea actualizar regularmente. La actualización puede ser una molestia, especialmente para las versiones principales, que requieren que reinstales todos tus paquetes, pero posponerlo solo lo empeora.\nRStudio RStudio es un entorno de desarrollo integrado, o IDE, para la programación R. Descargue e instálelo desde http://www.rstudio.com/download. RStudio se actualiza un par de veces al año. Cuando hay una nueva versión disponible, RStudio se lo informará. Es una buena idea actualizar regularmente para que pueda aprovechar las últimas y mejores funciones. Para este libro, asegúrese de tener RStudio 1.0.0.\nCuando inicie RStudio, verá dos regiones clave en la interfaz:\n`{r echo = FALSE, out.width =\u0026quot; 75% \u0026quot;} knitr :: include_graphics (\u0026quot;diagrams / rstudio-console.png\u0026quot;)  `\nPor ahora, todo lo que necesita saber es que escribe el código R en el panel de la consola y presiona Intro para ejecutarlo. ¡Aprenderás más a medida que avanzamos!\nEl tidyverse También necesitará instalar algunos paquetes de R. Un R paquete es una colección de funciones, datos y documentación que amplía las capacidades de la base R. El uso de paquetes es clave para el uso exitoso de R. La mayoría de los paquetes que aprenderá en este libro son parte de llamado tidyverse. Los paquetes en el tidyverse comparten una filosofía común de datos y programación R, y están diseñados para trabajar juntos de forma natural.\nPuede instalar el tidyverse completo con una sola línea de código:\n`{r, eval = FALSO} install.packages (\u0026quot;tidyverse\u0026quot;)  `\nEn su propia computadora, escriba esa línea de código en la consola y luego presione Intro para ejecutarla. R descargará los paquetes de CRAN y los instalará en su computadora. Si tiene problemas para instalar, asegúrese de estar conectado a Internet y de que https://cloud.r-project.org/ no esté bloqueado por su firewall o proxy.\nNo podrá utilizar las funciones, objetos y archivos de ayuda en un paquete hasta que lo cargue con library (). Una vez que haya instalado un paquete, puede cargarlo con la función library ():\n{r} biblioteca (tidyverse)  `\nEsto le indica que tidyverse está cargando los paquetes ggplot2, tibble, tidyr, readr, purrr y dplyr. Estos se consideran el core del tidyverse porque los usará en casi todos los análisis.\nLos paquetes en el tidyverse cambian con bastante frecuencia. Puede ver si hay actualizaciones disponibles y, opcionalmente, instalarlas ejecutando tidyverse_update ().\nOtros paquetes Hay muchos otros paquetes excelentes que no forman parte del tidyverse, porque resuelven problemas en un dominio diferente o están diseñados con un conjunto diferente de principios subyacentes. Esto no los hace mejores o peores, solo diferentes. En otras palabras, el complemento al tidyverse no es el messyverse, sino muchos otros universos de paquetes interrelacionados. A medida que aborde más proyectos de ciencia de datos con R, aprenderá nuevos paquetes y nuevas formas de pensar sobre los datos.\nEn este libro usaremos tres paquetes de datos externos al tidyverse:\n`{r, eval = FALSO} install.packages (c (\u0026quot;nycflights13\u0026quot;, \u0026quot;gapminder\u0026quot;, \u0026quot;Lahman\u0026quot;))  `\nEstos paquetes proporcionan datos sobre vuelos de aerolíneas, desarrollo mundial y béisbol que usaremos para ilustrar ideas clave de ciencia de datos.\nEjecutando código R La sección anterior le mostró un par de ejemplos de ejecución de código R. El código en el libro se ve así:\n`{r, eval = TRUE} 1 + 2 #\u0026gt; [1] 3  `\nSi ejecuta el mismo código en su consola local, se verá así:\n` \u0026gt; 1 + 2 [1] 3  `\nHay dos diferencias principales. En su consola, escribe después del \u0026gt;, llamado prompt; No mostramos el aviso en el libro. En el libro, la salida se comenta con #\u0026gt;; en tu consola aparece directamente después de tu código. Estas dos diferencias significan que si está trabajando con una versión electrónica del libro, puede copiar fácilmente el código del libro y en la consola.\nA lo largo del libro usamos un conjunto consistente de convenciones para referirnos al código:\n Las funciones están en una fuente de código y seguidas de paréntesis, como sum (), o mean ().\n Otros objetos R (como datos o argumentos de funciones) están en una fuente de código, sin paréntesis, como vuelos ox.  Si queremos dejar en claro de qué paquete proviene un objeto, usaremos el nombre del paquete seguido de dos puntos, como dplyr :: mutate (), o nycflights13 :: vuelos. Este también es un código R válido.\n  Obteniendo ayuda y aprendiendo más Este libro no es una isla; no existe un recurso único que le permita dominar R. Cuando comience a aplicar las técnicas descritas en este libro a sus propios datos, pronto encontrará preguntas que no contesto. Esta sección describe algunos consejos sobre cómo obtener ayuda y para ayudarlo a seguir aprendiendo.\nSi te quedas atascado, comienza con Google. Normalmente, agregar \u0026ldquo;R\u0026rdquo; a una consulta es suficiente para restringirla a resultados relevantes: si la búsqueda no es útil, a menudo significa que no hay resultados específicos de R disponibles. Google es particularmente útil para mensajes de error. Si recibe un mensaje de error y no tiene idea de lo que significa, intente buscarlo en Google. Lo más probable es que alguien más haya estado confundido en el pasado, y habrá ayuda en algún lugar de la web. (Si el mensaje de error no está en inglés, ejecute Sys.setenv (LANGUAGE =\u0026quot; en \u0026quot;) y vuelva a ejecutar el código; es más probable que encuentre ayuda para los mensajes de error en inglés).\nSi Google no ayuda, intente stackoverflow. Comience por pasar un poco de tiempo buscando una respuesta existente, incluyendo [R] para restringir su búsqueda a preguntas y respuestas que usen R. Si no encuentra nada útil, prepare un ejemplo reproducible mínimo o reprex. Un buen reprex hace que sea más fácil para otras personas ayudarte, y a menudo descubrirás el problema tú mismo mientras lo haces.\nHay tres cosas que debe incluir para que su ejemplo sea reproducible: paquetes, datos y código requeridos.\n ** Los paquetes ** deben cargarse en la parte superior del script, por lo que es fácil ver cuáles necesita el ejemplo. Este es un buen momento para comprobar que estás usando la última versión de cada paquete; es posible que hayas descubierto Un error que se ha solucionado desde que instaló el paquete. Para paquetes en tidyverse, la forma más fácil de verificar es ejecutar tidyverse_update ().\n La forma más fácil de incluir ** datos ** en una pregunta es usar dput () para generar el código R para recrearlo. Por ejemplo, para recrear los mtcars conjunto de datos en R, realizaría los siguientes pasos: 1. Ejecute dput (mtcars) en R 2. Copie la salida 3. En mi script reproducible, escriba mtcars \u0026lt;- y luego pegue. Intenta encontrar el subconjunto más pequeño de tus datos que aún revela el problema.\n Dedica un poco de tiempo a asegurarte de que tu ** código ** sea fácil para otros leer:\n   * Asegúrese de haber usado espacios y que sus nombres de variables sean concisos, aún informativo. * Use comentarios para indicar dónde radica su problema. * Haga todo lo posible para eliminar todo lo que no esté relacionado con el problema. Cuanto más corto sea su código, más fácil será comprenderlo y Más fácil es arreglarlo.\nTermine comprobando que realmente ha hecho un ejemplo reproducible comenzando una nueva sesión de R y copiando y pegando su script.\nTambién debe pasar algún tiempo preparándose para resolver los problemas antes de que ocurran. Invertir un poco de tiempo en aprender R cada día dará buenos resultados a largo plazo. Una forma es seguir lo que Hadley, Garrett y todos los demás en RStudio están haciendo en el blog RStudio. Aquí es donde publicamos anuncios sobre nuevos paquetes, nuevas funciones de IDE y cursos presenciales. También puede seguir a Hadley (\\ @hadleywickham) o Garrett (\\ @statgarrett) en Twitter, o seguir \\ @rstudiotips para mantenerse al día con las nuevas funciones en el IDE.\nPara mantenerse al día con la comunidad R en general, recomendamos leer http://www.r-bloggers.com: agrega más de 500 blogs sobre R de todo el mundo. Si eres un usuario activo de Twitter, sigue el hashtag # rstats. Twitter es una de las herramientas clave que Hadley utiliza para mantenerse al día con los nuevos desarrollos en la comunidad.\n","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"bbba931b484e2d59df549eee7cc9eebc","permalink":"https://www.marcusrb.com/cursos/r-studio/advanced-r/r201-tidy/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/cursos/r-studio/advanced-r/r201-tidy/","section":"cursos","summary":"La ciencia de datos es una disciplina emocionante que le permite convertir datos sin procesar en comprensión, conocimiento y conocimiento. El objetivo de \u0026ldquo;R for Data Science\u0026rdquo; es ayudarlo a aprender las herramientas más importantes en R que le permitirán hacer ciencia de datos. Después de leer este libro, tendrá las herramientas para abordar una amplia variedad de desafíos de la ciencia de datos, utilizando las mejores partes de R.","tags":null,"title":"Introducción","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.\nTus primeros comandos BigQuery Para usar BigQuery, importaremos el paquete de Python a continuación:\nfrom google.cloud import bigquery  El primer paso en el flujo de trabajo es crear un objeto Client. Como pronto verá, este objeto Client desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Trabajaremos con un conjunto de datos de publicaciones en Hacker News, un sitio web que se centra en noticias de informática y seguridad cibernética.\nEn BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos hacker_news está contenido en el proyecto bigquery-public-data. Para acceder al conjunto de datos,\n Comenzamos construyendo una referencia al conjunto de datos con el método dataset(). A continuación, utilizamos el método get_dataset(), junto con la referencia que acabamos de construir, para obtener el conjunto de datos.  # Construct a reference to the \u0026quot;hacker_news\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;hacker_news\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.\nUsamos el método list_tables() para listar las tablas en el conjunto de datos.\n# List all the tables in the \u0026quot;hacker_news\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there are four!) for table in tables: print(table.table_id)  OUTPUT\ncomments full full_201510 stories  De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla full en el conjunto de datos hacker_news.\n# Construct a reference to the \u0026quot;full\u0026quot; table table_ref = dataset_ref.table(\u0026quot;full\u0026quot;) # API request - fetch the table table = client.get_table(table_ref)  En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.\nEsquema de la tabla La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.\nEn este ejemplo, investigaremos la tabla completa full que obtuvimos anteriormente.\n# Print information on all the columns in the \u0026quot;full\u0026quot; table in the \u0026quot;hacker_news\u0026quot; dataset table.schema  OUTPUT\n[SchemaField('by', 'STRING', 'NULLABLE', \u0026quot;The username of the item's author.\u0026quot;, ()), SchemaField('score', 'INTEGER', 'NULLABLE', 'Story score', ()), SchemaField('time', 'INTEGER', 'NULLABLE', 'Unix time', ()), SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', 'Timestamp for the unix time', ()), SchemaField('title', 'STRING', 'NULLABLE', 'Story title', ()), SchemaField('type', 'STRING', 'NULLABLE', 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', ()), SchemaField('url', 'STRING', 'NULLABLE', 'Story url', ()), SchemaField('text', 'STRING', 'NULLABLE', 'Story or comment text', ()), SchemaField('parent', 'INTEGER', 'NULLABLE', 'Parent comment ID', ()), SchemaField('deleted', 'BOOLEAN', 'NULLABLE', 'Is deleted?', ()), SchemaField('dead', 'BOOLEAN', 'NULLABLE', 'Is dead?', ()), SchemaField('descendants', 'INTEGER', 'NULLABLE', 'Number of story or poll descendants', ()), SchemaField('id', 'INTEGER', 'NULLABLE', \u0026quot;The item's unique id.\u0026quot;, ()), SchemaField('ranking', 'INTEGER', 'NULLABLE', 'Comment ranking', ())]  Cada SchemaField nos informa sobre una columna específica (a la que también nos referimos como un campo field). En orden, la información es:\n El nombre de la columna. El tipo de campo (o tipo de datos) en la columna El modo de la columna (\u0026lsquo;NULLABLE\u0026rsquo; significa que una columna permite valores NULL y es el valor predeterminado) Una descripción de los datos en esa columna. El primer campo tiene el SchemaField:  SchemaField (\u0026lsquo;by\u0026rsquo;, \u0026lsquo;string\u0026rsquo;, \u0026lsquo;NULLABLE\u0026rsquo;, \u0026ldquo;El nombre de usuario del autor del elemento\u0026rdquo;, ()\nEsto nos dice:\n el campo (o columna) es llamado por los datos en este campo son cadenas, Se permiten valores NULL y Contiene los nombres de usuario correspondientes al autor de cada elemento.  Podemos usar el método list_rows() para verificar solo las primeras cinco líneas de la tabla completa full para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery RowIterator que se puede convertir rápidamente en un DataFrame de pandas con el método to_dataframe().\n# Preview the first five lines of the \u0026quot;full\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  El método list_rows() también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!\n# Preview the first five entries in the \u0026quot;by\u0026quot; column of the \u0026quot;full\u0026quot; table client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()  EXERCISE (Exercise_ Getting Started With SQL and BigQuery)\n","date":1568070000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"1f17eeb4bb6f4aa183b684f53c079d35","permalink":"https://www.marcusrb.com/cursos/business-analytics/intro-sql/sql101-0-start/","publishdate":"2019-09-10T00:00:00+01:00","relpermalink":"/cursos/business-analytics/intro-sql/sql101-0-start/","section":"cursos","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.","tags":null,"title":"Getting Started with SQL y Big Query","type":"docs"},{"authors":null,"categories":null,"content":" R es un programa de código abierto del que existen varias distribuciones, que se pueden descargar libremente. Veamos los procedimientos para instalarlo y actualizarlo.\nDownload Primero, por supuesto, debe descargar el paquete básico (70 Mb para la versión 3.3), elegir un \u0026ldquo;espejo\u0026rdquo; en http://cran.r-project.org/mirrors.html, o ir directamente a:\nespejo para sistemas Windows; espejos para sistemas Mac-Os; réplicas para sistemas Linux (Debian, Redhat, Suse, Ubuntu). Recientemente, Microsoft ha puesto a disposición una versión de R, de código abierto, gratuita y que tiene algunas características adicionales que facilitan la reproducibilidad de la búsqueda y el cálculo en paralelo (https://mran.microsoft.com/open).\nR portátil En Sourgeforce.net está disponible una versión portátil de R, que se puede instalar, con todas sus características, en un soporte de memoria externa (disco duro externo, unidad flash USB, etc.). Esta versión de R se puede integrar en la suite PortableApps.\nInstalación En Windows, el software se instala ejecutando el archivo ejecutable (exe) descargado.\nPara usar RCommander y RExcel, es preferible personalizar la instalación:\nEjecute el archivo ejecutable en modo administrador (haga clic en el archivo con el botón derecho del mouse y elija esta opción); Continúe con la instalación hasta la siguiente pantalla: ¿Desea configurar las opciones de arranque? Elige \u0026ldquo;Sí\u0026rdquo;: | Modo de visualización: elija SDI (varias ventanas) Estilo de ayuda: HTML (por defecto) Selección de procesos adicionales: almacena el número de versión en el registro: |\nActualización La forma más fácil de actualizar R y mantener bibliotecas es:\ninstale la nueva versión (se instalará en una nueva carpeta) copie los paquetes instalados desde la carpeta de la biblioteca anterior a la carpeta correspondiente de la nueva instalación iniciar R en modo administrador (en Windows Vista y Windows7) ejecutar - dentro de la nueva R - el comando paquetes de actualización (checkBuilt = TRUE, ask = FALSE) desinstale la versión anterior y elimine el directorio anterior. También puede actualizar R y paquetes con el paquete de instalación.\nR en la web Finalmente, existe la posibilidad de ejecutar R en línea (con diferentes interfaces), a través de sitios web que proporcionan una instalación de servidor R, como: https://www.tutorialspoint.com/execute_r_online.php\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"444bb1ed6a38865d504dc832b12139eb","permalink":"https://www.marcusrb.com/cursos/r-studio/instalacion-r/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/r-studio/instalacion-r/","section":"cursos","summary":"R es un programa de código abierto del que existen varias distribuciones, que se pueden descargar libremente. Veamos los procedimientos para instalarlo y actualizarlo.\nDownload Primero, por supuesto, debe descargar el paquete básico (70 Mb para la versión 3.3), elegir un \u0026ldquo;espejo\u0026rdquo; en http://cran.r-project.org/mirrors.html, o ir directamente a:\nespejo para sistemas Windows; espejos para sistemas Mac-Os; réplicas para sistemas Linux (Debian, Redhat, Suse, Ubuntu). Recientemente, Microsoft ha puesto a disposición una versión de R, de código abierto, gratuita y que tiene algunas características adicionales que facilitan la reproducibilidad de la búsqueda y el cálculo en paralelo (https://mran.","tags":null,"title":"Instalación de R","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"6b3100baa6d26c25b62d591eeb528782","permalink":"https://www.marcusrb.com/cursos/data-science/math-data-science/math-intro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/data-science/math-data-science/math-intro/","section":"cursos","summary":"","tags":null,"title":"Introducción Math for Data Science","type":"docs"},{"authors":null,"categories":null,"content":" Introducción Este curso cubre las habilidades clave de Python que necesitará para que pueda comenzar a usar Python para la ciencia de datos. El curso es ideal para alguien con experiencia previa en codificación que quiera agregar Python a su repertorio o subir de nivel sus habilidades básicas de Python. (Si es un programador por primera vez, puede consultar estos recursos de aprendizaje \u0026ldquo;Python para no programadores\u0026rdquo;).\nComenzaremos con una breve descripción general de la sintaxis de Python, la asignación de variables y los operadores aritméticos. Si tiene experiencia previa en Python, puede pasar directamente al ejercicio práctico.\n## Hello, Python! Python fue nombrado por la compañía de comedia británica Monty Python, por lo que haremos de nuestro primer programa Python un homenaje a su parodia sobre el spam.\nSolo por diversión, intente leer el código a continuación y predecir lo que hará cuando se ejecute. (Si no tienes idea, ¡está bien!)\nLuego haga clic en el botón \u0026ldquo;salida\u0026rdquo; para ver los resultados de nuestro programa.\nspam_amount = 0 print(spam_amount) # Ordering Spam, egg, Spam, Spam, bacon and Spam (4 more servings of Spam) spam_amount = spam_amount + 4 if spam_amount \u0026gt; 0: print(\u0026quot;But I don't want ANY spam!\u0026quot;) viking_song = \u0026quot;Spam \u0026quot; * spam_amount print(viking_song)  0 But I don't want ANY spam! Spam Spam Spam Spam  ¡Hay mucho que desempacar aquí! Este programa tonto demuestra muchos aspectos importantes de cómo se ve el código Python y cómo funciona. Revisemos el código de arriba a abajo.\nspam_amount = 0  Asignación de variables: aquí creamos una variable llamada spam_amount y le asignamos el valor de 0 usando =, que se llama operador de asignación.\nNota: si ha programado en ciertos otros lenguajes (como Java o C ++), puede estar notando algunas cosas que Python no requiere que hagamos aquí:\n no necesitamos \u0026ldquo;declarar\u0026rdquo; spam_amount antes de asignarle no necesitamos decirle a Python a qué tipo de valor se referirá spam_amount. De hecho, incluso podemos reasignar spam_amount para referirnos a un tipo diferente de cosas como una cadena o un booleano.  print(spam_amount)  0  Llamadas de función: print es una función de Python que muestra el valor que se le pasa en la pantalla. Llamamos a las funciones poniendo paréntesis después de su nombre y poniendo las entradas (o argumentos) a la función en esos paréntesis.\n# Ordering Spam, egg, Spam, Spam, bacon and Spam (4 more servings of Spam) spam_amount = spam_amount + 4  La primera línea de arriba es un comentario. En Python, los comentarios comienzan con el símbolo #.\nA continuación vemos un ejemplo de reasignación. La reasignación del valor de una variable existente tiene el mismo aspecto que la creación de una variable: todavía utiliza el operador de asignación =.\nEn este caso, el valor que estamos asignando a _spamamount implica una aritmética simple en su valor anterior. Cuando encuentra esta línea, Python evalúa la expresión en el lado derecho de = (0 + 4 = 4), y luego asigna ese valor a la variable en el lado izquierdo.\nif spam_amount \u0026gt; 0: print(\u0026quot;But I don't want ANY spam!\u0026quot;) viking_song = \u0026quot;Spam Spam Spam\u0026quot; print(viking_song)  But I don't want ANY spam! Spam Spam Spam  No hablaremos mucho sobre \u0026ldquo;condicionales\u0026rdquo; hasta más tarde, pero, incluso si nunca ha codificado antes, probablemente pueda adivinar lo que hace. Python es apreciado por su legibilidad y simplicidad.\nObserve cómo indicamos qué código pertenece al if. \u0026ldquo;¡Pero no quiero NINGÚN spam!\u0026rdquo; solo se supone que se imprime si _spamamount es positivo. Pero el código posterior (como _print (vikingsong)) debe ejecutarse sin importar qué. ¿Cómo lo sabemos (y Python)?\nLos dos puntos (:) al final de la línea if indican que se está iniciando un nuevo \u0026ldquo;bloque de código\u0026rdquo;. Las líneas posteriores que están sangradas son parte de ese bloque de código. Algunos otros idiomas usan {llaves \u0026ldquo;para marcar el comienzo y el final de los bloques de código. El uso de espacios en blanco significativos por Python puede ser sorprendente para los programadores que están acostumbrados a otros lenguajes, pero en la práctica puede conducir a un código más coherente y legible que los lenguajes que no imponen sangría de bloques de código.\nLas líneas posteriores que tratan con _vikingsong no están sangradas con 4 espacios adicionales, por lo que no forman parte del bloque de código if. Veremos más ejemplos de bloques de código sangrados más adelante cuando definamos funciones y usemos bucles.\nEste fragmento de código también es nuestro primer avistamiento de una cadena string en Python:\n\u0026quot;But I don't want ANY spam!\u0026quot;  \u0026quot;But I don't want ANY spam!\u0026quot;  Las cadenas se pueden marcar con comillas dobles o simples. (Pero debido a que esta cadena en particular contiene un carácter de comillas simples, podríamos confundir a Python tratando de rodearla con comillas simples, a menos que tengamos cuidado).\nviking_song = \u0026quot;Spam \u0026quot; * spam_amount print(viking_song)  Spam Spam Spam Spam  El operador * se puede usar para multiplicar dos números (3 * 3 se evalúa como 9), pero de manera bastante divertida, también podemos multiplicar una cadena por un número, para obtener una versión que se ha repetido tantas veces. Python ofrece una serie de trucos descarados y pequeños que ahorran tiempo como este, donde los operadores como * y + tienen un significado diferente según el tipo de cosas a las que se aplican. (El término técnico para esto es operador de sobrecarga)\nNúmeros y aritmética en Python Ya hemos visto un ejemplo de una variable que contiene un número arriba:\nspam_amount = 0  \u0026ldquo;Número\u0026rdquo; es un buen nombre informal para el tipo de cosas, pero si quisiéramos ser más técnicos, podríamos preguntarle a Python cómo describiría el tipo de cosas que es spam_amount:\ntype(spam_amount)  int  Es un int - abreviatura de entero. Hay otro tipo de número que comúnmente encontramos en Python:\nfrom matplotlib import pyplot as plt %matplotlib inline import seaborn as sns df = sns.load_dataset('iris') sns.lmplot(x = 'petal_length', y = 'petal_width', data = df , hue = 'species' , fit_reg = False)  \u0026lt;seaborn.axisgrid.FacetGrid at 0x1a16f2d908\u0026gt;  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"9dba43a8b0dacd1d330c03d11a734cb4","permalink":"https://www.marcusrb.com/cursos/python/py101/py101-1-intro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/python/py101/py101-1-intro/","section":"cursos","summary":"Introducción Este curso cubre las habilidades clave de Python que necesitará para que pueda comenzar a usar Python para la ciencia de datos. El curso es ideal para alguien con experiencia previa en codificación que quiera agregar Python a su repertorio o subir de nivel sus habilidades básicas de Python. (Si es un programador por primera vez, puede consultar estos recursos de aprendizaje \u0026ldquo;Python para no programadores\u0026rdquo;).\nComenzaremos con una breve descripción general de la sintaxis de Python, la asignación de variables y los operadores aritméticos.","tags":null,"title":"Python 101 - Introducción","type":"docs"},{"authors":null,"categories":null,"content":" Instrucciones\n Da clic a \u0026lsquo;Submit Answer\u0026rsquo; y date cuenta como la consola ejecuta el código de R del editor: la solución 7 aparece como la suma de 3 y 4. El uso más simple de R es como una calculadora y graficadora, pero por supuesto hay mucho, mucho, mucho más! Añade una linea que haga el calculo de 6 y 12.  Script R\n# Este es el editor y la parte de abajo es la consola # El símbolo de numeral # es utilizada para hacer comentarios # Calcula 3 + 4 3 + 4 # Calcula 6 + 12 6 + 12  Aritmética con R Como vimos el uso más simple que se le puede dar a R es el de una calculadora. Consideremos las siguientes operaciones:\nAdición: + Resta: - Multiplicación: * División: / Exponenciación: ^ Modulo: %% Los últimos dos necesitan una breve explicación:\nEl operador ^ eleva el número a la izquierda a la potencia a la derecha 3^2 es 9 El módulo (mod) %% calcula el residuo de la división del número a la izquierda por el número a la derecha, por ejemplo 5 mod 3 o 5%%3 es 2.\nInstrucciones\n Escribe 2^5 en el editor para calcular 2 a la quinta potencia. Escribe 28 %% 6 para calcular el 28 módulo 6. Da clic a \u0026lsquo;Submit Answer\u0026rsquo; para ver el resultado en la consola.  Script R\n# Adición 5 + 5 # Resta 5 - 5 # Multiplicación 3 * 5 # División (5 + 5) / 2 # Exponenciación 2^5 # Modulo 28 %% 6  Variables y asignaciones Un concepto básico en programación es el de variable.\nUna variable nos permite guardar valores (por ejemplo el número 4) o algún objeto (veremos mas adelante de que se trata) en R. Luego puedes acceder al valor guardado en la variable por medio del nombre de la misma.\nPodemos asignar el valor de 4 a la variable mi_variable con el siguiente comando: mi_variable \u0026lt;- 4\nInstrucciones\n Completa el código en el editor de tal manera que el valor de 42 quede asignado a la variable x. Da clic en \u0026lsquo;Submit Answer\u0026rsquo;. Nota al escribir x en R, se imprime el valor de 42 en la consola.  Script R\n# Asigna 42 a x x \u0026lt;- 42 # Imprime el valor de la variable x x  Variables y asignaciones (2) Supongamos que tienes una canasta con cinco manzanas. Para recordarlo, quizás quieras asignar el número de manzanas en una variable llamada numero_manzanas\nInstrucciones\n Escribe el siguiente código: numero_manzanas \u0026lt;- 5 para asignar el numero 5 a la variable numero_manzanas. Escribe: numero_manzanas abajo del segundo comentario. Da clic a \u0026lsquo;Submit Answer\u0026rsquo;, ve la consola: el numero que sea ha impreso es 5.  Script R\n# Asigna el valor de 5 a la variable numero_manzanas numero_manzanas \u0026lt;- 5 # Imprime el valor de la variable numero_manzanas numero_manzanas  Variables y asignaciones (3) Supongamos que ahora tienes 6 naranjas. De nuevo, para no olvidarlo se te ocurre crear una variable llamada numero_naranjas y asignar el valor de 6 a esa variable. Ahora podemos a empezar a utilizar las variables creadas para hacer algo con ellas. Usemos R para saber el número total de frutas con las que contamos, pidamos que haga la cuenta por nosotros: numero_manzanas + numero_naranjas. Al leer este codigo nos damos cuenta de la importancia de dar nombres útiles a nuestras variables.\nInstrucciones\n Asigna a numero_naranjas el valor de 6. R permite combinar estas variables numero_manzanas y numero_naranjas en una nueva variable numero_frutas. Crea la variable numero_frutas y asigna el valor del total de frutas que tenemos.  Script R\n# Asigna el valor de 5 y 6 a las variables numero_manzanas y numero_naranjas respectivamente numero_manzanas \u0026lt;- 5 numero_naranjas \u0026lt;- 6 # Suma estas dos variables e imprime el resultado. numero_manzanas + numero_manzanas #Crea la variable numero_frutas y asigna el resultado de la suma anterior. numero_frutas \u0026lt;- (numero_manzanas + numero_naranjas) numero_frutas  Manzanas y naranjas En la escuela primaria nos decían que no sumáramos manzanas con naranjas, pero es lo que acabamos de hacer :) \\n Sin embargo numero_manzanas y numero_naranjas son dos variables que contienen el mismo tipo de dato: un dato de tipo numérico. El operador + en R funciona con variables de este tipo. Si alguna de nuestras variables no es numérica sino por ejemplo caracter (ver el editor), entonces estaríamos tratando de asignar la suma de un caracter y un número a la variable numero_frutas, lo cual no es posible.\nInstrucciones\n Da clic a \u0026lsquo;Submit Answer\u0026rsquo; lee el mensaje de error, asegúrate de entender que dice (puedes copiar y pegar el texto en ingles en un traductor) Ajusta el código para que R deje de mostrar ese error.  Tipos de datos básicos en R R trabaja con muchos tipos de datos. Para empezar, algunos de los más básicos son:\n Decimales como 4.5 son llamados numeric (numéricos). Números enteros como 4 son llamados (sorpresa!) integer (enteros). Valores Booleanos (TRUE (Verdadero) o FALSE (Falso)) logical (lógicos). Texto (cadenas de caracteres) son characters (caracteres). Nota como utilizamos las comillas para denotar el texto en el editor.  Instrucciones\nCambia los valores de:\n mi_numerica a 42. mi_caracter a \u0026ldquo;cuarenta_y_dos\u0026rdquo;. Nota como utilizamos las comillas. mi_logica a FALSE (Falso). Ten en cuenta que R distingue entre mayúsculas y minúsculas!  Script R\n# Un valor numérico mi_numerica \u0026lt;- 42 # Asignando una cadena de caracteres (o simplemente caracteres) nota el uso de las comillas mi_caracter \u0026lt;- \u0026quot;cuarenta_y_dos\u0026quot; # Asignando un valor lógico verdadero mi_logica \u0026lt;- FALSE  ## ¿Cómo sé el tipo de dato? ¿Recuerdas que cuando añadiste 5 + \u0026ldquo;seis\u0026rdquo; obtuviste un error debido a que los tipos de datos no coincidian? Para evitar estas penosas situaciones :\\ puedes saber de antemano el tipo de dato que tienen tus variables utilizando el código class(nombre_variable)\nInstrucciones\n Completa el código en el editor para imprimir a la consola el tipo de dato de las variables mi_numerica, mi_caracter y mi_logica.  Script R\n# Declarando las variables de diferentes tipos mi_numerica \u0026lt;- 40 mi_caracter \u0026lt;- \u0026quot;cuarenta\u0026quot; mi_logica \u0026lt;- FALSE # Escribe el código para averiguar el tipo de dato de cada variable class(mi_numerica) class(mi_caracter) class(mi_logica)  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"10beda90f89353367e6f861b6626cdd8","permalink":"https://www.marcusrb.com/cursos/r-studio/intro-r/r101-intro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/r-studio/intro-r/r101-intro/","section":"cursos","summary":"Instrucciones\n Da clic a \u0026lsquo;Submit Answer\u0026rsquo; y date cuenta como la consola ejecuta el código de R del editor: la solución 7 aparece como la suma de 3 y 4. El uso más simple de R es como una calculadora y graficadora, pero por supuesto hay mucho, mucho, mucho más! Añade una linea que haga el calculo de 6 y 12.  Script R\n# Este es el editor y la parte de abajo es la consola # El símbolo de numeral # es utilizada para hacer comentarios # Calcula 3 + 4 3 + 4 # Calcula 6 + 12 6 + 12  Aritmética con R Como vimos el uso más simple que se le puede dar a R es el de una calculadora.","tags":null,"title":"Prácticas 1 - Introducción","type":"docs"},{"authors":null,"categories":null,"content":" El objetivo de este documento es ayudar a aquellas personas con un conocimiento básico sobre aprendizaje automático a aprovechar las recomendaciones de Google para aprendizaje automático. Presenta un estilo para aprendizaje automático, similar a la guía de estilo de Google para C++ y otras guías populares para la programación práctica. Si tomaste clases de aprendizaje automático, desarrollaste un modelo de aprendizaje automático o trabajaste en uno, tienes el conocimiento necesario para leer este documento.\nTerminología Los siguientes términos se mencionarán con frecuencia en nuestro debate sobre aprendizaje automático eficaz:\n Instancia: El aspecto sobre el que deseas hacer una predicción. Por ejemplo, la instancia puede ser una página web que deseas clasificar como \u0026ldquo;sobre gatos\u0026rdquo; o \u0026ldquo;no sobre gatos\u0026rdquo;. Etiqueta: Una respuesta a la tarea de predicción, que se puede generar mediante un sistema de aprendizaje automático o a partir de los datos de entrenamiento. Por ejemplo, la etiqueta sobre una página web puede ser \u0026ldquo;sobre gatos\u0026rdquo;. Atributo: Una propiedad de una instancia utilizada en una tarea de predicción. Por ejemplo, una página web puede tener un atributo \u0026ldquo;contiene la palabra gato\u0026rdquo;. Columna de atributos: Un conjunto de atributos relacionados, como el conjunto de todos los países posibles donde es posible que vivan los usuarios. Un ejemplo puede tener uno o más atributos presentes en una columna de atributos. \u0026ldquo;Columna de atributos\u0026rdquo; es terminología específica de Google. Una columna de atributos se conoce como un \u0026ldquo;espacio de nombres\u0026rdquo; en el sistema de VW (en Yahoo/Microsoft) o como un campo. Ejemplo: Una instancia (con sus atributos) y una etiqueta. Modelo: Una representación estadística de una tarea de predicción. Entrenas un modelo basado en ejemplos y, luego, lo usas para hacer predicciones. Métrica: El número que importa. Se puede o no optimizar directamente. Objetivo: Una métrica que tu algoritmo intenta optimizar. Canalización: La infraestructura que rodea al algoritmo de aprendizaje automático. Incluye la recopilación de datos del frontend, su incorporación a los archivos de datos de entrenamiento, el entrenamiento de uno o más modelos y la exportación de los modelos para la producción. Tasa de clics: El porcentaje de visitantes a una página web que hacen clic en un vínculo en un anuncio.  Resumen Para desarrollar buenos productos:\nEn aprendizaje automático, implemente todos los conocimientos de ingeniería que tiene, no los conocimientos de aprendizaje automático que no tiene.\nDe hecho, la mayoría de los problemas que debes resolver son de ingeniería. Incluso con todos los recursos de un experto en aprendizaje automático, la mayoría de los beneficios provienen de los atributos geniales, no de los algoritmos de aprendizaje automático geniales. Por lo tanto, la estrategia básica es la siguiente:\nAsegúrate de que tu canalización sea completamente estable. Comienza con un objetivo razonable. Agrega atributos de sentido común de una forma sencilla. Asegúrate de que la canalización siga siendo estable. Esta estrategia dará resultados positivos por un tiempo prolongado. Modifica esta estrategia solo cuando no queda otra opción para probar. Si sumas complejidad, se retrasan las siguientes versiones.\nUna vez que hayas agotado los trucos simples, puedes pasar a aprendizaje automático de vanguardia. Consulta la sección en los proyectos de aprendizaje automático de la Fase III.\nEste documento se organiza de la siguiente manera:\nLa primera parte te permite determinar si es el momento indicado para desarrollar un sistema de aprendizaje automático. La segunda parte se trata de implementar la primera canalización. La tercera parte cubre el lanzamiento y la iteración mientras se agregan atributos nuevos a la canalización, la evaluación de los modelos y la desviación entre el entrenamiento y la publicación. La parte final trata sobre qué hacer cuando alcanzas una meseta. Luego, hay una lista del trabajo relacionado y un apéndice con información sobre los sistemas que se usan comúnmente como ejemplos en este documento.\nAntes de aprendizaje automático Regla n.º 1: No tengas miedo de lanzar un producto sin aprendizaje automático.\nEl aprendizaje automático es genial, pero necesita datos. En teoría, puedes obtener datos de un problema diferente y, luego, modificar un poco el modelo para un producto nuevo, pero es probable que la heurística básica no funcione como es debido. Si piensas que aprendizaje automático te brindará un aumento del 100%, entonces una heurística te permitirá alcanzar el 50% de ese camino.\nPor ejemplo, si clasificas apps en un mercado de apps, puedes usar la tasa de instalación o la cantidad de instalaciones como la heurística. Si detectas spam, filtra los editores que enviaron spam antes. No tengas miedo de usar la edición humana. Si necesitas clasificar contactos, clasifica el que se usó más recientemente como el primero (o puedes clasificarlos por orden alfabético). Si el aprendizaje automático no es indispensable para tu producto, no lo uses hasta que tengas datos.\nRegla n.º 2: Primero, diseña métricas e impleméntalas.\nAntes de establecer las tareas de tu sistema de aprendizaje automático, realiza un seguimiento del sistema actual. Debes hacerlo por las siguientes razones:\n Es más fácil obtener permiso de los usuarios del sistema al comienzo. Si consideras que algo puede llegar a ser un problema en el futuro, es mejor obtener datos históricos ahora. Si diseñas el sistema con la instrumentación de métricas en mente, las cosas irán mejorando en el futuro. En especial, si no deseas realizar búsquedas globales de strings en registros para instrumentar las métricas. Notarás qué aspectos cambian y cuáles permanecen iguales. Por ejemplo, supongamos que deseas optimizar directamente los usuarios activos por un día. Sin embargo, durante las primeras manipulaciones del sistema, es posible que notes que los cambios drásticos en la experiencia del usuario no afectan de forma notoria esta métrica.  El equipo de Google Plus mide las expansiones por lectura, las veces que se compartió el contenido por lectura, las veces que alguien indico +1 por lectura, los comentarios por lectura, los comentarios por usuario, las veces que se compartió el contenido por usuario, etc. para calcular el desempeño de una publicación durante el tiempo de publicación. También ten en cuenta que es importante el marco de trabajo de un experimento, en el que puedes agrupar usuarios y sumar estadísticas por experimento. Consulta la regla n.º 12.\nSi recopilas métricas con mayor flexibilidad, puedes lograr una imagen más amplia del sistema. ¿Notas un problema? ¡Agrega una métrica para realizar un seguimiento! ¿Te entusiasma algún cambio cuantitativo en la actualización más reciente? ¡Agrega una métrica para realizar un seguimiento!\nRegla n.º 3: Elige aprendizaje automático antes que una heurística compleja. Una heurística sencilla puede ser la clave para lanzar el producto, pero una heurística compleja no es sostenible. Una vez que tengas los datos y una idea básica de lo que intentas lograr, pasa al aprendizaje automático. Como en la mayoría de las tareas de ingeniería de software, buscas actualizar la estrategia de forma constante, ya sea con un modelo heurístico o de aprendizaje automático. Notarás que el modelo de aprendizaje automático es más fácil de actualizar y mantener (consulta la regla n.º 16).\nFase I de aprendizaje automático: Tu primera canalización Concéntrate en la infraestructura de tu sistema para la primera canalización. Si bien es divertido pensar en todo aprendizaje automático teórico que implementarás, es difícil determinar qué sucede si no usas una canalización confiable.\nRegla n.º 4: Procura que el primer modelo se mantenga simple y acierta con la infraestructura. El primer modelo proporciona el mayor impulso para tu producto, por lo que no necesita ser muy sofisticado. Sin embargo, te encontrarás con más problemas de infraestructura de lo que esperas. Antes de que alguien pueda usar tu novedoso sistema de aprendizaje automático, debes determinar lo siguiente:\n Cómo obtener ejemplos para tu algoritmo de aprendizaje. Un primer borrador para definir qué es \u0026ldquo;bueno\u0026rdquo; y qué es \u0026ldquo;malo\u0026rdquo; para tu sistema. Cómo integrar tu modelo a la aplicación. Puedes aplicar el modelo en vivo o calcularlo previamente con ejemplos sin conexión y guardar los resultados en una tabla. Por ejemplo, tal vez deseas preclasificar páginas web y almacenar los resultados en una tabla, pero deseas clasificar los mensajes de chat en vivo.  Si eliges atributos simples, es más sencillo garantizar lo siguiente:\n Que los atributos se comuniquen con tu algoritmo de aprendizaje correctamente. Que el modelo aprenda ponderaciones razonables. Que los atributos se comuniquen con tu modelo en el servidor correctamente. Si logras que el sistema cumpla con estas tres condiciones, habrás completado la mayor parte del trabajo. El modelo simple brinda métricas y un comportamiento de punto de referencia que puedes usar para probar modelos más complejos. Algunos equipos apuntan a un primer lanzamiento \u0026ldquo;neutral\u0026rdquo;: un primer lanzamiento que desprioriza de forma explícita las ganancias de aprendizaje, para no distraerse.  Regla n.º 5: Prueba la infraestructura separada de aprendizaje automático.\nAsegúrate de poder probar la infraestructura y de que las partes de aprendizaje del sistema están aisladas, para que puedas probar las partes a su alrededor. De forma específica:\n Prueba obtener datos para el algoritmo. Comprueba que se completen las columnas de atributos que deben completarse. Cuando la privacidad lo permite, inspecciona de forma manual la entrada al algoritmo de entrenamiento. Si es posible, comprueba las estadísticas en tu canalización para compararlas con las estadísticas de los mismos datos procesados en otro lugar.\n Prueba obtener modelos a partir del algoritmo de entrenamiento. Asegúrate de que el modelo de tu entorno de entrenamiento muestre el mismo resultado que el modelo del entorno de publicación (consulta la regla n.º 37).\n  El aprendizaje automático es un tanto impredecible; por lo tanto, asegúrate de probar el código para crear ejemplos en el entrenamiento y la publicación. Asegúrate también de poder cargar y usar un modelo fijo durante la publicación. Además, es importante comprender los datos: consulta Consejos prácticos para el análisis de conjuntos de datos grandes y complejos.\nRegla n.º 6: Ten cuidado con la pérdida de datos al copiar canalizaciones.\nA menudo, para crear una canalización, copiamos una existente (es decir, la programación a ciegas), y la canalización anterior pierde datos que necesitamos en la canalización nueva. Por ejemplo, la canalización para la vista Lo más interesante de Google+ pierde las publicaciones anteriores (porque intenta calificar las publicaciones nuevas). Esta canalización se copió para usar las Novedades de Google+, donde las publicaciones anteriores todavía tienen valor. Sin embargo, la canalización sigue perdiendo publicaciones antiguas. Otro patrón común es solo registrar datos que vio el usuario. Estos datos no tienen valor si queremos desarrollar un modelo sobre por qué el usuario no vio una publicación específica, ya que se perdieron todos los ejemplos negativos. Ocurrió un problema similar en Play. Durante el trabajo en la página de inicio de Play Apps, se creó una canalización nueva que también contenía ejemplos de la página de destino de Play Juegos sin ningún atributo para que quedara claro de dónde provenía cada ejemplo.\nRegla n.º 7: Convierte la heurística en atributos o gestiónalos de forma externa.\nPor lo general, los problemas que el aprendizaje automático intenta resolver no son completamente nuevos. Ya existe un sistema para la clasificación, el ordenamiento o el problema que intentes resolver. Esto significa que existe una gran cantidad de reglas y heurísticas. Estas mismas heurísticas pueden ser de gran ayuda cuando se modifican con aprendizaje automático. Debes recolectar toda la información que tengan tus heurísticas por dos razones. Primero, facilitará la transición a un sistema de aprendizaje automático. Segundo, por lo general, estas reglas contienen mucha de la intuición del sistema de la que no conviene deshacerse. Existen cuatro formas de usar una heurística existente:\n Realiza un preprocesamiento con la heurística. Si el atributo es increíblemente asombroso, entonces esto es una opción. Por ejemplo, si, en un filtro de spam, el destinatario ya está en la lista negra, no intentes volver a aprender qué significa \u0026ldquo;estar en la lista negra\u0026rdquo;. Bloquea el mensaje. Esta estrategia tiene más sentido en tareas de clasificación binaria. Crea un atributo. Crear directamente un atributo a partir de la heurística es genial. Por ejemplo, si usas una heurística para calcular la calificación de relevancia para el resultado de una consulta, puedes incluir la calificación como el valor de un atributo. Más tarde, es posible que desees usar técnicas de aprendizaje automático para adaptar el valor (por ejemplo, al convertir el valor en uno de un conjunto finito de valores discretos o al combinarlo con otros atributos), pero comienza usando los valores sin procesar que produce la heurística. Recolecta las entradas sin procesar de la heurística. Si existe una heurística para apps que combine la cantidad de instalaciones, la cantidad de caracteres en el texto y el día de la semana, considera separar estos datos y enviar estas entradas al aprendizaje de forma separada. En este caso, puedes usar algunas técnicas que usan los conjuntos (consulta la regla n.º 40). Modifica la etiqueta. Esta es una opción cuando consideras que la heurística captura información que no está contenida actualmente en la etiqueta. Por ejemplo, si intentas maximizar la cantidad de descargas, pero también deseas obtener contenido de calidad, la solución puede ser multiplicar la etiqueta por la cantidad promedio de estrellas que recibió la app. Esta opción permite mucha libertad. Consulta la sección \u0026ldquo;Tu primer objetivo\u0026rdquo;.  Ten en cuenta la mayor complejidad que implica usar heurísticas en un sistema de aprendizaje automático. El uso de heurísticas antiguas en tu algoritmo de aprendizaje automático nuevo puede contribuir a una transición sin sobresaltos, pero piensa si existe una forma más sencilla de lograr el mismo efecto.\nSupervisión En general, mantén el sistema en buen estado, como usar alertas con opciones y contar con una página de panel.\nRegla n.º 8: Conoce los requisitos de antigüedad de tu sistema.\n¿Cuánto se degrada el rendimiento si el modelo tiene un día de antigüedad? ¿Y una semana? ¿Un trimestre? Esta información te puede ayudar a establecer las prioridades de la supervisión. Si pierdes una calidad significativa de los productos cuando el modelo no se actualiza por un día, se justifica contar con un ingeniero que haga un seguimiento constante. La mayoría de los sistemas de publicación de anuncios tiene que gestionar anuncios nuevos cada día, por lo que deben actualizarse. Por ejemplo, si el modelo de AA para Búsqueda de Google Play no está actualizado, puede tener un efecto negativo en menos de un mes. Algunos modelos de la sección Lo más interesante en Google+ no tienen un identificador de publicaciones, por lo que pueden exportar estos modelos de forma poco frecuente. Otros modelos tienen identificadores que se actualizan con una frecuencia mucho mayor. Ten en cuenta que la antigüedad puede cambiar con el tiempo, en especial cuando se agregan o quitan columnas de atributos en el modelo.\nRegla n.º 9: Detecta los problemas antes de exportar los modelos. Existe una etapa en muchos sistemas de aprendizaje automático en la que exportas el modelo para la publicación. Si existe un problema con un modelo exportado, es un problema que notará el usuario.\nComprueba el estado del modelo antes de exportarlo. Específicamente, asegúrate de que el rendimiento del modelo sea consistente con los datos existentes. Si tienes problemas persistentes con los datos, no exportes el modelo. Muchos equipos que implementan continuamente modelos comprueban el área bajo la curva ROC (o AUC) antes de la exportación Los problemas sobre modelos que no se han exportado requieren de una alerta de correo electrónico; los problemas de un modelo para el usuario requieren de una página. Por lo tanto, lo mejor es esperar y estar seguro, antes de hacer algo que afecte a los usuarios.\nRegla n.º 10: Busca los fallos silenciosos. Este es un problema que ocurre con más frecuencia en los sistemas de aprendizaje automático que en otros tipos de sistemas. Supongamos que una tabla particular que se une ya no se actualiza. El sistema de aprendizaje automático se ajustará y el comportamiento continuará siendo razonablemente bueno, con una degradación paulatina. A veces, hay tablas con meses de atraso y basta con una simple actualización en lugar de otro lanzamiento para mejorar el rendimiento en ese trimestre. La cobertura de un atributo puede cambiar debido a las modificaciones en la implementación: por ejemplo, una columna de atributo puede estar completa en un 90% con ejemplos y, de pronto, reducir la cantidad de ejemplos a un 60%. Una vez, había una tabla en Play con un atraso de 6 meses. Una mera actualización aportó un aumento del 2% en la tasa de instalación. Si realizas un seguimiento de las estadísticas de los datos e inspeccionas los datos manualmente cada tanto, puedes reducir este tipo de fallos.\nRegla n.º 11: Documenta y asigna propietarios para las columnas de atributos. Si el sistema es grande y existen muchas columnas de funciones, debes saber quién creó o mantiene cada columna de atributos. Si descubres que la persona que se encarga de una columna de atributos se va, asegúrate de que alguien reciba esa información. Si bien las columnas de atributos tienen nombres descriptivos, es recomendable tener una descripción más detallada de lo que hace un atributo, de dónde proviene y cuál es la contribución esperada.\nTu primer objetivo Si bien te importan varias métricas o mediciones sobre el sistema, el algoritmo de aprendizaje automático a menudo requiere un objetivo único, un número que el algoritmo \u0026ldquo;intenta\u0026rdquo; optimizar. Hago una diferencia entre objetivos y métricas: una métrica es cualquier número que genera el sistema, que puede o no ser importante. Consulta también la regla n.º 2.\nRegla n.º 12: No pienses demasiado qué objetivo debes optimizar directamente. Quieres ganar dinero, que tus usuarios estén contentos y que el mundo sea un lugar mejor. Hay cientos de métricas para tener en cuenta y debes medirlas a todas (consulta la regla n.º 2). Sin embargo, en el comienzo del proceso de aprendizaje automático, notarás que todas aumentan, incluso aquellas que no optimizaste directamente. Por ejemplo, supongamos que te importan la cantidad de clics y el tiempo de visita en el sitio. Si optimizas la cantidad de clics, es probable que también aumente el tiempo de visita.\nPor lo tanto, hay que simplificar y no pensar demasiado en el equilibrio entre las diferentes métricas cuando puedes aumentar fácilmente todas las métricas. Tampoco fuerces esta regla, es decir, no confundas tu objetivo con el estado general del sistema (consulta la regla n.º 39). Además, si aumentas la métrica optimizada directamente, pero decides no ejecutar el sistema, es posible que debas revisar los objetivos.\nRegla n.º 13: Elige una métrica simple, observable y con atributos para tu primer objetivo. Por lo general, no sabes cuál es el verdadero objetivo. Crees que sí, pero luego, al observar los datos y comparar el sistema anterior con el nuevo sistema de aprendizaje automático, te das cuenta de que debes modificar el objetivo. Además, muchas veces, los miembros del equipo no se ponen de acuerdo con el objetivo verdadero. El objetivo del aprendizaje automático debe ser algo que sea fácil de medir y una representación del \u0026ldquo;verdadero\u0026rdquo; objetivo. De hecho, a menudo no existe un \u0026ldquo;verdadero\u0026rdquo; objetivo (consulta la regla n.º 39). Por lo tanto, implementa el entrenamiento para un objetivo sencillo de aprendizaje automático y considera contar con una \u0026ldquo;capa de políticas\u0026rdquo; que te permita agregar más lógica (con suerte, una lógica muy sencilla) para la calificación final.\nLa forma más sencilla de lograr un modelo es el comportamiento del usuario que se observa directamente y se atribuye a una acción en el sistema:\n ¿El usuario hizo clic en este vínculo clasificado? ¿El usuario descargó este objeto clasificado? ¿El usuario reenvió, respondió o envió por correo electrónico este objeto clasificado? ¿El usuario calificó este objeto clasificado? ¿El usuario denunció este objeto mostrado como spam, pornografía u ofensivo?  Al comienzo, evita los efectos indirectos del modelado:\n ¿El usuario realizó otra visita al día siguiente? ¿Cuánto tiempo duró la visita al sitio del usuario? ¿Cuáles fueron los usuarios activos por día? Los efectos indirectos logran excelentes métricas y se pueden usar en pruebas A/B y decisiones de lanzamiento.  Por último, no intentes hacer que el aprendizaje automático responda a estas preguntas:\n ¿El usuario está feliz con el producto? ¿El usuario está satisfecho con la experiencia? ¿El producto mejora el bienestar general del usuario? ¿Cómo afectará el estado general de la empresa?  Estas preguntas son importantes, pero muy difíciles de medir. En su lugar, usa representantes: si el usuario está feliz, permanecerá en el sitio por más tiempo. Si el usuario está satisfecho, volverá a visitar el sitio mañana. En cuanto al bienestar y el estado de la empresa, se requiere el criterio humano para conectar el objetivo de aprendizaje automático con la naturaleza del producto que vendes y tu plan comercial.\nRegla n.º 14: Comenzar con un modelo interpretativo facilita la depuración. La regresión lineal, la regresión logística y la regresión de Poisson están directamente relacionadas con un modelo probabilístico. Cada predicción se interpreta como una probabilidad o un valor esperado. Esto facilita la depuración, en comparación con los modelos que usan objetivos (pérdida de cero uno, diferentes pérdidas de bisagra y más) que intentan optimizar directamente el rendimiento o la precisión de la clasificación. Por ejemplo, si las probabilidades en la capacitación son diferentes de las probabilidades predichas en la comparación o la inspección del sistema de producción, es posible que esta diferencia indique un problema.\nPor ejemplo, en la regresión lineal, logística o de Poisson, existen subconjuntos de datos donde la expectativa promedio de las predicciones equivale a la etiqueta promedio (calibrado con un momento o simplemente calibrado). Esto es verdadero en líneas generales siempre y cuando no tengas una regularización y el algoritmo se haya convergido. Si tienes un atributo que es 1 o 0 para cada ejemplo, significa que el conjunto de 3 ejemplos donde el atributo es 1 está calibrado. Además, si tienes un atributo que es 1 para cada ejemplo, entonces el conjunto de todos los ejemplos está calibrado.\nCon modelos simples, es más fácil lidiar con ciclos de reacción (consulta la regla n.º 36). A menudo, usamos estas predicciones probabilísticas para tomar una decisión: por ejemplo, calificar publicaciones según el valor esperado decreciente (es decir, la probabilidad de hacer clic, descargar, etc.). Sin embargo, recuerda que cuando debes elegir qué modelo usar, la decisión importa más que la probabilidad de los datos según el modelo (consulta la regla n.° 27).\nRegla n.º 15: Separa el filtro de spam y la clasificación de calidad en una capa de política. La clasificación de calidad es un arte delicado, pero el filtro de spam es una guerra. Las señales que usas para determinar las publicaciones de calidad alta serán obvias para los usuarios de tu sistema; ellos podrán modificar sus publicaciones para que tengan estas propiedades. Además, tu clasificación de calidad debe centrarse en calificar el contenido que se publica de buena fe. No subestimes al modelo por darle una clasificación demasiado alta al spam. De forma similar, el contenido \u0026ldquo;subido de tono\u0026rdquo; debe separarse de la clasificación de calidad. El filtro de spam es una historia diferente. Es esperable que los atributos que debes generar cambiarán constantemente. A menudo, incluyes reglas obvias en el sistema (por ejemplo, si una publicación tiene más de tres votos de spam, no hay que recuperarla). Los modelos aprendidos deben actualizarse a diario o de forma más frecuente. La reputación del creador del contenido tiene un peso importante.\nEn algún nivel, el resultado de estos dos sistemas debe integrarse. Ten en cuenta que el filtro de spam en resultados de la búsqueda debe ser más agresivo que en mensajes de correo electrónico. Esto es así si no tienes ninguna regularización y el algoritmo está convergido. En general, es de este modo. Además, es una práctica estándar para quitar el spam de los datos de entrenamiento para el clasificador de calidad.\nFase II de aprendizaje automático: Ingeniería de atributos En la primera fase del ciclo de vida de un sistema de aprendizaje automático, la prioridad es mandar los datos de entrenamiento al sistema de aprendizaje, lograr instrumentar las métricas de interés y crear una infraestructura de publicación. Una vez que cuentas con un sistema integral en funcionamiento con pruebas de unidades y del sistema instrumentadas, comienza la fase II.\nEn la segunda fase, hay muchas recompensas a corto plazo. Existe una variedad de atributos obvios que se pueden agregar al sistema. Además, la segunda fase de aprendizaje automático implica agregar tantos atributos como sea posible y combinarlos de formas intuitivas. Durante esta fase, todas las métricas deben continuar subiendo. Habrá muchos lanzamientos, y es una excelente oportunidad para incorporar muchos ingenieros que puedan recopilar todos los datos que necesitas para crear un sistema de aprendizaje verdaderamente sorprendente.\nRegla n.º 16: Planifica el lanzamiento y la iteración. No esperes que el modelo en el que trabajas ahora sea el último que lanzarás o, incluso, que dejarás de lanzar modelos. Por lo tanto, ten en cuenta que la complejidad de este lanzamiento retrasará los lanzamientos futuros. Muchos equipos han lanzado uno o más modelos por trimestre durante años. Existen tres razones básicas para lanzar modelos nuevos:\nTienes atributos nuevos. Deseas ajustar la regularización y combinar atributos antiguos de formas nuevas. Deseas ajustar el objetivo. Independientemente de la razón, es recomendable poner atención en el modelo: analizar los datos que se ingresan en el ejemplo te permite encontrar señales nuevas y detectar señales antiguas con problemas. Entonces, a medida que desarrollas el modelo, piensa en lo fácil que es agregar, quitar o recombinar atributos. Piensa en lo fácil que es crear una copia nueva de la canalización y verifica que sea correcta. Piensa en si es posible ejecutar dos o tres copias en paralelo. Por último, no te preocupes si no logras incluir todos los atributos en esta versión de la canalización. Las incluirás el próximo trimestre.\nRegla n.º 17: Comienza con los atributos directamente observados e informados, en lugar de los atributos aprendidos. Este puede ser un punto controversial, pero evita muchos problemas. Primero, describamos qué es un atributo aprendido. Un atributo aprendido es un atributo generado por un sistema externo (como un sistema de agrupación en clústeres sin supervisar) o el mismo modelo (por ejemplo, mediante un modelo factorizado o aprendizaje profundo). Ambas opciones pueden ser útiles, pero tienen muchos problemas, por lo que no es conveniente incluirlas en el primer modelo.\nSi usas un sistema externo para crear un atributo, recuerda que ese sistema tiene su propio objetivo. Es posible que el objetivo del sistema externo no tenga mucha relación con tu objetivo actual. Si realizas una instantánea del sistema externo, es posible que esté desactualizada. Si actualizas los atributos desde el sistema externo, es posible que los significados cambien. Si usas un sistema externo para proporcionar un atributo, debes tener mucho cuidado con ese enfoque.\nEl principal problema de los modelos multiplicados y los modelos profundos es que no son convexos. Por lo tanto, no hay garantía de encontrar una solución óptima (o de aproximarse a esta). Además, el mínimo local de cada iteración puede ser diferente. Esta variación no permite juzgar con exactitud si el impacto de un cambio en tu sistema es relevante o contingente. Si creas un modelo sin atributos profundos, obtienes un sistema de referencia con buen rendimiento. Una vez que alcanzas este punto de referencia, puedes probar enfoques menos ortodoxos.\nRegla n.º 18: Prueba atributos de contenido que generalicen en contextos. A menudo, un sistema de aprendizaje automático es una parte pequeña de una entidad mucho más grande. Por ejemplo, si imaginas una publicación que se puede usar en Lo Más Interesante, mucha gente hará +1, la compartirá o escribirá un comentario en ella antes de que llegue a mostrarse en Lo más interesante. Si proporcionas esas estadísticas al modelo, este puede promocionar publicaciones nuevas para las que no tiene datos en el contexto que está optimizando. YouTube Watch Next puede usar la cantidad de videos que miraste, o que miraste de forma secuencial (la cantidad de veces que se miró un video después de otro) en la búsqueda de YouTube. También puedes usar clasificaciones de usuarios explícitas. Por último, si usas una acción de usuario como etiqueta, ver esa acción en el documento en un contexto diferente puede ser un excelente atributo. Todos estos atributos te permiten aportar contenido nuevo al contexto. Ten en cuenta que esto no se trata de personalización: primero, descubre si a alguien le gusta el contenido en este contexto; luego, descubre a quién le gusta más o menos.\nRegla n.º 19: Usa atributos muy específicos cuando sea posible. Gracias a la gran cantidad de datos, es más fácil aprender millones de atributos sencillos que unos pocos atributos complejos. Los identificadores de documentos que se obtienen y las consultas canónicas no brindan mucha generalización, pero alinean las clasificaciones con las etiquetas en las consultas principales. Por lo tanto, no temas agrupar atributos si cada uno se aplica a una fracción muy pequeña de datos, pero la cobertura total es de más del 90%. Puedes usar la regularización para eliminar los atributos que se aplican a pocos ejemplos.\nRegla n.º 20: Combina y modifica los atributos existentes para crear atributos nuevos de una forma legible. Hay diferentes formas de combinar y modificar atributos. Los sistemas de aprendizaje automático como TensorFlow te permiten preprocesar los datos mediante transformaciones. Los dos enfoques estándar son las \u0026ldquo;discretizaciones\u0026rdquo; y las \u0026ldquo;combinaciones\u0026rdquo;.\nLa discretización consiste en tomar un atributo continuo y crear varios atributos discretos. Considera un atributo continuo como la edad. Puedes crear un atributo que es 1 cuando la edad es menor de 18, otro atributo que es 1 cuando la edad es entre 18 y 35, etc. No pienses demasiado en los límites de estos histogramas; los cuantiles básicos serán los más eficaces.\nLas combinaciones unen dos o más columnas de atributos. En la terminología de TensorFlow, una columna de atributos es un conjunto de atributos homogéneos (p. ej., {masculino, femenino}, {EE.UU., Canadá, México}, etc.). Una combinación es una nueva columna de atributos que incluye, p. ej., {masculino, femenino} × {EE.UU., Canadá, México}. Esta nueva columna de atributos contendrá el atributo (masculino, Canadá). Si usas TensorFlow y le indicas que cree esta combinación, este atributo (masculino, Canadá) aparecerá en los ejemplos que representan canadienses masculinos. Ten en cuenta que se necesita una enorme cantidad de datos para aprender modelos con combinaciones de tres, cuatro o más columnas de atributos básicos.\nLas combinaciones que producen columnas de atributos muy grandes pueden producir un sobreajuste. Por ejemplo, imagina que haces algún tipo de búsqueda y tienes una columna de atributos con palabras en la consulta y otra con palabras en el documento. Si unes estas columnas con una combinación, terminarás con muchos atributos (consulta la regla n.º 21).\nCuando trabajas con texto, existen dos alternativas. La más rigurosa es un producto escalar. En su forma más simple, un producto escalar simplemente cuenta la cantidad de palabras en común entre la consulta y el documento. Por lo tanto, este atributo se puede discretizar. Otro enfoque es una intersección: tenemos un atributo que está presente solo si la palabra \u0026ldquo;poni\u0026rdquo; aparece en el documento y en la consulta, y otro atributo que está presente solo si la palabra \u0026ldquo;el\u0026rdquo; aparece en el documento y en la consulta.\nRegla n.º 21: La cantidad de ponderaciones de atributos que puedes aprender en un modelo lineal es casi proporcional a la cantidad de datos que tienes. Existen resultados teóricos fascinantes sobre aprendizaje estadístico relacionados con el nivel de complejidad correspondiente de un modelo, pero esta regla es todo lo que necesitas saber. Tuve discusiones con personas que ponían en duda que se pudiera aprender algo con mil ejemplos o que nunca se necesitan más de un millón de ejemplos, porque están empecinados con un determinado método de aprendizaje. La clave es escalar el aprendizaje a la medida del tamaño de los datos:\nSi trabajas con un sistema de ranking de búsquedas, existen millones de palabras diferentes en los documentos y la consulta, y tienes 1000 ejemplos etiquetados, debes usar un producto escalar entre los atributos de consultas y de documentos, TF-IDF y una media docena de otros atributos desarrollados por humanos. 1000 ejemplos, una docena de atributos. Si tienes un millón de ejemplos, intersecta la columna de atributos de consultas con la de documentos, y aplica regularización y, posiblemente, selección de atributos. Esto te brindará un millón de atributos, pero, con la regularización, tendrás menos. 10 millones de ejemplos, tal vez 100,000 atributos. Si tienes miles o cientos de miles de millones, puedes combinar las columnas de atributos con tokens de consultas y de documentos mediante la regularización y la selección de atributos. Tendrás miles de millones de ejemplos y 10 millones de atributos. La teoría de aprendizaje estadístico raramente establece límites rígidos, pero ofrece un buen punto de partida. En última instancia, usa la regla n.º 28 para decir qué atributos usarás.\nRegla n.º 22: Quita los atributos que ya no uses. Los atributos sin usar crean deuda técnica. Si descubres que no estás usando un atributo y que no sirve combinarlo con otros atributos, quítalo de la infraestructura. Debes mantener limpia tu infraestructura para que puedas probar los atributos más prometedores tan rápido como sea posible. Si es necesario, tu atributo se puede volver a agregar en cualquier momento.\nTen en cuenta la cobertura cuando analices qué atributos agregarás o conservarás. ¿Cuántos ejemplos cubre el atributo? Por ejemplo, si tienes algunos atributos de personalización, pero solo el 8% de los usuarios tiene atributos de personalización, eso no será muy eficaz.\nAl mismo tiempo, algunos atributos te sorprenden gratamente. Por ejemplo, si tienes un atributo que cubre solo el 1% de los datos, pero el 90% de los ejemplos de este atributo son positivos, es un excelente atributo que agregar.\nAnálisis humano del sistema Antes de avanzar a la tercera fase de aprendizaje automático, es importante hablar de algo que no se enseña en ninguna clase de aprendizaje automático: cómo analizar un modelo existente y mejorarlo. Esto es más un arte que una ciencia. Aun así, existen muchos antipatrones que es conveniente evitar.\nRegla n.º 23: No eres el típico usuario final. Probablemente, esta es la razón más común por la que un equipo no progresa. Si bien usar un prototipo con tu equipo o usar un prototipo en tu empresa tiene muchos beneficios, los empleados deben analizar si el rendimiento es correcto. Si bien un cambio que sea evidentemente malo no debe usarse, cualquier función que parezca lista para la producción debe probarse aún más, ya sea contratando a gente común para que responda preguntas en una plataforma de participación colectiva o mediante un experimento en vivo con usuarios reales.\nHay dos razones para ello. Estás demasiado familiarizado con el código. Es posible que busques un aspecto específico de las publicaciones, o que estés muy involucrado (p. ej., sesgo de confirmación). La segunda razón es que tu tiempo es demasiado valioso. Considera el costo de nueve ingenieros en una reunión de una hora de duración y cuántas etiquetas logradas con trabajo humano puedes obtener en una plataforma de participación colectiva.\nSi realmente quieres comentarios de usuarios, implementa metodologías de experiencia de usuario. Crea usuarios persona (puedes encontrar una descripción en el libro Sketching User Experiences [Cómo diseñar experiencias de usuario] de Bill Buxton) al comienzo del proceso y, luego, implementa una prueba de usabilidad (puedes encontrar una descripción en el libro Don’t Make Me Think [No me hagas pensar] de Steve Krug). Los usuarios persona implican crear un usuario hipotético. Por ejemplo, si tu equipo se compone solo por hombres, será beneficioso diseñar un usuario persona femenina de 35 años (completa con atributos de usuario) y observar los resultados que genera, en lugar de los 10 resultados para hombres de 25 a 40 años. También puedes lograr una nueva perspectiva al evaluar cómo reaccionan las personas reales a tu sitio (de forma local o remota) en una prueba de usabilidad.\nRegla n.º 24: Mide el delta entre los modelos. Una de las formas más sencillas y, a veces, más útiles de realizar mediciones que puedes usar antes de que los usuarios vean tu nuevo modelo es calcular la diferencia entre los nuevos resultados y los obtenidos con el sistema en producción. Por ejemplo, si tienes un problema de ranking, ejecuta ambos modelos con una muestra de consultas en todo el sistema y observa el tamaño de la diferencia simétrica de los resultados (ponderados según la posición en el ranking). Si la diferencia es muy pequeña, entonces puedes deducir que habrá poco cambio, sin necesidad de ejecutar un experimento. Si la diferencia es muy grande, entonces debes asegurarte de que el cambio sea positivo. Analizar las consultas donde la diferencia simétrica es alta te puede ayudar a comprender de forma cualitativa cómo fue el cambio. Sin embargo, asegúrate de que el sistema sea estable. Cuando compares un modelo consigo mismo, asegúrate de que tenga una diferencia simétrica baja (idealmente cero).\nRegla n.º 25: Cuando elijas un modelo, el rendimiento utilitario predomina por sobre el poder de predicción. Tu modelo puede intentar predecir la tasa de clics. Sin embargo, en última instancia, la pregunta clave es lo que haces con esa predicción. Si la usas para clasificar documentos, entonces la calidad del ranking final importa más que la predicción en sí misma. Si predices la probabilidad de que un documento sea spam y, luego, tienes un punto límite sobre lo que se bloquea, entonces la precisión de lo que se permite tiene más importancia. La mayoría de las veces, estos dos aspectos coinciden; cuando no es así, es probable que haya una pequeña ganancia. Además, si hay algún cambio que mejore la pérdida logística, pero que reduzca el rendimiento del sistema, busca otro atributo. Si esto comienza a suceder con más frecuencia, es hora de volver a evaluar el objetivo del modelo.\nRegla n.º 26: Busca patrones en los errores observados y crea atributos nuevos. Supongamos que ves un ejemplo de entrenamiento que el modelo \u0026ldquo;no entendió\u0026rdquo;. En una tarea de clasificación, este error puede ser un falso positivo o un falso negativo. En una tarea de clasificación, el error puede ser un par donde un positivo tiene un ranking menor que un negativo. El punto más importante es que sea un ejemplo que el sistema de aprendizaje automático sepa que no lo entendió y que lo corrija si tiene la oportunidad. Si le agregas un atributo al modelo para que pueda corregir el error, el modelo intentará usarlo.\nPor otro lado, si intentas crear un atributo basado en ejemplos que el sistema no considera errores, el atributo se ignorará. Por ejemplo, supongamos que, en la búsqueda de apps de Play, alguien busca \u0026ldquo;juegos gratuitos\u0026rdquo;. Supongamos que uno de los primeros resultados es una app de bromas menos relevante. Entonces, creas un atributo para \u0026ldquo;apps de bromas\u0026rdquo;. Sin embargo, si maximizas la cantidad de instalaciones y las personas instalan una app de bromas cuando buscan juegos gratuitos, el atributo para \u0026ldquo;apps de bromas\u0026rdquo; no tendrá el efecto deseado.\nUna vez que tengas ejemplos que el modelo no haya entendido, busca las tendencias que estén fuera del conjunto de atributos actual. Por ejemplo, si parece que el sistema penaliza las publicaciones más largas, agrega la longitud de la publicación. No seas demasiado específico sobre los atributos que agregues. Si agregas la longitud de la publicación, no intentes adivinar qué significa \u0026ldquo;largo\u0026rdquo;, solo agrega una decena de atributos y permite que el modelo descubra qué hacer con ellos (consulta la regla n.º 21). Esta es la forma más fácil de obtener el resultado deseado.\nRegla n.º 27: Intenta cuantificar el comportamiento no deseado que observes. Algunos miembros de tu equipo comenzarán a frustrarse con las propiedades del sistema que no les gusten, ya que la función de pérdida existente no las captura. En este punto, deben hacer lo que sea necesario para convertir sus quejas en números sólidos. Por ejemplo, si consideran que se muestran demasiadas \u0026ldquo;apps de bromas\u0026rdquo; en la búsqueda de Play, se pueden contratar a evaluadores humanos para que identifiquen las apps de bromas. (Puedes usar datos etiquetados por humanos en este caso, ya que una fracción relativamente pequeña de consultas representan una gran fracción del tráfico). Si los problemas se pueden medir, puedes comenzar a usarlos como atributos, objetivos o métricas. La regla general es \u0026ldquo;medir primero, optimizar después\u0026rdquo;.\nRegla n.º 28: Ten en cuenta que el comportamiento idéntico a corto plazo no implica un comportamiento idéntico a largo plazo. Imagina que tienes un sistema nuevo que analiza cada doc_id y exact_query, y luego calcula la probabilidad de clic para cada documento y cada consulta. Descubres que este comportamiento es casi idéntico a tu sistema actual en la comparación y la prueba A/B; por lo tanto, dado su simplicidad, lo ejecutas. Sin embargo, notas que no se muestran apps nuevas. ¿Por qué? Bueno, dado que tu sistema solo muestra un documento basado en su propio historial con esa consulta, no hay forma de aprender qué documento nuevo debe mostrar.\nLa única forma de entender cómo debe funcionar un sistema a largo plazo es entrenarlo solo con datos adquiridos cuando el modelo está publicado. Esto es muy difícil.\nDesviación entre el entrenamiento y la publicación La desviación entre el entrenamiento y la publicación es la diferencia entre el rendimiento del entrenamiento y el de la publicación. Existen diferentes razones para esta desviación:\nuna discrepancia entre cómo manipulas los datos en las canalizaciones de entrenamiento y del servidor un cambio en los datos entre el momento del entrenamiento y el del servidor un ciclo de retroalimentación entre el modelo y el algoritmo Hemos observado sistemas de aprendizaje automático de producción en Google con una desviación entre el entrenamiento y la publicación que afecta negativamente el rendimiento. La mejor solución es supervisarlo de forma explícita para que los cambios en el sistema y en los datos no generen una desviación inadvertida.\nRegla n.º 29: La mejor manera de asegurarte de que el entrenamiento se asemeja a la publicación es guardar el conjunto de atributos que usas en la publicación y canalizar esos atributos en un registro para luego usarlos en el entrenamiento. Incluso si no lo puedes hacer para cada ejemplo, hazlo para una fracción pequeña, de forma tal que puedas verificar la coherencia entre la publicación y el entrenamiento (consulta la regla n.º 37). En Google, los equipos que hicieron esta medición se sorprendieron a menudo por los resultados. La página de inicio de YouTube cambió a atributos del registro en el servidor, lo que generó mejoras de calidad importantes y redujo la complejidad del código. Ahora, muchos equipos están cambiando sus infraestructuras.\nRegla n.º 30: ¡Realiza muestras con datos con ponderación por importancia, no los quites! Cuando tienes muchos datos, es tentador usar los archivos del 1 al 12 e ignorar los archivos del 13 al 99. Esto es un error. Si bien se pueden quitar los datos que nunca se mostraron al usuario, la ponderación por importancia es la mejor opción para el resto. La ponderación por importancia implica que, si decides hacer una muestra con el ejemplo X con una probabilidad del 30%, la ponderación será de 10\u0026frasl;3. Con la ponderación por importancia, se mantienen todas las propiedades de calibración que analizamos en la regla n.º 14.\nRegla n.º 31: Ten en cuenta que, si cruzas datos de una tabla durante el entrenamiento y la publicación, los datos en la tabla pueden cambiar. Digamos que cruzas ID de documentos a una tabla que contiene atributos para esos documentos (como la cantidad de comentarios o clics). Entre el entrenamiento y el servidor, es posible que cambien los atributos en la tabla. Por lo tanto, puede cambiar predicción del modelo para el mismo documento entre el entrenamiento y la publicación. La forma más sencilla de evitar este tipo de problemas es registrar los atributos durante la publicación (consulta la regla n.º 32). Si la tabla cambia lentamente, puedes tomar una instantánea de la tabla a cada hora o cada día para obtener datos razonablemente cercanos. Ten en cuenta que esto no resuelve completamente el problema.\nRegla n.º 32: Reutiliza el código entre la canalización de entrenamiento y la canalización del servidor, siempre que sea posible. El procesamiento por lotes es diferente al procesamiento en línea. En el procesamiento en línea, debes responder cada solicitud a medida que llega (p. ej., debes realizar una búsqueda separada para cada consulta). En el procesamiento por lotes, puedes combinar tareas (p. ej., unir funciones). En la publicación, implementas el procesamiento en línea, mientras que el entrenamiento es una tarea de procesamiento por lotes. Sin embargo, hay varias formas de reutilizar el código. Por ejemplo, puedes crear un objeto que sea específico para tu sistema, donde el resultado de cualquier consulta o unión se puede almacenar de una forma legible y donde los errores se pueden probar fácilmente. Luego, una vez que hayas reunido toda la información, durante el entrenamiento o la publicación, ejecutas un método común para conectar el objeto legible específico del sistema y el formato que espera el sistema de aprendizaje automático. Esto elimina una fuente de desviación entre el entrenamiento y la publicación. Como corolario, intenta no usar dos lenguajes de programación diferentes entre el entrenamiento y la publicación. Si haces esto, será casi imposible compartir el código.\nRegla n.º 33: Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. En general, mide el rendimiento de un modelo con datos reunidos en forma posterior a aquellos con los que se ha entrenado el modelo, ya que refleja de forma más precisa qué hará el sistema en la producción. Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. El rendimiento no debería ser tan bueno en los datos nuevos, pero no debería ser mucho peor. Como puede haber efectos diarios, es posible que no predigas la tasa de clics promedio o la tasa de conversión, pero el área bajo la curva, que representa la posibilidad de darle una puntuación más alta al ejemplo positivo que al ejemplo negativo, debería ser razonablemente parecida.\nRegla n.º 34: En la clasificación binaria para filtrado (como la detección de spam o la identificación de correos electrónicos de interés), realiza pequeños sacrificios a corto plazo en el rendimiento para lograr datos más claros. En la tarea de filtrado, los ejemplos que se marcan como negativos no se muestran al usuario. Supongamos que tienes un filtro que bloquea el 75% de los ejemplos negativos durante la publicación. Puede surgir la tentación de obtener más datos de entrenamiento a partir de las instancias que se muestran a los usuarios. Por ejemplo, si un usuario marca como spam un correo electrónico que permitió tu filtro, se puede aprender de esta acción.\nPero este enfoque introduce un sesgo en la muestra. Puedes obtener datos más claros si etiquetas el 1% de todo el tráfico como \u0026ldquo;retenido\u0026rdquo; durante la publicación y envías todos los ejemplos retenidos al usuario. Ahora, el filtro bloqueará al menos el 74% de los ejemplos negativos. Los ejemplos retenidos se convertirán en los datos de entrenamiento.\nSi el filtro bloquea el 95% o más de los ejemplos negativos, este enfoque se hace menos viable. Aun así, si deseas medir el rendimiento en la publicación, puedes hacer una muestra todavía más pequeña (p. ej., 0.1% o 0.001%). Diez mil ejemplos son suficientes para estimar el rendimiento de forma precisa.\nRegla n.º 35: Ten en cuenta la desviación inherente a los problemas de ranking. Si cambias el algoritmo de clasificación lo suficiente como para ver resultados diferentes, habrás logrado modificar los datos que el algoritmo verá en el futuro. Este tipo de desviación aparecerá, y debes tenerla en cuenta al diseñar el modelo. Existen varias estrategias diferentes que sirven para favorecer los datos que tu modelo ya vio.\nPermite tener una regularización más alta en los atributos que cubren más consultas, a diferencia de esos atributos que solo abarcan una consulta. De esta forma, el modelo favorecerá los atributos que son específicos a una o pocas consultas por sobre los atributos que se generalizan a todas las consultas. Esta estrategia permite evitar que los resultados muy populares acaben en consultas irrelevantes. Este enfoque es contrario a la sugerencia más tradicional de contar con más regularización en columnas de funciones con más valores únicos. Permite que los atributos solo tengan ponderaciones positivas. Además, cualquier atributo bueno será mejor que uno \u0026ldquo;desconocido\u0026rdquo;. No uses atributos asociados solo al documentos. Esto es una versión extrema de la regla n.º 1. Por ejemplo, incluso si una app determinada es una descarga popular, más allá de la consulta, no es necesario mostrarla en todos lados. Esto se simplifica al no tener atributos solo de documentos. La razón por la que no deseas mostrar una app popular específica en todos lados está relacionada con la importancia de lograr que las apps que deseas estén disponibles. Por ejemplo, si alguien busca \u0026ldquo;apps para observar pájaros\u0026rdquo;, es posible que descarguen \u0026ldquo;Angry birds\u0026rdquo;, pero de forma claramente accidental. Si se muestra esta app, es posible que mejore la tasa de descarga, pero no se cumplen con las necesidades del usuario. Regla n.º 36: Evita los ciclos de retroalimentación con atributos posicionales. La posición del contenido afecta enormemente la probabilidad de que el usuario interactúe con este. Si ubicas una app en la primera posición, se seleccionará con más frecuencia y te dará la impresión de que es más probable que se seleccione. Una forma de lidiar con eso es agregar atributos de posición, es decir, atributos sobre la posición del contenido en la página. Entrena el modelo con atributos de posición para que aprenda a darle una mayor ponderación al atributo \u0026ldquo;primera posición\u0026rdquo;, por ejemplo. Así, el modelo les asigna una ponderación menor a otros factores con ejemplos de \u0026ldquo;primera posición=verdadero\u0026rdquo;. Entonces, en la publicación, no asignas ninguna instancia al atributo de posición (o le asignas el mismo atributo predeterminado), porque calificas candidatos antes de que hayas decidido el orden en el que se mostrarán.\nTen en cuenta que es importante mantener cualquier atributo de posición separado del resto del modelo, debido a la asimetría entre el entrenamiento y la prueba. Lo ideal es que el modelo sea la suma de una función de los atributos posicionales y una función del resto de atributos. Por ejemplo, no cruces los atributos de posición con cualquier atributo de documentos.\nRegla n.º 37: Mide la desviación entre el entrenamiento y la publicación. En el sentido más general, existen diversas razones para la desviación. Además, se puede dividir en varias partes:\nLa diferencia entre el rendimiento en los datos de entrenamiento y los datos retenidos. En general, esta diferencia siempre existe y no siempre es negativa. La diferencia entre el rendimiento en los datos retenidos y los datos \u0026ldquo;del día siguiente\u0026rdquo;. De nuevo, esta diferencia siempre existe. Debes ajustar la regularización para maximizar el rendimiento del día siguiente. Sin embargo, caídas notables en el rendimiento entre los datos retenidos y los datos del día siguiente pueden ser un indicador de que algunos atributos dependen del tiempo y posiblemente afecten al rendimiento del modelo de forma negativa. La diferencia entre el rendimiento en los datos del día siguiente y los datos en vivo. Si aplicas un modelo a un ejemplo en los datos de entrenamiento y el mismo ejemplo en la publicación, deberías obtener el mismo resultado (consulta la regla n.º 5). Por lo tanto, si aparece una discrepancia, probablemente indique un error de ingeniería. Fase III de aprendizaje automático: Crecimiento reducido, refinamiento de la optimización y modelos complejos Existen ciertos indicios de que la segunda fase llega a su fin. Primero, las ganancias mensuales comienzan a disminuir. Las métricas comenzarán a emparejarse: en algunos experimentos, verás que algunas aumentan y otras disminuyen. Este es un momento interesante. Dado que las ganancias son más difíciles de obtener, el aprendizaje automático debe sofisticarse aún más. Una advertencia: esta sección contiene más reglas especulativas que las secciones anteriores. Hemos visto a muchos equipos realizar grandes progresos en la Fase I y la Fase II de aprendizaje automático. Una vez que alcanzan la Fase III, los equipos deben buscar su propio camino.\nRegla n.º 38: No pierdas tiempo en nuevos atributos si los objetivos no alineados son un problema. Cuando las mediciones comienzan a estabilizarse, el equipo comenzará a buscar problemas fuera del alcance de los objetivos de tu sistema de aprendizaje automático actual. Como indicamos anteriormente, si el objetivo algorítmico existente no abarca los propósitos del producto, debes cambiar el objetivo o los propósitos. Por ejemplo, puedes optimizar clics, +1 o descargas, pero toma las decisiones de lanzamiento según los evaluadores humanos.\nRegla n.º 39: Las decisiones de lanzamiento representan los objetivos a largo plazo del producto. Alice tiene una idea para reducir la pérdida logística de las instalaciones predichas. Agrega un atributo. Se reduce la pérdida logística. Cuando hace un experimento en vivo, observa un aumento en la tasa de instalación. Sin embargo, cuando realiza una reunión para evaluar el lanzamiento, alguien menciona que la cantidad de usuarios activos diarios cayó un 5%. El equipo decide no lanzar el modelo. Alice está decepcionada, pero ahora se da cuenta de que las decisiones de lanzamiento dependen de varios factores y solo algunos de ellos se pueden optimizar directamente con AA.\nLa verdad es que el mundo real no es un juego de rol; no hay \u0026ldquo;puntos de ataque\u0026rdquo; que indican el estado de tu producto. El equipo debe usar las estadísticas que reúne para intentar predecir de forma eficaz qué tan bueno será el sistema en el futuro. Deben preocuparse por la participación, el número de usuarios activos en un día (DAU), el número de usuarios activos en 30 días (30 DAU), la ganancia y el retorno de la inversión del anunciante. Estas métricas que se miden en las pruebas A/B representan los objetivos a largo plazo: satisfacer a los usuarios, aumentar la cantidad de usuarios, satisfacer a los socios y obtener ganancias. A su vez, puedes considerar estos propósitos como representantes de otros propósitos: lograr un producto útil y de calidad, y que la empresa prospere de aquí a cinco años.\nLas únicas decisiones de lanzamiento fáciles son cuando todas las métricas mejoran (o al menos no empeoran). Si el equipo puede elegir entre un algoritmo de aprendizaje automático sofisticado o una simple heurística (que funciona mejor en todas las métricas), debe elegir la heurística. Además, no existe una clasificación explícita para todos los valores de métricas posibles. En especial, considera estos dos escenarios siguientes:\nExperimento Usuarios activos por día Ganancia/día A 1 millón $4 millones B 2 millones $2 millones Si el sistema actual es A, entonces es poco probable que el equipo cambie a B. Si el sistema actual es B, entonces es poco probable que el equipo cambie a A. Esto parece contradecir el comportamiento racional; sin embargo, las predicciones de las métricas cambiantes pueden o no cumplirse. Por lo tanto, cada cambio conlleva un riesgo grande. Cada métrica abarca una cierta cantidad de riesgo que preocupa al equipo.\nAdemás, ninguna métrica representa la preocupación máxima del equipo, \u0026ldquo;¿dónde estará mi producto de aquí a cinco años?\u0026rdquo;.\nPor otro lado, las personas tienden a favorecer un objetivo que pueden optimizar directamente. La mayoría de las herramientas de aprendizaje automático favorecen dicho entorno. Un ingeniero agregando atributos nuevos puede lograr un flujo constante de lanzamiento en dicho entorno. Existe un tipo de aprendizaje automático, el aprendizaje de multiobjetivo, que comienza a resolver este problema. Por ejemplo, se puede formular un problema de satisfacción de restricciones con límites inferiores en cada métrica y optimiza alguna combinación lineal de las métricas. Pero, aun así, no todas las métricas se enmarcan fácilmente como objetivos de aprendizaje automático: si el usuario hace clic en un documento o instala una app, se debe a que se mostró el contenido. Pero es mucho más difícil determinar la razón por la que un usuario visita tu sitio. Cómo predecir el éxito futuro de un sitio de forma integral depende IA-completo: es tan difícil como la visión por computadora o el procesamiento de lenguajes naturales.\nRegla n.º 40: Mantén las combinaciones simples. Los modelos unificados que aceptan atributos sin procesar y clasifican contenido directamente son los modelos más sencillos de depurar y comprender. Sin embargo, una combinación de modelos (un modelo que combina los resultados de otros modelos) puede funcionar mejor. Para mantener las cosas simples, cada modelo debe ser una combinación de modelos que solo acepta como entrada el resultado de otros modelos o un modelo básico que acepta muchos atributos, pero no ambos. Si tienes modelos sobre otros modelos que se entrenan de forma separada y los combinas, se puede generar un comportamiento erróneo.\nUsa un modelo simple como combinación, que solo acepte los resultados de los modelos \u0026ldquo;básicos\u0026rdquo; como entradas. También debes implementar propiedades en esos modelos de conjuntos. Por ejemplo, un aumento en el resultado generado por un modelo básico no debe reducir el resultado del combinado. Además, es mejor que los modelos entrantes se puedan interpretar de forma semántica (p. ej., calibrados), para que los cambios en los modelos subyacentes no confundan al modelo combinado. Asegúrate también que un aumento en la probabilidad predicha de un clasificador subyacente no reduzca la probabilidad predicha del conjunto.\nRegla n.º 41: Cuando el rendimiento se estanque, busca nuevas fuentes de información de forma cualitativa para agregar, en lugar de refinar las señales existentes. Agregaste cierta información demográfica sobre el usuario. Agregaste cierta información sobre las palabras en el documento. Finalizaste la exploración de combinación de atributos y ajustaste la regularización. No observaste un lanzamiento con más de un 1% de mejora en las métricas clave, en varios trimestres. ¿Ahora qué?\nEs hora de desarrollar la infraestructura para atributos radicalmente diferentes, como el historial de los documentos a los que este usuario accedió en el último día, semana o año, o los datos de una propiedad diferente. Usa entidades de wikidatos o algún recurso interno de tu empresa (como el Gráfico de conocimiento de Google). Usa el aprendizaje profundo. Comienza a ajustar tus expectativas sobre el retorno de la inversión esperado y aumenta tus esfuerzos adecuadamente. Como en cualquier proyecto de ingeniería, debes comparar el beneficio de agregar nuevos atributos con el costo de una mayor complejidad.\nRegla n.º 42: No esperes que la diversidad, la personalización o la relevancia se correlacionen con la popularidad. La diversidad en un conjunto de contenidos puede significar muchas cosas; la más común es la diversidad de la fuente del contenido. La personalización implica que cada usuario obtiene sus propios resultados. La relevancia implica que los resultados para una consulta específica son más apropiados para esa consulta que para otra. Además, por definición, estas tres propiedades se diferencian de lo común.\nEl problema es que lo común tiende a ser difícil de superar.\nTen en cuenta que tu sistema mide clics, el tiempo dedicado, reproducciones, +1, veces que se comparte el contenido, etc., es decir, la popularidad del contenido. A veces, los equipos intentan aprender un modelo personal con diversidad. Para implementar la personalización, agregan atributos que le permiten al sistema la personalización (algunos atributos que representan el interés del usuario) o la diversificación (atributos que indican si este documento tiene algún atributo en común con otros documentos de los resultados, como el autor o el contenido), y descubren que esos atributos obtienen una ponderación menor (o a veces, un signo diferente) al que esperaban.\nEsto no significa que la diversidad, la personalización o la relevancia no sean valiosas. Como se indicó en la regla anterior, puedes hacer un posprocesamiento para aumentar la diversidad o la relevancia. Si observas que los objetivos a largo plazo aumentan, puedes declarar que la diversidad o la relevancia son valiosas, más allá de la popularidad. Puedes continuar usando el posprocesamiento o directamente modificar el objetivo según la diversidad o la relevancia.\nRegla n.º 43: Tus amigos tienden a ser los mismos en diferentes productos. No así tus intereses. Varios equipos en Google ganaron mucho terreno al tomar que un modelo que predice la cercanía de una conexión con un producto y lograr que funcione en otro producto. Tus amigos no cambian. Por otro lado, observé a muchos equipos lidiar con atributos de personalización entre productos. Sí, parece que debería funcionar. Por ahora, parece que no. Un método que a veces funciona es usar los datos sin procesar de una propiedad para predecir el comportamiento en otra. Ten en cuenta que también puede ayudar saber que un usuario tiene un historial en otra propiedad. Por ejemplo, la presencia de actividad de usuario en dos propiedades puede ser un indicativo en sí misma.\nTrabajo relacionado Existen muchos documentos sobre aprendizaje automático en Google y en otras fuentes.\nCurso intensivo de aprendizaje automático: Una introducción al aprendizaje automático aplicado. Aprendizaje automático: Un enfoque probabilístico de Kevin Murphy, para comprender el campo de aprendizaje automático. Consejos prácticos para el análisis de conjuntos de datos grandes y complejos: Un enfoque de ciencia de datos sobre los conjuntos de datos. Aprendizaje profundo de Ian Goodfellow et al, para aprendizaje de modelos no lineales. Documento de Google sobre la deuda técnica, con muchos consejos generales. Documentación de Tensorflow. Agradecimientos Agradezco a David Westbrook, Peter Brandt, Samuel Ieong, Chenyu Zhao, Li Wei, Michalis Potamias, Evan Rosen, Barry Rosenberg, Christine Robson, James Pine, Tal Shaked, Tushar Chandra, Mustafa Ispir, Jeremiah Harmsen, Konstantinos Katsiapis, Glen Anderson, Dan Duckworth, Shishir Birmiwal, Gal Elidan, Su Lin Wu, Jaihui Liu, Fernando Pereira y Hrishikesh Aradhye por las numerosas correcciones, sugerencias y ejemplos útiles para este documento. Agradezco también a Kristen Lefevre, Suddha Basu y Chris Berg quienes colaboraron en una versión anterior. Cualquier error, omisión u opiniones controversiales es mi responsabilidad.\nAnexo Existen diferentes referencias a productos de Google en este documento. Para proporcionar más contexto, agregué una descripción breve de los ejemplos más comunes a continuación.\nDescripción de YouTube YouTube es un servicio de transmisión de videos. Tanto los equipos de YouTube Watch Next y de la página principal de YouTube usan modelos de AA para clasificar recomendaciones de videos. Watch Next recomienda videos para ver después del que se está reproduciendo, la página principal recomienda videos para los usuarios que exploran la página principal.\nDescripción general de Google Play Google Play tiene muchos modelos para resolver diferentes problemas. Las búsqueda de apps en Play, las recomendaciones personalizadas en la página principal de Play y \u0026ldquo;Otros usuarios también instalaron\u0026rdquo; usan aprendizaje automático.\nDescripción de Google Plus Google Plus usa aprendizaje automático en diferentes situaciones: clasificar las publicaciones en la sección \u0026ldquo;Novedades\u0026rdquo; que ve el usuario, las publicaciones en la sección \u0026ldquo;Lo más interesante\u0026rdquo; (las publicaciones que son más populares), las personas que conoces, etc.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"53458969ff468af4a6f852fed51d3d32","permalink":"https://www.marcusrb.com/cursos/data-science/intro-machine-learning/ml101-0-reglas1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/data-science/intro-machine-learning/ml101-0-reglas1/","section":"cursos","summary":"El objetivo de este documento es ayudar a aquellas personas con un conocimiento básico sobre aprendizaje automático a aprovechar las recomendaciones de Google para aprendizaje automático. Presenta un estilo para aprendizaje automático, similar a la guía de estilo de Google para C++ y otras guías populares para la programación práctica. Si tomaste clases de aprendizaje automático, desarrollaste un modelo de aprendizaje automático o trabajaste en uno, tienes el conocimiento necesario para leer este documento.","tags":null,"title":"Reglas del aprendizaje automático - Fase I","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El objetivo de la primera parte de este libro es ponerlo al día con las herramientas básicas de __ exploración de datos__ lo más rápido posible. La exploración de datos es el arte de mirar sus datos, generar hipótesis rápidamente, probarlas rápidamente y luego repetir una y otra vez. El objetivo de la exploración de datos es generar muchos clientes potenciales prometedores que luego puede explorar con mayor profundidad.\nEn esta parte del libro aprenderá algunas herramientas útiles que tienen una recompensa inmediata:\n La visualización es un excelente lugar para comenzar con la programación R, porque el la recompensa es muy clara: puedes hacer tramas elegantes e informativas que ayudan Entiendes los datos. En [visualización de datos] te sumergirás en la visualización,aprender la estructura básica de un diagrama de ggplot2 y técnicas poderosas para convirtiendo datos en tramas.\n La visualización por sí sola no suele ser suficiente, por lo que en [transformación de datos] aprenderá los verbos clave que le permiten seleccionar variables importantes, filtrar observaciones clave, crear nuevas variables y calcular resúmenes.  Finalmente, en [análisis exploratorio de datos], combinará la visualización y transformación con tu curiosidad y escepticismo para preguntar y responder Preguntas interesantes sobre los datos.\n  El modelado es una parte importante del proceso exploratorio, pero aún no tiene las habilidades para aprenderlo o aplicarlo de manera efectiva. Volveremos a ello en modelado, una vez que esté mejor equipado con más herramientas de programación y disputas de datos.\nEntre estos tres capítulos que le enseñan las herramientas de exploración hay tres capítulos que se centran en su flujo de trabajo de R. En [workflow: basics], [workflow: scripts] y [workflow: projects] aprenderá buenas prácticas para escribir y organizar su código R. Estos lo prepararán para el éxito a largo plazo, ya que le brindarán las herramientas para mantenerse organizado cuando aborde proyectos reales.\n","date":1568502000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"3fbf01b120577875ae777c0249635464","permalink":"https://www.marcusrb.com/cursos/r-studio/advanced-r/r201-program/","publishdate":"2019-09-15T00:00:00+01:00","relpermalink":"/cursos/r-studio/advanced-r/r201-program/","section":"cursos","summary":"Introducción El objetivo de la primera parte de este libro es ponerlo al día con las herramientas básicas de __ exploración de datos__ lo más rápido posible. La exploración de datos es el arte de mirar sus datos, generar hipótesis rápidamente, probarlas rápidamente y luego repetir una y otra vez. El objetivo de la exploración de datos es generar muchos clientes potenciales prometedores que luego puede explorar con mayor profundidad.","tags":null,"title":"Programa R para Data Science","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568070000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"5530e9d3deb636f72045ee194d4ee27f","permalink":"https://www.marcusrb.com/cursos/python/py101/py101-2-funciones/","publishdate":"2019-09-10T00:00:00+01:00","relpermalink":"/cursos/python/py101/py101-2-funciones/","section":"cursos","summary":"","tags":null,"title":"Python 101 - Funciones","type":"docs"},{"authors":null,"categories":null,"content":" Introducción Ahora que sabe cómo acceder y examinar un conjunto de datos, ¡está listo para escribir su primera consulta SQL! Como pronto verá, las consultas SQL lo ayudarán a clasificar un conjunto de datos masivo, para recuperar solo la información que necesita.\nComenzaremos usando las palabras clave SELECT, FROM y WHERE para obtener datos de columnas específicas según las condiciones que especifique.\nPara mayor claridad, trabajaremos con un pequeño conjunto de datos imaginario pet_records que contiene solo una tabla, llamada mascotas pets.\n   ID Name Animal     1 Dr. Harris Rabbit   2 Moon Dog   3 Ripley Cat   4 Tom Cat    SELECT\u0026hellip;.FROM La consulta SQL más básica selecciona una sola columna de una sola tabla. Para hacer esto,\n especifique la columna que desea después de la palabra SELECT, y luego especifique la tabla después de la palabra FROM. Por ejemplo, para seleccionar la columna Name (de la tabla de mascotas pets en la base de datos pet_records en el proyecto bigquery-public-data), nuestra consulta aparecerá de la siguiente manera:  Tenga en cuenta que al escribir una consulta SQL, el argumento que pasamos a FROM no está entre comillas simples o dobles (\u0026lsquo;o \u0026ldquo;). Está en comillas invertidas (`).\nWHERE \u0026hellip; Los conjuntos de datos de BigQuery son grandes, por lo que generalmente querrá devolver solo las filas que cumplan condiciones específicas. Puede hacerlo utilizando la cláusula WHERE.\nLa consulta a continuación devuelve las entradas de la columna Nombre Name que están en filas donde la columna Animal tiene el texto \u0026lsquo;Cat\u0026rsquo;.\nEjemplo: ¿Cuáles son todas las ciudades de EE. UU. En el conjunto de datos OpenAQ? Ahora que ya tiene lo básico, veamos un ejemplo con un conjunto de datos real. Utilizaremos un conjunto de datos OpenAQ sobre la calidad del aire.\nPrimero, configuraremos todo lo que necesitamos para ejecutar consultas y echar un vistazo rápido a las tablas que hay en nuestra base de datos.\nfrom google.cloud import bigquery # Create a \u0026quot;Client\u0026quot; object client = bigquery.Client() # Construct a reference to the \u0026quot;openaq\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;openaq\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref) # List all the tables in the \u0026quot;openaq\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there's only one!) for table in tables: print(table.table_id)  El conjunto de datos contiene solo una tabla, llamada global_air_quality. Buscaremos la tabla y echaremos un vistazo a las primeras filas para ver qué tipo de datos contiene.\n# Construct a reference to the \u0026quot;global_air_quality\u0026quot; table table_ref = dataset_ref.table(\u0026quot;global_air_quality\u0026quot;) # API request - fetch the table table = client.get_table(table_ref) # Preview the first five lines of the \u0026quot;global_air_quality\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  ¡Todo se ve bien! Entonces, hagamos una consulta. Supongamos que queremos seleccionar todos los valores de la columna de la ciudad city que están en filas donde la columna del país country es \u0026lsquo;US\u0026rsquo; (Para \u0026ldquo;Estados Unidos\u0026rdquo;).\n# Query to select all the items from the \u0026quot;city\u0026quot; column where the \u0026quot;country\u0026quot; column is 'US' query = \u0026quot;\u0026quot;\u0026quot; SELECT city FROM `bigquery-public-data.openaq.global_air_quality` WHERE country = 'US' \u0026quot;\u0026quot;\u0026quot;  Tómese el tiempo ahora para asegurarse de que esta consulta se alinee con lo que aprendió anteriormente.\nEnviando la consulta al conjunto de datos¶ Estamos listos para usar esta consulta para obtener información del conjunto de datos OpenAQ. Como en el tutorial anterior, el primer paso es crear un objeto Client.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Comenzamos configurando la consulta con el método query(). Ejecutamos el método con los parámetros predeterminados, pero este método también nos permite especificar configuraciones más complicadas sobre las que puede leer en la documentación. Volveremos sobre esto más tarde.\n# Set up the query query_job = client.query(query)  A continuación, ejecutamos la consulta y convertimos los resultados en un DataFrame de pandas.\n# API request - run the query, and return a pandas DataFrame us_cities = query_job.to_dataframe()  Ahora tenemos un DataFrame de pandas llamado us_cities, que podemos usar como cualquier otro DataFrame.\n# What five cities have the most measurements? us_cities.city.value_counts().head()  ","date":1568070000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"c84766e9f905480b508b10863a921fc0","permalink":"https://www.marcusrb.com/cursos/business-analytics/intro-sql/sql101-1-select/","publishdate":"2019-09-10T00:00:00+01:00","relpermalink":"/cursos/business-analytics/intro-sql/sql101-1-select/","section":"cursos","summary":"Introducción Ahora que sabe cómo acceder y examinar un conjunto de datos, ¡está listo para escribir su primera consulta SQL! Como pronto verá, las consultas SQL lo ayudarán a clasificar un conjunto de datos masivo, para recuperar solo la información que necesita.\nComenzaremos usando las palabras clave SELECT, FROM y WHERE para obtener datos de columnas específicas según las condiciones que especifique.\nPara mayor claridad, trabajaremos con un pequeño conjunto de datos imaginario pet_records que contiene solo una tabla, llamada mascotas pets.","tags":null,"title":"Select, From \u0026 Where","type":"docs"},{"authors":null,"categories":null,"content":" R es un programa modular. Hay muchas funciones disponibles en la distribución estándar (consulte Instalación de R), pero se pueden agregar muchas más gracias a paquetes y complementos adicionales.\nInstalación La forma más fácil de instalar paquetes, con una conexión a Internet activa, es a través del menú R: Paquetes → Instalar paquetes.\nA veces, el funcionamiento de un paquete depende de la presencia de otros paquetes (dependencias). Para asegurarse de instalar tanto el paquete como las dependencias, es preferible usar el comando:\ninstall.packages(\u0026quot;Rcmdr\u0026quot;, dependencies = TRUE)  Uso Los paquetes adicionales no se activan automáticamente cuando se abre R, pero deben \u0026ldquo;llamarse\u0026rdquo; con la biblioteca de comandos (). Por ejemplo, para usar RCommander:\nlibrary(Rcmdr)  Al llamar a un paquete, si faltan las dependencias (cualquier paquete del que dependa), se abrirá un cuadro de diálogo que le permitirá descargarlo e instalarlo automáticamente.\nTambién se pueden llamar paquetes desde el menú de la consola: Paquetes → Cargar paquete.\nActualización Periódicamente, es necesario verificar la existencia de versiones más actualizadas de los paquetes R. Es preferible ejecutar este procedimiento desde la ventana R, y haber cerrado RCommander, que a su vez debe actualizarse.\nPara actualizar paquetes en Windows, debe ejecutar R desde los administradores.\nEl comando para actualizar los paquetes está en el menú Paquetes (Actualizar paquetes \u0026hellip;), y el procedimiento es completamente automático: solo siga las instrucciones en pantalla.\nTambién es posible actualizar los paquetes escribiendo el comando\nupdate.packages()  Comandos principales Para saber qué paquetes están instalados en su sistema:\nlibrary()  Para cargar la ayuda del paquete:\nhelp(package)  Para eliminar el paquete del sistema\nremove.packages( \u0026quot;\\_nombre_paquete\\_\u0026quot;)  Para averiguar dónde está instalado un archivo de paquete:\nsystem.file(package = \u0026quot;nombre_paquete\u0026quot;)  Para aprender más\nTodos los paquetes disponibles\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"4b1c5ae56b8bdfe426863d64b315d892","permalink":"https://www.marcusrb.com/cursos/r-studio/paquetes/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/r-studio/paquetes/","section":"cursos","summary":"R es un programa modular. Hay muchas funciones disponibles en la distribución estándar (consulte Instalación de R), pero se pueden agregar muchas más gracias a paquetes y complementos adicionales.\nInstalación La forma más fácil de instalar paquetes, con una conexión a Internet activa, es a través del menú R: Paquetes → Instalar paquetes.\nA veces, el funcionamiento de un paquete depende de la presencia de otros paquetes (dependencias). Para asegurarse de instalar tanto el paquete como las dependencias, es preferible usar el comando:","tags":null,"title":"Paquetes","type":"docs"},{"authors":null,"categories":null,"content":" Fase II de aprendizaje automático: Ingeniería de atributos En la primera fase del ciclo de vida de un sistema de aprendizaje automático, la prioridad es mandar los datos de entrenamiento al sistema de aprendizaje, lograr instrumentar las métricas de interés y crear una infraestructura de publicación. Una vez que cuentas con un sistema integral en funcionamiento con pruebas de unidades y del sistema instrumentadas, comienza la fase II.\nEn la segunda fase, hay muchas recompensas a corto plazo. Existe una variedad de atributos obvios que se pueden agregar al sistema. Además, la segunda fase de aprendizaje automático implica agregar tantos atributos como sea posible y combinarlos de formas intuitivas. Durante esta fase, todas las métricas deben continuar subiendo. Habrá muchos lanzamientos, y es una excelente oportunidad para incorporar muchos ingenieros que puedan recopilar todos los datos que necesitas para crear un sistema de aprendizaje verdaderamente sorprendente.\nRegla n.º 16: Planifica el lanzamiento y la iteración. No esperes que el modelo en el que trabajas ahora sea el último que lanzarás o, incluso, que dejarás de lanzar modelos. Por lo tanto, ten en cuenta que la complejidad de este lanzamiento retrasará los lanzamientos futuros. Muchos equipos han lanzado uno o más modelos por trimestre durante años. Existen tres razones básicas para lanzar modelos nuevos:\nTienes atributos nuevos. Deseas ajustar la regularización y combinar atributos antiguos de formas nuevas. Deseas ajustar el objetivo. Independientemente de la razón, es recomendable poner atención en el modelo: analizar los datos que se ingresan en el ejemplo te permite encontrar señales nuevas y detectar señales antiguas con problemas. Entonces, a medida que desarrollas el modelo, piensa en lo fácil que es agregar, quitar o recombinar atributos. Piensa en lo fácil que es crear una copia nueva de la canalización y verifica que sea correcta. Piensa en si es posible ejecutar dos o tres copias en paralelo. Por último, no te preocupes si no logras incluir todos los atributos en esta versión de la canalización. Las incluirás el próximo trimestre.\nRegla n.º 17: Comienza con los atributos directamente observados e informados, en lugar de los atributos aprendidos. Este puede ser un punto controversial, pero evita muchos problemas. Primero, describamos qué es un atributo aprendido. Un atributo aprendido es un atributo generado por un sistema externo (como un sistema de agrupación en clústeres sin supervisar) o el mismo modelo (por ejemplo, mediante un modelo factorizado o aprendizaje profundo). Ambas opciones pueden ser útiles, pero tienen muchos problemas, por lo que no es conveniente incluirlas en el primer modelo.\nSi usas un sistema externo para crear un atributo, recuerda que ese sistema tiene su propio objetivo. Es posible que el objetivo del sistema externo no tenga mucha relación con tu objetivo actual. Si realizas una instantánea del sistema externo, es posible que esté desactualizada. Si actualizas los atributos desde el sistema externo, es posible que los significados cambien. Si usas un sistema externo para proporcionar un atributo, debes tener mucho cuidado con ese enfoque.\nEl principal problema de los modelos multiplicados y los modelos profundos es que no son convexos. Por lo tanto, no hay garantía de encontrar una solución óptima (o de aproximarse a esta). Además, el mínimo local de cada iteración puede ser diferente. Esta variación no permite juzgar con exactitud si el impacto de un cambio en tu sistema es relevante o contingente. Si creas un modelo sin atributos profundos, obtienes un sistema de referencia con buen rendimiento. Una vez que alcanzas este punto de referencia, puedes probar enfoques menos ortodoxos.\nRegla n.º 18: Prueba atributos de contenido que generalicen en contextos. A menudo, un sistema de aprendizaje automático es una parte pequeña de una entidad mucho más grande. Por ejemplo, si imaginas una publicación que se puede usar en Lo Más Interesante, mucha gente hará +1, la compartirá o escribirá un comentario en ella antes de que llegue a mostrarse en Lo más interesante. Si proporcionas esas estadísticas al modelo, este puede promocionar publicaciones nuevas para las que no tiene datos en el contexto que está optimizando. YouTube Watch Next puede usar la cantidad de videos que miraste, o que miraste de forma secuencial (la cantidad de veces que se miró un video después de otro) en la búsqueda de YouTube. También puedes usar clasificaciones de usuarios explícitas. Por último, si usas una acción de usuario como etiqueta, ver esa acción en el documento en un contexto diferente puede ser un excelente atributo. Todos estos atributos te permiten aportar contenido nuevo al contexto. Ten en cuenta que esto no se trata de personalización: primero, descubre si a alguien le gusta el contenido en este contexto; luego, descubre a quién le gusta más o menos.\nRegla n.º 19: Usa atributos muy específicos cuando sea posible. Gracias a la gran cantidad de datos, es más fácil aprender millones de atributos sencillos que unos pocos atributos complejos. Los identificadores de documentos que se obtienen y las consultas canónicas no brindan mucha generalización, pero alinean las clasificaciones con las etiquetas en las consultas principales. Por lo tanto, no temas agrupar atributos si cada uno se aplica a una fracción muy pequeña de datos, pero la cobertura total es de más del 90%. Puedes usar la regularización para eliminar los atributos que se aplican a pocos ejemplos.\nRegla n.º 20: Combina y modifica los atributos existentes para crear atributos nuevos de una forma legible. Hay diferentes formas de combinar y modificar atributos. Los sistemas de aprendizaje automático como TensorFlow te permiten preprocesar los datos mediante transformaciones. Los dos enfoques estándar son las \u0026ldquo;discretizaciones\u0026rdquo; y las \u0026ldquo;combinaciones\u0026rdquo;.\nLa discretización consiste en tomar un atributo continuo y crear varios atributos discretos. Considera un atributo continuo como la edad. Puedes crear un atributo que es 1 cuando la edad es menor de 18, otro atributo que es 1 cuando la edad es entre 18 y 35, etc. No pienses demasiado en los límites de estos histogramas; los cuantiles básicos serán los más eficaces.\nLas combinaciones unen dos o más columnas de atributos. En la terminología de TensorFlow, una columna de atributos es un conjunto de atributos homogéneos (p. ej., {masculino, femenino}, {EE.UU., Canadá, México}, etc.). Una combinación es una nueva columna de atributos que incluye, p. ej., {masculino, femenino} × {EE.UU., Canadá, México}. Esta nueva columna de atributos contendrá el atributo (masculino, Canadá). Si usas TensorFlow y le indicas que cree esta combinación, este atributo (masculino, Canadá) aparecerá en los ejemplos que representan canadienses masculinos. Ten en cuenta que se necesita una enorme cantidad de datos para aprender modelos con combinaciones de tres, cuatro o más columnas de atributos básicos.\nLas combinaciones que producen columnas de atributos muy grandes pueden producir un sobreajuste. Por ejemplo, imagina que haces algún tipo de búsqueda y tienes una columna de atributos con palabras en la consulta y otra con palabras en el documento. Si unes estas columnas con una combinación, terminarás con muchos atributos (consulta la regla n.º 21).\nCuando trabajas con texto, existen dos alternativas. La más rigurosa es un producto escalar. En su forma más simple, un producto escalar simplemente cuenta la cantidad de palabras en común entre la consulta y el documento. Por lo tanto, este atributo se puede discretizar. Otro enfoque es una intersección: tenemos un atributo que está presente solo si la palabra \u0026ldquo;poni\u0026rdquo; aparece en el documento y en la consulta, y otro atributo que está presente solo si la palabra \u0026ldquo;el\u0026rdquo; aparece en el documento y en la consulta.\nRegla n.º 21: La cantidad de ponderaciones de atributos que puedes aprender en un modelo lineal es casi proporcional a la cantidad de datos que tienes. Existen resultados teóricos fascinantes sobre aprendizaje estadístico relacionados con el nivel de complejidad correspondiente de un modelo, pero esta regla es todo lo que necesitas saber. Tuve discusiones con personas que ponían en duda que se pudiera aprender algo con mil ejemplos o que nunca se necesitan más de un millón de ejemplos, porque están empecinados con un determinado método de aprendizaje. La clave es escalar el aprendizaje a la medida del tamaño de los datos:\nSi trabajas con un sistema de ranking de búsquedas, existen millones de palabras diferentes en los documentos y la consulta, y tienes 1000 ejemplos etiquetados, debes usar un producto escalar entre los atributos de consultas y de documentos, TF-IDF y una media docena de otros atributos desarrollados por humanos. 1000 ejemplos, una docena de atributos. Si tienes un millón de ejemplos, intersecta la columna de atributos de consultas con la de documentos, y aplica regularización y, posiblemente, selección de atributos. Esto te brindará un millón de atributos, pero, con la regularización, tendrás menos. 10 millones de ejemplos, tal vez 100,000 atributos. Si tienes miles o cientos de miles de millones, puedes combinar las columnas de atributos con tokens de consultas y de documentos mediante la regularización y la selección de atributos. Tendrás miles de millones de ejemplos y 10 millones de atributos. La teoría de aprendizaje estadístico raramente establece límites rígidos, pero ofrece un buen punto de partida. En última instancia, usa la regla n.º 28 para decir qué atributos usarás.\nRegla n.º 22: Quita los atributos que ya no uses. Los atributos sin usar crean deuda técnica. Si descubres que no estás usando un atributo y que no sirve combinarlo con otros atributos, quítalo de la infraestructura. Debes mantener limpia tu infraestructura para que puedas probar los atributos más prometedores tan rápido como sea posible. Si es necesario, tu atributo se puede volver a agregar en cualquier momento.\nTen en cuenta la cobertura cuando analices qué atributos agregarás o conservarás. ¿Cuántos ejemplos cubre el atributo? Por ejemplo, si tienes algunos atributos de personalización, pero solo el 8% de los usuarios tiene atributos de personalización, eso no será muy eficaz.\nAl mismo tiempo, algunos atributos te sorprenden gratamente. Por ejemplo, si tienes un atributo que cubre solo el 1% de los datos, pero el 90% de los ejemplos de este atributo son positivos, es un excelente atributo que agregar.\nAnálisis humano del sistema Antes de avanzar a la tercera fase de aprendizaje automático, es importante hablar de algo que no se enseña en ninguna clase de aprendizaje automático: cómo analizar un modelo existente y mejorarlo. Esto es más un arte que una ciencia. Aun así, existen muchos antipatrones que es conveniente evitar.\nRegla n.º 23: No eres el típico usuario final. Probablemente, esta es la razón más común por la que un equipo no progresa. Si bien usar un prototipo con tu equipo o usar un prototipo en tu empresa tiene muchos beneficios, los empleados deben analizar si el rendimiento es correcto. Si bien un cambio que sea evidentemente malo no debe usarse, cualquier función que parezca lista para la producción debe probarse aún más, ya sea contratando a gente común para que responda preguntas en una plataforma de participación colectiva o mediante un experimento en vivo con usuarios reales.\nHay dos razones para ello. Estás demasiado familiarizado con el código. Es posible que busques un aspecto específico de las publicaciones, o que estés muy involucrado (p. ej., sesgo de confirmación). La segunda razón es que tu tiempo es demasiado valioso. Considera el costo de nueve ingenieros en una reunión de una hora de duración y cuántas etiquetas logradas con trabajo humano puedes obtener en una plataforma de participación colectiva.\nSi realmente quieres comentarios de usuarios, implementa metodologías de experiencia de usuario. Crea usuarios persona (puedes encontrar una descripción en el libro Sketching User Experiences [Cómo diseñar experiencias de usuario] de Bill Buxton) al comienzo del proceso y, luego, implementa una prueba de usabilidad (puedes encontrar una descripción en el libro Don’t Make Me Think [No me hagas pensar] de Steve Krug). Los usuarios persona implican crear un usuario hipotético. Por ejemplo, si tu equipo se compone solo por hombres, será beneficioso diseñar un usuario persona femenina de 35 años (completa con atributos de usuario) y observar los resultados que genera, en lugar de los 10 resultados para hombres de 25 a 40 años. También puedes lograr una nueva perspectiva al evaluar cómo reaccionan las personas reales a tu sitio (de forma local o remota) en una prueba de usabilidad.\nRegla n.º 24: Mide el delta entre los modelos. Una de las formas más sencillas y, a veces, más útiles de realizar mediciones que puedes usar antes de que los usuarios vean tu nuevo modelo es calcular la diferencia entre los nuevos resultados y los obtenidos con el sistema en producción. Por ejemplo, si tienes un problema de ranking, ejecuta ambos modelos con una muestra de consultas en todo el sistema y observa el tamaño de la diferencia simétrica de los resultados (ponderados según la posición en el ranking). Si la diferencia es muy pequeña, entonces puedes deducir que habrá poco cambio, sin necesidad de ejecutar un experimento. Si la diferencia es muy grande, entonces debes asegurarte de que el cambio sea positivo. Analizar las consultas donde la diferencia simétrica es alta te puede ayudar a comprender de forma cualitativa cómo fue el cambio. Sin embargo, asegúrate de que el sistema sea estable. Cuando compares un modelo consigo mismo, asegúrate de que tenga una diferencia simétrica baja (idealmente cero).\nRegla n.º 25: Cuando elijas un modelo, el rendimiento utilitario predomina por sobre el poder de predicción. Tu modelo puede intentar predecir la tasa de clics. Sin embargo, en última instancia, la pregunta clave es lo que haces con esa predicción. Si la usas para clasificar documentos, entonces la calidad del ranking final importa más que la predicción en sí misma. Si predices la probabilidad de que un documento sea spam y, luego, tienes un punto límite sobre lo que se bloquea, entonces la precisión de lo que se permite tiene más importancia. La mayoría de las veces, estos dos aspectos coinciden; cuando no es así, es probable que haya una pequeña ganancia. Además, si hay algún cambio que mejore la pérdida logística, pero que reduzca el rendimiento del sistema, busca otro atributo. Si esto comienza a suceder con más frecuencia, es hora de volver a evaluar el objetivo del modelo.\nRegla n.º 26: Busca patrones en los errores observados y crea atributos nuevos. Supongamos que ves un ejemplo de entrenamiento que el modelo \u0026ldquo;no entendió\u0026rdquo;. En una tarea de clasificación, este error puede ser un falso positivo o un falso negativo. En una tarea de clasificación, el error puede ser un par donde un positivo tiene un ranking menor que un negativo. El punto más importante es que sea un ejemplo que el sistema de aprendizaje automático sepa que no lo entendió y que lo corrija si tiene la oportunidad. Si le agregas un atributo al modelo para que pueda corregir el error, el modelo intentará usarlo.\nPor otro lado, si intentas crear un atributo basado en ejemplos que el sistema no considera errores, el atributo se ignorará. Por ejemplo, supongamos que, en la búsqueda de apps de Play, alguien busca \u0026ldquo;juegos gratuitos\u0026rdquo;. Supongamos que uno de los primeros resultados es una app de bromas menos relevante. Entonces, creas un atributo para \u0026ldquo;apps de bromas\u0026rdquo;. Sin embargo, si maximizas la cantidad de instalaciones y las personas instalan una app de bromas cuando buscan juegos gratuitos, el atributo para \u0026ldquo;apps de bromas\u0026rdquo; no tendrá el efecto deseado.\nUna vez que tengas ejemplos que el modelo no haya entendido, busca las tendencias que estén fuera del conjunto de atributos actual. Por ejemplo, si parece que el sistema penaliza las publicaciones más largas, agrega la longitud de la publicación. No seas demasiado específico sobre los atributos que agregues. Si agregas la longitud de la publicación, no intentes adivinar qué significa \u0026ldquo;largo\u0026rdquo;, solo agrega una decena de atributos y permite que el modelo descubra qué hacer con ellos (consulta la regla n.º 21). Esta es la forma más fácil de obtener el resultado deseado.\nRegla n.º 27: Intenta cuantificar el comportamiento no deseado que observes. Algunos miembros de tu equipo comenzarán a frustrarse con las propiedades del sistema que no les gusten, ya que la función de pérdida existente no las captura. En este punto, deben hacer lo que sea necesario para convertir sus quejas en números sólidos. Por ejemplo, si consideran que se muestran demasiadas \u0026ldquo;apps de bromas\u0026rdquo; en la búsqueda de Play, se pueden contratar a evaluadores humanos para que identifiquen las apps de bromas. (Puedes usar datos etiquetados por humanos en este caso, ya que una fracción relativamente pequeña de consultas representan una gran fracción del tráfico). Si los problemas se pueden medir, puedes comenzar a usarlos como atributos, objetivos o métricas. La regla general es \u0026ldquo;medir primero, optimizar después\u0026rdquo;.\nRegla n.º 28: Ten en cuenta que el comportamiento idéntico a corto plazo no implica un comportamiento idéntico a largo plazo. Imagina que tienes un sistema nuevo que analiza cada doc_id y exact_query, y luego calcula la probabilidad de clic para cada documento y cada consulta. Descubres que este comportamiento es casi idéntico a tu sistema actual en la comparación y la prueba A/B; por lo tanto, dado su simplicidad, lo ejecutas. Sin embargo, notas que no se muestran apps nuevas. ¿Por qué? Bueno, dado que tu sistema solo muestra un documento basado en su propio historial con esa consulta, no hay forma de aprender qué documento nuevo debe mostrar.\nLa única forma de entender cómo debe funcionar un sistema a largo plazo es entrenarlo solo con datos adquiridos cuando el modelo está publicado. Esto es muy difícil.\nDesviación entre el entrenamiento y la publicación La desviación entre el entrenamiento y la publicación es la diferencia entre el rendimiento del entrenamiento y el de la publicación. Existen diferentes razones para esta desviación:\nuna discrepancia entre cómo manipulas los datos en las canalizaciones de entrenamiento y del servidor un cambio en los datos entre el momento del entrenamiento y el del servidor un ciclo de retroalimentación entre el modelo y el algoritmo Hemos observado sistemas de aprendizaje automático de producción en Google con una desviación entre el entrenamiento y la publicación que afecta negativamente el rendimiento. La mejor solución es supervisarlo de forma explícita para que los cambios en el sistema y en los datos no generen una desviación inadvertida.\nRegla n.º 29: La mejor manera de asegurarte de que el entrenamiento se asemeja a la publicación es guardar el conjunto de atributos que usas en la publicación y canalizar esos atributos en un registro para luego usarlos en el entrenamiento. Incluso si no lo puedes hacer para cada ejemplo, hazlo para una fracción pequeña, de forma tal que puedas verificar la coherencia entre la publicación y el entrenamiento (consulta la regla n.º 37). En Google, los equipos que hicieron esta medición se sorprendieron a menudo por los resultados. La página de inicio de YouTube cambió a atributos del registro en el servidor, lo que generó mejoras de calidad importantes y redujo la complejidad del código. Ahora, muchos equipos están cambiando sus infraestructuras.\nRegla n.º 30: ¡Realiza muestras con datos con ponderación por importancia, no los quites! Cuando tienes muchos datos, es tentador usar los archivos del 1 al 12 e ignorar los archivos del 13 al 99. Esto es un error. Si bien se pueden quitar los datos que nunca se mostraron al usuario, la ponderación por importancia es la mejor opción para el resto. La ponderación por importancia implica que, si decides hacer una muestra con el ejemplo X con una probabilidad del 30%, la ponderación será de 10\u0026frasl;3. Con la ponderación por importancia, se mantienen todas las propiedades de calibración que analizamos en la regla n.º 14.\nRegla n.º 31: Ten en cuenta que, si cruzas datos de una tabla durante el entrenamiento y la publicación, los datos en la tabla pueden cambiar. Digamos que cruzas ID de documentos a una tabla que contiene atributos para esos documentos (como la cantidad de comentarios o clics). Entre el entrenamiento y el servidor, es posible que cambien los atributos en la tabla. Por lo tanto, puede cambiar predicción del modelo para el mismo documento entre el entrenamiento y la publicación. La forma más sencilla de evitar este tipo de problemas es registrar los atributos durante la publicación (consulta la regla n.º 32). Si la tabla cambia lentamente, puedes tomar una instantánea de la tabla a cada hora o cada día para obtener datos razonablemente cercanos. Ten en cuenta que esto no resuelve completamente el problema.\nRegla n.º 32: Reutiliza el código entre la canalización de entrenamiento y la canalización del servidor, siempre que sea posible. El procesamiento por lotes es diferente al procesamiento en línea. En el procesamiento en línea, debes responder cada solicitud a medida que llega (p. ej., debes realizar una búsqueda separada para cada consulta). En el procesamiento por lotes, puedes combinar tareas (p. ej., unir funciones). En la publicación, implementas el procesamiento en línea, mientras que el entrenamiento es una tarea de procesamiento por lotes. Sin embargo, hay varias formas de reutilizar el código. Por ejemplo, puedes crear un objeto que sea específico para tu sistema, donde el resultado de cualquier consulta o unión se puede almacenar de una forma legible y donde los errores se pueden probar fácilmente. Luego, una vez que hayas reunido toda la información, durante el entrenamiento o la publicación, ejecutas un método común para conectar el objeto legible específico del sistema y el formato que espera el sistema de aprendizaje automático. Esto elimina una fuente de desviación entre el entrenamiento y la publicación. Como corolario, intenta no usar dos lenguajes de programación diferentes entre el entrenamiento y la publicación. Si haces esto, será casi imposible compartir el código.\nRegla n.º 33: Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. En general, mide el rendimiento de un modelo con datos reunidos en forma posterior a aquellos con los que se ha entrenado el modelo, ya que refleja de forma más precisa qué hará el sistema en la producción. Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. El rendimiento no debería ser tan bueno en los datos nuevos, pero no debería ser mucho peor. Como puede haber efectos diarios, es posible que no predigas la tasa de clics promedio o la tasa de conversión, pero el área bajo la curva, que representa la posibilidad de darle una puntuación más alta al ejemplo positivo que al ejemplo negativo, debería ser razonablemente parecida.\nRegla n.º 34: En la clasificación binaria para filtrado (como la detección de spam o la identificación de correos electrónicos de interés), realiza pequeños sacrificios a corto plazo en el rendimiento para lograr datos más claros. En la tarea de filtrado, los ejemplos que se marcan como negativos no se muestran al usuario. Supongamos que tienes un filtro que bloquea el 75% de los ejemplos negativos durante la publicación. Puede surgir la tentación de obtener más datos de entrenamiento a partir de las instancias que se muestran a los usuarios. Por ejemplo, si un usuario marca como spam un correo electrónico que permitió tu filtro, se puede aprender de esta acción.\nPero este enfoque introduce un sesgo en la muestra. Puedes obtener datos más claros si etiquetas el 1% de todo el tráfico como \u0026ldquo;retenido\u0026rdquo; durante la publicación y envías todos los ejemplos retenidos al usuario. Ahora, el filtro bloqueará al menos el 74% de los ejemplos negativos. Los ejemplos retenidos se convertirán en los datos de entrenamiento.\nSi el filtro bloquea el 95% o más de los ejemplos negativos, este enfoque se hace menos viable. Aun así, si deseas medir el rendimiento en la publicación, puedes hacer una muestra todavía más pequeña (p. ej., 0.1% o 0.001%). Diez mil ejemplos son suficientes para estimar el rendimiento de forma precisa.\nRegla n.º 35: Ten en cuenta la desviación inherente a los problemas de ranking. Si cambias el algoritmo de clasificación lo suficiente como para ver resultados diferentes, habrás logrado modificar los datos que el algoritmo verá en el futuro. Este tipo de desviación aparecerá, y debes tenerla en cuenta al diseñar el modelo. Existen varias estrategias diferentes que sirven para favorecer los datos que tu modelo ya vio.\nPermite tener una regularización más alta en los atributos que cubren más consultas, a diferencia de esos atributos que solo abarcan una consulta. De esta forma, el modelo favorecerá los atributos que son específicos a una o pocas consultas por sobre los atributos que se generalizan a todas las consultas. Esta estrategia permite evitar que los resultados muy populares acaben en consultas irrelevantes. Este enfoque es contrario a la sugerencia más tradicional de contar con más regularización en columnas de funciones con más valores únicos. Permite que los atributos solo tengan ponderaciones positivas. Además, cualquier atributo bueno será mejor que uno \u0026ldquo;desconocido\u0026rdquo;. No uses atributos asociados solo al documentos. Esto es una versión extrema de la regla n.º 1. Por ejemplo, incluso si una app determinada es una descarga popular, más allá de la consulta, no es necesario mostrarla en todos lados. Esto se simplifica al no tener atributos solo de documentos. La razón por la que no deseas mostrar una app popular específica en todos lados está relacionada con la importancia de lograr que las apps que deseas estén disponibles. Por ejemplo, si alguien busca \u0026ldquo;apps para observar pájaros\u0026rdquo;, es posible que descarguen \u0026ldquo;Angry birds\u0026rdquo;, pero de forma claramente accidental. Si se muestra esta app, es posible que mejore la tasa de descarga, pero no se cumplen con las necesidades del usuario. Regla n.º 36: Evita los ciclos de retroalimentación con atributos posicionales. La posición del contenido afecta enormemente la probabilidad de que el usuario interactúe con este. Si ubicas una app en la primera posición, se seleccionará con más frecuencia y te dará la impresión de que es más probable que se seleccione. Una forma de lidiar con eso es agregar atributos de posición, es decir, atributos sobre la posición del contenido en la página. Entrena el modelo con atributos de posición para que aprenda a darle una mayor ponderación al atributo \u0026ldquo;primera posición\u0026rdquo;, por ejemplo. Así, el modelo les asigna una ponderación menor a otros factores con ejemplos de \u0026ldquo;primera posición=verdadero\u0026rdquo;. Entonces, en la publicación, no asignas ninguna instancia al atributo de posición (o le asignas el mismo atributo predeterminado), porque calificas candidatos antes de que hayas decidido el orden en el que se mostrarán.\nTen en cuenta que es importante mantener cualquier atributo de posición separado del resto del modelo, debido a la asimetría entre el entrenamiento y la prueba. Lo ideal es que el modelo sea la suma de una función de los atributos posicionales y una función del resto de atributos. Por ejemplo, no cruces los atributos de posición con cualquier atributo de documentos.\nRegla n.º 37: Mide la desviación entre el entrenamiento y la publicación. En el sentido más general, existen diversas razones para la desviación. Además, se puede dividir en varias partes:\nLa diferencia entre el rendimiento en los datos de entrenamiento y los datos retenidos. En general, esta diferencia siempre existe y no siempre es negativa. La diferencia entre el rendimiento en los datos retenidos y los datos \u0026ldquo;del día siguiente\u0026rdquo;. De nuevo, esta diferencia siempre existe. Debes ajustar la regularización para maximizar el rendimiento del día siguiente. Sin embargo, caídas notables en el rendimiento entre los datos retenidos y los datos del día siguiente pueden ser un indicador de que algunos atributos dependen del tiempo y posiblemente afecten al rendimiento del modelo de forma negativa. La diferencia entre el rendimiento en los datos del día siguiente y los datos en vivo. Si aplicas un modelo a un ejemplo en los datos de entrenamiento y el mismo ejemplo en la publicación, deberías obtener el mismo resultado (consulta la regla n.º 5). Por lo tanto, si aparece una discrepancia, probablemente indique un error de ingeniería. Fase III de aprendizaje automático: Crecimiento reducido, refinamiento de la optimización y modelos complejos Existen ciertos indicios de que la segunda fase llega a su fin. Primero, las ganancias mensuales comienzan a disminuir. Las métricas comenzarán a emparejarse: en algunos experimentos, verás que algunas aumentan y otras disminuyen. Este es un momento interesante. Dado que las ganancias son más difíciles de obtener, el aprendizaje automático debe sofisticarse aún más. Una advertencia: esta sección contiene más reglas especulativas que las secciones anteriores. Hemos visto a muchos equipos realizar grandes progresos en la Fase I y la Fase II de aprendizaje automático. Una vez que alcanzan la Fase III, los equipos deben buscar su propio camino.\nRegla n.º 38: No pierdas tiempo en nuevos atributos si los objetivos no alineados son un problema. Cuando las mediciones comienzan a estabilizarse, el equipo comenzará a buscar problemas fuera del alcance de los objetivos de tu sistema de aprendizaje automático actual. Como indicamos anteriormente, si el objetivo algorítmico existente no abarca los propósitos del producto, debes cambiar el objetivo o los propósitos. Por ejemplo, puedes optimizar clics, +1 o descargas, pero toma las decisiones de lanzamiento según los evaluadores humanos.\nRegla n.º 39: Las decisiones de lanzamiento representan los objetivos a largo plazo del producto. Alice tiene una idea para reducir la pérdida logística de las instalaciones predichas. Agrega un atributo. Se reduce la pérdida logística. Cuando hace un experimento en vivo, observa un aumento en la tasa de instalación. Sin embargo, cuando realiza una reunión para evaluar el lanzamiento, alguien menciona que la cantidad de usuarios activos diarios cayó un 5%. El equipo decide no lanzar el modelo. Alice está decepcionada, pero ahora se da cuenta de que las decisiones de lanzamiento dependen de varios factores y solo algunos de ellos se pueden optimizar directamente con AA.\nLa verdad es que el mundo real no es un juego de rol; no hay \u0026ldquo;puntos de ataque\u0026rdquo; que indican el estado de tu producto. El equipo debe usar las estadísticas que reúne para intentar predecir de forma eficaz qué tan bueno será el sistema en el futuro. Deben preocuparse por la participación, el número de usuarios activos en un día (DAU), el número de usuarios activos en 30 días (30 DAU), la ganancia y el retorno de la inversión del anunciante. Estas métricas que se miden en las pruebas A/B representan los objetivos a largo plazo: satisfacer a los usuarios, aumentar la cantidad de usuarios, satisfacer a los socios y obtener ganancias. A su vez, puedes considerar estos propósitos como representantes de otros propósitos: lograr un producto útil y de calidad, y que la empresa prospere de aquí a cinco años.\nLas únicas decisiones de lanzamiento fáciles son cuando todas las métricas mejoran (o al menos no empeoran). Si el equipo puede elegir entre un algoritmo de aprendizaje automático sofisticado o una simple heurística (que funciona mejor en todas las métricas), debe elegir la heurística. Además, no existe una clasificación explícita para todos los valores de métricas posibles. En especial, considera estos dos escenarios siguientes:\nExperimento Usuarios activos por día Ganancia/día A 1 millón $4 millones B 2 millones $2 millones Si el sistema actual es A, entonces es poco probable que el equipo cambie a B. Si el sistema actual es B, entonces es poco probable que el equipo cambie a A. Esto parece contradecir el comportamiento racional; sin embargo, las predicciones de las métricas cambiantes pueden o no cumplirse. Por lo tanto, cada cambio conlleva un riesgo grande. Cada métrica abarca una cierta cantidad de riesgo que preocupa al equipo.\nAdemás, ninguna métrica representa la preocupación máxima del equipo, \u0026ldquo;¿dónde estará mi producto de aquí a cinco años?\u0026rdquo;.\nPor otro lado, las personas tienden a favorecer un objetivo que pueden optimizar directamente. La mayoría de las herramientas de aprendizaje automático favorecen dicho entorno. Un ingeniero agregando atributos nuevos puede lograr un flujo constante de lanzamiento en dicho entorno. Existe un tipo de aprendizaje automático, el aprendizaje de multiobjetivo, que comienza a resolver este problema. Por ejemplo, se puede formular un problema de satisfacción de restricciones con límites inferiores en cada métrica y optimiza alguna combinación lineal de las métricas. Pero, aun así, no todas las métricas se enmarcan fácilmente como objetivos de aprendizaje automático: si el usuario hace clic en un documento o instala una app, se debe a que se mostró el contenido. Pero es mucho más difícil determinar la razón por la que un usuario visita tu sitio. Cómo predecir el éxito futuro de un sitio de forma integral depende IA-completo: es tan difícil como la visión por computadora o el procesamiento de lenguajes naturales.\nRegla n.º 40: Mantén las combinaciones simples. Los modelos unificados que aceptan atributos sin procesar y clasifican contenido directamente son los modelos más sencillos de depurar y comprender. Sin embargo, una combinación de modelos (un modelo que combina los resultados de otros modelos) puede funcionar mejor. Para mantener las cosas simples, cada modelo debe ser una combinación de modelos que solo acepta como entrada el resultado de otros modelos o un modelo básico que acepta muchos atributos, pero no ambos. Si tienes modelos sobre otros modelos que se entrenan de forma separada y los combinas, se puede generar un comportamiento erróneo.\nUsa un modelo simple como combinación, que solo acepte los resultados de los modelos \u0026ldquo;básicos\u0026rdquo; como entradas. También debes implementar propiedades en esos modelos de conjuntos. Por ejemplo, un aumento en el resultado generado por un modelo básico no debe reducir el resultado del combinado. Además, es mejor que los modelos entrantes se puedan interpretar de forma semántica (p. ej., calibrados), para que los cambios en los modelos subyacentes no confundan al modelo combinado. Asegúrate también que un aumento en la probabilidad predicha de un clasificador subyacente no reduzca la probabilidad predicha del conjunto.\nRegla n.º 41: Cuando el rendimiento se estanque, busca nuevas fuentes de información de forma cualitativa para agregar, en lugar de refinar las señales existentes. Agregaste cierta información demográfica sobre el usuario. Agregaste cierta información sobre las palabras en el documento. Finalizaste la exploración de combinación de atributos y ajustaste la regularización. No observaste un lanzamiento con más de un 1% de mejora en las métricas clave, en varios trimestres. ¿Ahora qué?\nEs hora de desarrollar la infraestructura para atributos radicalmente diferentes, como el historial de los documentos a los que este usuario accedió en el último día, semana o año, o los datos de una propiedad diferente. Usa entidades de wikidatos o algún recurso interno de tu empresa (como el Gráfico de conocimiento de Google). Usa el aprendizaje profundo. Comienza a ajustar tus expectativas sobre el retorno de la inversión esperado y aumenta tus esfuerzos adecuadamente. Como en cualquier proyecto de ingeniería, debes comparar el beneficio de agregar nuevos atributos con el costo de una mayor complejidad.\nRegla n.º 42: No esperes que la diversidad, la personalización o la relevancia se correlacionen con la popularidad. La diversidad en un conjunto de contenidos puede significar muchas cosas; la más común es la diversidad de la fuente del contenido. La personalización implica que cada usuario obtiene sus propios resultados. La relevancia implica que los resultados para una consulta específica son más apropiados para esa consulta que para otra. Además, por definición, estas tres propiedades se diferencian de lo común.\nEl problema es que lo común tiende a ser difícil de superar.\nTen en cuenta que tu sistema mide clics, el tiempo dedicado, reproducciones, +1, veces que se comparte el contenido, etc., es decir, la popularidad del contenido. A veces, los equipos intentan aprender un modelo personal con diversidad. Para implementar la personalización, agregan atributos que le permiten al sistema la personalización (algunos atributos que representan el interés del usuario) o la diversificación (atributos que indican si este documento tiene algún atributo en común con otros documentos de los resultados, como el autor o el contenido), y descubren que esos atributos obtienen una ponderación menor (o a veces, un signo diferente) al que esperaban.\nEsto no significa que la diversidad, la personalización o la relevancia no sean valiosas. Como se indicó en la regla anterior, puedes hacer un posprocesamiento para aumentar la diversidad o la relevancia. Si observas que los objetivos a largo plazo aumentan, puedes declarar que la diversidad o la relevancia son valiosas, más allá de la popularidad. Puedes continuar usando el posprocesamiento o directamente modificar el objetivo según la diversidad o la relevancia.\nRegla n.º 43: Tus amigos tienden a ser los mismos en diferentes productos. No así tus intereses. Varios equipos en Google ganaron mucho terreno al tomar que un modelo que predice la cercanía de una conexión con un producto y lograr que funcione en otro producto. Tus amigos no cambian. Por otro lado, observé a muchos equipos lidiar con atributos de personalización entre productos. Sí, parece que debería funcionar. Por ahora, parece que no. Un método que a veces funciona es usar los datos sin procesar de una propiedad para predecir el comportamiento en otra. Ten en cuenta que también puede ayudar saber que un usuario tiene un historial en otra propiedad. Por ejemplo, la presencia de actividad de usuario en dos propiedades puede ser un indicativo en sí misma.\nTrabajo relacionado Existen muchos documentos sobre aprendizaje automático en Google y en otras fuentes.\nCurso intensivo de aprendizaje automático: Una introducción al aprendizaje automático aplicado. Aprendizaje automático: Un enfoque probabilístico de Kevin Murphy, para comprender el campo de aprendizaje automático. Consejos prácticos para el análisis de conjuntos de datos grandes y complejos: Un enfoque de ciencia de datos sobre los conjuntos de datos. Aprendizaje profundo de Ian Goodfellow et al, para aprendizaje de modelos no lineales. Documento de Google sobre la deuda técnica, con muchos consejos generales. Documentación de Tensorflow. Agradecimientos Agradezco a David Westbrook, Peter Brandt, Samuel Ieong, Chenyu Zhao, Li Wei, Michalis Potamias, Evan Rosen, Barry Rosenberg, Christine Robson, James Pine, Tal Shaked, Tushar Chandra, Mustafa Ispir, Jeremiah Harmsen, Konstantinos Katsiapis, Glen Anderson, Dan Duckworth, Shishir Birmiwal, Gal Elidan, Su Lin Wu, Jaihui Liu, Fernando Pereira y Hrishikesh Aradhye por las numerosas correcciones, sugerencias y ejemplos útiles para este documento. Agradezco también a Kristen Lefevre, Suddha Basu y Chris Berg quienes colaboraron en una versión anterior. Cualquier error, omisión u opiniones controversiales es mi responsabilidad.\nAnexo Existen diferentes referencias a productos de Google en este documento. Para proporcionar más contexto, agregué una descripción breve de los ejemplos más comunes a continuación.\nDescripción de YouTube YouTube es un servicio de transmisión de videos. Tanto los equipos de YouTube Watch Next y de la página principal de YouTube usan modelos de AA para clasificar recomendaciones de videos. Watch Next recomienda videos para ver después del que se está reproduciendo, la página principal recomienda videos para los usuarios que exploran la página principal.\nDescripción general de Google Play Google Play tiene muchos modelos para resolver diferentes problemas. Las búsqueda de apps en Play, las recomendaciones personalizadas en la página principal de Play y \u0026ldquo;Otros usuarios también instalaron\u0026rdquo; usan aprendizaje automático.\nDescripción de Google Plus Google Plus usa aprendizaje automático en diferentes situaciones: clasificar las publicaciones en la sección \u0026ldquo;Novedades\u0026rdquo; que ve el usuario, las publicaciones en la sección \u0026ldquo;Lo más interesante\u0026rdquo; (las publicaciones que son más populares), las personas que conoces, etc.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"cdf1b317720d0402f5b02d434cbb3944","permalink":"https://www.marcusrb.com/cursos/data-science/intro-machine-learning/ml101-0-reglas2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/data-science/intro-machine-learning/ml101-0-reglas2/","section":"cursos","summary":"Fase II de aprendizaje automático: Ingeniería de atributos En la primera fase del ciclo de vida de un sistema de aprendizaje automático, la prioridad es mandar los datos de entrenamiento al sistema de aprendizaje, lograr instrumentar las métricas de interés y crear una infraestructura de publicación. Una vez que cuentas con un sistema integral en funcionamiento con pruebas de unidades y del sistema instrumentadas, comienza la fase II.\nEn la segunda fase, hay muchas recompensas a corto plazo.","tags":null,"title":"Reglas del aprendizaje automático - Fase II","type":"docs"},{"authors":null,"categories":null,"content":" Creando vectores Te sientes de suerte?\nEso espero, porque en este capítulo vamos de viaje a la Ciudad del Pecado, también conocida como el \u0026ldquo;Paraíso del estadístico\u0026rdquo; ;-).\nGracias a R y a tus nuevas habilidades analíticas, vas a aprender cómo mejorar tus ganancias en el casino y empezar una lucrativa carrera como jugador profesional. En este capítulo veremos cómo puedes fácilmente llevar la cuenta de tus apuestas y como hacer análisis simples de tus jugadas. Próxima parada\u0026hellip; Viva las Vegas!!!\nInstrucciones\n Esperamos que recuerdes lo que aprendiste en el capítulo anterior. Asigna el valor \u0026ldquo;Alla vamos!\u0026rdquo; a la variable Vegas  Script R\n# Define la variable Vegas Vegas \u0026lt;- \u0026quot;Alla vamos!\u0026quot;  Creando vectores (2) Primero concentrémonos.\nEn nuestro camino a para quebrar al casino, haremos uso extensivo de los vectores. Un vector es un arreglo unidimensional que puede contener datos numéricos, caracteres, o valores lógicos. En otras palabras un vector es una herramienta simple para guardar un conjunto de datos del mismo tipo. Por ejemplo podemos llevar la cuenta de las ganancias y pérdidas en los juegos de casino.\nEn R, se crea un vector con la función c(). La función se llama c por \u0026ldquo;combinar\u0026rdquo;. Se ponen los valores o elementos del vector dentro de paréntesis, separados por coma. Por ejemplo:\nvector_numerico \u0026lt;- c(1, 2, 3) vector_caracter \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) vector_logico \u0026lt;- c(TRUE, FALSE)  Una vez has creado estos vectores en R puedes usarlos para hacer cálculos.\nInstrucciones\n Completa el código de tal manera que el vector_logico contenga tres elementos: TRUE, FALSE y TRUE (en ese orden).  Script R\nvector_numerico \u0026lt;- c(1, 10, 49) vector_caracter \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) # Completa el código para vector_logico vector_logico \u0026lt;- c(TRUE, FALSE, TRUE)  Creando vectores (3) Después de una semana en Las Vegas y cero Ferraris en tu garage, decides que es hora de empezar a utilizar tus super-poderes analíticos.\nAntes de hacer un primer análisis, decides llevar un registro de todas tus ganancias y pérdidas de la semana pasada:\nEn el poker:\n Lunes: ganaste 140 Martes: perdiste 50 Miercoles: ganaste 20 Jueves: perdiste 120 Viernes: ganaste 240  En la ruleta:\n Lunes: perdiste 24 Martes: perdiste 50 Miercoles: ganaste 100 Jueves: perdiste 350 Viernes: ganaste 10  Solamente has jugado poker y ruleta, porque un grupo de adivinos se ha apoderado de la mesa de dados. Para poder utilizar esos datos en R, decides crear las variables vector_poker y vector_ruleta.\nInstrucciones\n Ahora asigna los días de la semana como nombres a los vectores vector_poker y vector_ruleta. Usa los nombres con mayúscula inicial y sin acentos (esto no es clase de ortografía\u0026hellip;): Lunes, Martes, Miercoles, Jueves y Viernes.\n Imprime los vectores en la consola\n  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Asigna los nombres a los vectores names(vector_poker) \u0026lt;- c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;,\u0026quot;Miercoles\u0026quot;,\u0026quot;Jueves\u0026quot;,\u0026quot;Viernes\u0026quot;) names(vector_ruleta) \u0026lt;- c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;,\u0026quot;Miercoles\u0026quot;,\u0026quot;Jueves\u0026quot;,\u0026quot;Viernes\u0026quot;) # Imprime los vectores en la consola vector_poker vector_ruleta  Nombrando elementos (2) Si quieres ser un buen estadístico, tienes que ser un poco perezoso. (Si ya eres perezoso, quizás hayas nacido con esos excepcionales talentos estadísticos.)\nEn los ejercicios anteriores probablemente sentiste que es aburrido escribir una y otra vez información como los días de la semana. Sin embargo, existe una manera mas eficiente de lograr esto, que tal si asignamos el vector que contiene los días de la semana a una variable.\nAsí como lo hiciste con las ganancias/pérdidas de la ruleta y el poker, puedes también crear un vector que contenga los días de la semana. Al asignarlo a una variable, éste puede ser reusado.\nInstrucciones\n Crea la variable vector_dias que contenga un vector con los días de la semana, de Lunes a Viernes. Usa la variable vector_dias para asignar nombres a los elementos de vector_poker y vector_ruleta.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Crea la variable vector_dias vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;,\u0026quot;Miercoles\u0026quot;,\u0026quot;Jueves\u0026quot;,\u0026quot;Viernes\u0026quot;) # Asigna los nombres a los elementos de vector_poker y vector_ruleta names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias  Calculando las ganancias totales Ahora que tienes los resultados del poker y la ruleta en un vector con elementos apropiadamente nombrados, puedes empezar a hacer análisis.\nVeremos como encontrar la siguiente información:\n La ganancia/pérdida total por cada día de la semana. La ganancia/pérdida total en la semana. Sabremos si estas ganando o perdiendo dinero en el poker y en la ruleta.  Para saber lo que nos proponemos tenemos que hacer cálculos aritméticos con los vectores. Cuando sumamos dos vectores en R, éste hace la suma elemento por elemento, por ejemplo las siguientes líneas de código son completamente equivalentes:\nc(1, 2, 3) + c(4, 5, 6) c(1 + 4, 2 + 5, 3 + 6) c(5, 7, 9)  Primero experimentemos sumando!\nInstrucciones\n Toma la suma de las variables vector_A y vector_B y asígnala a vector_total. Mira el resultado imprimiendo el valor de vector_total en la consola.  Script R\nvector_A \u0026lt;- c(1, 2, 3) vector_B \u0026lt;- c(4, 5, 6) # Asigna la suma de vector_A y vector_B vector_total \u0026lt;- vector_A + vector_B # Imprime el vector_total a la consola vector_total  Calculando las ganancias totales (2) ¿Entendiste como R realiza aritmética con los vectores?\nYa es hora de tener uno de esos Ferraris en el garage! Primero, hay que saber cuál fue la ganancia/pérdida por cada día. La ganancia neta de cada día es la suma de las ganancias/pérdidas que hiciste en el poker y en la ruleta.\nEn R, este cálculo es simplemente la suma de vector_ruleta y vector_poker.\nInstrucciones\n Asigna a la variable total_diario lo que ganaste/perdiste cada día en total (poker y ruleta combinado).\n Nombra los elementos de total_diario utilizando vector_dias.\n Imprime total_diario en la consola para que puedas ver tus resultados totales en cada día de la semana. Cuáles fueron los días buenos y los días malos?\n  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Calculando las ganancias/pérdidas diarias: total_diario \u0026lt;- vector_ruleta + vector_poker # Dando nombres a total_diario names(total_diario) \u0026lt;- vector_dias # Imprime total_diario a la consola para ver tus resultados por día total_diario  Calculando ganancias totales (3) Basado en los análisis previos, parece que tuviste una mezcla de días buenos y malos. Esto no era lo que tu ego esperaba, y te preguntas si quizás hay una (muy pequeña) posibilidad de que hayas perdido dinero en la semana.\nLa función que nos ayudará a contestar esta pregunta es sum(). Ésta función calcula la suma de todos los elementos de un vector. Por ejemplo para calcular (y asignar a una variable) el monto total de dinero que has ganado/perdido en el poker en la semana, escribe:\ntotal_poker \u0026lt;- sum(vector_poker)  Instrucciones\n Calcula el monto total de dinero que has ganado/perdido en la ruleta, asígnalo a la variable _totalruleta. Ahora que tienes los totales para la ruleta y el poker, puedes fácilmente calcular el valor de _totalsemana, que es la suma de todas las ganancias y pérdidas en la semana. Imprime el valor de _totalsemana a la consola. ¿En total ganaste o perdiste en la semana?  Script R\n# Resultados en la mesa de Poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Ganancias totales en el poker total_poker \u0026lt;- sum(vector_poker) # Ahora completa el código: total_ruleta \u0026lt;- sum(vector_ruleta) total_semana \u0026lt;- sum(total_poker + total_ruleta) #Imprime el total que ganaste/perdiste en la semana total_semana  Comparando ganancias totales Mmmmm\u0026hellip;. parece que estás perdiendo dinero. Que sorpresa! Hora de repensar tu estrategia! Esto va a requerir un análisis más profundo.\nDespués de una breve lluvia de ideas en el jacuzzi del hotel, piensas que una posible explicación puede ser que tus habilidades en la ruleta no están tan bien desarrolladas como las de poker. Así que talvez tus ganancias totales en el poker son más grandes (\u0026gt;) que en la ruleta.\nInstrucciones\n Calcula total_poker y total_ruleta como en el ejercicio anterior. Fíjate si tus ganancias totales en el poker son más altas que en la ruleta haciendo una comparación. Asigna el resultado de esta comparación a la variable respuesta. ¿Cuál es tu conclusión? ¿Deberías dedicarte a la ruleta o al poker? Imprime respuesta a la consola.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Ganancias totales en el poker y en la ruleta total_poker \u0026lt;- sum(vector_poker) total_ruleta \u0026lt;- sum(vector_ruleta) # Probando si las ganancias en el poker son más altas que en la ruleta: respuesta \u0026lt;- total_poker \u0026gt; total_ruleta # Imprime respuesta a la consola respuesta  Seleccionando elementos: los buenos tiempos La corazonada parece que fue cierta. Parece que te va mejor en el poker que en la ruleta.\nOtro posible aspecto a investigar son tus resultados al comienzo de la semana comparados con los últimos días. Talvez te excediste en los tequilas al final de la semana\u0026hellip;\nPara responder esta pregunta, concentrémonos en solo unos elementos del vector_total. En otras palabras, nuestro objetivo es seleccionar elementos específicos de los vectores. Para seleccionar elementos de un vector (y luego matrices, data frames, etc.), puedes utilizar los corchetes. Entre corchetes indicamos los elementos que queremos seleccionar. Por ejemplo, para seleccionar el primer elemento de un vector, escribes vector_poker[1]. Para seleccionar el segundo elemento escribimos vector_poker[2], etc.\nInstrucciones\n Asigna tu ganancia del Miércoles a la variable _pokermiercoles  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Asigna el valor de lo ganado el Miercoles en el poker poker_miercoles \u0026lt;- vector_poker[3]  Seleccionando elementos: los buenos tiempos (2) ¿Qué tal si analizamos los resultados en los días medios de la semana?\nPara seleccionar múltiples elementos de un vector, puedes hacerlo usando corchetes. En el ejercicio anterior pusimos un número, llamado índice, entre los corchetes y obtuvimos el elemento del vector correspondiente a ese índice. Para seleccionar varios elementos podemos utilizar otro vector cuyos elementos son los índices que deseamos seleccionar. Por ejemplo, para seleccionar el primero y el quinto elemento usamos el vector c(1,5) dentro de los corchetes. El siguiente código selecciona el primero y quinto elementos de vector_poker:\nvector_poker[c(1,5)]  Instrucciones\n Asigna los resultados de obtenidos del poker de los días Martes, Miercoles y Jueves a la variable dias_medios_poker. Imprime el vector dias_medios_poker a la consola para ver sus elementos.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Define la nueva variable: dias_medios_poker \u0026lt;- vector_poker[c(2,3,4)] # Imprime dias_medios_poker dias_medios_poker  Seleccionando elementos: los buenos tiempos (3) Seleccionar varios elementos del vector_poker con c(2,3,4) quizás no sea muy conveniente. Recordemos como estadísticos nuestras raíces perezosas, así que usemos una notación que R utiliza para crear vectores de números consecutivos c(2,3,4) se puede crear con el código 2:4, que genera el vector que contiene los números naturales del 2 al 4 (incluyendo ambos extremos).\nAsí que ahora tenemos otra manera de seleccionar los elementos que corresponden a los días medios de la semana: vector_poker[2:4].\nQuizás en este ejemplo no haya mucha diferencia entre c(2,3,4) y 2:4 pero imagina que tuviéramos que extraer los primeros 80 elementos consecutivos de un vector, con la primera notación tendríamos que escribir todos los números del 1 al 80, sin embargo ahora sabemos que podemos hacerlo así de simple: 1:80.\nInstrucciones\n Asigna los resultados del Martes al Viernes del vector_ruleta a la variable martes_a_viernes_ruleta. Utiliza la notación de dos puntos:. Imprime martes_a_viernes_ruleta en la consola.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Define la nueva variable siguiendo las instrucciones martes_a_viernes_ruleta \u0026lt;- vector_ruleta[c(2:5)] #Imprime martes_a_viernes_ruleta en la consola martes_a_viernes_ruleta  Seleccionando elementos: los buenos tiempos (4) Otra forma de abordar este problema es usar los nombres de los elementos (Lunes, Martes, etc.) en lugar de sus índices. Por ejemplo:\nvector_poker[\u0026quot;Lunes\u0026quot;]  Seleccionará el primer elemento de vector_poker porque el primer elemento es el que tiene el nombre \u0026ldquo;Lunes\u0026rdquo;. De la misma manera en que lo hicimos en el ejercicio anterior, puedes utilizar un vector que tenga los nombres de los elementos que deseas extraer:\nvector_poker[c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;)]  Instrucciones\n Calcula tus ganancias promedio durante los primeros tres días de la semana seleccionando los elementos por nombre. Asigna este valor a promedio_primeros3_dias. Puedes usar la función mean() para obtener el promedio de los elementos de un vector.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Only poker results promedio_primeros3_dias \u0026lt;- mean((vector_poker)[c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;,\u0026quot;Miercoles\u0026quot;)])  Selección por comparación - Paso 1 Al utilizar los operadores de comparación, podemos abordar las preguntas anteriores de una manera más interesante.\nLos operadores de comparación en R son los siguientes:\n \u0026lt; menor que \u0026gt; mayor que \u0026gt;= mayor o igual que == igualdad != no igual  Como ya hemos visto, 6 \u0026gt; 5 da un valor verdadero: TRUE. Una característica muy buena de R es que puedes utilizar operaciones de comparación también en vectores. por ejemplo c(4,5,6) \u0026gt; 5 resulta en: FALSE, FALSE, TRUE. En otras palabras R hace la comparación por cada elemento y responde TRUE o FALSE dependiendo del resultado de la comparación. Esto es muy interesante y puede ser difícil de entender, practícalo en la consola!\nInternamente, R recicla el valor 5 cuando ejecuta c(4,5,6) \u0026gt; 5. R quiere hacer una comparación elemento por elemento de c(4,5,6) con 5, pero 5 no es un vector de tamaño 3. Para resolver esto R crea el vector c(5,5,5) y luego hace la comparación elemento por elemento.\nInstrucciones\n Obtengamos los días en que obtuvimos valores positivos (\u0026gt; 0) en el poker (vector_poker) y asignémoslo a la variable positivos_poker.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Que días obtiviste ganancias en el poker? positivos_poker \u0026lt;- vector_poker \u0026gt; 0 positivos_poker  Selección por comparación - Paso 2 Trabajar con comparaciones hará tu vida analítica más fácil. En lugar de seleccionar \u0026ldquo;a mano\u0026rdquo; un conjunto de días para analizar (como en los primeros ejercicios), puedes decirle a R que te seleccione solo aquellos días en los que tuviste ganancias en el poker.\nEn el ejercicio anterior usaste positivos_poker \u0026lt;- vector_poker \u0026gt;0 para encontrar aquellos días en los cuales tuviste ganancias. Ahora, nos gustaría saber no solo los días, sino las cantidades que ganaste en esos días.\nPuedes seleccionar los elementos deseados usando positivos_poker entre corchetes para seleccionar los elementos de vector_poker. Esto funciona, porque R seleccionará solo aquellos elementos en los cuales el vector positivos_poker tiene un valor verdadero (TRUE).\nInstrucciones - Asigna las ganancias en el poker a la variable ganancias_poker.\nScript R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Que días obtuviste ganancias en el poker? selection_vector \u0026lt;- vector_poker \u0026gt; 0 # Selecciona de vector_poker los días con ganancias ganancias_poker \u0026lt;- vector_poker[selection_vector] #Imprime ganancias_poker  Selección Avanzada Así como lo hiciste con el poker, también quisieras saber aquellos días en los que tuviste ganancia en la ruleta.\nInstrucciones\n Asigna los valores de las ganancias que tuviste en la ruleta a la variable ganancias_ruleta. Es decir selecciona solo aquellos valores de vector_ruleta que son positivos y asignalos a la variable ganancias_ruleta.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Que días obtiviste ganancias en la ruleta? positivos_ruleta \u0026lt;- vector_ruleta \u0026gt; 0 # Selecciona los valores de vector_ruleta que fueron ganancias ganancias_ruleta \u0026lt;- vector_ruleta[positivos_ruleta] #Imprime ganancias_ruleta  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"38cd2009a27a9f96afc6247c7b1864ca","permalink":"https://www.marcusrb.com/cursos/r-studio/intro-r/r101-vectores/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/r-studio/intro-r/r101-vectores/","section":"cursos","summary":"Creando vectores Te sientes de suerte?\nEso espero, porque en este capítulo vamos de viaje a la Ciudad del Pecado, también conocida como el \u0026ldquo;Paraíso del estadístico\u0026rdquo; ;-).\nGracias a R y a tus nuevas habilidades analíticas, vas a aprender cómo mejorar tus ganancias en el casino y empezar una lucrativa carrera como jugador profesional. En este capítulo veremos cómo puedes fácilmente llevar la cuenta de tus apuestas y como hacer análisis simples de tus jugadas.","tags":null,"title":"Prácticas 2 - Vectores","type":"docs"},{"authors":null,"categories":null,"content":" El workspace es el área de trabajo de la sesión R e incluye todos los objetos en uso. En RStudio, los objetos en el espacio de trabajo se pueden explorar en el panel Entorno:\nPanel de entorno RStudio [IMG]\nR no guarda objetos relacionados con una sesión de trabajo individualmente: los conjuntos de datos y las salidas se almacenan en un solo archivo, cuyo nombre predeterminado es .RData. Este archivo es la imagen del área de trabajo activa durante una sesión.\nGuardar y cargar el espacio de trabajo Al final de una sesión R, puede guardar una imagen del espacio de trabajo, que se volverá a cargar automáticamente en el próximo inicio. Ej.:\nsave.image( \u0026quot;C: //.../myfile.RData\u0026quot;)  Para cargar la imagen del espacio de trabajo, use el comando de carga:\nload( \u0026quot;C: //.../myfile.RData\u0026quot;)  El menú Archivo de R La imagen del espacio de trabajo también se puede guardar o cargar desde el menú R, posiblemente con un nombre elegido por el usuario (por defecto no tiene nombre: .RData).\nEnumerar los objetos presentes. Para obtener una lista de los objetos y clases contenidos en el espacio de trabajo, puede usar el comando de menú Varios / Lista de objetos, o escribir el comando\nls()  Enumere los objetos que contienen (por ejemplo: la palabra tab):\nls(pattern = \u0026quot;tab\u0026quot;)  ## Eliminar los objetos\nPara eliminar los objetos:\nrm(objeto) # Podemos utilizar también remove(objeto)  Vaciar el área de trabajo. Para eliminar todos los objetos y limpiar el espacio de trabajo (corresponde a borrar el espacio de trabajo o borrar el entorno en otros programas):\nrm(list = ls())  Elimine los objetos que contienen (por ejemplo: la palabra tab):\nrm(list = ls(pattern = \u0026quot;tab\u0026quot;))  objeto = nombre del objeto, sin comillas El argumento de lista especifica la lista de objetos que se eliminarán.\nÁrea de trabajo de R y RCommander RCommander comparte el espacio de trabajo R, y los marcos de datos que forman parte del espacio de trabajo (área de trabajo) se pueden mostrar en la esquina superior izquierda (conjunto de datos o conjunto de datos).\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"cde998243d6d746f6c211dcb7818867e","permalink":"https://www.marcusrb.com/cursos/r-studio/workspace/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/r-studio/workspace/","section":"cursos","summary":"El workspace es el área de trabajo de la sesión R e incluye todos los objetos en uso. En RStudio, los objetos en el espacio de trabajo se pueden explorar en el panel Entorno:\nPanel de entorno RStudio [IMG]\nR no guarda objetos relacionados con una sesión de trabajo individualmente: los conjuntos de datos y las salidas se almacenan en un solo archivo, cuyo nombre predeterminado es .RData. Este archivo es la imagen del área de trabajo activa durante una sesión.","tags":null,"title":"Área de trabajo","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"c12af28839981b132d4e7e6964bdcf7b","permalink":"https://www.marcusrb.com/cursos/r-studio/intro-r/r101-matrices/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/cursos/r-studio/intro-r/r101-matrices/","section":"cursos","summary":"","tags":null,"title":"Prácticas 3 - Matrices","type":"docs"},{"authors":null,"categories":null,"content":" Introducción Comenzaremos con una descripción general de cómo funcionan los modelos de aprendizaje automático y cómo se usan. Esto puede parecer básico si ya ha realizado modelos estadísticos o aprendizaje automático. No se preocupe, progresaremos para construir modelos potentes pronto.\nEste micro curso le permitirá construir modelos a medida que avance en el siguiente escenario:\nSu primo ha ganado millones de dólares especulando con bienes raíces. Se ofreció a convertirse en socio comercial con usted debido a su interés en la ciencia de datos. Él proporcionará el dinero, y usted proporcionará modelos que predicen cuánto valen varias casas.\nLe preguntas a tu primo cómo ha predicho los valores inmobiliarios en el pasado. y dice que es solo intuición. Pero más preguntas revelan que ha identificado patrones de precios de casas que ha visto en el pasado, y usa esos patrones para hacer predicciones para las casas nuevas que está considerando.\nEl aprendizaje automático funciona de la misma manera. Comenzaremos con un modelo llamado Árbol de decisión. Hay modelos más elegantes que dan predicciones más precisas. Pero los árboles de decisión son fáciles de entender y son el bloque de construcción básico para algunos de los mejores modelos en ciencia de datos.\nPara simplificar, comenzaremos con el árbol de decisión más simple posible.\nDivide las casas en solo dos categorías. El precio previsto para cualquier casa en consideración es el precio promedio histórico de las casas en la misma categoría.\nUsamos datos para decidir cómo dividir las casas en dos grupos, y luego nuevamente para determinar el precio previsto en cada grupo. Este paso de capturar patrones de datos se llama ajuste (fitting) o capacitación del modelo (training). Los datos utilizados para ajustarse fit al modelo se denominan datos de entrenamiento.\nLos detalles de cómo se ajusta el modelo (por ejemplo, cómo dividir los datos) son lo suficientemente complejos como para guardarlos para más adelante. Después de que el modelo se haya ajustado, puede aplicarlo a nuevos datos para predecir los precios de viviendas adicionales.\nMejorando el árbol de decisiones ¿Cuál de los siguientes dos árboles de decisiones es más probable que resulte de ajustar los datos de capacitación de bienes raíces?\nEl árbol de decisión de la izquierda (Árbol de decisión 1) probablemente tenga más sentido, porque captura la realidad de que las casas con más habitaciones tienden a venderse a precios más altos que las casas con menos habitaciones. El mayor inconveniente de este modelo es que no captura la mayoría de los factores que afectan el precio de la vivienda, como la cantidad de baños, el tamaño del lote, la ubicación, etc.\nPuede capturar más factores utilizando un árbol que tiene más \u0026ldquo;divisiones\u0026rdquo;. Estos se llaman árboles \u0026ldquo;más profundos\u0026rdquo;. Un árbol de decisión que también considera el tamaño total del lote de cada casa podría verse así:\nUsted predice el precio de cualquier casa rastreando a través del árbol de decisión, siempre eligiendo la ruta correspondiente a las características de esa casa. El precio previsto para la casa está en la parte inferior del árbol. El punto en la parte inferior donde hacemos una predicción se llama hoja (leaf).\nLas divisiones y los valores en las hojas estarán determinados por los datos, por lo que es hora de que revise los datos con los que trabajará.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"d6b42f5b97fb5577a6fbfd7cc1882e41","permalink":"https://www.marcusrb.com/cursos/data-science/intro-machine-learning/ml101-1-como-funciona/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/data-science/intro-machine-learning/ml101-1-como-funciona/","section":"cursos","summary":"Introducción Comenzaremos con una descripción general de cómo funcionan los modelos de aprendizaje automático y cómo se usan. Esto puede parecer básico si ya ha realizado modelos estadísticos o aprendizaje automático. No se preocupe, progresaremos para construir modelos potentes pronto.\nEste micro curso le permitirá construir modelos a medida que avance en el siguiente escenario:\nSu primo ha ganado millones de dólares especulando con bienes raíces. Se ofreció a convertirse en socio comercial con usted debido a su interés en la ciencia de datos.","tags":null,"title":"Cómo funcionan los modelos","type":"docs"},{"authors":null,"categories":null,"content":" Estructura de los recursos de Big Data En esta sección encontraremos los recursos y tutoriales relacionados con el mundo Big Data para los percorsos o Learning Path de:\n Data Engineering Cloud Architecture Machine Learning Engineer Cloud Developer Big Data Engineering  - Introducción al Big Data - Arquitectura Lambda - Origenes de Hadoop - Modelados de datos en Apache Hadoop - Apache Hive - Apache Spark - Spark Streaming - Apache Sqoop - Apache Flume - Apache Nifi - Kafka - Apache Flink  ","date":1609714800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609872091,"objectID":"88e051cea87ff46813747bd579a842e9","permalink":"https://www.marcusrb.com/big-data/","publishdate":"2021-01-04T00:00:00+01:00","relpermalink":"/big-data/","section":"recursos","summary":"El programa formativo de Power BI eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Recursos de Big Data","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Tableau te guiaré paso a paso para obtener habilidades de Tableau. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Tableau con éxito.\nLa aplicación de dashboard con Tableau es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.\nEstructura del programa La estructura del programa formativo se compone en dos grandes bloques: - fundamentos - avanzado\nambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.\n#\n","date":1609714800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"daf467ee7273e1e265aad8622b8323f2","permalink":"https://www.marcusrb.com/recursos-business-intelligence/","publishdate":"2021-01-04T00:00:00+01:00","relpermalink":"/recursos-business-intelligence/","section":"recursos","summary":"El programa formativo de Tableau eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Recursos de Business Intelligence","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Power BI te guiaré paso a paso para obtener habilidades de Power Bi. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Power BI con éxito.\nLa aplicación de dashboard con Power BI es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.\nEstructura del programa y cursos La estructura del programa formativo se compone en dos grandes bloques:\n Power BI fundamentos 101 Power BI avanzado 201  ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.\nCurso avanzado en DAX y Power Query de Power BI (~70 horas) Incluye el curso de fundamentos 101 más:\n","date":1609714800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"a4f4dcc1d1765c5de1bd9e79c9bad377","permalink":"https://www.marcusrb.com/recursos-inteligencia-artificial/","publishdate":"2021-01-04T00:00:00+01:00","relpermalink":"/recursos-inteligencia-artificial/","section":"recursos","summary":"El programa formativo de Power BI eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Recursos de Inteligencia Artificial","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Google Data Studio es una herramienta gratuita de visualización e informes de datos basada en la nube que se conecta a muchas fuentes de datos diferentes, y convierte esos datos en paneles informativos e informes que son fáciles de entender y compartir, y son totalmente personalizables.\n- Guía de Microsoft Power BI - Guía de Tableau - Guía de Google Data Studio - Guía de Grafana  ","date":1609714800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"824ca2d762b299c90da6734353e3906b","permalink":"https://www.marcusrb.com/recursos-visualizacion-datos/","publishdate":"2021-01-04T00:00:00+01:00","relpermalink":"/recursos-visualizacion-datos/","section":"recursos","summary":"El programa formativo de Data Studio cubre todos los aspectos de la herramienta. Curso a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Recursos de Visualización de datos","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Power BI te guiaré paso a paso para obtener habilidades de Power Bi. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Power BI con éxito.\nLa aplicación de dashboard con Power BI es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.\nEstructura del programa y cursos La estructura del programa formativo se compone en dos grandes bloques:\n Power BI fundamentos 101 Power BI avanzado 201  ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.\nCurso de fundamentos en Power BI (~20 horas)  Introducción a Analítica de datos Business Intelligence vs Big Data Fundamentales de Visualización de datos Proyectos de Data Discovery y Agilismo Introducción a Power BI y conexión a fuentes de datos Transformación de datos con Power Query Modelado de datos y SQL Analysis Services DAX, funciones y mejores prácticas Reportes y cuadro de mandos en Power BI Power BI App y conexiones on-premise Power BI Flow y Data Pipeline Labs y Prácticas  Realización de PoC (Proof of Concept) con Power BI También realizo pruebas de conceptos en su empresa, con una muestra de datos (también del tipo dummies), me ocupo de transformar su necesidad en un panel de control dinámico y eficiente.\nSe realizará una sesión de 4 horas máximo de data discovery y otras sesiones de detección de KPI e indicadores importantes.\n¿Está interesado en capacitarse en Power BI ? Puedes adquirir el curso de Power BI en la plataforma UDEMY a un precio reducido y totalmente online.\nSi necesitas un curso en remoto, en tu empresa o a nivel profesional puedes llamar hoy o utilizar el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Formación de fundamentos en Microsoft PowerBI\nDEMO Data Analysis con Power BI   ","date":1591743600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"339ff1bc910ccef854dc02d458330124","permalink":"https://www.marcusrb.com/curso-power-bi-fundamentos/","publishdate":"2020-06-10T00:00:00+01:00","relpermalink":"/curso-power-bi-fundamentos/","section":"cursos","summary":"El programa formativo de Power BI eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Formación fundamentos en Microsoft Power BI","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Power BI te guiaré paso a paso para obtener habilidades de Power Bi. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Power BI con éxito.\nLa aplicación de dashboard con Power BI es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.\nEstructura del programa y cursos La estructura del programa formativo se compone en dos grandes bloques:\n Power BI fundamentos 101 Power BI avanzado 201  ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.\nCurso avanzado en DAX y Power Query de Power BI (~70 horas) Incluye el curso de fundamentos 101 más:\nCentralización de datos Creación de flujos de datos, configuración de puerta de enlace, actualización de flujos de datos, creación de informes a partir de flujos de datos, centralización de conjuntos de datos, certificación de conjuntos de datos, creación de informes a partir de conjuntos de datos\nEl lenguaje de fórmulas de Power Query Usando La Barra De Fórmula; Usando El Editor Avanzado; Descripción general del lenguaje M; Explorando M usando #shared\nComprensión del código generado automáticamente Excel.Workbook; File.Contents; Table.TransformColumns; Table.TransformColumnTypes; Table.UnpivotColumns; Table.UnpivotOtherColumns\nCrear funciones personalizadas en M Definiendo una función; Definición de parámetros de entrada; El operador de acceso; Definir el cuerpo de la función; Usando parámetros opcionales; Funciones de llamada\nTécnicas de iteración Beneficio de generar listas; Generando listas de números; Generando listas de fechas; Generando listas alfanuméricas; Usando cada función; Aplicar una función a una lista de archivos\nDAX avanzado Usando DAX Studio; Escribir fórmulas complejas; Usando variables; Calcular promedios móviles; Cálculo de totales acumulados; Cálculos de percentiles; Creación de fórmulas avanzadas de inteligencia de tiempo; Usando múltiples tablas de fechas; Trabajando con calendarios no estándar\nTrabajando con tablas calculadas Crear tablas calculadas; Funciones DAX que devuelven tablas; La función CALCULATETABLE; La función ADDCOLUMNS; La función RESUMEN; RESUMEN con ROLLUP; VALORES y funciones DISTINCT; La función CROSSJOIN; La función TOPN; La función ROW; Usar tablas calculadas dentro del modelo de datos\nUsar tablas de parámetros ¿Qué es una tabla de parámetros? Cuándo usar tablas de parámetros; Usando la función HASONEVALUE; Usando la función VALUES; Crear rebanadoras personalizadas; Crear múltiples soluciones de tabla de parámetros\nModelado y visualización de datos avanzados Trabajar con múltiples tablas de hechos, Usar relaciones activas e inactivas, Usar la función USERELATIONSHIP, Crear propiedades visuales dinámicas, Usar BLANK () para hacer que la visibilidad sea dinámica\nDashboards avanzados Agregar enlaces personalizados a un tablero de instrumentos; Usando el widget de contenido web; Usando el widget de video; Mosaicos de panel de transmisión en tiempo real, integración de Power Automation y PowerApps\nProgramas personalizados en empresas y pymes Si requieres una formación personalizada tanto sea de fundamentos que conceptos avanzados de inteligencia de negocio, SQL Server Analysis, DAX, modelado de datos y ETL con Power Query, puedo adaptar el contenido de Power BI para un mínimo de 4 personas hasta un máximo de 20.\nRealización de PoC (Proof of Concept) con Power BI También realizo pruebas de conceptos en su empresa, con una muestra de datos (también del tipo dummies), me ocupo de transformar su necesidad en un panel de control dinámico y eficiente.\nSe realizará una sesión de 4 horas máximo de data discovery y otras sesiones de detección de KPI e indicadores importantes.\n¿Está interesado en capacitarse en Power BI ? Si necesitas un curso en remoto, en tu empresa o a nivel profesional puedes llamar hoy o utilizar el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Formación avanzada en Microsoft PowerBI\nDEMO Data Analysis con Power BI   ","date":1591743600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"952e069b0c0002287ae48f9d066773a9","permalink":"https://www.marcusrb.com/curso-power-bi-avanzado/","publishdate":"2020-06-10T00:00:00+01:00","relpermalink":"/curso-power-bi-avanzado/","section":"cursos","summary":"El programa formativo de Power BI eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Formación avanzada en Microsoft Power BI","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Tableau te guiaré paso a paso para obtener habilidades de Tableau. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Tableau con éxito.\nLa aplicación de dashboard con Tableau es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.\nEstructura del programa La estructura del programa formativo se compone en dos grandes bloques: - fundamentos - avanzado\nambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.\nCurso en Tableau Public, Desktop y Prep (~30 horas)  Introducción a Analítica de datos Business Intelligence vs Big Data Fundamentales de Visualización de datos Proyectos de Data Discovery y Agilismo Introducción a Tableau y conexión a fuentes de datos Transformación de datos con Tableau Prep creación campos calculados, medidas, parámetros y filtros dinámicos Reportes y cuadro de mandos en Tableau Tableau Public, Tableau App, Tableau Reader Data Pipeline Labs y Prácticas  Programas personalizados en empresas y pymes Si requieres una formación personalizada tanto sea de fundamentos que conceptos avanzados de inteligencia de negocio, base de datos, modelado de datos y ETL con Tableau Prep, puedo adaptar el contenido de Tableau Desktop para un mínimo de 4 personas hasta un máximo de 20.\nRealización de PoC (Proof of Concept) con Tableau También realizo pruebas de conceptos en su empresa, con una muestra de datos (también del tipo dummies), me ocupo de transformar su necesidad en un panel de control dinámico y eficiente.\nSe realizará una sesión de 4 horas máximo de data discovery y otras sesiones de detección de KPI e indicadores importantes.\n¿Está interesado en capacitarse en Tableau ? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Formación en Tableau\n","date":1591743600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"b92115a5e7ad0ccc69c697e189b4845b","permalink":"https://www.marcusrb.com/curso-tableau/","publishdate":"2020-06-10T00:00:00+01:00","relpermalink":"/curso-tableau/","section":"cursos","summary":"El programa formativo de Tableau eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Formación Tableau Desktop y Tableau Prep","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"177c9357768ca420ca83dbd5bcf9c900","permalink":"https://www.marcusrb.com/cursos/r-studio/intro-r/r101-factores/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/cursos/r-studio/intro-r/r101-factores/","section":"cursos","summary":"","tags":null,"title":"Prácticas 4 - Factores","type":"docs"},{"authors":null,"categories":null,"content":" Explorando los datos Usando pandas para familiarizarse con sus datos El primer paso en cualquier proyecto de aprendizaje automático es familiarizarse con los datos. Usarás la biblioteca Pandas para esto. Pandas es la herramienta principal de datos que los científicos usan para explorar y manipular datos. La mayoría de las personas abrevian pandas en su código como pd. Hacemos esto con el comando\nimport pandas as pd  La parte más importante de la biblioteca Pandas es el DataFrame. Un DataFrame contiene el tipo de datos que podría considerar como una tabla. Esto es similar a una hoja en Excel, o una tabla en una base de datos SQL.\nPandas tiene métodos poderosos para la mayoría de las cosas que querrás hacer con este tipo de datos.\nComo ejemplo, veremos datos sobre los precios de la vivienda en Melbourne, Australia. En los ejercicios prácticos, aplicará los mismos procesos a un nuevo conjunto de datos, que tiene los precios de las viviendas en Iowa.\nLos datos de ejemplo (Melbourne) están en la ruta del archivo ../input/melbourne-housing-snapshot/melb_data.csv.\nCargamos y exploramos los datos con los siguientes comandos:\n# save filepath to variable for easier access melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv' # read the data and store data in DataFrame titled melbourne_data melbourne_data = pd.read_csv(melbourne_file_path) # print a summary of the data in Melbourne data melbourne_data.describe()  Interpretación de la descripción de datos Los resultados muestran 8 números para cada columna en su conjunto de datos original. El primer número, el recuento, muestra cuántas filas tienen valores no faltantes.\nLos valores perdidos surgen por muchas razones. Por ejemplo, el tamaño de la segunda habitación no se recogería al inspeccionar una casa de 1 habitación. Volveremos al tema de los datos faltantes.\nEl segundo valor es la media, que es el promedio. Debajo de eso, std es la desviación estándar, que mide la extensión numérica de los valores.\nPara interpretar los valores mínimo, 25%, 50%, 75% y máximo, imagine ordenar cada columna del valor más bajo al más alto. El primer valor (el más pequeño) es el mínimo. Si recorre un cuarto de camino en la lista, encontrará un número que es mayor que el 25% de los valores y menor que el 75% de los valores. Ese es el valor del 25% (pronunciado \u0026ldquo;percentil 25\u0026rdquo;). Los percentiles 50 y 75 se definen de forma análoga, y el máximo es el número más grande.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"87e04c13d4e364974c56b95c9b8dda91","permalink":"https://www.marcusrb.com/cursos/data-science/intro-machine-learning/ml101-2-basic-data-exploration/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/data-science/intro-machine-learning/ml101-2-basic-data-exploration/","section":"cursos","summary":"Explorando los datos Usando pandas para familiarizarse con sus datos El primer paso en cualquier proyecto de aprendizaje automático es familiarizarse con los datos. Usarás la biblioteca Pandas para esto. Pandas es la herramienta principal de datos que los científicos usan para explorar y manipular datos. La mayoría de las personas abrevian pandas en su código como pd. Hacemos esto con el comando\nimport pandas as pd  La parte más importante de la biblioteca Pandas es el DataFrame.","tags":null,"title":"Explorando los datos","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Google Data Studio es una herramienta gratuita de visualización e informes de datos basada en la nube que se conecta a muchas fuentes de datos diferentes, y convierte esos datos en paneles informativos e informes que son fáciles de entender y compartir, y son totalmente personalizables.\nCaracterísticas Data Studio Google Data Studio es intuitivo, rápido, flexible y permite una gran cantidad de opciones de diseño y presentación.\nAmplia gama de conectores de datos. Data Studio tiene 17 conectores de datos internos y alrededor de 108 de terceros para elegir Funciones fáciles de usar Data Studio proporciona docenas de funciones matemáticas, de cadena, de fecha y otras para transformar sus datos en valores más útiles. Variedad de formas, imágenes y texto. Data Studio le permite agregar formas, imágenes y texto a sus informes y paneles para que sean más fáciles de leer. Niveles de permisos Aprovechando la tecnología Google Drive, puede administrar fácilmente a todos sus usuarios y su nivel de acceso Mezcla de datos, ahora una realidad Data Studio le permite agregar datos de múltiples fuentes para tener una vista comparativa de ellos a la vez\nLos recursos de este curso están disponibles en:\n Cursos online Google Data Studio Video-Tutoriales Google Data Studio Recursos para Google Academy for Ads  Novedades de Google Data studio\ngoogle-data-studio-1\nDEMO Data Analysis con Google Data Studio   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"bc2ea7232c2f17782c0c713392bce972","permalink":"https://www.marcusrb.com/curso-google-data-studio/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/curso-google-data-studio/","section":"cursos","summary":"El programa formativo de Data Studio cubre todos los aspectos de la herramienta. Curso a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Formación en Google Data Studio","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"1c4026c8d6eb86debb56126ae1c7d68f","permalink":"https://www.marcusrb.com/cursos/r-studio/intro-r/r101-dataframes/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/cursos/r-studio/intro-r/r101-dataframes/","section":"cursos","summary":"","tags":null,"title":"Prácticas 5 - Data Frames","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"bef4da6f6643aecc039d506c29ba22af","permalink":"https://www.marcusrb.com/cursos/data-science/intro-machine-learning/ml101-3-exercise-1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/data-science/intro-machine-learning/ml101-3-exercise-1/","section":"cursos","summary":"","tags":null,"title":"Caso Práctico 1 - Explorando los datos","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"57ddd30986a0ea38254b1607223028c3","permalink":"https://www.marcusrb.com/cursos/r-studio/intro-r/r101-listas/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/cursos/r-studio/intro-r/r101-listas/","section":"cursos","summary":"","tags":null,"title":"Prácticas 6 - Listas","type":"docs"},{"authors":null,"categories":null,"content":" Seleccionar datos para modelar Su conjunto de datos tenía demasiadas variables para entenderlo, o incluso para imprimirlo bien. ¿Cómo puede reducir esta cantidad abrumadora de datos a algo que pueda entender?\nComenzaremos eligiendo algunas variables usando nuestra intuición. Los cursos posteriores le mostrarán técnicas estadísticas para priorizar automáticamente las variables.\nPara elegir variables / columnas, necesitaremos ver una lista de todas las columnas en el conjunto de datos. Eso se hace con la propiedad de columnas del DataFrame (la línea inferior del código a continuación).\nimport pandas as pd melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv' melbourne_data = pd.read_csv(melbourne_file_path) melbourne_data.columns  # The Melbourne data has some missing values (some houses for which some variables weren't recorded.) # We'll learn to handle missing values in a later tutorial. # Your Iowa data doesn't have missing values in the columns you use. # So we will take the simplest option for now, and drop houses from our data. # Don't worry about this much for now, though the code is: # dropna drops missing values (think of na as \u0026quot;not available\u0026quot;) melbourne_data = melbourne_data.dropna(axis=0)  Hay muchas formas de seleccionar un subconjunto de sus datos. El Micro Curso de Pandas los cubre con más profundidad, pero por ahora nos centraremos en dos enfoques.\nNotación de puntos, que usamos para seleccionar el \u0026ldquo;objetivo de predicción\u0026rdquo; Seleccionando con una lista de columnas, que usamos para seleccionar las \u0026ldquo;características\u0026rdquo;\nSelecting The Prediction Target You can pull out a variable with dot-notation. This single column is stored in a Series, which is broadly like a DataFrame with only a single column of data.\nWe\u0026rsquo;ll use the dot notation to select the column we want to predict, which is called the prediction target. By convention, the prediction target is called y. So the code we need to save the house prices in the Melbourne data is\ny = melbourne_data.Price  Elegir \u0026ldquo;Características\u0026rdquo; Las columnas que se ingresan en nuestro modelo (y luego se usan para hacer predicciones) se denominan \u0026ldquo;características\u0026rdquo;. En nuestro caso, esas serían las columnas utilizadas para determinar el precio de la vivienda. A veces, usará todas las columnas excepto el objetivo como características. Otras veces estará mejor con menos funciones.\nPor ahora, crearemos un modelo con solo algunas características. Más adelante verá cómo iterar y comparar modelos creados con diferentes características.\nSeleccionamos múltiples características al proporcionar una lista de nombres de columnas entre paréntesis. Cada elemento de esa lista debe ser una cadena (con comillas).\nAquí hay un ejemplo:\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']  Por conveniencia atribuimos los datos a la variable X\nX = melbourne_data[melbourne_features]  Revisemos rápidamente los datos que usaremos para predecir los precios de la vivienda utilizando el método de descripción y el método de encabezado, que muestra las pocas filas superiores.\nX.describe()  X.head()  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"0232902daec273303c8717311a261b70","permalink":"https://www.marcusrb.com/cursos/data-science/intro-machine-learning/ml101-4-datos-modelo/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cursos/data-science/intro-machine-learning/ml101-4-datos-modelo/","section":"cursos","summary":"Seleccionar datos para modelar Su conjunto de datos tenía demasiadas variables para entenderlo, o incluso para imprimirlo bien. ¿Cómo puede reducir esta cantidad abrumadora de datos a algo que pueda entender?\nComenzaremos eligiendo algunas variables usando nuestra intuición. Los cursos posteriores le mostrarán técnicas estadísticas para priorizar automáticamente las variables.\nPara elegir variables / columnas, necesitaremos ver una lista de todas las columnas en el conjunto de datos. Eso se hace con la propiedad de columnas del DataFrame (la línea inferior del código a continuación).","tags":null,"title":"Seleccionar los datos para el modelo","type":"docs"},{"authors":["marcusRB"],"categories":["Data Visualization"],"content":" ¿Google Data Studio, ¿cómo sacarle todo el partido? Hace unas semanas os mostramos cómo crear campos calculados con Google Data Studio mediante una planificación con nuevos atributos y nuevas métricas, mejor conocidos como \u0026ldquo;campos calculados\u0026rdquo;, y, además, cuándo crearlos. Tenemos varias opciones de creación y, entre ellas, pusimos los ejemplos de las 5 funciones más utilizadas. Hoy os explicamos en concreto los casos de uso y resolvemos algunas de las preguntas más frecuentes.\nEjemplos de casos de uso en Data Studio con los campos calculados. Ratio de conversión personalizado El ratio de conversión es una métrica estándar y muy utilizada tanto en Google Analytics como en las herramientas de publicidad.\nCuando creamos objetivos, sean micro o macro, es muy importante definir unas serie de métricas, entre ellas, el ratio de conversión. Muchas veces estas métricas no proveen insights y necesitamos buscar alternativas.\nUn ejemplo es comparar la performance de dos o más anuncios / banners de publicidad tipo display o creatividades y obtener una KPI muy interesante para tal fin, CPI o Conversión Por Impresión.\nSolución:\nSegún la fuente de datos que tengamos, sea Google Ads, Google Analytics o Facebook y similares, crearemos un nuevo campo calculado:\n nombre campo: CPI Conversión Per Impression. fórmula (fx): Conversions / Impressions.  Comparar el tráfico de dispositivos móviles entre países/regiones o áreas geográficas Segmentar y recopilar información al detalle de una parte de nuestros datos e informes es la tarea más importante para un analista de datos. Imaginemos representar gráficamente el tráfico de dispositivos móviles según el periodo seleccionado y comparado con un periodo anterior. ¿Quieres medir el market share?\nEl modo eficiente de realizarlo en Data Studio es a través de un conjunto de campos calculados y añadir más datos a través de la técnica data blending (datos combinados). Sobre este punto se realizan videotutoriales que te explicarán paso a paso en qué consiste y las novedades actuales. Te recomiendo suscribirte al canal YouTube de Paradigma y seguir esta sección de Playlist de Tutoriales.\nSolución:\nCrea los segmentos en Google Analytics (GA). Para este ejemplo he creado 5 segmentos de Usuarios por cada Continente. Sucesivamente, he creado cada reporte con su segmento en Data Studio, tal como puedes apreciar en estas imágenes.\nSiempre es bueno renombrar cada gráfico con las propias métricas y segmentos aplicados, como Usuarios Europe, Sesiones Móvil Asia, etc.\nHaz click en Data Blending y este es el resultado:\nCrea sucesivamente campos calculados a nivel de gráfico, como por ejemplo Serie temporal, ya que los campos calculados a nivel de fuente de datos no son posibles con datos combinados.\nCambia el tipo de unidad de medida de Numérico a Porcentaje:\nFinalmente aplica estos campos calculados a las métricas de tu gráfico:\nRatio de abandono del funnel ¿Quiere mostrar el punto de abandono del funnel de conversión? Si bien los pasos del funnel de objetivos de Google Analytics pueden proporcionar información, solo funcionan si todas las etapas de su embudo equivalen a una página vista, ya que se quedan limitados cuando estos se basan en eventos\nEn nuestro ejemplo, asumimos que has creado un objetivo de Google Analytics para cada una de las etapas de su embudo.\nSolución:\nCrea un campo calculado donde se realice el ratio de abandono del embudo de conversiones de todos los pasos.\nCrea el campo calculado y lo renombras según el tipo de stage donde se aplicaría:\n1-(Funnel Stage 2 (Goal 2 Completions))/Funnel Stage 1 (Goal 1 Completions)  Cambia la unidad de la métrica de Numérica a Porcentaje.\nPuedes mostrar los campos calculados creados en los reportes junto con un gráfico de barras horizontales que muestra los números absolutos de consecución de objetivos en la etapa del embudo como en este ejemplo:\nLimpieza de URLs Probablemente, todos nos hemos encontrado con informes que distrajeron nuestra atención de la conclusión principal y nos vieron enfocarnos en errores leves. Las URL inconsistentes en el informe permiten a los usuarios de estos informes y cuadros de mando dudar de nuestras conclusiones, incluso si el efecto de sesgo es mínimo.\nLimpiar estas URL con campos calculados es un ejercicio sencillo. Estamos abordando soluciones para tres escenarios comunes en los reportes de la dimensión de página.\nPáginas reestructuradas\nPor una razón u otra, la URL de una página ha cambiado con el tiempo. Sin embargo, en términos de informes, ambas URL deben doblarse en una. Supongamos para este ejercicio que Classic XL Backpack se movió de la categoría habitual a la categoría de marca de propia de Google.\n Anterior URL: https://your.googlemerchandisestore.com/details/Lifestyle/Bags/GGL1757_Classic XL Backpack Nueva URL: https://your.googlemerchandisestore.com/Google/Bags/GGL1757_Classic XL Backpack  Ya que en nuestro dashboard de Data Studio queremos redireccionar todo este tráfico anterior al nuevo aplicamos la solución a través de nuestra función RegExp_Replace.\nSolución:\n Creamos una dimensión personalizada del tipo campo calculado. Seleccionamos desde data source nuevo campo calculados desde campos y podemos escribir nuestra fórmula:  REGEXP_REPLACE(Page, '/details/Lifestyle/Bags/GGL1757_Classic%20XL%20Backpack*', '/Google/Bags/GGL1757_Classic%20XL%20Backpack')  URLs duplicadas ¿Se puede acceder a algunas páginas a través de diferentes URL? En función del CMS utilizado y de la configuración del servidor, no es raro que se pueda acceder a las páginas con y sin barras diagonales finales. Dependiendo de cómo estén vinculadas esas páginas interna y externamente, se determinará cuánto tráfico se informa para cada versión. Arreglar esto de forma retroactiva para los informes garantizará la coherencia de sus datos. También es importante tener una buena configuración de la propia herramienta de analítica que reduzca estos errores; por ejemplo, en este post de se habla de la guía de Google Analytics y cómo configurarla correctamente.\nSolución:\n Utilizamos la función en Data Studio para eliminar el forward slash o comúnmente llamada slash /. Creamos desde Data Source un campo calculado del tipo dimensión con la función REGEXP_REPLACE  REGEXP_REPLACE(Page,\u0026quot;./\u0026quot;,\u0026quot;\u0026quot;)  Inconsistencia de mayúsculas y minúsculas Si el entorno de su servidor web distingue entre mayúsculas y minúsculas, es posible que las URL aparezcan con casos incoherentes en sus informes. Dedicar unos minutos a consolidar los datos en sus paneles es un tiempo bien invertido. La otra vía sería realizar un filtro a nivel de informe desde la herramienta de analítica convirtiendo todo el tráfico entrante en mayúscula o minúscula, así reduciendo los errores y los sesgos en los informes.\nSolución:\n Vamos a utilizar la función para convertir nuestras páginas en minúsculas. Creamos un campo calculado del tipo dimensión en nuestro data source y seleccionamos la función LOWER  Lower(page)  Separadores en las URLs Ha habido mucho debate sobre los mejores separadores de palabras de URL (+ , - o _ ) desde una perspectiva de SEO. Si se trata de un sitio web antiguo, es probable que los separadores de palabras de URL empleados hayan cambiado con el tiempo. Ha llegado el momento de consolidarlos en Data Studio.\nSolución:\n Utilizamos la función para reemplazar todos los separadores + y _ en un guion -. Creamos desde Data Source una nueva dimensión que nos ayude en esta tarea con la función REGEXP_REPLACE  REGEXP_REPLACE(Page,'(\\\\+|_)','-')  Eliminar los parámetros en las URLs La eliminación de parámetros de consulta de una lista de URL es un caso de uso común cuando se desglosan las métricas por dimensión de página. Esto se puede hacer con los filtros de Google Analytics para datos futuros o nuevas configuraciones, sin embargo, no es una solución para los datos ya recopilados.\nSolución:\n Ejemplo aplicado con la función REGEXP_REPLACE en nuestro data source:  REGEXP_REPLACE(page, '\\\\?.+', '')  Agrupación de contenidos ¿Necesitas agrupar las métricas por tipos de contenido específicos para obtener mejores insights? La configuración de grupos de contenido es la forma estándar de hacer esto para datos futuros así como los actuales. También podemos aplicarlos con la ayuda de campos calculados en Google Data Studio. Solución:\n Creamos la agrupación de contenidos en Data Studio. A través de la función CASE desde la fuente de datos, creamos una dimensión personalizada.  CASE WHEN (Page=\u0026quot;/es\u0026quot; OR Page=\u0026quot;/de\u0026quot; OR Page=\u0026quot;/pt\u0026quot;) THEN \u0026quot;Inicio\u0026quot; WHEN (REGEXP_MATCH(Page, '^/../shop/.*')) THEN \u0026quot;Ecommerce\u0026quot; WHEN (REGEXP_MATCH(Page, '^/../blog/.*')) THEN \u0026quot;Blog\u0026quot; WHEN (REGEXP_MATCH(Page, '(^/../company/.*|.*empleo.*)')) THEN \u0026quot;Empresa\u0026quot; WHEN (REGEXP_MATCH(Page, '^/../support/.*')) THEN \u0026quot;Soporte\u0026quot; WHEN (REGEXP_MATCH(Page, '.*/contact``` )) THEN \u0026quot;Contactos\u0026quot; ELSE \u0026quot;Otros\u0026quot; END  Estamos usando la función CASE junto con la función REGEX_MATCH para definir los grupos de contenido. El poder de las expresiones regulares te permite definir reglas de coincidencia más complejas. Ten en cuenta las condiciones superpuestas en las diferentes declaraciones WHEN: la función CASE siempre ejecutará la primera cláusula que coincida.\nEjemplo: la página URL /blog/consejos-buscar-empleo se agrupará en la categoría Blog, aunque también coincide con la categoría Empresa, por tener en su URL la palabra empleo. Sin embargo, tal como hemos estructurado nuestra función, la primera condición TRUE se ejecutará y nos devolverá el primer resultado. Podemos personalizar todo lo mencionado anteriormente para que se ajuste mejor a nuestro caso de uso específico.\nPreguntas frecuentes Ya que me llegaron varios comentarios anteriormente a este post, intentaré resumir algunas de las preguntas más frecuentes que he ido recopilando y daré una explicación sobre el uso de las métricas calculadas.\n ¿Por qué existen limitaciones en cuanto a qué métricas/dimensiones puedo utilizar de forma conjunta en un campo calculado de Google Data Studio?  Normalmente podemos utilizar métricas o dimensiones en una misma función o a un nuevo campo calculado, pero cuando intentamos introducir las dos conjuntamente muchas veces devuelve un error. Cuando normalmente trabajamos con la herramienta de analitica digital, Google Analytics por ejemplo, y queremos crear campos calculados hay una lógica detrás: puede resumir usuarios y sesiones, restar páginas vistas y dividir por salidas si lo desea. También existe un límite de alcances que no permite realizar un resultado, por ejemplo la jerarquía habitual en analítica digital es User \u0026gt; Sessions \u0026gt; Events. Alterar este orden conlleva a un error.\nLos campos calculados de la fuente de datos también se pueden utilizar en otros campos calculados del mismo tipo (dimensión frente a métrica); sin embargo, esto no funciona con campos calculados a nivel de gráfico.\n ¿Por qué no funciona mi función CASE?  Una razón que he encontrado es cuando incluí campos del tipo Fecha en mi fórmula CASE; es decir, día de la semana. Esos campos se representan internamente con un número en lugar del nombre del día; es decir, 1 en lugar de lunes. Para resolver esto, puede usar el número en tu declaración de condición o usar el campo Nombre del día de la semana en su lugar.\n ¿Por qué mi función REGEX no coincide?  Todos hemos estado allí y la resolución de problemas puede ser tediosa. Si se trata de un caso de uso de Google Analytics se puede probar su expresión regular con los filtros avanzados en GA. Alternativamente, se puede utilizar otras herramientas de testing. Yo utilizo esta de Regex101 , pero hay muchas más.\n ¿Puedo crear un campo calculado a partir de dos fuentes diferentes?  Sí. Sin embargo, esto solo funciona en una configuración específica. En primer lugar, deberá unir tus fuentes de datos a una fuente de datos combinada (data blending). Luego, puedes crear un campo calculado a nivel de gráfico (con sus limitaciones) sobre su nueva fuente de datos combinados. Lamentablemente, no puedes crear campos calculados de fuente de datos en datos combinados.\n ¿Por qué mi campo calculado está en blanco?  Lo más probable es que su función de campo calculado no devuelva ningún valor para el período de tiempo seleccionado. De forma predeterminada, Data Studio muestra *null** en las tablas si este es el caso. Sin embargo, también se puede cambiar a nada y transformar este valor en blank o -no data-.\nConclusión El uso de los campos calculados de Google Data Studio nos ha ayudado a llevar nuestros informes al siguiente nivel. Ya sea mediante su uso para aumentar sustancialmente el interés de las partes interesadas y la demanda de informes personalizados o para obtener información completamente nueva sobre sus datos son imprescindibles si utilizas Data Studio.\nComo especialista en marketing digital, el éxito siempre dependerá de su capacidad para presentar datos relevantes de la forma más intuitiva y atractiva. Los campos calculados de Data Studio pueden ser una parte esencial para facilitar la toma de decisiones basada en datos. Pero no se aplica solo a esta herramienta SaaS, como analista digital, analista de datos o también si trabajas en inteligencia de negocio, crear nuevas dimensiones y métricas es algo muy habitual. Como he mencionado en otro post sobre la comparativa de las diferentes plataformas de Business Intelligence, también podrás aplicar estas mismas técnicas si la herramienta de visualización a utilizar será Power BI, Tableau, AWS QuickSight, Looker\u0026hellip;\n¿Has encontrado algo nuevo sobre los campos calculados de Google Data Studio? Si es así, puedes comentar en este mismo post y abrir un debate.\nY si desea aumentar aún más su conocimiento de esta increíble herramienta de informes, consulte nuestro canal de YouTube y de los webinar realizados y/o futuros.\nFUENTE ORIGINAL\n","date":1607597574,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"24dcddcce267639e8e01aaa6e6e4800f","permalink":"https://www.marcusrb.com/google-data-studio-como-sacarle-todo-partido/","publishdate":"2020-12-10T11:52:54+01:00","relpermalink":"/google-data-studio-como-sacarle-todo-partido/","section":"post","summary":"¿Google Data Studio, ¿cómo sacarle todo el partido? Hace unas semanas os mostramos cómo crear campos calculados con Google Data Studio mediante una planificación con nuevos atributos y nuevas métricas, mejor conocidos como \u0026ldquo;campos calculados\u0026rdquo;, y, además, cuándo crearlos. Tenemos varias opciones de creación y, entre ellas, pusimos los ejemplos de las 5 funciones más utilizadas. Hoy os explicamos en concreto los casos de uso y resolvemos algunas de las preguntas más frecuentes.","tags":["data studio","data visualization","google","dashboard"],"title":"Google Data Studio, ¿cómo sacarle todo el partido?","type":"post"},{"authors":["marcusRB"],"categories":["Data Visualization","Webinar","Machine Learning"],"content":" Cómo visualizar los modelos de machine learning en Power BI En este Webinar haremos un recorrido desde Azure ML a Databricks, así como BigQuery ML pasando por Knime, hasta los modelos realizados de forma nativa con scikit-learn, PyTorch con Python y R, y otras técnicas más de minería y análisis.\nSeguramente ya has trabajado con todas estas técnicas o con algunas de ellas. Todas las opciones son válidas para mostrar los resultados de aprendizaje automático en una herramienta de visualización, como por ejemplo en Microsoft Power BI.\nEnfocaremos el tema desde el lado del usuario y de la parte del analista de negocio. Hay diferentes maneras de conseguirlo, por lo que veremos algunas de éstas y también comentaremos las restricciones, por ejemplo, de aquellos que cuentan con licencia Pro y Premium, y también las de aquellos usuarios que siguen investigando y evaluando esta herramienta de manera gratuita.\nEste evento tendrá una hora aprox. de duración donde veremos casos de uso, maneras de realizar dashboards y reportes y métodos para poder compartirlo con el resto de usuarios en la nube. Estará abierto a todo el mundo que tenga curiosidad acerca los datos, visualización y machine learning (en esta última parte mencionaré modelos sencillos de aplicar).\nPara poder acceder al Webinar es necesario que te inscribas en el formulario que aparece al final de esta página. ¡Te esperamos!\nPuedes visionar los webinars anteriores sobre machine learning y Power BI, así como consultar nuestros artículos relacionados:\n https://youtu.be/AFTanu_iBD4 https://www.paradigmadigital.com/eventos/mejora-tus-tecnicas-de-modelado-con-dax-y-power-query-en-power-bi/ https://www.paradigmadigital.com/eventos/automl-para-la-clasificacion-de-imagenes-entrena-modelos-sin-una-linea-de-codigo/-  Link oficial en GitHub\n  ","date":1606323174,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"c7329f790a5bc3beaef0ad646da9a3f9","permalink":"https://www.marcusrb.com/webinar-visualiza-azure-machine-learning-power-bi/","publishdate":"2020-11-25T17:52:54+01:00","relpermalink":"/webinar-visualiza-azure-machine-learning-power-bi/","section":"post","summary":"Cómo visualizar los modelos de machine learning en Power BI En este Webinar haremos un recorrido desde Azure ML a Databricks, así como BigQuery ML pasando por Knime, hasta los modelos realizados de forma nativa con scikit-learn, PyTorch con Python y R, y otras técnicas más de minería y análisis.\nSeguramente ya has trabajado con todas estas técnicas o con algunas de ellas. Todas las opciones son válidas para mostrar los resultados de aprendizaje automático en una herramienta de visualización, como por ejemplo en Microsoft Power BI.","tags":["power-bi","data visualization","azure","machine learning"],"title":"[Webinar] - Visualiza Machine Learning en Power BI","type":"post"},{"authors":["marcusRB"],"categories":["Data Visualization","Webinar"],"content":" Cómo integrar R y Python en tus análisis de datos de Power BI En este webinar veremos unas recomendaciones útiles para el uso correcto de los lenguajes de programación y estadístico más utilizados en Data Analytics, Python y R.\nVamos a explotar los datos en la herramienta de visualización de inteligencia de negocio Microsoft Power BI y veremos sus capacidades, límites y técnicas de uso. Cuando se usan correctamente, se logran obtener respuestas rápidas en la interpretación, validación / cálculos, análisis / inferencias estadísticos de los datos. Sin embargo, antes de usar Python / R en Power BI, hay que tener en cuenta los problemas de rendimiento y privacidad de los datos.\nSi te perdiste el webinar, no te preocupes. Aquí tienes el vídeo de la charla. ¡Esperamos que sea de tu interés!\n  ","date":1602348774,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"0d7bb7f89f4fa39cfdacbab4b164e48e","permalink":"https://www.marcusrb.com/webinar-integrar-r-python-power-bi/","publishdate":"2020-10-10T17:52:54+01:00","relpermalink":"/webinar-integrar-r-python-power-bi/","section":"post","summary":"Cómo integrar R y Python en tus análisis de datos de Power BI En este webinar veremos unas recomendaciones útiles para el uso correcto de los lenguajes de programación y estadístico más utilizados en Data Analytics, Python y R.\nVamos a explotar los datos en la herramienta de visualización de inteligencia de negocio Microsoft Power BI y veremos sus capacidades, límites y técnicas de uso. Cuando se usan correctamente, se logran obtener respuestas rápidas en la interpretación, validación / cálculos, análisis / inferencias estadísticos de los datos.","tags":["power-bi","data visualization","r-studio","python"],"title":"[Webinar] - PoweR-Py - Integrar R y Python en Power BI","type":"post"},{"authors":["marcusRB"],"categories":["Data Visualization"],"content":" ¿Cómo crear campos calculados en Google Data Studio? Es muy probable que ya conozcas la herramienta de visualización Google Data Studio, un servicio gestionado de Google alojado en la nube.\nHemos visto su gran potencial para la generación de reportes/informes y análisis de visualización (que dejó su versión beta en 2018). Además, su uso no está extendido solamente a analistas de datos o a profesionales del marketing digital (creando informes de Google Ads, Facebook y Google Analytics), sino que también podemos aprovecharlo para integrar diferentes fuentes de datos como de producción (API tipo Jira u otras de project management e integradas con Google Cloud Platform). Existe un punto común: todos están aprovechando los campos calculados de Google Data Studio, ya que son una herramienta súper poderosa que se debería usar.\nEs cierto que en el contexto de la inteligencia de negocio, así como en ámbitos más técnicos, el rol de un analista de datos debería estar familiarizado con: programación SQL y conceptos de visualización; el funcionamiento de las APIs, resto de conectores y entorno cloud; y con el contexto del origen de los datos.\nCada herramienta de visualización tiene su complejidad a la hora de realizar una Extracción, Transformación y Carga o ETL (Extract, Transform, Load), donde los campos calculados y la creación de nuevas medidas entran en juego. Imagina tener unos objetivos y filtros mal configurados, agrupaciones de contenido no retroactivas y parámetros inconsistentes en tu cuenta de Google Analytics. O, en el peor de los casos, realizar todas las operaciones de forma manual, de manera desordenada, en hojas de cálculo de Google o funciones de VLOOKUP con referencias que devuelven un valor nulo\u0026hellip;\nPor estos motivos, nace la idea de realizar una guía completa y actualizada de los campos calculados de Google Data Studio, que te será muy útil para optimizar informes y obtener mejores resultados.\n¿Qué son los campos calculados de Google Data Studio? Los campos calculados te permiten aplicar cálculos y otras funciones a los datos para crear nuevas métricas y dimensiones. Además, se pueden usar para ampliar y transformar la información en sus fuentes de datos y permite replicarlos en otras herramientas, como Power BI con DAX o en Tableau.\nEntre otras ventajas, pueden ayudarte a:\n Calcular la tasa de conversión entre dos métricas. Analizar la tasa de caída del paso del funnel. Limpiar URLs y cadenas de texto. Agrupación de contenidos similares. Cálculos con sentencias condicionales. Manipulación de fechas.  Actualmente Google Data Studio aumenta de forma significativa el número de las funciones, al igual que mejora la otra tool de tratamiento de datos, Google Spreadsheets (que en su propia web indica más de 50 funciones y que se requieren para elaborar un dashboard más completo posible).\n¿Por qué utilizar los campos calculados en Data Studio? Siempre que quieras presentar datos que aún no están disponibles en datos de origen, los campos calculados son el mejor aliado, ya que te permiten hacerlo:\n Más bonito (es decir, limpiando su informe de URL). Más perspicaz (es decir, concatenando nombre de host + ruta de página de destino). Más procesable (es decir, al relacionar los resultados con los objetivos establecidos).  Presentar los datos de la manera más fácil y sin ambigüedades es primordial si desea que los clientes / stakeholders confíen en las recomendaciones y tomen las medidas apropiadas al respecto.\n¿Cuándo y cómo puedo utilizarlos? Los puedes utilizar en cualquier tipo de gráfico. Sin embargo, solo los campos calculados a nivel de gráfico se pueden usar en fuentes de datos combinadas (o data blending). Por tanto, es importante diferenciar:\n Los campos calculados de la fuente de datos se agregan a la fuente de datos y se pueden reutilizar en otros gráficos e informes. Los campos calculados a nivel de gráfico solo se pueden usar en un gráfico en particular, sin embargo, funcionan con datos combinados.  Primer campo calculado, paso a paso Planificación Antes de realizar un cuadro de mando, hay que conocer los datos y realizar una sesión de data discovery para detectar en una matriz tipo DAFO las necesidades, dificultades, indicadores y riesgos. Para poner un ejemplo, hagamos un inventario de todos los KPIs que no están directamente disponibles en una sola fuente de datos. Definamos qué fuentes de datos, dimensiones, métricas y funciones necesitaremos para crearlas.\nEjemplo: el sitio web de generación de leads de su cliente tiene tres formularios de contacto que se rastrean como objetivos separados en Google Analytics. Sin embargo, en el dashboard se requiere que el detalle de la ‘tasa de conversión’ del formulario de contacto para los tres objetivos sea del formulario de contacto combinado\nPara crear un campo calculado ‘tasa de conversión de objetivos’ (con todos los formularios de contacto), se necesitará:\n Una vista de Google Analytics como fuente de datos. Las métricas de cumplimiento de objetivos individuales y la de métrica de sesión. Algún cálculo aritmético básico en un campo calculado que nos llevará al resultado deseado.  Un consejo: un error común de cualquier analista de datos es importar todos los campos en el dashboard y comenzar a pintar, aumentando el tiempo de respuesta de los diferentes cálculos, empeorando la performance y obviando la creación de nuevas métricas.\nImplantación del campo calculado Añadimos el nuevo campo calculado al data source o al gráfico. Podemos tomar como guía esta tabla comparativa para saber cuándo utilizar uno u otro:\n    Campo calculado por fuente de datos Campo calculado por gráfico     ¿Necesita permisos al Data Source? ✔️ ❌   ¿Necesita combinar datos? ❌ ✔️   ¿Se requieren otros campos calculados? ✔️ ❌   ¿Deberían ser utilizados en otros informes o gráficos? ✔️ ❌    A continuación, seguimos y creamos un campo calculado a nivel de fuente de datos.\nAñadimos cualquier gráfico al lienzo en blanco que pueda tomar múltiples dimensiones y métricas como entradas (por ejemplo, una tabla).\n Hacemos click en Add A Field o Crear nuevo campo, tal como indica en la captura.   Creamos el campo calculado, utilizando una de las funciones disponibles, otro campo calculado o una medida calculada.   La alternativa por campo calculado aplicado al gráfico si queremos que sea Dimensión o Métrica, en caso del tipo gráfico, se puede realizar desde el panel indicado en la captura.   Y, en este último caso, directamente creamos el campo calculado.  Siguiendo con el ejemplo anterior, si queremos que sea Tasa de conversión de objetivos (todos los forms), que sea utilizado en otros informes y requerimos permisos será campo calculado del tipo Data Source.\nAñadimos la función En el campo de Fórmula podemos definir cómo deben manipularse sus dimensiones o métricas existentes. Es posible que sea muy básico, pero siempre es útil entender cómo funciona ya que es simplemente hacer un cálculo aritmético entre las métricas.\nLa verificación verde (check abajo de la propia ventana) indica que la fórmula no contiene ningún error de sintaxis. Presionamos Guardar y Data Studio realizará otra comprobación de detección, pero esta vez también considerando los datos reales.\n Del ejemplo anterior realizamos una operación aritmética basada en la suma de los diferentes objetivos dividida entre las sesiones. Importante: debemos fijarnos en el uso correcto de los paréntesis.  Añadimos el campo a nuestro dashboard Una vez realizados los pasos anteriores, guardamos y lo aplicamos en nuestra tabla. Hay que realizar unos pequeños cambios, como tener que indicarle el tipo de dato o campo (siendo un ratio será en formato porcentaje).\n¿Qué son las funciones de Google Data Studio? Las funciones de Data Studio son fórmulas que se pueden usar en campos calculados para manipular y combinar campos existentes de formas más avanzadas y complejas de lo que permite la aritmética básica.\nAl igual que las fórmulas en el software de hojas de cálculo como Excel o Google Sheets, las funciones de Data Studio tienen un propósito y una sintaxis específicas. Google las clasifica en 6 categorías:\n Agregación (cómo MAX, MIN, AVG, SUM). Aritméticos (ej. ROUND para realizar redondeo de los valores numéricos). Fechas (ej. WEEKDAY para devolver el nombre del día de la semana). Geográfico (ej. TOCOUNTRY para devolver el código ISO del país). Texto (ej. CONCAT para realizar la concatenación de textos). Otras (ej. CASE para devolver valores basados en condiciones).  Las cinco funciones más utilizadas en Data Studio  UPPER/LOWER  Para ser ordenados, organizados y tiquismiquis con nuestros datos tenemos que utilizar categorías en mayúsculas, así como las dimensiones más importante, y dejar el resto en minúsculas. Entonces, puedes realizar todo esto con las funciones UPPER/LOWER que realizará la conversión de las cadenas de textos como input, en el formato requerido.\nEjemplo: realizar la conversión a mayúscula de los dispositivos.\nUPPER(Device Category)   CONCAT  ¿Necesitas unir múltiples campos de texto para que tus datos sean más comprensibles? Si quieres combinar dos dimensiones (o convertir una métrica en dimensión) en una columna de la tabla es un caso de uso común para la función CONCAT. Puede tener múltiples dimensiones como entradas y la función devuelve la concatenación de todos los campos.\nEjemplo: desde un data source Google Sheets, importamos la tabla de wikipedia y concatenamos el idioma y el código.\nCONCAT(\u0026quot;Idioma: \u0026quot;,Language,\u0026quot;-\u0026quot;,Wiki)   REGEX_EXTRACT  ¿Necesitas extraer parte de un texto?, ¿o valores numéricos que tenga un patrón? Las expresiones regulares son muy importantes y aunque existen muchas guías para entender su funcionamiento, os recomiendo esta guía de RegExp.\nEjemplo: extraemos solamente aquellas páginas que solo tienen un patrón google+redesign y nos devuelve la parte restante.\nREGEXP_EXTRACT(Page,'^/google\\+redesign/(.*)+')  Input:\nOutput:\n REGEX_MATCH  Si necesitamos evaluar una cierta condición pero el input a pasar no es exacto, entonces podemos utilizar la expresión REGEX_MATCH. Esta evalúa la condición y devolverá un valor sea True o False, al igual que la anterior busca la condición y devuelve el resultado.\nLo habitual es que sea utilizada junto con otras funciones, como para encontrar un patrón y devolver otro resultado. Ejemplo, el uso del CASE \u0026ndash;\u0026gt; WHEN \u0026ndash;\u0026gt; THEN.\n CASE  Como hemos visto anteriormente, la función CASE indica una expresión condicional no muy compleja, pero hay que entender bien su funcionamiento para poder obtener el resultado final correctamente.\nEjemplo: buscamos patrones y devolvemos un valor final.\nCASEWHEN REGEXP_MATCH(Language, \u0026quot;(English|German|Dutch|Swedish)\u0026quot;)THEN \u0026quot;Germanic\u0026quot;WHEN REGEXP_MATCH(Language, \u0026quot;(Italian|French|Spanish)\u0026quot;)THEN \u0026quot;Romance\u0026quot;ELSE \u0026quot;Other family\u0026quot;END  Conclusiones Este post es la primera parte de los campos calculados en Google Data Studio. Hemos observado que en cualquier fichero, conector o fuente de datos en esta herramienta (o cualquier otra de visualización) será necesario primero identificar los indicadores y sucesivamente crear las métricas o nuevas columnas. Pronto, una segunda parte en la que veremos ejemplos aplicados a marketing digital y más casos de uso.\nFUENTE ORIGINAL\n","date":1602348774,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"ce2bc5e0eb6f1e5727fd6725a42fc2a3","permalink":"https://www.marcusrb.com/como-crear-campos-calculados-en-data-studio/","publishdate":"2020-10-10T17:52:54+01:00","relpermalink":"/como-crear-campos-calculados-en-data-studio/","section":"post","summary":"¿Cómo crear campos calculados en Google Data Studio? Es muy probable que ya conozcas la herramienta de visualización Google Data Studio, un servicio gestionado de Google alojado en la nube.\nHemos visto su gran potencial para la generación de reportes/informes y análisis de visualización (que dejó su versión beta en 2018). Además, su uso no está extendido solamente a analistas de datos o a profesionales del marketing digital (creando informes de Google Ads, Facebook y Google Analytics), sino que también podemos aprovecharlo para integrar diferentes fuentes de datos como de producción (API tipo Jira u otras de project management e integradas con Google Cloud Platform).","tags":["data studio","data visualization","google","campos calculados"],"title":"¿Cómo crear campos calculados en Google Data Studio?","type":"post"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Aprende a enviar los valores de e commerce al pixel de Google Ads con Tag Manager. El paso más tedioso de todos marketeros es registrar correctamente las transacciones desde la web a Google Analytics o cualquier herramienta de analítica. Pero un solo check no es suficiente para poder llevar todas tipos de personalizaciones de comercio electrónico y aprovechar las bondades del dataLayer presente en nuestro site. Un ejemplo es la personalización de los eventos de comercio electrónico mejorado a eventos, o mejor, hacía las plataforams de Advertising, como Google Ads, Facebook, Linkedin, hasta Twitter, Tik Tok y Bing Ads. Es importante tener el dataLayer implementado (bien por plugin o bien via desarrollo), y el resto lo haremos en Google Tag Manager a través de pequeñas funciones en JavaScript. ¿Estás listo para ser el ninja de Google Tag Manager? En este video tutorial te explicaremos como realizar unos pasos más habituales y el resto podrás realizarlos sin dificultades.\nVideo   ","date":1600279200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"14ac9a62cf80c580ef682793a4cbc54d","permalink":"https://www.marcusrb.com/talk/google-ads-gtm/","publishdate":"2020-09-01T20:00:00Z","relpermalink":"/talk/google-ads-gtm/","section":"talk","summary":"Tutorial - Enviamos los valores de e-commerce al pixel de Google Ads con Tag Manager.","tags":["googleads","googletagmanager"],"title":"Tutorial - Enviamos los valores de e-commerce al pixel de Google Ads con Tag Manager.","type":"talk"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Aprende a rear una etiqueta en Google Tag Manager para registrar las llamadas. ¿Necesitas registrar las llamadas efectuadas directamente desde la web? Con la personalización de la etiqueta de seguimiento de eventos de llamadas es posible tanto para Google Ads, como este video tutorial, como el resto de plataformas de publicidad. Aunque la etiqueta está bien escondida dentro del repositorio en Google Tag Manager, veamos como poder realizar con simples pasos su personalización y tener constancia del tráfico entrante de nuestros clientes potenciales y grabar sus interacciones correctamente para la mejor atribución de las conversiones en Google Ads u lo que sea. No hace apoyo técnico, solo una cuenta de Google Ads, Google Tag Manager y seguir los pasos del vídeo. En caso de tener otro tipo de evento, como slider o eventos tipo Ajax, entonces se requiere una personalización avanzada con la llamada a la librería de Ajax o jQuery.\nVideo   ","date":1599674400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"0c1f7356902c60a296debdd373a4d529","permalink":"https://www.marcusrb.com/talk/llamadas-gtm/","publishdate":"2020-09-01T20:00:00Z","relpermalink":"/talk/llamadas-gtm/","section":"talk","summary":"Tutorial - crear una etiqueta en Google Tag Manager para registrar las llamadas.","tags":["leads","googletagmanager","llamadas"],"title":"Tutorial - crear una etiqueta en Google Tag Manager para registrar las llamadas.","type":"talk"},{"authors":["marcusRB"],"categories":["Data Analytics"],"content":" Modelo de clasificación con Google Analytics y BigQuery ML. Si eres analista de datos o trabajas codo a codo con el departamento de Marketing Digital, quizás sea típico en tu día a día tener que utilizar la herramienta de análisis Google Analytics, acompañada de otras tools de exploración de datos (hojas de cálculos, API nativas, herramientas de terceros).\nUna de las limitaciones que se nos presenta con Google Analytics es que todos los datos se encuentran en los propios servidores en EEUU, por lo que tenemos muy poca flexibilidad a la hora de extraer.\nAquí, la solución o la única manera para poder explotar estos datos de forma local (además de realizar lo mismo con otras como Data Studio, Tableau, Power BI, etc.) es utilizar técnicas de segmentaciones de clientes, predicciones y clasificaciones utilizando Python, R y otras muchas habilidades en programación SQL o JavaScript, entre otras.\n¿Qué es BigQuery? Tal como la define la propia web de Google Cloud Platform:\n BigQuery es la base de datos analítica de bajo costo totalmente administrada, NoOps de Google. Con BigQuery puede consultar terabytes y petabytes de datos sin tener ninguna infraestructura para administrar o necesitar un administrador de base de datos. BigQuery usa SQL y puede aprovechar el modelo de pago por uso. BigQuery le permite concentrarse en analizar datos para encontrar ideas significativas.  En la versión premium, podemos conectar esta herramienta a la base de datos NoOps, Google BigQuery. Es decir que con si aprovechamos el potencial de Google Analytics y tenemos una cuenta de Google BigQuery podemos realizar operaciones de Machine Learning aplicadas a datos de nuestra web. Con un pero: esta funcionalidad está solo habilitada en la versión premium, Google Analytics 360; con la otra versión existe otra opción de volcado, ingesta de datos a través de conectores y posterior explotación (más manual).\n¿Qué podemos realizar con BigQuery?  Consulta y explora el conjunto de datos de comercio electrónico. Crear un conjunto de datos de capacitación y evaluación que se utilizará para la predicción por lotes. Crear un modelo de clasificación (regresión logística) en BQML. Evaluar el rendimiento de su modelo de aprendizaje automático. Predecir y clasificar la probabilidad de que un visitante realice una compra.  Si necesitas conocer cómo explotar al máximo BigQuery, aquí unos consejos muy útiles de Tomás Calleja.\n¿No tienes Google Analytics? Opta por el dataset de prueba Hace muchos años que Google ofreció su propia cuenta de analítica digital para realizar pruebas y que, además, permite aprender un montón sobre la propia herramienta y las configuraciones.\nLos datos son \u0026ldquo;reales\u0026rdquo;. Aquí podría discutir si es cierto o no, pero observando las cifras de ventas, no me extrañaría\u0026hellip; La web que utiliza Google para su comercio electrónico es la denominada Google Merchandise Store y podría decir que tienen muchas cosas interesantes.\nRespecto a la cuenta de Analytics, con solo abrir y validar una cuenta de Google y obtener Google Analytics, ya se pueden explorar los datos simplemente con el enlace proporcionado y disponibles en este enlace Google Analytics DEMO. Se vincularía automáticamente, ¡y listo para operar!\nComo tarea, puedes realizar una navegación al mismo sitio web del merchandise store y así ver en tiempo real que funciona correctamente.\nTambién puedes modificar los parámetros de fuente y medio (con un pequeño trick) y colarte en sus informes, tal como se muestran en las capturas.\nEl dataset está disponible para su explotación desde el propio Google Cloud Platform, en el repositorio público (el merchandise store de Google para realizar pruebas de concepto).\n¿Cómo realizar la conexión entre Google Analytics 360 y BigQuery? Aquí te explicaré los pasos para poder realizar la conexión en caso de querer tener los datos en un data lake como BigQuery.\nDesde Google Analytics, tenemos que fijarnos en:\n Propiedad \u0026gt; Vinculación con otros productos \u0026gt; Todos los productos y buscamos Configurar enlace de BigQuery.  Desde esta ventana seleccionamos la vista deseada para su vínculo y realizamos estos pasos indicados en esta captura:\n Vista de Google Analytics:   Contactos:   Preferencia de ingesta, si en streaming o en lotes:  Ahí, confirmamos del lado de BigQuery unos pasos:\n Habilitar los permisos necesarios a Analytics para poder leer y extraer los datos:   Indicar los datos de la vista de Google Analytics:  Se confirma todo y ya tenemos los datos volcados en unas horas en BigQuery y, así, en caso de seleccionar varias veces al día y según el tamaño de nuestro Analytics, todos los días recibiremos una notificación de carga de los datos.\n¿Cómo explotar los datos a través de SQL de BigQuery ? Veamos ahora unos ejemplos de cómo realizar consultas con BigQuery, con pequeñas sentencias en #standardSQL. Los datos serán volcados por días y tenemos diferentes opciones para poder realizar las consultas. Aún así, si no estás familiarizado con la API de Google Analytics, aquí un esquema de todos los campos que puedes utilizar y obtener informaciones.\n Identificar los campos duplicados:  #standardSQL SELECT COUNT(*) as num_duplicate_rows, * FROM `data-to-insights.ecommerce.all_sessions_raw` GROUP BY fullVisitorId, channelGrouping, time, country, city, totalTransactionRevenue, transactions, timeOnSite, pageviews, sessionQualityDim, date, visitId, type, productRefundAmount, productQuantity, productPrice, productRevenue, productSKU, v2ProductName, v2ProductCategory, productVariant, currencyCode, itemQuantity, itemRevenue, transactionRevenue, transactionId, pageTitle, searchKeyword, pagePathLevel1, eCommerceAction_type, eCommerceAction_step, eCommerceAction_option HAVING num_duplicate_rows \u0026gt; 1;  Copy.\n Analizar todas las nuevas sesiones:  #standardSQL # schema: https://support.google.com/analytics/answer/3437719?hl=en SELECT fullVisitorId, # the unique visitor ID visitId, # a visitor can have multiple visits date, # session date stored as string YYYYMMDD time, # time of the individual site hit (can be 0 to many per visitor session) v2ProductName, # not unique since a product can have variants like Color productSKU, # unique for each product type, # a visitor can visit Pages and/or can trigger Events (even at the same time) eCommerceAction_type, # maps to ‘add to cart', ‘completed checkout' eCommerceAction_step, eCommerceAction_option, transactionRevenue, # revenue of the order transactionId, # unique identifier for revenue bearing transaction COUNT(*) as row_count FROM `data-to-insights.ecommerce.all_sessions` GROUP BY 1,2,3 ,4, 5, 6, 7, 8, 9, 10,11,12 HAVING row_count \u0026gt; 1 # find duplicates  Copy.\n Obtener los visitantes únicos por agrupación de canal:  #standardSQL SELECT COUNT(DISTINCT fullVisitorId) AS unique_visitors, channelGrouping FROM `data-to-insights.ecommerce.all_sessions` GROUP BY channelGrouping ORDER BY channelGrouping DESC;  Copy.\n Hasta incluso realizar operaciones con los pedidos y obtener ratios:  #standardSQL SELECT COUNT(*) AS product_views, COUNT(productQuantity) AS orders, SUM(productQuantity) AS quantity_product_ordered, SUM(productQuantity) / COUNT(productQuantity) AS avg_per_order, (v2ProductName) AS ProductName FROM `data-to-insights.ecommerce.all_sessions` WHERE type = 'PAGE' GROUP BY v2ProductName ORDER BY product_views DESC LIMIT 5;  Copy.\n Obtener productos comprados y unidades:  #standardSQL SELECT COUNT(*) AS product_views, COUNT(productQuantity) AS orders, SUM(productQuantity) AS quantity_product_ordered, v2ProductName FROM `data-to-insights.ecommerce.all_sessions` WHERE type = 'PAGE' GROUP BY v2ProductName ORDER BY product_views DESC LIMIT 5;  Copy.\nHasta aquí tenemos una visión general de nuestro grande \u0026ldquo;lago de datos\u0026rdquo;. Gracias al potencial de BigQuery podemos sacar ventaja a la hora de realizar análisis exploratorio o, inclusive, conectarnos con R o Python con la propia API de BigQuery y realizar visualizaciones muy interesantes.\n¿Cómo seleccionar el modelo de BigQuery ML? Para poder aprovechar el BigQuery ML, el aprendizaje automático aplicado a nuestro Google Analytics, podemos seleccionar un problema o modelo entre Forecasting o Clasificación.\nMientras que el primero se basará en obtener valores numéricos como ventas basados en datos históricos; el segundo, si un usuario con unas características comprará o no en nuestro ecommerce, y así potenciar mejor las campañas de marketing.\nVeamos cómo poder crear un modelo que aproximadamente tardará unos 5 - 10 minutos:\nCREATE OR REPLACE MODEL `ecommerce.classification_model` OPTIONS ( model_type='logistic_reg', labels = ['will_buy_on_return_visit'] ) AS #standardSQL SELECT * EXCEPT(fullVisitorId) FROM # features (SELECT fullVisitorId, IFNULL(totals.bounces, 0) AS bounces, IFNULL(totals.timeOnSite, 0) AS time_on_site FROM `data-to-insights.ecommerce.web_analytics` WHERE totals.newVisits = 1 AND date BETWEEN '20160801' AND '20170430') # train on first 9 months JOIN (SELECT fullvisitorid, IF(COUNTIF(totals.transactions \u0026gt; 0 AND totals.newVisits IS NULL) \u0026gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid) USING (fullVisitorId) ;  Copy.\nEste modelo nos ayudará a entrenar correctamente con datos históricos donde utilizamos los visitantes, teniendo en cuenta la tasa de rebote y el tiempo de permanencia o duración de sesión.\nEl resultado de este ejemplo es de apenas un 73%; no es muy satisfactorio, la verdad.\nUna vez creado el modelo, tenemos otra nueva tabla. En este caso classification_model e incluso podemos evaluar nuestra curva de ROC (que pide el ratio entre los True Positive y False Positive), como este:\n¿Cómo podemos realizar mejoras en nuestro modelo de BigQuery ML? Como todos modelos de Machine Learning, también en este podemos realizar mejoras, aplicando unos campos adicionales específicos de navegación, por ejemplo:\n En qué punto ha abandonado el proceso de compra. De dónde vino el visitante (fuente de tráfico, búsqueda orgánica, sitio de referencia). Categoría de dispositivo (móvil, tablet, escritorio). Información geográfica (país, ciudad, estado, región).  Creamos un segundo modelo con estos detalles:\nCREATE OR REPLACE MODEL `ecommerce.classification_model_2` OPTIONS (model_type='logistic_reg', labels = ['will_buy_on_return_visit']) AS WITH all_visitor_stats AS ( SELECT fullvisitorid, IF(COUNTIF(totals.transactions \u0026gt; 0 AND totals.newVisits IS NULL) \u0026gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid ) # add in new features SELECT * EXCEPT(unique_session_id) FROM ( SELECT CONCAT(fullvisitorid, CAST(visitId AS STRING)) AS unique_session_id, # labels will_buy_on_return_visit, MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress, # behavior on the site IFNULL(totals.bounces, 0) AS bounces, IFNULL(totals.timeOnSite, 0) AS time_on_site, IFNULL(totals.pageviews, 0) AS pageviews, # where the visitor came from trafficSource.source, trafficSource.medium, channelGrouping, # mobile or desktop device.deviceCategory, # geographic IFNULL(geoNetwork.country, \u0026quot;\u0026quot;) AS country FROM `data-to-insights.ecommerce.web_analytics`, UNNEST(hits) AS h JOIN all_visitor_stats USING(fullvisitorid) WHERE 1=1 # only predict for new visits AND totals.newVisits = 1 AND date BETWEEN '20160801' AND '20170430' # train 9 months GROUP BY unique_session_id, will_buy_on_return_visit, bounces, time_on_site, totals.pageviews, trafficSource.source, trafficSource.medium, channelGrouping, device.deviceCategory, country );  Copy.\nPodemos realizar una evaluación del modelo, teniendo en cuenta que tardará siempre entre 5-10 minutos, dependiendo del tamaño de la tabla, y aplicamos unos filtros a nuestro resultado ROC, como en este ejemplo:\n#standardSQL SELECT roc_auc, CASE WHEN roc_auc \u0026gt; .9 THEN 'good' WHEN roc_auc \u0026gt; .8 THEN 'fair' WHEN roc_auc \u0026gt; .7 THEN 'decent' WHEN roc_auc \u0026gt; .6 THEN 'not great' ELSE 'poor' END AS model_quality FROM ML.EVALUATE(MODEL ecommerce.classification_model_2, ( WITH all_visitor_stats AS ( SELECT fullvisitorid, IF(COUNTIF(totals.transactions \u0026gt; 0 AND totals.newVisits IS NULL) \u0026gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid ) # add in new features SELECT * EXCEPT(unique_session_id) FROM ( SELECT CONCAT(fullvisitorid, CAST(visitId AS STRING)) AS unique_session_id, # labels will_buy_on_return_visit, MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress, # behavior on the site IFNULL(totals.bounces, 0) AS bounces, IFNULL(totals.timeOnSite, 0) AS time_on_site, totals.pageviews, # where the visitor came from trafficSource.source, trafficSource.medium, channelGrouping, # mobile or desktop device.deviceCategory, # geographic IFNULL(geoNetwork.country, \u0026quot;\u0026quot;) AS country FROM `data-to-insights.ecommerce.web_analytics`, UNNEST(hits) AS h JOIN all_visitor_stats USING(fullvisitorid) WHERE 1=1 # only predict for new visits AND totals.newVisits = 1 AND date BETWEEN '20170501' AND '20170630' # eval 2 months GROUP BY unique_session_id, will_buy_on_return_visit, bounces, time_on_site, totals.pageviews, trafficSource.source, trafficSource.medium, channelGrouping, device.deviceCategory, country ) ));  Copy.\nEl resultado ha alcanzado un 91%, mucho mejor que el anterior.\nResultados finales: Teniendo en cuenta esto que hemos visto, llega el momento de sacar unos insights y futuras mejoras:\n Del 6% de los primeros visitantes (en orden decreciente de probabilidad prevista), más del 6% realiza una compra en una visita posterior. Estos usuarios representan casi el 50% de todos los visitantes nuevos que realizan una compra en una visita posterior. En general, solo el 0,7% de los visitantes nuevos hacen una compra en una visita posterior.  Y muy importante, el target objetivo del 6% aumentaría el ROI de marketing 9 veces respecto al resto de objetivos.\nMis conclusiones respecto a integrar Google Analytics con el Machine Learning de BigQuery es que es un arma muy potente para todos departamentos de Marketing: primero, para aprovechar al máximo estas herramientas; segundo, mejorar los análisis; y tercero, tomar decisiones adecuadas frente a las campañas de advertising e, incluso, personalizadas ad-hoc.\nY tú, ¿has utilizado en tu departamento de marketing BigQuery ML de Google?, ¿has comprobado si el ROAS ha sido mayor respecto al utilizado en exclusiva Google Analytics?\nFUENTE ORIGINAL\n","date":1599065574,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"55fe0e6ad214cdd2a259b0ee7a2969a5","permalink":"https://www.marcusrb.com/construir-modelo-clasificacion-analytics-bigquery-ml/","publishdate":"2020-09-02T17:52:54+01:00","relpermalink":"/construir-modelo-clasificacion-analytics-bigquery-ml/","section":"post","summary":"Modelo de clasificación con Google Analytics y BigQuery ML. Si eres analista de datos o trabajas codo a codo con el departamento de Marketing Digital, quizás sea típico en tu día a día tener que utilizar la herramienta de análisis Google Analytics, acompañada de otras tools de exploración de datos (hojas de cálculos, API nativas, herramientas de terceros).\nUna de las limitaciones que se nos presenta con Google Analytics es que todos los datos se encuentran en los propios servidores en EEUU, por lo que tenemos muy poca flexibilidad a la hora de extraer.","tags":["machine learning"," google analytics","bigquery"],"title":"Modelo de clasificación con Google Analytics y BigQuery ML","type":"post"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Aprende cómo personalizar el pixel de Bing Ads con Google Tag Manager. En este tutorial vemos como personalizar el pixel de seguimiento del segundo buscador más importante, Bing.\nAunque no sea muy extendido su uso, Bing Ads cada vez más está ganando posiciones de mercado, como en muchos paises europeos es tan importante más que Google (por la cuestión privacidad), siendo el canal habitual en Francia, Alemania y UK. Gracias al gesto de etiquetas Google Tag Manager, podemos sin intervención de un desarrollador implementar el pixel o los diferentes de eventos, donde necesitamos recoger las interacciones de los usuarios, eventos de usabilidad y todas aquellos relacionados con el comercio electrónico. Gracias a la etiqueta nativa presente en Google Tag Manager, podemos en pocos clics crear diferentes pixel, el principal como base, y el resto para las interacciones. Es importante contar también con el dataLayer de comercio electrónico (clásico o mejorado), para recolectar el resto de eventos específicos delos pasos del funnel, al igual que hacemos con Google Ads y Google Analytics..\n¿Quién es el ponente? Marco Russo\nConsultor y Especialista en Data \u0026amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.\nVideo   ","date":1598983200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"b626f185ec2961823763029e120b0c0b","permalink":"https://www.marcusrb.com/talk/bing_ads_gtm/","publishdate":"2020-09-01T20:00:00Z","relpermalink":"/talk/bing_ads_gtm/","section":"talk","summary":"Tutorial - Aprende cómo personalizar el pixel de Bing Ads con Google Tag Manager.","tags":["bingads","googletagmanager"],"title":"Tutorial - Aprende cómo personalizar el pixel de Bing Ads con Google Tag Manager.","type":"talk"},{"authors":["marcusRB"],"categories":["Data Science","Webinar"],"content":" Cómo automatizar tareas de Machine Learning en KNIME ¿Es posible automatizar completamente el ciclo de vida de la ciencia de datos? ¿Es posible construir automáticamente un modelo de aprendizaje automático a partir de un conjunto de datos?\nDe hecho, en los últimos meses, han aparecido muchas herramientas que afirman automatizar todo o parte del proceso de ciencia de datos. ¿Cómo trabajan? ¿Podrías construir uno tú mismo? Si adopta una de estas herramientas, ¿cuánto trabajo sería necesario para adaptarlo a su propio problema y su propio conjunto de datos?\nPor lo general, el precio a pagar por el aprendizaje automático es la pérdida de control de un modelo de \u0026ldquo;caja negra\u0026rdquo;. Lo que gana en la automatización, lo pierde en el ajuste o la interpretabilidad. Aunque dicho precio podría ser aceptable para problemas de ciencia de datos circunscritos en dominios bien definidos, podría convertirse en una limitación para problemas más complejos en una variedad más amplia de dominios. En estos casos, es deseable una cierta cantidad de interacción con el usuario final.\nEn KNIME, la herramienta que tiene parte de automatizaciones y muchas de intervención humana, utiliza una interfaz totalmente automatizada para guiar a los usuarios a través de la selección, capacitación, prueba y optimización de varios modelos de aprendizaje automático. El flujo de trabajo ha sido diseñado para que los analistas de negocios creen fácilmente soluciones de análisis predictivo aplicando su conocimiento de dominio.\nEn este Webinar mostraremos los pasos de esta aplicación explicando en detalle las técnicas utilizadas para la extracción de nuevas features, aprendizaje automático, detección de valores atípicos, selección de características, optimización de parámetros y evaluación de modelos.\nPara este webinar la aplicación a descargar es totalmente FREE tanto para entornos Windows, MacOSx que Linux, y disponible en este enlace oficial\nPara acceder al evento, es necesario que te inscribas en el formulario que encontrarás al final de la página.\nAquí el extracto del video, no dude de comentar y compartir!\n  ","date":1593103974,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"495a067072b54a50fc6979689bf31423","permalink":"https://www.marcusrb.com/webinar-automatizar-tareas-machine-learning-knime/","publishdate":"2020-06-25T17:52:54+01:00","relpermalink":"/webinar-automatizar-tareas-machine-learning-knime/","section":"post","summary":"Cómo automatizar tareas de Machine Learning en KNIME ¿Es posible automatizar completamente el ciclo de vida de la ciencia de datos? ¿Es posible construir automáticamente un modelo de aprendizaje automático a partir de un conjunto de datos?\nDe hecho, en los últimos meses, han aparecido muchas herramientas que afirman automatizar todo o parte del proceso de ciencia de datos. ¿Cómo trabajan? ¿Podrías construir uno tú mismo? Si adopta una de estas herramientas, ¿cuánto trabajo sería necesario para adaptarlo a su propio problema y su propio conjunto de datos?","tags":["machine learning","knime","webinar"],"title":"[Webinar] - Automatizar tareas de Machine Learning con Knime","type":"post"},{"authors":null,"categories":null,"content":" Como realizar una consultoría o auditoría de Google Analytics ¿Necesita ayuda con Google Analytics? Permita que le muestre cómo utilizar los datos analíticos para cuantificar sus esfuerzos de SEO, aumentar el tráfico web, convertir a más visitantes en clientes potenciales y mejorar de forma mensurable su desempeño de marketing.\n¿Qué es Google Analytics? Google Analytics es una herramienta de análisis web que es extremadamente poderosa, además de ser gratuita. Los propietarios de negocios, ejecutivos de marketing y webmasters pueden usarlo para: medir y rastrear la actividad del sitio web, optimizar el rendimiento del sitio web, mejorar las tasas de conversión (por ejemplo, visitante a líder o visitante a venta) e incluso mejorar el rendimiento del marketing offline. Google Analytics está \u0026ldquo;basado en cookies\u0026rdquo; (a diferencia de una herramienta de análisis web que analiza los archivos de registro o logs).\nPara usar Google Analytics, simplemente hay que implementar una pequeña porción del código de JavaScript en cada página de su sitio web que desea rastrear. Relativamente hablando, esto hace que Google Analytics sea extremadamente fácil de usar y algunos sitios web se pueden configurar en menos de 30 minutos. Google Analytics se puede personalizar de varias maneras, por lo que es casi tan poderoso como muchas soluciones analíticas de sitios web pagados.\n¿Necesitas ayuda con Google Analytics y la nueva versión GA4? Si eres como la mayoría de los dueños de negocios, pones Google Analytics en tu sitio web, lo miras por unos meses y luego te ocupas de otras cosas. ¿Quién tiene tiempo para ver toda esta información? ¿Suena familiar? Únete a la multitud. Una de las quejas más comunes que tienen los propietarios de negocios locales sobre los datos de análisis de marketing y Google Analytics en particular es que hay una tonelada de datos: demasiados datos y no suficientes conocimientos procesables. ¡No necesita otro informe para analizar ni una hoja de cálculo para interpretar!\nTe ofrezco una solución de \u0026ldquo;hágalo por mí\u0026rdquo; que ayuda a los propietarios de negocios pequeños (y no tan pequeños) a personalizar Google Analytics para satisfacer mejor sus necesidades, examinar los informes interminables para identificar los más relevantes para el cliente y convertir los datos en resultados accionables! Actualizado también a la nueva versión Google Analytics 4 o GA4, incluido Firebase Analytics.\n¿Está interesado en consultar o capacitarse en Google Analytics o análisis digital? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Google Analytics - GA4\nRealizar una consultoría y/o auditoría de Google Tag Manager Google Tag Manager, la herramienta más utilizada para etiquetar eventos e interacciones, será el rol más importante para detectar nuevos insight en tu negocio, tener claro desde principio que medir y como medirlo,\nEl Administrador de etiquetas de Google es una gran herramienta para que los dueños de negocios o los equipos de marketing vean cómo funciona su sitio web: qué áreas están funcionando, qué rendimiento tiene bajo rendimiento, qué funciones no se están usando como creía, etc. Lo vemos como una excelente forma de recopilar datos de experiencia de usuario UX \u0026ldquo;User Experience\u0026rdquo;, y mejorar el ratio de conversión, CRO \u0026ldquo;Conversion Rate Optimization\u0026rdquo; en vivo en su sitio de clientes reales.\nGoogle proporciona una guía de desarrollador para usar la herramienta de análisis, tengo experiencia de casi 5 años en la herramienta de analísis para grandes clientes y cuentas, además de tener un background de programad web, especializado en la configuración de Google Tag Manager, así como en el análisis de los datos que proporciona (porque, sin información accionable, ¿qué valor tiene? datos proporcionan en sí mismo para obtener más de su sitio web?).\nQue servicio incluye el servicio de consultoría de Tag Manager? Como servicio de consultoría de Tag Manager, primero tendré que ver su objetivo empresarial y qué quieres que se analice, puedo analizar todo un sitio web, pero la pregunta sería: ¿Todos los datos valen lo mismo para su empresa? Así que ofrezco:\n Auditoría de sitios web y mapeo de etiquetas\n Configuración y configuración\n Aplicación y monitoreo\n Formación  Plan de implementación de Tag Manager Google Tag Manager permite el etiquetado inmediato y la inserción de fragmentos de código a su sitio web, pero sin un experto en la mejor forma de configurarlo o vincularlo con su seguimiento actual de Google Analytics, puede parecer perdido. Realizaré la configuración y la migración de datos del Administrador de etiquetas de Google, lo que permite un seguimiento y generación de informes analíticos eficientes. En caso de ayuda, podré necesitaré apoyo de vustro departamento IT.\nOs asesoraré para que el Administrador de etiquetas de Google será realizado de la mejor configuración junto con el mapeo de las etiquetas en función del objetivo de su sitio (lo que le permite realizar un seguimiento de todo, desde diferentes formularios de captura de clientes hasta clics de enlaces específicos).\nAuditoría de Google Tag Manager Teniendo en cuenta que además de un servicio de consultoría e implementación, también podré asesorarle sobre los puntos claves para su correcta instalación, en el caso ya esté realizado el mismo por vosotros o terceros. Así que me dedicaré con este servicio a la búsqueda optima de su correcta implementación, auditando todo el proceso de recogida de los datos, hasta la correcta visualización en las etiquetas, sean de Google Analytics, Google Ads, Facebook Ads u otras etiquetas o pixel de conversiones, además de todos los puntos de contacto llamados micro-conversiones, pasos previo a su conversión.\nFormación in-company de Google Tag Manager Si tiene un equipo de marketing familiarizado con Google Analytics y sus productos relacionados y solo está buscando ayuda para comenzar a implementar y usar el Administrador de etiquetas de Google, también ofrezco sesiones de capacitación personalizadas. Para estas sesiones, trabajaré con usted personalmente o en equipo para asegurarnos de que confía en su configuración y en el uso del Administrador de etiquetas de Google en su sitio. Incluso ofrezco controles de seguimiento y monitoreo después de mis capacitaciones para garantizar que se sienta cómodo con la herramienta y cómo puede desarrollarse para que coincida con el crecimiento y los cambios de su sitio web.\nMis servicios de formación in-company serán presenciales en vuestras oficinas en un máximo de 8 - 12 horas, siendo posible más horas a distancia via streaming, u online a través de la plataforma Moodle de propiedad de una escuela digital.\n¿Está interesado en consultar o capacitarse en Google Tag Manager? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Google Tag Manager\n","date":1591807974,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"993977fc1e57f59ed4427bc81929b1d6","permalink":"https://www.marcusrb.com/servicios-freelance-google-analytics/","publishdate":"2020-06-10T17:52:54+01:00","relpermalink":"/servicios-freelance-google-analytics/","section":"servicios","summary":"Ofrezco servicios de auditoria, implementación y formación de la herramienta de análisis web, Google Analytics y creación de plan de medición en Google Tag Manager.","tags":null,"title":"Consultor freelance de analítica digital, Google Analytics, GA4 y GTM","type":"page"},{"authors":["marcusRB"],"categories":["Data Visualization"],"content":" Otro año más, el octavo consecutivo, la herramienta de visualizaciones Tableau, junto con la otra rival de Microsoft, Power BI, ha sido premiada por Gartner como mejor suite de BI para realizar dashboard y ayudar a los analistas de Business Analytics. Esta consultora recopila, en su cuadrante mágico, los diferentes puntos (a favor y en contra) que han encontrado los usuarios.\nPero, ¿cuáles son las funcionalidades que permiten a Tableau de ser una de las mejores herramientas? En anteriores post de visualización ya hemos visto diferentes puntos de vista. Pero, en este post, veremos en profundidad cómo sacarle más partido gracias a estos consejos.\n1 Reports, Dashboards, Historias El primer tip de Tableau (además de ser nativa) es diferenciar entre crear un report, que incluye detalles de un indicador, de una dimensión, una métrica, una métrica nueva calculada o un filtro; o un parámetro, que básicamente será el elemento que incluiremos en uno o más dashboards. Aquí está el secreto mejor guardado de esta herramienta. Si eres nuevo, no te preocupes, lo explicaré con íconos.\nEl primer elemento que observamos sirve para crear nuestra nueva hoja de trabajo. Básicamente es un report de una dimensión o más, con una o más métricas o métricas calculadas, con filtros y segmentos. Cuando creamos una nueva hoja, podemos montar algo como esto:\nUn simple reporte de usuarios que visitan en un periodo n, segmentado por navegador y versión.\nSi quiero crear otra que me indique en un periodo de tiempo los login efectuados, indicando IP, fecha, día de inicio y último, siempre será con métricas calculadas (otra de las especialidades de esta herramienta) como esto:\nAhora que tenemos dos reportes, creamos un dashboard:\nNo es ni más ni menos que realizar un drag-n-drop de los dos reportes al recuadro. Y listo, lo formateamos un poco y ya tenemos nuestro dashboard.\nAhora que queremos realizar un nuevo insight en Tableau, debemos solo importar el dashboard en la historia. Esta última será como nivel de presentación a nuestros clientes, departamento, directivos o a quiénes queremos transmitir el mensaje. La utilidad permite poder compartirlo en formato keynote, pdf, imprimir o via server o email.\nDe la imagen de arriba arrastramos el Dashboard Frecuencia, que creamos hace poco, y obtenemos esto:\n2 ¿Qué gráfico utilizar? Aunque no es exclusivo de Tableau, cada report no incluirá solo tablas o indicadores, sino que a través de gráficos es mucho más intuitivo para el usuario final.\nElegir el tipo de gráfico correcto te ayudará a encontrar la historia en tus datos, revelará patrones y tendencias, por lo que, al instante, entenderás el significado del conjunto de datos que estás visualizando. En esta sección, descubriremos cuándo usar el tipos de gráficos más comunes.\nAquí unos cuantos ejemplos de gráficos:\n Comparaciones entre artículos: ¿Cómo comparar valores en diferentes categorías? Lo más habitual a través de barchart o radar.   Comparaciones en el tiempo: ¿Cómo comparar los cambios en un período de tiempo? Ejemplo line-chart o area chart.   Composición: ¿Cómo mostrar la relación part-to-whole/treemap/stacked bar?   Correlación: ¿Cómo entender la relación entre dos o más variables? A través de Scatter plot/bubble chart.   Distribución: ¿Cómo entender la frecuencia de los valores en un conjunto de datos? Aquí el histograma (que muchas veces lo confundimos con el bar chart o gráfico de barras) y boxplot (conocido como caja de bigote).   Ubicación: ¿Dónde ocurren las cosas? Podemos ver también dónde las cosas no ocurren y , así, descubrir la próxima oportunidad. En este caso, es muy potente el motor de Tableau de mapas Mapbox o de servidor Web Map Services (WMS).   Indicadores clave de rendimiento (KPI): ¿Cómo mostrar si estás en blanco? Podemos incluso crear otros indicadores a medida, con colores y formatos diferentes.  3 ¿Y respecto a los colores en los reports? En Tableau es importante no caer en el error de que “más es mejor”. La regla minimalista del “menos es más” es esencial y fundamental para un dashboard ganador.\n¿Cómo mejorar el formato? Muy simple en Tableau. Primero, en cuanto a las diferentes tonalidades que podemos personalizar y utilizar, no debemos dejar por defecto la paleta de la izquierda, sino utilizar otras (existen muchas más),\nHay que pensar siempre en los usuarios y no todos pueden interpretar bien los colores. El formato de sus gráficos tiene que ser fácil de comprender y, además, ‘estéticamente’ agradables. Los datos se pueden representar por color, puedes usar colores para resaltar diferentes categorías o para representar una métrica secundaria en tu gráfico. Aunque, ten cuidado, el color que se usa mal puede oscurecer el gráfico y crear confusiones a los usuarios.\nComo nota, adjunto aquí un pequeño resumen del buen uso de colores y cuándo o en qué ocasión utilizar una paleta u otra, así como añadir una leyenda (pequeña, obviamente) y que no distraiga la lectura del gráfico.\n4 Dimensiones calculadas Cuando ingestamos datos en Tableau, además de tener un gran abanico de conectores disponibles (exactamente con la versión utilizada 2019.4 y hasta la fecha dispone de 82 nativos y el resto vía API a medida) podemos realizar unas operaciones de transformación de las variables, y entre ellas las categóricas o dimensiones.\nSin quitar el mérito a la otra herramienta muy potente del fabricante Trifacta, la herramienta utilizada también en cloud de Google Cloud Platform DataPrep, la tool de transformación Tableau Prep nos ayudará en gran medida en este proceso de gran magnitud y permite complementar con otras pequeñas transformaciones de columnas, filas, filtros o cálculos sencillos en la fase previa a la realización de un reporte.\nOs pongo un ejemplo de Tableau Prep Builder de una transformación realizada para el Ayuntamiento de Gijón (prueba de concepto realizada en mayo 2019 y con gran éxito) donde en los propios flujos de ETL y transformación, la herramienta ha resultado muy satisfactoria.\nUn ejemplo muy sencillo que se podría realizar antes de procesar los datos en nuestro reporte sería crear duplicados de columnas o cambiar el formato de los tipo de datos, ampliar las columnas como las fechas o realizar pequeñas limpiezas de campos vacíos.\nEn este ejemplo he querido poner un cálculo muy sencillo de transformación, separando el mes del año:\nY ya que entramos en tema de campos calculados, veamos cómo utilizar los campos calculados para crear las métricas calculadas.\n5 Métricas calculadas Por métricas calculadas entendemos estos nuevos campos donde realizaremos operaciones para enriquecer nuestros reportes con más detalles e informaciones, pero han de ser tratadas o calculadas. Entre las funciones disponemos de un número muy alto, que similar a las hojas de cálculos o bases de datos SQL, realizaremos prácticamente sin complejidad\nLas funciones van por orden y disponibles por categoría. Aquí una muestra:\nCon estos dos ejemplos crearemos la métrica para obtener el mínimo y máximo de fechas y poder calcular sucesivamente la métrica FirstLogin y LastLogin:\nAquí el resultado para obtener las IPs activas, las frecuencias o los logins de nuestros usuarios (los datos son obtenidos de logs de servidor previamente tratados).\nY este es el resultado final. Hemos ido creando varios dashboards que nos ayudarán a entender mejor el tráfico que estamos obteniendo en un sitio web y el comportamiento de los propios usuarios. Podemos realizar estas pequeñas tareas de crear métricas específicas para poder detectar patrones y así generar nuevos conocimientos de estos datos, creando nuevos insights.\n¿Qué más nos ofrece Tableau? Es evidente que solo vemos la punta del iceberg: esto es el comienzo y en este post vemos un resumen de algunas de las funcionalidades de la herramienta líder en inteligencia de negocio y análitica avanzada, además de disponer y ofrecer un gran abanico de funcionalidades.\nEn las próximas entregas iremos avanzados con técnicas útiles a la hora de calcular nuevas métricas y combinar con parámetros avanzados, con el fin de tener los indicadores más importantes en nuestro dashboard sin perder en ningún momento el propósito inicial, detectar tendencias, patrones y crear nuevos insights, desde simples indicadores hasta métricas de seguimiento más al detalle.\nFUENTE ORIGINAL\n","date":1591807974,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"56b79ea36279de13a18d354135ca830e","permalink":"https://www.marcusrb.com/tableau-cinco-tips-mejorar-dashboard/","publishdate":"2020-06-10T17:52:54+01:00","relpermalink":"/tableau-cinco-tips-mejorar-dashboard/","section":"post","summary":"Otro año más, el octavo consecutivo, la herramienta de visualizaciones Tableau, junto con la otra rival de Microsoft, Power BI, ha sido premiada por Gartner como mejor suite de BI para realizar dashboard y ayudar a los analistas de Business Analytics. Esta consultora recopila, en su cuadrante mágico, los diferentes puntos (a favor y en contra) que han encontrado los usuarios.\nPero, ¿cuáles son las funcionalidades que permiten a Tableau de ser una de las mejores herramientas?","tags":["dashboard","tableau","tips"],"title":"Tableau, cinco tips que te ayudarán a mejorar tus dashboards","type":"post"},{"authors":["marcusRB"],"categories":["Data Visualization","Webinar"],"content":"Unos de los retos más ambiciosos para un analista de datos, analista de negocio y todos aquellos que tienen pensando en realizar un cuadro de mando o dashboard, sea la herramienta comercial más utilizada (Power BI, como también Tableau, QuickSight, Locker, Qlik), también open-source o free (Data Studio), nos olvidamos de pasos que son imprenscindibles y fundamentlas, crear un modelo de datos. Claro que haría falta un ejercicio previo de Data Discovery, crear un backlog y con agilismo realizar un plan de alcance para poder llegar a un MVP (producto mínimo viable) como también una POC (prueba de concepto), pero el conocer que datos dispongamos y podemos crear nuevas medidas, campos calculados o formatear mejor nuestra tabla, es esencial.\nEn este taller de aproximadamente una hora veremos las técnicas más utilizadas y los casos de uso de las expresiones DAX, modelado de datos y procesos de ETL con Power Query en Power BI, para que así puedas mejorar la visualización e interpretación de los datos.\nMicrosoft Power BI sigue un año más como líder entre las herramientas de inteligencia de negocio (fuente Gartner), y la suite sigue mejorando su aspecto tanto de usabilidad, como para enriquecer nuevas funcionalidades.\nUnos de los aspectos más importante para no tener miedo a esta herramienta comienza con el flujo de trabajo general de extremo a extremo que utiliza Power BI, el modelado de datos. La idea de ir creando más talleres de técnicas especializadas en Power BI, como crear medidas calculadas con Data Analysis eXpressions (DAX), manipular datos con el poderoso Power Query (o como algunos lo conocen lenguaje M) y aprovechar de muchos otros servicios hasta aplicar técnicas de Machine Learning o realizar operaciones con R y Python.\nAunque el ponente es \u0026ldquo;homónimo\u0026rdquo; de otro gran consultor de Power BI y SQL, seguro que aprovecharás este taller para aclarar algunas dudas a la hora de modelar tus datos.\nSi os perdisteis este evento, ¡no os preocupéis! En el vídeo que encontraréis a continuación podréis visualizar el Webinar.\nAquí el extracto del video, no dude de comentar y compartir!\n  ","date":1591116774,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"9e480921d3f5e2d56869c4bdc2e24878","permalink":"https://www.marcusrb.com/webinar-tecnicas-modelado-dax-power-bi/","publishdate":"2020-06-02T17:52:54+01:00","relpermalink":"/webinar-tecnicas-modelado-dax-power-bi/","section":"post","summary":"Unos de los retos más ambiciosos para un analista de datos, analista de negocio y todos aquellos que tienen pensando en realizar un cuadro de mando o dashboard, sea la herramienta comercial más utilizada (Power BI, como también Tableau, QuickSight, Locker, Qlik), también open-source o free (Data Studio), nos olvidamos de pasos que son imprenscindibles y fundamentlas, crear un modelo de datos. Claro que haría falta un ejercicio previo de Data Discovery, crear un backlog y con agilismo realizar un plan de alcance para poder llegar a un MVP (producto mínimo viable) como también una POC (prueba de concepto), pero el conocer que datos dispongamos y podemos crear nuevas medidas, campos calculados o formatear mejor nuestra tabla, es esencial.","tags":["modelo datos","powerbi","tips"],"title":"[Webinar] - Mejora tu modelo DAX - Power Query en Power BI.","type":"post"},{"authors":null,"categories":null,"content":" En los últimos 10 años, he participado en los programas formativos de muchas escuelas de negocio, universidades y centro de formación de la administración pública. He realizado servicio de tutoría, docencia de asignaturas, formación a grandes empresas, pymes - autónomos, particulares.\nDocencia para programas de posgrados, máster, cursos y webinar. Con más de 10 años de expieriencia en sector formativo, colaboro con diferentes escuelas de negocio y universidades, así como administración públicas para realizar cursos, seminarios o participar en congresos en materia de analítica digital, analítica de datos y visualización.\nHe preparado varios programas, módulos y cursos personalizados de fundamentos hasta niveles más avanzados con duración máxima de 120 horas de formación a un público de casi 100 alumnos en algunos casos. Las formaciones pueden ser presenciales, a distancia, grabadas o en formato blended.\nGestiono y preparo: - el cuadro formativo - tutoría y asesoramiento - los módulos en formato presentación - los casos prácticos - los laboratorios durante las horas lectivas - las prácticas y soluciones - grabaciones\nFormación in-company en Analítica de negocio Tanto si tiene un equipo de marketing familiarizado con herramientas de análisis digitales como Google Analytics y sus productos relacionados y solo está buscando ayuda para comenzar a implementar y usar el Administrador de etiquetas de Tag Manager, también ofrezco sesiones de capacitación personalizadas. Para estas sesiones, trabajaré con usted personalmente o en equipo para asegurarnos de que confía en su configuración y en el uso del Administrador de etiquetas de Google en su sitio. Incluso ofrezco controles de seguimiento y monitoreo después de mis capacitaciones para garantizar que se sienta cómodo con la herramienta y cómo puede desarrollarse para que coincida con el crecimiento y los cambios de su sitio web. Tengo amplia experiencia también en otras herramientas de publicidad Google Ads y FacebookAds , me limito a formación y sesiones de auditoría.\nPara el resto de formaciones a medidas con un mínimo de 10 horas, relacionadas con el resto de disciplinas de Data, tengo años de experiencia en profesorado y formaciones en:\n Sesiones de Data Discovery Preparación de indicadores y fuentes de datos SQL y bases de datos Modelado de datos ETL, preparación y transformación de datos Visualización con Power BI, Tableau, Data Studio, AWS QuickSight Minería de datos y Machine Learning Deep Learning Python para data science y analísis de datos, Pandas - Numpy - Scipy - Scikit-learn, visualización Matplotlib / Seaborn R Studio Cloud Engineer en AWS, Google Cloud Platform, Microsoft Azure y DataBricks  Mis servicios de formación in-company serán presenciales en vuestras oficinas, siendo posible a distancia via streaming u online a través de las plataformas habituales (Zoom, Google Meet, Teams, etc.)\n¿Está interesado en capacitarse en una de estas disciplinas o quieres contarme algo más? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Formación en Datos\n","date":1591036301,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"75ebb8aaaa7d5b1538653120f36b9d80","permalink":"https://www.marcusrb.com/formacion-in-company-docencia-escuelas-negocio/","publishdate":"2020-06-01T18:31:41Z","relpermalink":"/formacion-in-company-docencia-escuelas-negocio/","section":"servicios","summary":"Formación a medida para tu empresa y/o clases particulares de analítica digital, analítica de datos y visualización con Power BI, Tableau y Data Studio.","tags":null,"title":"Formación in-company \u0026 docencia en escuelas de negocios","type":"page"},{"authors":["marcusRB"],"categories":["Analítica Digital","Tag Manager"],"content":" Desde 2011, los gestores de etiquetas irrumpieron en los departamentos de Marketing Digital con la capacidad de absorber gran parte del trabajo realizado por desarrolladores front/back como acompañamiento en una de las tareas más tediosas: la implementación del píxel de conversión.\nEste mismo año, Google lanzó su propia plataforma Tag Manager, en una prometedora versión beta, con muchas ganas de crecer y mejorar.\nAl año siguiente salió la versión definitiva y, hasta la fecha, ha ido mejorando tanto en la usabilidad como en la propia capacidad de crear e implementar diferentes script.\nEste gestor, adaptado a todo el público con conocimientos mínimos de diseño front (HTML, CSS, JS, etc.) permite organizar, implementar y activar las etiquetas propias y las de terceros (se calculan unas 100 hasta la fecha).\nEste post nos adentraremos, de una forma muy general, en el universo de Google Tag Manager, su interfaz, su utilidad básica y algunas de las técnicas más utilizadas.\nGoogle Tag Manager es muy similar al resto de los competidores premium como Tealium, Adobe DTM, IBM o Ensighten\u0026hellip; ya que comparte muchas funcionalidades, se diferencia de ellos en que es free.\nAdemás, con las numerosas guías, vídeos tutoriales, webinar cualquier especialista en analítica puede utilizarlo. Pero, aún así, en el año 2019, la administración de estas etiquetas siguen siendo un caos.\nGracias a esta herramienta, hemos pasado de esta situación diaria de cualquier profesional de Marketing\u0026hellip;\n… a esta más ordenada:\n¿Qué son las etiquetas? Vamos por pasos. Primero, ¿qué son realmente las etiquetas?\n Son esos pequeños fragmentos de código requeridos para personalizar el contenido, recopilar datos analíticos y publicar anuncios y habilitar campañas de remarketing. Están en lenguaje JavaScript, pasan de ser esenciales para el tratamiento de los usuarios y navegantes de nuestras web y apps, otros miden el impacto y repercusión en las inversiones publicitarias. Si las etiquetas están mal implementadas, causan distorsiones de los cálculos en las herramientas de análisis y complican aún más la tarea de optimización de campañas o de toma de decisiones por tener resultados de los insight erróneos.  ¿Cuáles son las funciones de un TMS? TMS se compone de una interfaz SaaS principalmente en jQuery, Go o AngularJS, que tiene un sistema de versionado tipo GIT (control de calidad QA, Jenkins).\nFuncionalidades Las etiquetas de Digital Marketing se utilizan para agregar funcionalidad a sitios web, contenido de vídeo y aplicaciones móviles.\nEsta funcionalidad puede incluir análisis web, análisis de campañas, medición de audiencia, personalización, pruebas A/B, servidores de anuncios (DMP), retargeting de comportamiento y seguimiento de conversiones. Además, añade pruebas funcionales como detector de errores JS, timing, performance, etc.\nLos sistemas de administración reemplazan estas etiquetas múltiples con una de un solo contenedor y, posteriormente, priorizan y \u0026ldquo;disparan\u0026rdquo; etiquetas individuales según sea apropiado en función de las reglas comerciales, los eventos de navegación y los datos conocidos.\nLa funcionalidad típica incluye el entorno de prueba (sandboxing), el seguimiento de auditoría y el control de versiones, la capacidad para probar diferentes soluciones A / B, la deduplicación de etiquetas y el acceso a los datos basados en roles.\nBeneficios Los beneficios de los sistemas de gestión de etiquetas incluyen:\n Agilidad. Cada vez trabajamos más con la metodología Agile, así que la menor dependencia de los recursos técnicos y de los ciclos de TI confiere una mayor agilidad a los usuarios empresariales. Rendimiento. Tiempos de carga de página reducidos gracias a la carga asíncrona de etiquetas, la carga condicional y la funcionalidad de tiempo de espera. Ahorro de costes. Capacidad de duplicar etiquetas utilizadas para atribuir comisiones. Control de datos. Capacidad para controlar la fuga de datos a terceros y cumplir con la legislación de privacidad de datos (consentimiento de cookies). Los administradores también proporcionan otra capa de abstracción para gestionar la complejidad de los sitios web grandes. Vista previa segura. Algunos administradores, como el de Google, incluyen un modo de vista previa que permite verificar los problemas de formato y seguridad antes de implementar etiquetas en producción.  Pero, hay una parte mala también. Las etiquetas tienen una forma de aparecer inesperadamente, dejando incluso caer la web sin previo aviso o provocando errores en la recogida de los datos por un cambio en la parte frontal no comunicado.\nHay que tener en cuenta que estamos “modificando” el core de la web y, muchas veces, nuestro código podría ser no compatible o entrar en conflicto con algún plugin, script existente, etc.\nAdemás, hay que considerar el tamaño del sitio web. Si es pequeño, el error podrá ser ínfimo, pero si hablamos de una web con cientos de miles de visitas al días, entonces, seguramente conocerás algunas de estas causas de errores más comunes:\n Etiquetas mal configuradas en el sitio web. Etiquetas no autorizadas en el sitio web. Etiquetas duplicadas en el sitio web. Etiquetas que faltan en el sitio web.  Para arreglar esto, con millones de instancias de analíticas y etiquetas de marketing en el sitio web, ¿por dónde empezamos? Lo primero, será comenzar con el marco de gobernanza de etiquetas (Tag Governance Framework).\n¿Qué es el gobierno de etiquetas? Al oír la palabra “gobernanza”, ¿qué es lo primero que te viene a la mente? ¿Control? ¿Vigilancia? ¿Protección? ¿Fuerza?\nEsto es, exactamente, lo que los especialistas en marketing, analistas, arquitectos de sistema, ingenieros y desarrolladores desean para su gestor de etiquetas.\nLa gobernanza de etiquetas es una subdisciplina dentro de la gobernanza de datos, centrada específicamente en los datos recopilados por etiquetas de análisis y marketing digital y enviados a través de una solicitud de red.\nComo ocurre con la gestión de etiquetas, la gobernanza es una disciplina que depende del software. De la misma manera que la verdadera gestión de etiquetas se basa en un TMS también requiere el uso de una aplicación (web, app).\n¿Para hacer qué? Las soluciones de gobernanza analizan las solicitudes de red enviadas desde sitios web y aplicaciones en sus diversas etapas de desarrollo para identificar posibles errores de etiquetado.\nCaracterísticas  Escanear solicitudes de red. Las soluciones de gobernanza rastrean sitios web que buscan solicitudes de red enviadas por las etiquetas. Cuando una etiqueta de medición o de marketing digital envía una solicitud, una solución de gobernanza de etiquetas capturará esos datos y los analizará en sus valores de componentes. Estos valores se comparan con reglas predefinidas para determinar si son correctas o no. En varias etapas de desarrollo. Idealmente, cualquier empresa que implemente nuevas etiquetas en su sitio web o aplicación debería hacerlo al comienzo del ciclo de desarrollo.  Una buena solución de gobernanza debería poder escanear sitios web y aplicaciones dentro de entornos de preproducción (como un entorno de preparación).\nAl hacerlo, los equipos de tecnología pueden resolver los problemas antes de que un sitio web o aplicación se active.\nAdemás, debería supervisar el rendimiento de las etiquetas en el entorno de producción, notificando a las partes interesadas apropiadas si algo sale mal.\n Identificar posibles errores de etiquetado. Un error de etiquetado es cualquier desviación de las mejores prácticas de etiquetado o de los requisitos comerciales internos de una empresa (las reglas predefinidas que mencionamos anteriormente). Cuando una solución de gobernanza de etiquetas descubre un posible error de etiquetado, notificará a las partes interesadas apropiadas.  ¿Es posible la gobernanza manual? Si bien, en teoría, podría verificar manualmente los registros de la red en busca de errores de etiquetado (muchas compañías lo hacen), este proceso es altamente ineficiente y propenso a errores humanos.\nEs mejor adoptar la automatización y evitar un triple bypass en su implementación de etiquetado.\nPor qué usar una solución de gobierno de etiquetas Con las soluciones y los procesos correctamente implementados, es posible controlar las etiquetas a escala.\nA medida que lo hagas podrás confiar en los datos, obtener un ROI en el gasto en tecnología y hacerlo todo de manera más eficiente que si fuera de manera manual. Por esto, obtendrás las etiquetas por las que pagaste dinero.\nEste marco abarca los componentes esenciales de un programa de gobernanza de etiquetas efectivo. En las secciones siguientes detallaré una descripción de cada fase del marco, así como la forma en que las soluciones de gobernabilidad de etiquetas juegan un papel clave para lograr los objetivos de esa fase.\nFases del marco de gobernanza Plan La gobernanza de etiquetas desempeña un papel importante para ayudar a las empresas a planificar su estrategia de medición algo que, lamentablemente, no hacen suficientes empresas.\nDemasiadas compañías no planean Según diferentes encuestas de varias consultoras, las empresas no documentan su estrategia de análisis. Esto se refleja negativamente en el rendimiento de la etiqueta y disminuye la eficiencia debido a la redundancia en las estrategias de medición.\nAntes de comenzar a implementar etiquetas o configurar variables en su sitio, se debe crear el plan de etiquetado o measurement plan. Este plan documenta su estrategia de medición y comercialización, esbozando qué etiqueta está haciendo qué y por qué.\nLa fase de planificación permite mapear preguntas comerciales como, por ejemplo, ¿qué segmento de nuestros clientes se convierte mejor en condiciones x?, en contra de las variables que recopilan datos relevantes.\nEl mismo principio se aplica a las etiquetas que agregan características a su sitio: deben tener un plan determinando para establecer cuándo y dónde deben implementarse.\nConstruyendo un plan de etiquetado Los planes de etiquetado ayudan a evitar la sobrecarga de datos. Trabajando para un cliente, una vez, tuve la tarea de ayudar a convertir aproximadamente cien paneles diferentes, con alrededor de 20-30 KPI cada uno, en datos de Analytics.\nHabía tantas métricas que nadie sabía para qué servían. Ese es el resultado de una mala planificación sin una documentación clara.\nLo mismo puede suceder con las etiquetas y variables de análisis: las personas comienzan a medir todo tipo de cosas sin, realmente, tener una buena razón.\nTener un plan te puede ayudar a minimizar la sobrecarga de datos porque se da un paso adicional al planificar antes de implementar. Parte de ese proceso de planificación implica descubrir lo que, actualmente, está recopilando y documentando. Hacerlo, te ayudará a reducir el peso analítico.\nLas soluciones de gobernanza de etiquetas te permiten escanear tu sitio web para averiguar qué datos está recopilando actualmente. De esa manera, puedes evaluar lo que ya tienes implementado, lo que podría ser prescindible o cómo satisfacer las necesidades actuales sin implementar nueva tecnología.\nComply El gobierno de etiquetas puede ayudar a las empresas a realizar los cambios necesarios para cumplir con los controles internos y externos.\nHablemos de GDPR El Reglamento General de Protección de Datos (GDPR) ha provocado mucho pánico con respecto a la privacidad de datos y las regulaciones de protección de datos.\nLas personas se están acercando a estos mandatos de la manera incorrecta. Deberíamos ser más proactivos para proteger a los clientes que estamos tratando de impresionar, en lugar de hacerlo solo porque las administraciones lo han impuesto.\nLa regla de oro de la privacidad de datos El otro día, un amigo estaba hablando de lo genial que era el tipo de datos que podíamos recopilar sobre las personas, pero con la advertencia de que no querría que le hicieran lo mismo.\nEse tipo de mentalidad puede ser peligrosa para las empresas, no solo en términos de legalidad, sino también por la óptica de la marca y, tal vez, incluso, por la ética.\nLa mayoría de los controles deben provenir de fuentes internas. Por supuesto, tenemos que cumplir externamente, pero debemos usar todo este concepto de privacidad para ser un diferenciador estratégico.\nTag Governance es el nuevo monitor de sala Como parte del plan de etiquetado, tienes que saber exactamente qué tipos de variables debes recopilar (ID de usuario, ID de producto, etc.) y cuáles nunca debes recopilar (SSN, dirección de correo electrónico, datos bancarios, etc.).\nCon una solución de gobernanza de etiquetas, puedes escanear las etiquetas y variables analíticas para patrones específicos. Entonces, si un valor variable coincide con un SSN lo escuchará.\nDeploy Una empresa que aplica la gobernanza de etiquetas en entornos de preproducción puede corregir los errores de etiquetado antes de que sean caros. Esto ocurre, justo, cuando llegan al entorno de producción.\nNo todo comienza con los desarrolladores Como ya hemos establecido, los desarrolladores no son la zona cero. Hay (o debería haber) una planificación significativa que entra en una estrategia de medición y marketing antes de que los desarrolladores pongan la punta de sus dedos en su teclado mecánico.\n¿Cómo llega esa planificación al desarrollador? A través del gerente de producto que debe preocuparse por los requisitos de medición igual que por otro requisito de características del producto.\nDesafortunadamente, esta no es siempre la actitud que adoptan los gerentes de producto y el rendimiento de la etiqueta, a menudo, se relega a un segundo plano.\nUn buen gerente de producto o arquitecto de análisis se asegurará de que el desarrollador sepa, exactamente, qué tipo de requisitos de medición de control de calidad se deben cumplir con cada característica.\nEntonces, cuando los desarrolladores van a crear una implementación de datos tendrán pautas específicas a seguir.\nDetectando errores de etiquetado antes de que sucedan ¿Cómo encaja la gobernanza en el flujo de trabajo de un desarrollador? Cuando un equipo de desarrollo está equipado con una solución de gobernanza, pueden escanear sus implementaciones en un entorno de desarrollo para asegurarse de que las etiquetas y las variables de la capa de datos se llenen de acuerdo con los requisitos incluidos en el plan de etiquetado.\nEncontrar problemas en un entorno de desarrollo tiene menos riesgo porque los datos son falsos. No está trabajando con datos reales de clientes, sino está probando cuentas.\nNo solo puede asegurarse de que sus etiquetas funcionan como se esperaba, sino que también puede probar instancias de datos personales no conformes antes de pasar a producción y molestar a alguien.\nQA Las soluciones de gobernanza de etiquetas brindan a los ingenieros de control de calidad herramientas para automatizar las pruebas de etiquetas en entornos de preparación.\nQA: su última línea de defensa antes de la producción El trabajo de este ingeniero es similar al del desarrollador, ya que está probando el rendimiento de la etiqueta con los requisitos del plan de etiquetado antes de pasar a un entorno de producción.\nLa diferencia es que el aseguramiento de la calidad llevará las pruebas de rendimiento a un nivel completamente nuevo: ejecutar pruebas iterativas en diversas condiciones.\nEsas pruebas deben incluir etiquetas. Cuando el ingeniero verifica si un botón funciona como se esperaba, también debería estar atento para ver si ocurrió alguna medición específica.\nUn trabajo para QA, no para analistas Históricamente, los analistas y los vendedores digitales se encargaban de probar los errores de etiquetado en un entorno de producción porque sus datos estaban en juego.\nEl problema con este enfoque es que los errores de etiquetado en la producción son caros de localizar y reparar. Las empresas respondieron y comenzaron a confiar más en los ingenieros para probar los requisitos de medición durante el control de calidad.\nGobierno de etiqueta y control de calidad El ingeniero de control de calidad tiene un papel importante en las pruebas de análisis y etiquetas de marketing, como si fuera el guardián final antes de que un sitio web entre en producción.\nLas soluciones de gobernanza de etiquetas ayudan a automatizar sus pruebas iterativas para que puedan asegurarse de que las etiquetas funcionan en diversas condiciones. Esto mantiene los ciclos de lanzamiento del sitio web ágiles, al tiempo que minimiza el código roto.\nValidate Acabas de publicar una nueva versión en el entorno de producción y, ¿ahora qué? Debes asegurarte de que nada se estropeó al ponerlo en marcha.\nAlgunas etiquetas tienen miedo escénico Cuando se activa un sitio los errores de etiquetado son comunes. Para contrarrestar el problema, los interesados en tecnología pueden asumir la responsabilidad de verificar manualmente los errores. Pero es una batalla complicada.\nEl gran tamaño de un sitio web y el volumen de etiquetas en el sitio hacen que una auditoría integral no sea realista. Además, las personas no están preparadas para escanear cantidades masivas de datos en busca de errores.\nEl error humano ocurre A los humanos les gusta tomar atajos y hacer excepciones, no son sistemáticos como un programa informático.\nEntonces, incluso, si tuviéramos los recursos humanos para verificar los errores de etiquetado en cada página de un sitio web, no obtendríamos los resultados de una prueba automatizada.\nReemplazar la verificación manual por una auditoría, monitoreo y validación de etiquetas puede ayudar a probar la implementación de su etiqueta a una velocidad más precisa y mucho más rápida.\nMonitor Debe supervisar el rendimiento de la etiqueta a intervalos regulares. Cuando surgen errores de etiquetado, debe ser el primero en saberlo.\nFlash de noticias: ruptura de sitios web En un mundo perfecto, un sitio web en pleno funcionamiento en producción nunca tendría problemas. Pero, nunca es un mundo perfecto.\nDe alguna manera, algunos datos personales volátiles lo convierten en una variable analítica o una etiqueta adicional que surge de una etiqueta de publicación de anuncios.\nEn consecuencia, debes realizar pruebas continuas en el entorno de producción para asegurarte de que todo esté de acuerdo con el plan de etiquetado que se creó hace tanto tiempo.\nRevísalo de nuevo. Entonces otra vez. Y una vez más. Los errores de etiquetado aparecen en producción todo el tiempo. Pero, tampoco puede haber alguien que esté todo el tiempo monitoreando las etiquetas para asegurarse de que estén funcionando.\nLa solución de gobernanza de etiquetas puede hacer este trabajo pesado, escaneando periódicamente su sitio en vivo y verificando si hay errores de etiquetado.\nImplementación del marco de gobierno de etiquetas Cuando se implementan correctamente el análisis digital y la tecnología de marketing presentan un valor significativo para la empresa.\nEstas tecnologías se basan en etiquetas y las etiquetas se basan en una implementación adecuada.\nSi usas una solución automatizada para controlar las etiquetas en todas las fases del marco, puedes lograr una mayor precisión de los datos y capacidad de acción en su organización.\n¿Cuántas etiquetas rotas tiene en su sitio web? Realmente, no puedes saberlo hasta que lo verifiques. ¿Tu empresa o departamento de marketing o analítica cuenta con un marco de gobernanza para la gestión de etiquetas?\nFUENTE ORIGINAL\n","date":1589993574,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"14ad06343d682dfdf1bd7c574950a467","permalink":"https://www.marcusrb.com/marco-gobernanza-google-tag-manager/","publishdate":"2020-05-20T17:52:54+01:00","relpermalink":"/marco-gobernanza-google-tag-manager/","section":"post","summary":"Desde 2011, los gestores de etiquetas irrumpieron en los departamentos de Marketing Digital con la capacidad de absorber gran parte del trabajo realizado por desarrolladores front/back como acompañamiento en una de las tareas más tediosas: la implementación del píxel de conversión.\nEste mismo año, Google lanzó su propia plataforma Tag Manager, en una prometedora versión beta, con muchas ganas de crecer y mejorar.\nAl año siguiente salió la versión definitiva y, hasta la fecha, ha ido mejorando tanto en la usabilidad como en la propia capacidad de crear e implementar diferentes script.","tags":["google tag manager","gobierno del dato","agilismo"],"title":"El marco de gobernanza en Google Tag Manager","type":"post"},{"authors":["marcusRB"],"categories":["Data Science","Webinar"],"content":"Este Webinar es la segunda parte de la serie \u0026ldquo;Data Science con Kaggle\u0026rdquo;, donde veremos la optimización y el tuneado de parámetros de Machine Learning. Además, también participaremos en una competición de regresión y en otra de reconocimiento de imágenes en vivo con redes neuronales. Los trabajos, como siempre, serán compartidos en un repositorio en Github.\nEl único requisito que se pide, además de ver y seguir los pasos del webinar kaggle L1 anterior), es tener:\n Conocimientos de técnicas de optimización de modelos de Machine Learning con Scikit-Learn.\n Saber utilizar las diferentes librerías de Python, (ej. Pandas y Numpy).\n Técnicas de preprocesamiento y manipulación de los datos.\n Visualización e interpretación de insights con Matplotlib y Seaborn.\n Imputaciones de los missing values en sus diferentes técnicas.\n Representación de los resultados de machine learning, confusion matrix, multiclases, etc.\n Subida y mejora de los resultados en la competición.\n  Repositorio primera parte de la serie \u0026ldquo;Data Science con Kaggle\u0026rdquo;: https://github.com/marcusRB/WEBINAR_KAGGLE\n  ","date":1588438374,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"14ba20f37e13b2df5eab0eb75e56f87c","permalink":"https://www.marcusrb.com/webinar-kaggle-data-science-tuneado-parametros/","publishdate":"2020-05-02T17:52:54+01:00","relpermalink":"/webinar-kaggle-data-science-tuneado-parametros/","section":"post","summary":"Este Webinar es la segunda parte de la serie \u0026ldquo;Data Science con Kaggle\u0026rdquo;, donde veremos la optimización y el tuneado de parámetros de Machine Learning. Además, también participaremos en una competición de regresión y en otra de reconocimiento de imágenes en vivo con redes neuronales. Los trabajos, como siempre, serán compartidos en un repositorio en Github.\nEl único requisito que se pide, además de ver y seguir los pasos del webinar kaggle L1 anterior), es tener:","tags":["kaggle","data science","machine learning"],"title":"[Webinar] - Kaggle Data Science optimización y tuneado de parámetros.","type":"post"},{"authors":["marcusRB"],"categories":["Data Science","Webinar"],"content":"Tanto si ya tienes algo de experiencia en Data Analytics como si no, no puedes perderte este evento de Data Science en el que trabajaremos directamente en la plataforma Kaggle con Python.\nHaremos el reto más \u0026ldquo;famoso\u0026rdquo;, pero también descubriremos métodos y técnicas útiles para otros retos.\n¡Por cierto! habrá un premio para el primero.\nSe requiere la instalación previa en vuestro ordenador del programa Anaconda, Docker con Jupyter y Visual Studio.\nNo dudes en contactar con nosotros antes del Webinar por si necesitas instalar alguna herramienta adicional.\nQué veremos:\n Organización del entorno de trabajo. Aplicación de Metodologías para la exploración de datos. Entrenamiento de tu primer modelo de aprendizaje automático. Enfrentarse a las competiciones de \u0026ldquo;Primeros pasos\u0026hellip;\u0026rdquo; Competir para maximizar los aprendizajes.  Repositorio primera parte de la serie \u0026ldquo;Data Science con Kaggle\u0026rdquo;: https://github.com/marcusRB/WEBINAR_KAGGLE\n  ","date":1587055974,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"1b3c6fa0f11cdefc73a7f3496ed4eaa7","permalink":"https://www.marcusrb.com/webinar-kaggle-participar-retos-data-science/","publishdate":"2020-04-16T17:52:54+01:00","relpermalink":"/webinar-kaggle-participar-retos-data-science/","section":"post","summary":"Tanto si ya tienes algo de experiencia en Data Analytics como si no, no puedes perderte este evento de Data Science en el que trabajaremos directamente en la plataforma Kaggle con Python.\nHaremos el reto más \u0026ldquo;famoso\u0026rdquo;, pero también descubriremos métodos y técnicas útiles para otros retos.\n¡Por cierto! habrá un premio para el primero.\nSe requiere la instalación previa en vuestro ordenador del programa Anaconda, Docker con Jupyter y Visual Studio.","tags":["kaggle","retos","machine learning"],"title":"[Webinar] - Kaggle Data Science participar retos data science.","type":"post"},{"authors":null,"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Cómo participar en retos Kaggle deData Science. Nivel 1.\n16 de Abril, 12-1h\nCómo participar en retos Kaggle de Data Science. Nivel 1. Tanto si ya tienes algo de experiencia en Data Analytics como si no, no puedes perderte este evento de Data Science en el que trabajaremos directamente en la plataforma Kaggle con Python. Haremos el reto más \u0026ldquo;famoso\u0026rdquo;, pero también descubriremos métodos y técnicas útiles para otros retos. ¡Por cierto! habrá un premio para el primero.\nSe requiere la instalación previa en vuestro ordenador del programa Anaconda, Docker con Jupyter y Visual Studio. No dudes en contactar con nosotros antes del Webinar por si necesitas instalar alguna herramienta adicional.\nQué veremos:\n Organización del entorno de trabajo Aplicación de Metodologías para la exploración de datos Entrenamiento de tu primer modelo de aprendizaje automático Enfrentarse a las competiciones de \u0026ldquo;Primeros pasos\u0026hellip;\u0026rdquo; Competir para maximizar los aprendizajes  Para apuntaros al webinar, tenéis que acceder al siguiente formulario (https://eventos.paradigmadigital.com/kaggle-data-science) e introducir vuestros datos. El día del evento, recibiréis por email la url del webinar para que podáis conectaros y participar en el mismo.\n¿Quién es el ponente? Marco Russo\nConsultor y Especialista en Data \u0026amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.\nVideo   ","date":1587038400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"94724ef16381452eedae5ec3e16c691e","permalink":"https://www.marcusrb.com/talk/kaggle_1/","publishdate":"2020-04-16T00:00:00Z","relpermalink":"/talk/kaggle_1/","section":"talk","summary":"Como participar en retos Kaggle de Data Science - Nivel 1","tags":["datascience","kaggle"],"title":"Kaggle nivel 1","type":"talk"},{"authors":["marcusRB"],"categories":["Data Visualization"],"content":" Este año la consultora Gartner ha valorado la herramienta de Microsoft de inteligencia de negocio, Power BI, como líder en el cuadrante mágico, donde quedan Tableau muy cerca, y Qlik y Thoughtspot un poco más alejadas. Entre las mejores valoraciones encontramos la usabilidad y poder subir nuestro trabajo en los espacios de trabajo personales de cloud. Veamos en este post algunos de las funcionalidades más destacadas.\n1 Slicer con filtros Una de las primeras funcionalidades que permite dominar la herramienta de visualizaciones Power BI es la completa personalización (tanto en la opción Desktop como en desarrollo).\nSlicer es una de ellas. Permite crear menús laterales donde podemos incluir, por ejemplo, filtros y segmentos y dejar más espacio a nuestro gráfico.\nVeamos en este ejemplo paso a paso cómo construirlo:\n El ejemplo es uno de los dashboard de muestra. Al parecer, tenemos demasiados gráficos e indicadores, por lo que sería útil ampliar algo más, creando un menú lateral izquierdo y mover el filtro.   Creamos un botón, representado con una flecha derecha y lo ampliamos.   Ahora le damos formato al menú, con color, moviendo el resto del gráfico para que el botón quede a la izquierda y que cumpla su función.   Con el Bookmark y Selection operaremos para darle forma y creamos un evento. Cuando pinchemos en el botón, se desplegará un elemento (que incluye el filtro) y una flecha. Para ello, debemos esconder el resto de elementos y viceversa.   Jugamos con background, fill o border, y si es necesario, introducimos el nombre del texto. Entre los marcadores tenemos dos, Visible y Hidden, a estos hay que añadir un action. Ahora creamos un text box donde incluiremos el filtro y le damos colores y formato.   Es el momento de crear los eventos, uno para que despliegue y otro que esconda el elemento. Para este último necesitaremos otro botón, el de la izquierda.   Una última cosa: en el panel de Selection es bueno poner orden( especialmente, secuencial para poder dar prioridad a los elementos). Asignamos al Action el marcador creado Hidden y, muy importante, damos prioridad a los elementos, visibles o invisibles.   Ahora, actualizamos el marcador para cuando realicemos algún cambio en los elementos, como por ejemplo, si esconder o dejar visible.   Una vez realizada la operación, el proceso de prueba sería con tecla CTRL o CMD más click a la flecha:   Ahora que funciona el slicer, introducimos el filtro y le damos nuevamente una orden (activarlo o esconderlo) y habilitarlo en Selection.   Y para que el filtro funcione constantemente y no se reinicie cada vez que lo visualizamos, necesitamos realizar el siguiente paso.   Básicamente deshabilitamos la opción Data, para que no afecte a los datos, sino que solo tenga el filtro como función principal.  El resultado está disponible en este fichero template y la demo realizada en mi entorno cloud una vez publicada:\n2 Jerarquías en los gráficos Cuando tratamos fechas, por ejemplo, automáticamente el sistema Power BI reconoce esta como tal y crea de forma automática su propia jerarquía: año, trimestre, mes y día. Si esta fecha la utilizamos en los filtros o en la coordenada de las X, podemos utilizarla también como segmento.\nVeamos el ejemplo del dashboard anterior:\n Realizamos las mismas operaciones de la imágen:   Una vez publicado, podemos utilizar los selectores drop down o drill up para que podamos segmentar por fechas. Por trimestre:   Por mes:  3 Crear campos calculados con fórmulas DAX La utilidad de Power BI no es solo por la usabilidad y funcionalidades, también cuenta con herramientas muy potentes de cálculos para crear nuevas métricas, heredada de la suite Office365, como la denominada DAX (Data Analysis Expressions), junto con PowerQuery, PowerPivot y el resto de tools que tienen la capacidad de realizar pequeñas funciones.\nEl listado completo de las funciones, disponibles en su web oficial y recogidas en su documentación, se dividen en estas categorías:\n Fecha y hora. Inteligencia de tiempo. Filtro. Información. Lógica. Matemática y trigonométrica. Estadística. Texto.  La gran cantidad de información (y una gran comunidad) permite a esta herramienta ser muy potente en los departamentos de análisis. Además, incluye tools para conexión con módulos de R y Python (ampliando las opciones de exploración).\nVeamos un sencillo ejemplo de fórmula DAX. Creamos un convertidor de moneda, basado en botones y vemos cómo adaptarlo a nuestro gráfico.\n Creamos una nueva tabla moneda, muy sencilla. Una columna, con el nombre currency_code, de USD, GBP y EUR, y otra columna, con el nombre values, con valores fijos (de momento operamos con estos).   Creamos un filtro o slicer con los tres valores, le damos una capa de formato y colores, y que cumpla la función de selección simple.   Creamos ahora una tabla con las monedas y valores y vemos cómo se comporta cuando utilizamos el filtro.   Seleccionando uno de estos tres, en la tabla de prueba de abajo, vemos el valor correspondiente. ¿Cómo podemos sacar ventaja con este tip? Creamos una función DAX para que al seleccionar uno de los valores permita realizar operaciones a nuestros valores de ventas que vimos anteriormente. Para ello, debemos crear una nueva medida selector_currency que recogerá como variable el selector de la moneda y, además de guardarla, permite devolver un resultado a otra función (la sucesiva que realizaremos).   La nueva medida será creada exactamente igual que la imágen y la función será la encargada de almacenar el valor.   Una vez creada la nueva medida, vamos con la otra que podemos utilizar en condiciones IF ELSE o SWITCH CASE, en caso de querer realizar operaciones más complejas. En este caso, recogerá el total de las ventas TotalSales y se convertirá en USD o GBP, una vez que venga seleccionado el filtro correspondiente.La fórmula es la que sigue con la evaluación SWITCH CASE:   Veamos cómo funciona correctamente teniendo en cuenta que la tabla donde nos devolverá los resultados será una tabla de tipo matriz con dos campos recién creados (código de moneda y valor de la conversión). Con libras:   Y con dólares:   Hemos visto que funciona correctamente. Aunque, los valores de conversiones no son los reales actuales. Si los queremos, podríamos conectarnos con la tabla de conversiones en tiempo real para que nos devuelva los valores exactos (y mejores que los utilizados anteriormente).  4 Gestionar las relaciones de tablas Una buena opción es la gestión de relaciones de tablas una vez que tengamos conectadas diferentes bases de datos y queremos realizar operaciones con ellas (la opción está disponible en nuestro dashboard).\nY una vez realizada la operación de relaciones, dispondremos de una tabla similar a esta:\nPara crear varias medidas y mejorar el control, la gestión de relaciones permite ahorrar tiempo en los cálculos. Repasamos las siguientes configuraciones, llamada cardinalidad:\n Muchos a uno (*: 1): una relación de muchos a uno es el tipo de relación por defecto más común. Significa que la columna en una tabla puede tener más de una instancia de un valor y la otra tabla relacionada, a menudo conocida como la tabla de búsqueda, solo tiene una instancia de un valor. Uno a uno (1: 1): en una relación uno a uno, la columna en una tabla tiene solo una instancia de un valor particular y la otra tabla relacionada tiene solo una instancia de un valor particular. Uno a muchos (1: *): en una relación uno a muchos, la columna en una tabla tiene solo una instancia de un valor particular y la otra tabla relacionada puede tener más de una instancia de un valor. Muchos a muchos (*: *): con los modelos compuestos, puede establecer una relación de muchos a muchos entre las tablas, lo que elimina los requisitos de valores únicos en las tablas. También elimina las soluciones alternativas anteriores, como la introducción de nuevas tablas solo para establecer relaciones. Para obtener más información, consulte Relaciones con una cardinalidad de muchos.  5 Importar Datos vs Direct Query Existen dos formas de modelar los datos, bien realizando llamadas query a la base de datos desde las diferentes fuentes de datos o bien importarlos en nuestra herramienta. Después hay que realizar la transformación a través de uno de los lenguajes nativos (Power Query o DAX) y realizar los cálculos necesarios.\n¿Cuál es la diferencia? ¿Cuál es el mejor método?\nPrimero, hay que conocer bien la necesidad del propio negocio y la estructura organizacional (¿Cómo de grandes son las fuentes de datos?).\nUtilizamos Direct Query cuando tenemos ya definida una estructura de tablas. Por este motivo, no sería factible su importación y, además, no tiene la capacidad de realizar una pre-agrupación.\nOtra limitación sería que dada una cantidad ingente de datos, mejor utilizar la técnica DIVIDE and CONQUER (“divide y vencerás”), ya que pequeñas consultas query son más eficientes que realizar una llamada de todo el dataset, así evitamos SELECT * FROM.\nSi ya tenemos definido el modelo de datos, es decir, todos los cálculos vienen en tabla, podemos realizar las operaciones de transformación a través de query. En este caso, es más sencillo lanzar una query que tener que realizar una importación de los datos.\nEn caso de realizar operaciones OLAP (cubos multidimensionales), típico en un departamento de negocio, entonces es mejor tener que importar los datos que realizar consultas directas, ya que nos llevaría a que se produzca un error de cálculo.\nLas capacidades de series temporales son tareas complejas y es preferible importar los datos (especialmente para tratar años, trimestres, meses, semanas, días y horas) porque no son compatibles con Direct Query.\nEl mayor límite lo tenemos en la versión desktop (free), de las consultas de hasta un millón de filas, por lo tanto es preferible importar datos.\nEl otro límite es que las fórmulas DAX no son del todo compatible con las consultas Direct Query. Por lo tanto, es importante, durante la fase de recogida de las tablas a través de consultas, obtener un esquema de relaciones como esta imágen:\nNos permite realizar sucesivamente cálculos (campos calculados, tablas y columnas) a través de funciones nativas DAX, pero no transformaciones (es decir modelado de datos), porque estamos realizando las consultas al vuelo.\nSi tienes una sola tabla, pocos datos y necesitas realizar muchos cálculos y transformaciones, entonces, importar datos será tu solución y caso de uso. Es sencilla en Power Query (antes llamada Power Pivot en lenguaje M) ya que las operaciones son realizadas a través de pocos clicks o son operaciones combinadas de transformación.\nPodemos utilizar la edición avanzada:\nO, también, podemos realizar una operación sencilla de transformación con Power Query, como recoger un valor, como valor año, y transformarlo en valor en columna, por ejemplo:\nUna herramienta para todo Aquí una recopilación de tips muy útiles y sencillos de aplicar a nuestros dashboard, pero, esto no es todo. Próximamente, realizaré un post más específico con casos de usos, tutoriales y hasta webinar para mostrar la utilidad de esta herramienta muy potente de análisis, no solo para inteligencia de negocio, sino también utilizada en departamentos de Marketing, Finanzas, TI, Recursos Humanos, etc.\nFUENTE ORIGINAL\n","date":1586364774,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"819490328a0ef8fed87bf1ebefe2e127","permalink":"https://www.marcusrb.com/5-tips-utiles-power-bi/","publishdate":"2020-04-08T17:52:54+01:00","relpermalink":"/5-tips-utiles-power-bi/","section":"post","summary":"Este año la consultora Gartner ha valorado la herramienta de Microsoft de inteligencia de negocio, Power BI, como líder en el cuadrante mágico, donde quedan Tableau muy cerca, y Qlik y Thoughtspot un poco más alejadas. Entre las mejores valoraciones encontramos la usabilidad y poder subir nuestro trabajo en los espacios de trabajo personales de cloud. Veamos en este post algunos de las funcionalidades más destacadas.\n1 Slicer con filtros Una de las primeras funcionalidades que permite dominar la herramienta de visualizaciones Power BI es la completa personalización (tanto en la opción Desktop como en desarrollo).","tags":["dashboard","powerbi","tips"],"title":"5 tips en Power BI, sencillos y muy útiles de aplicar","type":"post"},{"authors":["marcusRB"],"categories":["Data Visualization"],"content":" En este post (y próximas entradas) profundizaré más sobre los aspectos y detalles de la herramienta de Business Intelligence, Google Data Studio. Desde que Google adquiriera Looker en 2019 para poder competir con las 3 bigs ya mencionadas para la consultora Gartner en post anteriores (PowerBI, Tableau y Qlik), Google Data Studio sigue siendo una de las soluciones de uso libre, en formato SaaS, con muchas más novedades y que deja bastante libertad a la hora de crear report y, tal vez, dashboard dinámicos. Pero, ¿cómo podemos sacarle partido para la creación de nuevos dashboards y que no sean siempre las herramientas de medición web como Google Analytics o ficheros csv importados?\nGoogle BigQuery como fuente de datos de Data Studio Ya comentamos las diferencias entre un proveedor de BI y otro, además del coste, usabilidad, movilidad y capacidad de extraer conocimiento de los datos que importamos. Pero, la diferencia principal la encontramos en el número de conectores disponibles. A la hora de desarrollar la API, disponemos de un gran abanico de conectores que tienen las diferentes herramientas para ahorrar tiempo en la ingesta de los datos de diferentes fuentes. Actualmente Data Studio, siendo propiedad de Google, dispone de más de 50 conectores asociados a la familia Cloud, como BigQuery o SQL.\nPrimer Tutorial con BigQuery y pequeñas queries en SQL El tutorial de hoy será sencillo y servirá como punto de partida la “nube”, utilizando uno de los dataset en formato libre de uso disponible en la web oficial donde existen actualmente un gran abanico para explorar y realizar nuestras pequeñas operaciones de carga parciales / totales, análisis de BI y creación de gráficos.\nEste tutorial será un avance de lo que podríamos ser capaces en lo que respecta a crear nuevos gráficos e insights. Además, el dataset utilizado podría superar el terabyte de información, así que la mejor forma es a través de un sistema de ingesta de datos rápido y eficiente, ¡y no muy caro!\nEl ejemplo es sencillo. Realizaremos un volcado a la herramienta de BI; o, también, podemos realizar consultas SQL para detectar patrones.\nLanzando las consultas queries en SQL Es importante distinguir entre MySQL y SQL de BigQuery. Exactamente tienen las mismas sintaxis, menos algunas excepciones a la hora de manipular fechas, arrays, listas anidadas, y que, además, BigQuery tiene su propia librería de auto Machine Learning que veremos en un próximo post.\nAquí va la primera consulta query: una vez conectados seremos capaces de encontrar el número de árboles plantados cada mes.\nLos objetivos serán:\n ¿Qué especies de árboles se plantaron? ¿Quién es el cuidador de los árboles? Dirección de los árboles plantados. Información del sitio del árbol.  SELECT TIMESTAMP_TRUNC(plant_date, MONTH) as plant_month, COUNT(tree_id) AS total_trees, species, care_taker, address, site_info FROM `bigquery-public-data.san_francisco_trees.street_trees` WHERE address IS NOT NULL AND plant_date \u0026gt;= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 365 DAY) AND plant_date \u0026lt; TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), DAY) GROUP BY plant_month, species, care_taker, address, site_info  Copy.\nLa consulta realizada será en formato SQL standard después de utilizar su propio motor BigQuery engine. Aquí podéis encontrar más detalles sobre su funcionamiento.\nLa vista que obtendremos será muy similar a esta:\nGeneramos datos en real time Si queremos obtener datos en real time y un dashboard siempre actualizado, entonces con esta pequeña modificación en la consulta seremos capaces de obtener datos de manera incremental cada día. La query es la siguiente, donde entre sus condiciones WHERE estamos indicando el parámetro Date.\nSELECT TIMESTAMP_TRUNC(plant_date, MONTH) as plant_month, COUNT(tree_id) AS total_trees, species, care_taker, address, site_info FROM `bigquery-public-data.san_francisco_trees.street_trees` WHERE address IS NOT NULL AND plant_date \u0026gt;= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY) AND plant_date \u0026lt; TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), DAY) GROUP BY plant_month, species, care_taker, address, site_info  Copy.\nSucesivamente habilitaremos la opción NEW SCHEDULED QUERY para que tengamos la opción de real time en nuestros informes.\nLa opción siguiente es para que los datos nuevos serán añadidos en lugar de tener que suscribirlos.\nAnalizando los datos en Google BigQuery Podemos, en este punto, realizar todo tipos de operaciones, realizar nuestros descubrimientos, detectar patrones y si queremos ir avanzando con los análisis, buscar pequeños estadísticos básicos, tareas de limpieza y preparación, manipulación, discretización y factorización, métodos de clustering y asociaciones, hasta aplicar algoritmos de clasificaciones, predicciones, etc. A vosotros, analistas, os dejaré esta tarea como pendiente de realizar ;-).\nRealizamos la conexión en Google Data Studio Una vez estudiadas las variables y las base de datos, realizamos la conexión. Para ello vamos en Crear:\nUna vez que escogemos la opción de Data Source:\nProcedemos con la conexión en BigQuery:\nLa conexión que estamos realizando es para utilizar el repositorio público disponible como Public Data-Sets, y una vez seleccionado el proyecto asociado a nuestra cuenta, buscaríamos tree y ya nos saldrán los tres datasets (podemos indistintamente utilizar new york o san francisco).\nY lanzamos nuestra consulta directamente con BigQuery si queremos realizar nuestra personalización, pero directamente en Data Studio. También podemos realizar tareas de asignación de parámetros y jugar con ellas para variables dinámicas.\nUna vez que obtengamos los datos vamos a realizar las separaciones correctamente entre las dimensiones y métricas. Según la guía oficial de Data Studio, las dimensiones son nuestras variables categóricas (fechas, nombres y tipo de árboles, lat y lon, ciudades, booleanos, etc), básicamente todas aquellas que son nombres y están marcada en VERDE. Las métricas serán todas variables numéricas (enteros, decimales, porcentajes, monedas), y estarán marcadas en AZUL.\nPodemos, o bien, importar todas por defecto o abrimos otra tarea a nuestro backlog para realizar todo tipo de operaciones de creación de nuevos campos calculados o nuevas métricas. Es importante esta tarea, que podemos podemos siempre volver atrás para realizarla.\nAdemás, a través del BigQuery Engine, podemos actualizar estas nuevas variables con el tiempo sin miedo de perder la consistencia de los datos (siempre que el origen se mantenga como tal).\nLa otra novedad que tiene Data Studio es el poder de refrescar los datos de forma automática: cada hora, cada 4 horas o cada 12. Para no lanzar muchas consultas, podemos dejar por defecto la última opción, ya que la tarea de planificación en BigQuery es diaria.\nExisten muchas trampas a la hora de convertir una dimensión / métrica en un tipo de variable. Si hace falta, es preferible comprobarlas antes de \u0026ldquo;dibujar\u0026rdquo;.\nLlegados a este punto, podemos crear un report o realizar una exploración. La diferencia es que en la primera hay que realizar todo tipo de gráficos, segmentos y filtros, y será nuestra obra maestra final; y en la segunda, nos da la oportunidad de visualizar en vista previa los datos. Muy útil si queremos ver el formato antes de crear el report. De la misma forma también se puede realizar desde BigQuery, ya que dispone del mismo \u0026ldquo;botón\u0026rdquo;.\nMasterpiece time! Es hora de \u0026ldquo;dibujar\u0026rdquo; la obra maestra Seguramente tú lo harías mejor, pero este sería un ejemplo de cómo presentar los datos desde la fuente BigQuery de Google y Data Studio.\nOs adjunto el link para consultarlo online también: http://paradig.ma/dashboard-data-studio\nGracias y ¡hasta el próximo post!\n\nFUENTE ORIGINAL\n","date":1583427174,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"1468bd29d98f0d20f95553fa96feb017","permalink":"https://www.marcusrb.com/generamos-dashboard-data-studio/","publishdate":"2020-03-05T17:52:54+01:00","relpermalink":"/generamos-dashboard-data-studio/","section":"post","summary":"En este post (y próximas entradas) profundizaré más sobre los aspectos y detalles de la herramienta de Business Intelligence, Google Data Studio. Desde que Google adquiriera Looker en 2019 para poder competir con las 3 bigs ya mencionadas para la consultora Gartner en post anteriores (PowerBI, Tableau y Qlik), Google Data Studio sigue siendo una de las soluciones de uso libre, en formato SaaS, con muchas más novedades y que deja bastante libertad a la hora de crear report y, tal vez, dashboard dinámicos.","tags":["dashboard","google data studio","big query"],"title":"Cómo crear un dashboard con Data Studio con la fuente de Bigquery de Google","type":"post"},{"authors":["marcusRB"],"categories":["Data Science"],"content":" Tal como mencionaba The New York Times hace unos años, “Científico de datos, la profesión más sexy del siglo XXI”, el auge de esta profesión ha ido aumentado. Llevamos a cabo una entrevista a Marco Russo, consultor de datos en Paradigma y docente de varios programas y cursos de Analítica de datos, entre ellos profesor del Máster de Data Science en NEOLAND.\n entr.: El Data Science, ¿es algo pasajero o deberíamos tenerlo en cuenta para el futuro?  Antes de contestar a esta pregunta, veamos estas dos gráficas muy interesantes. La primera es la tendencia de búsqueda en España referente a dos términos (data science y ciencias de datos). El aumento del interés de búsqueda se ha duplicado en desde octubre de 2018.\nPero en la misma medida, las búsquedas de Máster Data Science también ha aumentado un 60%, por lo que sí hay interés de parte de los internautas de entender porque está muy demandado este puesto.\nEn Linkedin si queremos buscar por científico de datos o data scientist los resultados a nivel España con estas dos palabras dan casi unos 1500 posiciones laborales abiertas (27 de octubre 2019). Entre ellas hay posiciones de consultorías, pero sí que son las empresas que demandan esta posición, y no creo que ahora mismo la demanda supere la oferta, se necesitará profesionales y con un buen nivel formativo.\n entr.¿Qué ha pasado exactamente en España, hay una moda referente a ciencias de datos?  No, no es una moda, quizás deberíamos pensar cosa están buscando exactamente las empresas. Las posiciones abiertas de científico de datos además de tener una buena remuneración (de promedio en España está en 37,000 euros brutos más beneficios), proyectos que pueden tener una duración de unos 2 - 3 años a más, y posiciones con niveles desde junior a senior, refiriéndome a expertise en diferentes áreas y herramientas utilizadas, quizás 2 a más de 5 años de experiencia, y entre los skills requeridos, bien aquí bastante que hablar (que conozca un sector o no, que tenga pensamiento computacional, estadístico, matemático, desarrollador en cloud, entre otras cosas). Más que moda es la realidad que están afrontando ahora las empresas. Las mayoría ha tenido que realizar años de ingesta de datos, (sí lo que llamamos Big Data). Miles de millones de bytes recogidos en una o varios repositorios (data lake), seguramente con una cierta calidad (cuestionable), en una plataforma on-premise o en cloud, y que está esperando a profesionales que de un sentido a este trabajo, respuestas, mejoras y rentabilidad. El nuevo petróleo de este decenio, serán los “datos”.\n entr. ¿Podría un científico de datos solucionar los problemas de una empresa?  Estaba leyendo un artículo (mismo escenario que ha pasado tanto en EEUU que en Europa), contaba que las empresas que no supieron organizar correctamente y gestionar proyectos de Big Data, o no tenían una planificación clara de cómo abordar un proyecto de Data Science, solamente el 15% de los proyectos logran ejecutarse. Es increíble, pensar que la parte restante, el 85% vaya a la “basura”. Por lo que, volviendo a la pregunta anterior, sí para muchas empresas ha sido una moda, no tuvieron en cuenta que los datos necesitan ser tratados de una forma, requería una inversión en tecnología e infraestructura, pero que además ya que tienen los datos, piensan que un científico de datos puede solucionar todos sus problemas de TI, o que todos los proyectos que quieren abordar sea fácil y alcanzable en poco tiempo. Creo que sigan esperando este “profesional”\u0026hellip;\n entr: ¿Pero, cuál es el rol de un científico de datos en una organización?  Buena pregunta. Se supone que el científico de datos podría dividirse en dos figuras más, o macro-roles:\n científico de datos de negocio\n científico de datos de TI\n  Y porque no podría ser uno solo, uno es puramente comercial y el otro trabajaría más con la rama de ingeniería, por lo que sería más lógico en desarrollo y TI. Ahora, ambas figuras podrían coexistir en una misma organización o solamente una (dependiendo de la envergadura del proyecto, organización, team, etc). Supongamos que estamos hablando de una grande empresa de telecomunicaciones, como es obvio no podría tener un solo científico de datos, así que serían varios según el proyecto, pero tendríamos estos roles principales:\n analistas de negocio y business analyst, (ambos tienen capacidades de entender cuál es el problema de negocio); data analyst, (tiene el expertise de extraer información valiosa de los datos), que junto con los últimos juega un papel fundamental al estar en un punto intermedio entre negocio y TI; data engineer y data architect, ambos son aquellos que organizan la infraestructura, cloud o on-premise y preparan los datos al servicio de los anteriores data scientist, que podría coordinar, organizar, planificar el team según un proyecto determinado, y a la vez, podría descubrir nuevas fuentes de datos, enriquecer lo que ya tiene, etc.  Podría existir el lead data scientist o el CDO junto con el CTO serían las figuras con más responsabilidades y liderando el departamento de data.\n entr.: ¿Cómo es de importante un científico de datos en un proyecto?  Supongamos que necesitamos incrementar las ventas de una grande distribución, tenemos datos basado en las transacciones de los actuales clientes, y los analistas han detectado que existen patrones, además han detectado otras posibles fuentes de datos capaces de mejorar el resultado, pero no saben bien cómo introducirlos o ingestarlos, tampoco saben si añadir unas variables más, mejorarían el resultado. Podría ser un ejemplo muy básico, pero un analista de datos podría lograr a través de las técnicas de Data Mining, crear modelos basados en estas transacciones, utilizando uno o más algoritmos de clasificación y de regresión y obtener algunos de los resultados. Hay un pero, no estamos considerando las variables de otras fuentes de datos, y tampoco sabemos bien cómo llegar a ellas. Un data engineer podría realizar la tarea, pero tampoco sabe si funcionará o no. Es por este motivo que un data scientist al tener conocimientos (y no solo fundamentos), de Big Data, Estadística, Matemática, Minería de Datos, Algoritmos y Negocio, además de ser un muy buen Comunicador, podría validar el proyecto desde otro punto de vista, modificando los algoritmos según su criterio o simplemente creando uno nuevo, mejorando el flujo de ingesta de datos, mejorando la seguridad, mejorando la velocidad y la calidad. Todo esto supondría un coste y es probable que no sea del todo factible realizarlo. Es por esto que tendrá que coordinar con los analistas de negocios para ponderar costes e ingresos del proyecto y verificar si el margen operativo valga la pena.Desde este ejemplo, podemos entender perfectamente el rol de uno científico de datos, o llamado el Data Wizard, el mago de los datos.\n entrev.: ¿Estudiar un Máster de Data Science podría cubrir la actual oferta?  Una parte seguramente sí, los roles son básicamente tres, analista de datos, científico de datos e ingeniero de datos. El primero tiene muy buenas capacidades analíticas, dará unas primeras pinceladas a la solución del problema, deberá conocer algoritmos, estadísticas y las herramientas principales de ingesta, manipulación, limpieza y visualización de los datos. El último perfil es puramente técnico y desarrollador, tendrá conocimientos avanzados de desarrollo en cloud, tema de seguridad, tema de coste, arquitectura etc. El científico de datos, además de los otros, tendrá que realizar operaciones de nuevas creaciones de modelos, mejorar los algoritmos actuales, etc. Un máster está abierto a todos aquellos que tienen uno de los 3 pilares bien definidos: Comunicación, Pensamiento estadístico y creativo, Desarrollo. Creo que con estos cualquier podría serlo, pero un máster no te convierte automáticamente en un científico de datos. Se necesita algo más que formación, pondría otras cualidades: perseverancia, constancia y esfuerzo - sacrificio. Esta pequeña parte que nos olvidamos siempre cuando estamos cursando un máster, la continuidad de practicar desde casa, en la oficina, y ser curiosos para realizar pequeños nuevos descubrimientos y probar cosas nuevas. Todo esto nos hace mejor profesional de los datos. Quizás las empresas exageran un poco el término de Data Scientist, podría ser incluso un simple Consultor de datos, Especialistas en datos, etc., pero el marketing es una arma potente de venta, ¿no? Sin embargo, estar al día con las nuevas tecnologías, experimentar, seguir formándose y tener disciplina, es algo que finalmente tiene su recompensa.\n entrev.:Tú que eres docente y profesor de varios cursos y de un Máster de Data Science, ¿qué recomendaciones podrías dar a tus alumno/as y futuros?  Como siempre digo, podemos disponer de miles de recursos, tanto online o como presencial, charlas, cursos, másteres, etc. Pero ahí no termina. Como decía antes, necesitaremos marcar unos objetivos a la hora de cursar un Mooc, leer recursos, etc. Quizás nos faltará tiempo, pero nadie se ha convertido de médico o ingeniero en pocos meses, ¿verdad? Es por esto que hay que considerar que estamos hablando de una formación que complementa algo que deberíamos tener. Si somos programadores, o matemáticos o ingenieros, partimos con ventajas, o no. El resto de tendrán que armarse de constancia y practicar mucho, aprender los fundamentos de estadística y matemáticas de bachillerato, eso es importante.\n entrev.:¿Utilizas alguna metodología que puedas mencionar?  Yo adopto la archiconocida Learn to Doing, ha funcionado conmigo y por qué no debería funcionar con el resto. Con la práctica se aprende mejor, y de una simple práctica realizada en aula (presencial o en remoto), podemos profundizar más con los recursos bibliográficos, materiales, modificar y adaptar nuestras mejoras, etc. Desde la práctica de fundamentos, casos reales y recursos para leer desde casa (¡parte importante!).\n entrev.: Y, ¿referente a la planificación de estudio?  También es importante la planificación. Para alcanzar pequeños objetivos a través de la práctica nos ayuda a entender lo que seguirá después, es importante tener una planificación, dentros de unos límites marcados y con una cierta lógica. El Máster de Data Science que estoy impartiendo en NEOLAND comienza con los fundamentos de las dos herramientas más utilizadas, R Studio y Python. Es probable que más adelante nos venga la curiosidad para otros lenguajes, pensaremos en Java, Javascript, Scala o Julia. Sigue Big Data, con la fase de ingesta, en la que deberíamos tener una visión 360º del entorno cloud, de Amazon AWS, Google Cloud Platform GCP y Microsoft Azure, mostrando cómo funciona realmente un proyecto de Big Data y de Análisis de Datos, y también cómo aplicar Auto Machine Learning. Los módulos de Data Mining y Deep Learning son los dos pilares más importantes del máster, así como visualizar y validar a través de los datos los modelos, interpretar sus resultados y mejorarlos con otras técnicas, además de visualizarlos. El proyecto final es el premio, lo que llevará el estudiante en su curriculum y portfolio conjuntamente con el resto de casos que realizará durante el máster. No creo que haga falta algo más, abordar demasiado sería incluso asfixiante para el alumno y no tener una visión clara de lo que será después.\nMuchas gracias Marco por contestar a nuestras preguntas, desde luego queda claro que la introducción del Data Science en el ámbito empresarial, es un paso fundamental para las empresas que quieran mantenerse dentro del paradigma futuro y una gran opción para formarse en ella.\nFUENTE ORIGINAL\n","date":1578415974,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"300000aa4ba0a8541cdccfa6eb45a3fc","permalink":"https://www.marcusrb.com/entrevista-estudiar-data-science-profesion-actual-moda-pasajera/","publishdate":"2020-01-07T17:52:54+01:00","relpermalink":"/entrevista-estudiar-data-science-profesion-actual-moda-pasajera/","section":"post","summary":"Tal como mencionaba The New York Times hace unos años, “Científico de datos, la profesión más sexy del siglo XXI”, el auge de esta profesión ha ido aumentado. Llevamos a cabo una entrevista a Marco Russo, consultor de datos en Paradigma y docente de varios programas y cursos de Analítica de datos, entre ellos profesor del Máster de Data Science en NEOLAND.\n entr.: El Data Science, ¿es algo pasajero o deberíamos tenerlo en cuenta para el futuro?","tags":["master data science","estudiar bootcamp","curso analisis de datos"],"title":"[Entrevista] - Estudiar Data Science, profesión del momento o moda pasajera?","type":"post"},{"authors":["marcusRB"],"categories":["Business Intelligence","Business Analytics","Data Visualization"],"content":" Uno de los retos de un analista digital es pasar por diferentes procesos de limpieza, exploración y análisis de contenido para sacar buenas conclusiones y detectar nuevos escenarios e insights. Además, tiene que comprobar si las fuentes de datos son de dudosa calidad o no.\nY si hablamos de un analista digital, es muy probable que toda esa exploración pase por la herramienta más utilizada en el mundo de la analítica: Google Analytics. Pero seguro que también estará conectado a otras fuentes como Google Sheet, un CRM o base de datos, u otros ficheros de texto, en formato csv, y más.\nHoy en día, un analista digital no solo tendrá que tener conocimientos solo de usabilidad de la web, también de negocios, estadística, programación, base de datos, visualización de datos, además estar actualizado con las nuevas tecnologías.\nEntre las herramientas que normalmente podemos incluir en nuestro \u0026ldquo;maletín\u0026rdquo; de explorador de los datos, sería la programación. Y algunos de nuestros aliados deben ser el lenguaje R o Python, siendo \u0026ldquo;sencillos\u0026rdquo; en su aprendizaje y muy útiles a la hora de ejecutar pequeños script, ya que tienen muchas librerías open-source.\nVeamos qué ofrecen cada uno de ellos y cómo pueden ayudarnos al análisis y visualización de datos.\n¿Por qué R? R es un lenguaje generalista, con diversas librerías de análisis estadístico bastante potentes que pueden suplir el campo de aplicación R, cosa que no sucede con otros lenguajes como Python, por ejemplo.\nR está pensado para explotar su potencial que es la \u0026ldquo;estadística\u0026rdquo;. Este fantástico lenguaje nos permite una primera toma de contacto con los datos debido a su flexibilidad por la exploración, limpieza y análisis a diferentes fuentes de datos, así como aplicar modelos y algoritmos predictivos puede ser de gran ayuda en el mundo de la análisis de datos.\nIntro de R Studio R Studio es un entorno gráfico para el lenguaje de programación R que facilita la creación y ejecución de scripts. También simplifica la instalación de los paquetes necesarios para la ejecución de aquellos scripts que los requieran.\nR Studio utiliza una partición de la pantalla en diferentes secciones, de forma que todos los elementos necesarios se encuentran disponibles a un solo clic, incluidos el código fuente, los datos cargados y generados por dicho código, los resultados obtenidos, los gráficos generados, etc.\nTambién facilita la integración con otros sistemas para la creación de informes en diferentes formatos (principalmente HTML o PDF).\nLibrería de Google Analytics en R Entre las diferentes librerías en R para poder conectar y explorar los datos desde Google Analytics, hay dos en particular de lo que hablaré hoy.\nAmbos están en el repositorio oficial del CRAN (googleAuthR y googleAnalyticsR). Su función es que se necesitan en una el token Google Analytics, mientras el otro habilitar Google Cloud y su API, con lo cual necesitaríamos tener activados:\n Google Cloud, habilitar el proyecto. Permiso de edición de Google Analytics.  Aspectos a considerar de Google Analytics\nDurante la fase de exploración, consultaremos dimensiones y métricas de Google Analytics. SI no estás familiarizado con estas dos partes más importantes de analítica web, mi consejo es que consultes la guía oficial para conocer las más representativas.\nSi, por el contrario, ya conoces la interfaz de Analytics y quieres ir más allá, puedes consultar la tool externa de exploración de estos datos a través de la otra API de Google Analytics, Query Explorer,** **y la extensión o complemento para Google Sheet que permite tener datos directamente en una hoja de cálculo.\nFase de Instalación y Autorización de GA\nEn esta fase cargaremos los paquetes necesarios, previamente necesitaremos una cuenta de Google Cloud (que nos vendrá bien también si en un futuro queremos utilizar Big Query).\nEs importante tener una cuenta de Google Analytics que no sea ni demo, ni solo de lectura, ya que podría tener problemas con los permisos.\n Cargamos las librerías y configurar los valores opcionales:\ninstall.packages(\u0026ldquo;googleAuthR\u0026rdquo;) install.packages(\u0026ldquo;googleAnalyticsR\u0026rdquo;) library(googleAnalyticsR) library(RGoogleAnalytics) library(ggplot2) # para representar gráficamente los datos library(forecast) # para las predicciones seriales library(\u0026ldquo;tidyverse\u0026rdquo;)\n Autorización GA con Google Cloud y ejecutamos\nAutorizamos a través del token con nuestro account Google ga_auth()\n  Comenzamos con la primera query de Google Analytics in R\nVeamos el listado de los account de GA y la guardamos en una nueva variable:\naccount_list Lo que estamos realizando aquí es simplemente a través del token generado por Google Analytics para habilitar desde nuestro account de Google Analytics el listado de Cuentas, Propiedades y Vistas para poder así trabajar con una cuenta específica. ## EDA (Exploratory Data Analysis) El trabajo de exploración del dataset en R requiere de varios pasos para comprobar si existen valores nulos o vacíos, discrepancias que podemos arreglar, sustituir o eliminar. A este proceso se le llama EDA. # Create a list of the parameters to be used in the Google Analytics query # Get the Sessions by Month in 2018 gadata Podéis ver el código [en este repositorio de GitHub][7]. Con nuestra pequeña query, que guardaremos en una nueva variable **gadata**, almacenaremos las sesiones en un periodo de un año, por ejemplo. Paralelamente podemos observar si existen sesiones igual a cero. Comprobado que efectivamente no existen valores nulos ni ceros, procederemos a una representación gráfica con el librería **ggplot** del paquete instalado _ggplot2._   Representación gráfica de la dimensión date y métrica sessions\ngadata %\u0026gt;% ggplot(aes(x=date, y=sessions)) + geom_point()\n   Y si queremos visualizar cada valor por tamaño según la sesión, entonces tendríamos este:\ngadata %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(x=date, y=sessions, size = sessions)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  Y, si además le añadimos gradación por color de más oscuro a más claro, según el tamaño de las sesiones, obtendremos esto:\ngadata %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(x=date, y=sessions, size = sessions, color = sessions)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  Siguiendo la misma lógica, podemos añadir otras métricas de tráfico importantes (duración media, usuarios, páginas vistas, transacciones, eventos, etc), así que para tener una idea de la evolución o tendencia por periodo, podemos representar el gráfico de líneas:\ngadata %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(x=date,y=sessions,group=1)) + geom_line() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) # some styles to rotate x-axis labels  Ahora sí que se nota el pico máximo entre octubre y diciembre, nos hace pensar que este tráfico puede deberse al periodo entre el pre-Black Friday y durante Navidad (si es un retail tiene lógica). Y si queremos representar la tendencia, añadimos la línea de tendencia para que vayamos viendo la evolución del tráfico a lo largo del periodo observado:\ngadata %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(x = date, y = sessions) ) + geom_point() + geom_smooth() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  Con los primeros datos podemos observar que ha habido un periodo estable, con acciones puntuales (serán promociones) y el pico hasta navidad.\nAhora nos interesa conocer algo más de nuestros usuarios, segmentando así el tráfico según periodos más cortos.\nPasamos a detectar si existen diferencia durante los días de la semana y hora del día. Creamos una nuevo dataset con las métricas sesiones y duración media de sesión, por día de la semana.\n# Añadimos la dimensión día de la semana y fecha - solo 1er semestre gadata_2 ## Segmentos de sesiones por día de la semana, franja horaria y categorías de dispositivos ![][13] Podemos observar que los boxplot indicados son muy relevantes, aunque necesitamos profundizar más sobre temas de cuartiles, min y max, media y mediana. Los puntos son nuestro \u0026quot;amigos\u0026quot; **outliers, **así que en casos puntuales estos tendrán que ser excluidos en algunos modelos y análisis. Lunes, martes y miércoles a primera vista tienen el mismo impacto, al igual que viernes y sábado. ## Duración media de sesión por día de la semana ![][14] Los datos en la segunda métrica son expresados en segundos, y los días de la semana están según el formato anglosajón (0 = Domingo , 6 = Sábado). Nos interesa ahora conocer la duración media de sesión por hora del día y el día de la semana. Una representación gráfica podría ser una matriz, con dimensiones día de la semana y hora, veamos un periodo de tiempo más corto, por ejemplo 6 meses.   Cargamos las librería correspondientes y guardamos en una nueva variable el dataframe:\nlibrary(\u0026ldquo;RColorBrewer\u0026rdquo;)\ngadata_3\nPodría ejecutarlo para verlo, pero para una correcta lectura de los días de la semana, sustituyamos los números por nombres, ordenando así los días de la semana:\ngadata_3$dayOfWeekName\nCon esta matriz se puede observar que las franjas horarias de 8:00 a 12:00 de lunes a viernes tienen mayor impacto por duración promedio, aunque se observan picos desde las 21:00 a las 00:00 los martes, miércoles y jueves.\nSábado es el día más tranquilo comenzando desde el viernes por la tarde, siguiendo hasta el domingo donde se observa un pico a partir de las 17:00 hasta las 22:00. Interesante para preparar campañas publicitarias o remarketing en estos horarios.\nSeguimos ahora con la segmentación por categoría de dispositivos. No es lo mismo con equipo de escritorio que con móvil o tablet. Así que vamos con la creación de una nueva variable y con dimensión **deviceCategory. **\nOtra visualización a realizar e interesante, sería la comparación por dispositivo. gadata_4 % ggplot(aes(deviceCategory, sessions)) +\ngeom_bar(aes(fill = deviceCategory), stat=\u0026ldquo;identity\u0026rdquo;)\n  # plot avgSessionDuration with `deviceCategory` gadata_4 %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(deviceCategory, avgSessionDuration)) + geom_bar(aes(fill = deviceCategory), stat=\u0026quot;identity\u0026quot;)  Con la segmentación por categoría de dispositivos podemos observar que tenemos mucho tráfico entrante por móvil, pero la duración promedio es muy baja.\nDato interesante si queremos utilizar como medio de \u0026ldquo;prospecting\u0026rdquo; el dispositivo móvil, pero también mejorar la conversión en desktop. El objetivo es crear finalmente un proyecto CRO y analizar muchos otros aspectos de usabilidad.\nEn este primer post hemos realizado una exploración de los datos integrando la API de Google Analytics en RStudio y como resultado unos cuantos ejemplos útiles a la hora de generar Insights.\nEn la segunda parte veremos algo más de inferencia estadística y aplicaremos algún modelo de predicción, que nos será de utilidad a la hora de buscar patrones y tendencias.\nSi quieres tener acceso al repositorio, puedes acceder a él a través de este link.\n(FUENTE ORIGINAL)[https://www.paradigmadigital.com/dev/analitica-web-r-analisis-visualizacion-datos/]\n","date":1570639974,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"460b39ea68f470128dcdc25ced46fbfe","permalink":"https://www.marcusrb.com/analitica-web-rstudio-analisis-visualizacion-datos/","publishdate":"2019-10-09T17:52:54+01:00","relpermalink":"/analitica-web-rstudio-analisis-visualizacion-datos/","section":"post","summary":"Uno de los retos de un analista digital es pasar por diferentes procesos de limpieza, exploración y análisis de contenido para sacar buenas conclusiones y detectar nuevos escenarios e insights. Además, tiene que comprobar si las fuentes de datos son de dudosa calidad o no.\nY si hablamos de un analista digital, es muy probable que toda esa exploración pase por la herramienta más utilizada en el mundo de la analítica: Google Analytics.","tags":["power bi","gartner","pytohn"],"title":"Comparativa de plataformas Business Intelligence y análisis de datos: introducción","type":"post"},{"authors":["marcusRB"],"categories":["Data Science"],"content":" Cómo ser Data Scientist desde cero La guía de ruta de aprendizaje definitiva que detalla todas las habilidades, conocimientos y capacitación que necesita para convertirse en un científico de datos ¿Busca una carrera que sea interesante, desafiante y muy solicitada? Una carrera de Data Scientist cumple todos esos requisitos y más.\nYa sea que lo sepas o no, estamos en medio de la 4ta Revolución Industrial (o Industria 4.0) que está siendo impulsada por el Internet de las Cosas (IoT) y la IA. Ambos se caracterizan por la recopilación, análisis e intercambio de datos. Gran cantidad de datos.\nSeguir una carrera en ciencia de datos es un movimiento sabio en estos días. Los trabajos relacionados con la ciencia de datos ocupan un lugar destacado en el informe de empleos emergentes de LinkedIn. El especialista en ciencia de datos y el gerente de ciencia de datos figuran en el top 15.\nNo hay duda de que las habilidades de ciencia de datos tienen una demanda alta y creciente. Todo tipo de empresas los necesitan, desde fabricantes hasta minoristas de Internet, desde empresas de nueva creación hasta agencias gubernamentales. También es una carrera bien remunerada, con un científico de datos promedio que gana un salario de $ 113,436 en los Estados Unidos.\nPor lo tanto, si está interesado en ayudar a las empresas a planificar su comercialización mediante la interpretación de grandes cantidades de datos, o ayudar a los gobiernos a enfocar sus recursos en las áreas correctas mediante el estudio de correlaciones o patrones de datos, existe una gran variedad.\nPero, ¿cómo te calificas y estableces una carrera como científico de datos? Esta guía detallada explicará los pasos necesarios, así como algunos cursos sugeridos para acelerar su progreso.\nLearning Path o pasos para el Data Science Pasos para convertirse en un científico de datos 1. Obtener calificaciones En primer lugar, necesitará algunas calificaciones técnicas. La ruta más común es estudiar para obtener una licenciatura o maestría. De hecho, el 88% de los científicos de datos tienen un mínimo de una maestría, y el 46% tiene un doctorado.\nPara obtener la mayoría de las habilidades y conocimientos necesarios para el trabajo, debe estudiar para obtener un título en Matemáticas y Estadística, Ciencias de la Computación o Ingeniería. Otras calificaciones pueden ser suficientes, pero estas son las más comunes. Alternativamente, como hay una escasez de científicos de datos, cada vez más empresas se enfrentan a personas que no tienen calificaciones formales. En su lugar, necesitará tener una buena experiencia en un rol relevante (programador de computadoras, ingeniero) o ser capaz de demostrar buenas habilidades de matemática e informática. También deberás completar algunos cursos especializados.\nEn estos días, puede encontrar cursos en línea totalmente certificados que imparten expertos en el campo de la ciencia de datos. Las plataformas de aprendizaje electrónico se han convertido en la mejor manera de obtener habilidades especializadas a un precio asequible, y están superando a las instituciones educativas formales como la forma número uno de obtener conocimientos y habilidades en profundidad.\n Desarrollar habilidades y conocimientos Además de las calificaciones, deberá ser capaz de demostrar habilidades específicas y conocimientos especializados.  Muchas personas persiguen una maestría en ciencia de datos, pero hay otras rutas, como cursos de aprendizaje electrónico, para adquirir el conocimiento relevante. Dependiendo de los requisitos del rol, es posible que necesite saber: Cómo codificar con un lenguaje como Python o C # ser capaz de usar SQL experiencia con Hadoop o plataformas similares experiencia de aprendizaje automático / IA visualizar y presentar datos con software o plataformas como ggplot, d3.js o tableau.\nEn términos de habilidades no técnicas, las siguientes son generalmente altas en las listas de empleadores: Atención al detalle: debe ser capaz de garantizar la precisión e integridad de los datos. Habilidades de organización: tratar con grandes conjuntos de datos, con potencialmente millones de puntos de datos, requiere habilidades de organización de alto nivel y un enfoque lógico y metódico Resolución de problemas: una parte importante del rol es encontrar nuevas formas de recopilar, interpretar y presentar datos. Esto requiere la capacidad de resolver problemas y \u0026ldquo;pensar fuera de la caja\u0026rdquo; a veces. Deseo de aprender: nuestro mundo tecnológico está en constante cambio, incluidos los métodos de recopilación de datos y las demandas impuestas al uso de estos datos. Los científicos de datos deben estar preparados para estudiar y practicar continuamente nuevas tecnologías y técnicas. Resiliencia y enfoque: estos rasgos de carácter son esenciales para los científicos de datos, ya que a menudo pasarán mucho tiempo en un problema, intentando diferentes formas de resolverlo. Comunicación y trabajo en equipo: la mayoría de los trabajos de ciencia de datos requerirán que trabajes con otros, a menudo de diferentes departamentos y disciplinas.\n Ganar experiencia laboral Durante sus estudios y posteriormente, es una buena idea adquirir algo de experiencia laboral.  Es posible que tenga la suerte de encontrar trabajo remunerado para cualquier número de empresas que necesiten científicos de datos. Estas empresas operan en todas las áreas de la economía, incluidas las finanzas, el comercio minorista, la fabricación, la ingeniería, etc. Las organizaciones sin fines de lucro y de caridad son un buen lugar para buscar si tiene dificultades para encontrar experiencia laboral, aunque es posible que tenga que conformarse con Trabajo no remunerado.\nOtra forma de obtener una valiosa experiencia en el campo de la ciencia de datos es inscribirse en cursos que ofrezcan talleres como parte del plan de estudios. Los cursos SuperDataScience ofrecen actividades prácticas de la vida real que le permiten desarrollar su nivel de experiencia.\nLa variedad de proyectos especializados es demasiado numerosa para enumerarla en detalle, pero aquí hay algunos ejemplos para despertar su apetito: Limpieza de datos: los sistemas de bases de datos grandes y complejos necesitarán limpieza frecuente, remodelación y archivo de conjuntos de datos. Los proyectos de limpieza de datos requieren un buen conocimiento de Python o R. Creación de visualizaciones de datos interactivas: si le gusta presentar datos en formatos únicos e interesantes, este tipo de proyecto le conviene. Utilizará software de tablero de algún tipo, p. Dash b Plotly, para crear visualizaciones de datos para organizaciones. Análisis de datos exploratorios (EDA): implica la interpretación de los datos, la formulación de preguntas relevantes que pueden revelar información comercial y luego responder las preguntas utilizando SQL, Python u otro lenguaje de programación. Aprendizaje automático: existen diferentes niveles de complejidad de los proyectos de aprendizaje automático. Como principiante, adhiérase a los proyectos de regresión lineal y logística, ya que son ideales. Este tipo de proyectos a menudo se utilizan para crear modelos para interpretar datos y comunicar ideas a los gerentes.\nEs útil crear una cartera profesional que incluya algunos tipos diferentes de proyectos exitosos, por lo que no tenga miedo de probar algunas especialidades diferentes para comenzar. Esto es especialmente cierto si no está seguro de en qué especialidad centrarse inicialmente.\nLa buena noticia es que todos los cursos de SuperDataScience actualizan automáticamente su cartera profesional al finalizar cualquier taller especializado en el que participe durante el curso. Haga clic aquí para obtener más información sobre nuestros cursos.\nCursos especializados de aprendizaje electrónico de ciencia de datos Necesita estudiar habilidades especializadas para convertirse en un Data Scientist competente y exitoso. También necesita renovar y actualizar continuamente sus conocimientos y habilidades. En SuperDataScience, ofrecemos una amplia gama de cursos que se especializan en ciencia de datos. Nuestro objetivo es hacer que las materias complejas sean fáciles de aprender.\nRuta de aprendizaje definitiva Nuestro exclusivo Ultimate Learning Path es el programa de estudios de aprendizaje electrónico más completo disponible para la ciencia de datos. Obtendrá todas las habilidades y conocimientos que necesita para convertirse en un científico de datos totalmente calificado. Incluye todo lo que cubrirías en un curso de Data Science Master en una institución educativa tradicional. Además, obtendrá la certificación para cada curso que complete, y la participación en los talleres especializados se agregará instantáneamente a su cartera profesional.\nAquí hay un breve resumen de algunos de nuestros cursos en línea más populares y lo que puede obtener de ellos.\nCurso de aprendizaje automático El aprendizaje automático es un campo en crecimiento dentro de la ciencia de datos. A medida que la IA se vuelve más popular y ampliamente implementada, es esencial comprender el aprendizaje automático y cómo aplicarlo. Este curso lo lleva paso a paso a través de teorías complejas, algoritmos y bibliotecas de codificación, lo que lo hace fácil de entender y digerir. Obtendrá una enseñanza simple pero profunda en todo lo que necesita saber para asumir proyectos complejos de aprendizaje automático. Los temas del curso incluyen:\nRegresión lineal Regresión lineal múltiple Agrupamiento de medias K Agrupación jerárquica Vecino K-más cercano Árboles de decisión Bosque al azar\nR Programación de la A a la Z R es un lenguaje de programación ampliamente utilizado para computación estadística y gráficos. Debido a la curva de aprendizaje empinada de R, hemos tenido cuidado de asegurarnos de que este sea un curso secuencial que gradualmente desarrolle su conocimiento, sin abrumarlo. Cada módulo se centra en un concepto diferente que puede aplicarse instantáneamente.\nTendrás manos a la obra con desafíos analíticos de la vida real, dándote la oportunidad de dominar R y desarrollar tus habilidades para resolver problemas.\nCiencia de datos de la A a la Z Este curso cubre todo lo que necesita saber para conseguir un trabajo como científico de datos. De hecho, serás puesto en la piel de un científico de datos, aprendiendo a lidiar con todos los desafíos que enfrentan regularmente, como datos corruptos, anomalías, irregularidades, ¡lo que sea!\nAprenderá a usar una variedad de herramientas como SQL, SSIS y Tableau. Y al final del curso sabrás cómo: limpiar y preparar datos para análisis realizar una visualización básica de datos datos del modelo datos de ajuste de curva Presentar hallazgos y conocimientos de datos. Estadísticas para Business Analytics \u0026amp; Data Science A-Z En lugar de intentar aprender (o volver a aprender) cada concepto y habilidad estadística, dominará solo los que necesita para trabajos de Data Scientist o Business Analyst. Los temas cubiertos incluyen: distribuciones la prueba z Teorema del límite central prueba de hipótesis intervalos de confianza significancia estadística y muchos otros temas relevantes \u0026hellip;\nTambién podrás aplicar este conocimiento a situaciones de la vida real, preparándote bien para cualquier trabajo o proyecto que realices. Este curso lo capacitará para una carrera exitosa en ciencias de datos o análisis de negocios.\nProgramación de Python de la A a la Z Python es un gran lenguaje de programación para aprender para los científicos de datos. Es ampliamente utilizado con muchas plataformas estadísticas y herramientas que dependen de él. Como con la mayoría de los lenguajes de programación, hay una curva de aprendizaje empinada. Para evitar sentirse abrumado, hemos estructurado este curso para que desarrolle conceptos gradualmente. A lo largo del curso, podrá aplicar sus conocimientos y habilidades con desafíos analíticos de la vida real. Aprendizaje profundo de la A a la Z El aprendizaje profundo se está convirtiendo en una característica importante de la Inteligencia Artificial (IA) a medida que los problemas se vuelven cada vez más complejos de resolver.\nAutomóviles autónomos, motores de diagnóstico médico, IA basada en la teoría de juegos: todos deben ser impulsados ​​por el aprendizaje profundo. Este curso cubre todo de la A a la Z para una comprensión completa del aprendizaje profundo. El curso incluye: Desarrollar una comprensión intuitiva de los conceptos complejos. 6 emocionantes desafíos del mundo real (incluido el uso de redes neuronales recurrentes para predecir los precios de las acciones y la creación de mapas autoorganizados para investigar el fraude) Codificación práctica Soporte en curso de expertos en ciencia de datos Dominio de herramientas importantes, incluidas Tensorflow y Pytorch. Comience su viaje de ciencia de datos Asegúrese de obtener la capacitación científica de datos más actualizada y completa disponible. Inscríbase en nuestros cursos hoy.\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\n","date":1568047974,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"c8c32edaa99f20cf1d0c3d5bcee37c66","permalink":"https://www.marcusrb.com/como-convertirse-en-data-scientist/","publishdate":"2019-09-09T17:52:54+01:00","relpermalink":"/como-convertirse-en-data-scientist/","section":"post","summary":"La guía de ruta de aprendizaje definitiva que detalla todas las habilidades, conocimientos y capacitación que necesita para convertirse en un científico de datos","tags":["formación data science","científico de datos","big data"],"title":"Cómo convertirse en un científico de datos","type":"post"},{"authors":["marcusRB"],"categories":["Data Science"],"content":" La guía de ruta de aprendizaje definitiva que detalla todas las habilidades, conocimientos y capacitación que necesita para convertirse en un estadístico\n Si las matemáticas y los datos “flotan en tu bote”, entonces una carrera como estadístico podría ser justo lo que estás buscando.\nLos estadísticos a menudo trabajan junto con otros especialistas en datos, como científicos de datos, ingenieros de aprendizaje automático y analistas de inteligencia empresarial. Una gran parte de su función puede ser ayudar a interpretar, preparar y presentar datos, pero a menudo se emplean para agregar músculo matemático a un equipo de ciencia de datos.\n Un buen estadístico tendrá un profundo conocimiento general de la rama estadística de las matemáticas, fuertes habilidades para resolver problemas y un interés en la tecnología.\n Los estadísticos han existido durante mucho tiempo y a menudo se considera una de las viejas carreras tradicionales. A pesar de esto, todavía hay una gran demanda de estadísticos. La Oficina de Estadísticas Laborales de EE. UU. predice que será la sexta carrera de mayor crecimiento en los próximos 10 años.\n ¿Qué hace un estadístico? El papel de un estadístico puede ser muy variado, dependiendo de las demandas del empleador.\nAquí están algunos ejemplos:\n Las empresas tecnológicas pueden necesitar un estadístico para diseñar algoritmos que aprovechen los datos de un producto o para generar información comercial. Las compañías encuestadoras necesitan estadísticos para diseñar encuestas justas e imparciales, analizar los datos resultantes y preparar una variedad de visualizaciones de datos para diferentes audiencias. Los servicios de suscripción como Netflix o Spotify utilizan estadísticos y científicos de datos para segmentar datos, crear algoritmos y preparar datos para funciones como listas de recomendaciones. Los sitios de redes sociales como Facebook y Twitter emplean estadísticos para construir modelos usando R (un lenguaje de programación estadística). Las agencias gubernamentales reclutan estadísticos para recopilar y analizar datos, así como para realizar simulaciones, crear modelos y algoritmos, etc. Las empresas manufactureras pueden contratar estadísticos para analizar datos operativos, construir modelos y ayudar a aumentar la productividad.   Esta es solo una breve lista de los tipos de cosas que hacen los estadísticos. La verdad es que, donde haya datos (especialmente grandes cantidades de datos), se necesitan estadísticos.\n ## Perspectivas de empleo y salario estadístico\nSi está considerando seriamente una carrera como estadístico, hay buenas noticias. Los estadísticos tienen una gran demanda. En abril de 2019, USA Today colocó al estadístico como el trabajo número 5 en los EE. UU., Por las siguientes razones:\n \u0026ldquo;Muy buen ambiente de trabajo, muy bajo estrés en el trabajo y muy buen crecimiento del empleo proyectado, lo que lo convierte en una de las pocas carreras en recibir las mejores calificaciones disponibles para las tres categorías\u0026rdquo;.\n Los estadísticos agregan una enorme cantidad de valor a un negocio, por lo que se les reembolsa bien por sus esfuerzos. El salario promedio de un estadístico es de $ 83,000, con estadísticos de alto nivel que ganan $ 100k + por año.\n  Pasos para convertirse en un estadístico  1. Obtener calificaciones Para obtener el conocimiento matemático y las habilidades que necesita para ser estadístico, necesitará un mínimo de un título de posgrado en una materia relacionada. Una licenciatura en Matemáticas y Estadística es el punto de partida más común.\n Muchos estadísticos continúan estudiando para obtener una maestría o un doctorado en temas como estadística aplicada, programación estadística o análisis de modelos de varianza. Si ya está trabajando en un trabajo relacionado con datos o tecnología y desea pasar a ser estadístico, puede ser aceptado con un título en otro campo matemático como ingeniería o física.\n Una vez que haya completado su título y tenga una idea del tipo de rol de estadístico en el que le gustaría especializarse, entonces es una buena idea tomar algunos cursos especializados como Python o R o diseño avanzado de bases de datos.\n Las plataformas de aprendizaje electrónico como SuperDataScience ofrecen la mejor manera de ampliar sus estudios en las áreas estadísticas que buscan los empleadores.\n  2. Desarrollar habilidades y conocimiento Necesitará desarrollar habilidades técnicas y personales para tener una carrera exitosa como estadístico. Como con la mayoría de los trabajos en estos días, debe actualizar constantemente sus conocimientos a lo largo de su carrera. Estas son algunas de las habilidades técnicas que se espera que tenga:\n  Programación en Python para diseñar y construir algoritmos Programación R para modelado estadístico y simulación SQL para construir y administrar bases de datos complejas Habilidades de análisis empresarial para generar conocimientos   La mayoría de los roles estadísticos anunciados en los sitios web de trabajo en estos días requieren habilidades y experiencia con algún tipo de lenguaje de programación estadística, como R. Sin habilidades de codificación estadística, puede tener dificultades para encontrar un trabajo estadístico, especialmente uno que pague más.\n La mayoría de los roles estadísticos anunciados en los sitios web de trabajo en estos días requieren habilidades y experiencia con algún tipo de lenguaje de programación estadística, como R. Sin habilidades de codificación estadística, puede tener dificultades para encontrar un trabajo estadístico, especialmente uno que pague más.\n Además de las habilidades técnicas, también debe trabajar en el desarrollo de habilidades blandas como:\n Trabajo en equipo y comunicación: se espera que trabajes como parte de un equipo de ciencia de datos, por lo que debes desarrollar tus habilidades interpersonales. Atención al detalle - la precisión es importante para mantener la integridad de los datos. Debería poder verificar su propio trabajo críticamente. Resolución de problemas: la creación de algoritmos y modelos estadísticos puede ser bastante difícil, especialmente cuando se trata de conjuntos de datos grandes y diversos. Resiliencia: trabajar como estadístico exige altos niveles de concentración y persistencia. Adaptabilidad: el panorama tecnológico cambia constantemente, lo que significa que los estadísticos necesitan adquirir con frecuencia nuevos conocimientos y aprender nuevas habilidades.   3. Cree una cartera de experiencia laboral Adquirir experiencia laboral y registrarlo en una cartera profesional es una buena manera de mostrar sus habilidades y demostrar una actitud proactiva. Esto lo colocará por encima de otros candidatos cuando solicite empleo.\n La experiencia laboral no necesariamente significa empleo. Cualquier proyecto que realice que implique trabajo estadístico puede incluirse en su cartera. Esto incluye:\n  Cursos: los cursos en línea o en vivo a menudo incluyen ejercicios y mini proyectos. A menudo recibirá un certificado al finalizar, que se puede incluir en su cartera. Talleres: las sesiones de estudio o proyectos grupales que se centran en habilidades específicas son excelentes para incluir, ya que demuestran un profundo conocimiento del tema. Trabajo remunerado o no remunerado - una vez que alcanza un buen nivel de experiencia a través del estudio, comienza a aplicar sus habilidades. Puede obtener una pasantía o trabajar gratis para que una organización voluntaria gane experiencia. Otra posibilidad es hacer algunos pequeños proyectos independientes en forma paralela, a través de sitios web como Upwork. Lectura extensa o estudio adicional - se puede incluir cualquier estudio o lectura individual adicional que haga. Esto muestra iniciativa y pasión por el tema. Pasatiempos o intereses relacionados - tal vez disfrutes codificando juegos simples o jugando ajedrez a un alto nivel. Es bueno incluir cualquier pasatiempo que demuestre cualquiera de las habilidades mencionadas anteriormente (resolución de problemas, resistencia, atención al detalle, etc.).   Hay buenas noticias si decides inscribirte en algún curso. Obtendrá una cartera profesional creada para usted, que se actualiza automáticamente con cualquier taller especializado en el que participe.\n","date":1567529574,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"b2b1285b76df84580bf28dac50d32238","permalink":"https://www.marcusrb.com/como-convertirse-en-estadistico/","publishdate":"2019-09-03T17:52:54+01:00","relpermalink":"/como-convertirse-en-estadistico/","section":"post","summary":"La guía de ruta de aprendizaje definitiva que detalla todas las habilidades, conocimientos y capacitación que necesita para convertirse en un estadístico","tags":["formacion data science","estadística","big data"],"title":"Cómo convertirse en un estadístico","type":"post"},{"authors":null,"categories":null,"content":" Se creará una sección especial con varios ejemplos del lenguaje R, para aprender la análisis de datos. R es un lenguaje de programación para la gestión y la análisis de datos, además de visualización de gráficos. Es un software libre y disponible en diferentes entornos (Unix, Linux, MacOS, Windows).\nEsta primera sección se especificará como instalar y como utilizarlas. Además de contribuir a añadir varios ejemplos de script para la exploración de datos, limpieza, uso de funciones matemáticas y estadísticas, aprendizaje automático y casos de uso como solución de negocio.\nUnos de los primeros proyectos realizados será la exploración de los datos, o EDA (Explorationa Data Analysis), pero con datos de Google Analytics, es decir exploraremos los datos de un sitio web y que conclusiones podemos sacar con esto.\nCheatsheets\n","date":1567375200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"4e3516537972809789c94b1eed37faa1","permalink":"https://www.marcusrb.com/projects/r-studio/","publishdate":"2019-09-02T00:00:00+02:00","relpermalink":"/projects/r-studio/","section":"projects","summary":"Directorio de proyectos realizado en R studio","tags":["machine learning","lenguaje r","data mining","exploración datos"],"title":"Proyectos realizados en R Studio","type":"projects"},{"authors":null,"categories":null,"content":" Se creará una sección especial con varios ejemplos del lenguaje R, para aprender la análisis de datos. R es un lenguaje de programación para la gestión y la análisis de datos, además de visualización de gráficos. Es un software libre y disponible en diferentes entornos (Unix, Linux, MacOS, Windows).\nEsta primera sección se especificará como instalar y como utilizarlas. Además de contribuir a añadir varios ejemplos de script para la exploración de datos, limpieza, uso de funciones matemáticas y estadísticas, aprendizaje automático y casos de uso como solución de negocio.\nUnos de los primeros proyectos realizados será la exploración de los datos, o EDA (Explorationa Data Analysis), pero con datos de Google Analytics, es decir exploraremos los datos de un sitio web y que conclusiones podemos sacar con esto.\nCheatsheets\n","date":1567375200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"391e4c6dae7273bd037a94fe47b74195","permalink":"https://www.marcusrb.com/proyectos/r-studio/","publishdate":"2019-09-02T00:00:00+02:00","relpermalink":"/proyectos/r-studio/","section":"proyectos","summary":"Directorio de proyectos realizado en R studio","tags":["machine learning","lenguaje r","data mining","exploración datos"],"title":"Proyectos realizados en R Studio","type":"proyectos"},{"authors":null,"categories":null,"content":" Machine Learning and Deep Learning Consulting ¿Cuál es la diferencia entre ML y consultoría de IA?\nAunque el aprendizaje automático (ML) es el subcampo de la IA con la mayoría de las aplicaciones comerciales, es mejor distinguirlas.\nIA: incluye todas las aplicaciones en las que la computadora imita la inteligencia humana ML: aplicaciones que utilizan datos conocidos para crear modelos que se pueden utilizar para clasificar / procesar nuevos datos  ¿ML consulting = consultoría de aprendizaje profundo?\nNo exactamente. El aprendizaje profundo es un subconjunto del aprendizaje automático. Sin embargo, el aprendizaje profundo es la técnica de aprendizaje automático más exitosa en términos de precisión a partir de 2019 en la mayoría de las áreas.\nNo es raro ver que en la industria se implementen técnicas alternativas como los bosques de decisiones en lugar del aprendizaje profundo. Esto se debe a que la falta de explicación de los resultados es un desafío para los modelos de aprendizaje profundo. Hay casos en los que los modelos de aprendizaje profundo no se implementan en producción.\nya que los gerentes no se sienten cómodos con modelos que no comprenden y que no brindan una explicación de los resultados. en los casos en que se requiera auditabilidad. Por ejemplo, la legislación laboral prohíbe la discriminación. Cualquier algoritmo que utilice criterios que se hayan utilizado anteriormente para la discriminación (es decir, género o raza) no puede tomar decisiones de recursos humanos legalmente sin proporcionar una justificación que involucre razones distintas a esos criterios. Lamentablemente, excluir del modelo criterios potencialmente discriminatorios no resuelve el problema. Por ejemplo, el nombre, los patrones en PTO, la brecha salarial y muchos otros puntos de datos podrían usarse para incluir indirectamente el género en la toma de decisiones. Los modelos de caja negra, sin importar cuán precisos o útiles sean sus resultados, no se pueden implementar en tales situaciones.  Explicar el aprendizaje profundo es un área activa de investigación llamada XAI (IA explicable). ¿Cuáles son las barreras para la adopción del AA?\nComo destaca Deloitte, estas son las barreras mencionadas con mayor frecuencia según los profesionales:\nEscasez de talento: a agosto de 2018, había 150.000 puestos de trabajo de ciencia de datos sin completar en los EE. UU. Que Linkedin describió como una escasez aguda en las grandes ciudades de EE. UU. Inmadurez de la infraestructura y los procesos de ML: ML es un nuevo paradigma de programación, que deriva reglas de los datos en lugar de la entrada del programador. Nos tomó decenas de años crear Scrum, el enfoque de programación ágil, que la mayoría de los equipos utilizan actualmente. Del mismo modo, se necesitará tiempo para que los procesos y marcos de ML alcancen la madurez. TensorFlow, uno de los marcos de aprendizaje automático más utilizados, se publicó a finales de 2015. La mayoría de las técnicas de aprendizaje automático consumen mucha información: los datos de entrenamiento etiquetados con precisión requieren mucho tiempo y son costosos de generar. Los profesionales del aprendizaje automático deben ser creativos al aprovechar los datos públicos o etiquetar los datos necesarios. Es por eso que se fundaron numerosas empresas de etiquetado de datos desde la década de 2010. Otra solución a esto es el aprendizaje de una sola vez y otros enfoques que requieren menos datos; sin embargo, esta es un área de investigación en curso. El aprendizaje profundo no se puede explicar. Como se discutió, esto está obstaculizando el progreso y XAI intenta abordarlo.  ¿Cuál es el futuro de la consultoría de aprendizaje automático?\nLa consultoría de ML crecerá al abordar los problemas identificados:\nExpansión del grupo de talentos: la mayoría de las consultorías están analizando su fuerza laboral en detalle para identificar a aquellos que son capaces de la ciencia de datos. Una formación en programación, estadística o matemáticas tiende a ser suficiente para que las personas trabajen como científicos de datos después de una formación relativamente rápida.\nMejorar la infraestructura y los procesos de ML: a medida que ML madura como paradigma de programación, mejores procesos, mejores recursos informáticos (es decir, GPU y chips de IA) y más automatización harán que ML sea más rápido y más fácil.\nSer creativo con los datos: los avances en el procesamiento del lenguaje natural (NLP) se debieron a la amplia disponibilidad de documentos gubernamentales traducidos en Canadá y Europa. Si bien la búsqueda de datos es una solución relativamente sencilla, las áreas de investigación de la inteligencia artificial, como el aprendizaje por transferencia o la síntesis de datos, podrían ser soluciones más técnicas.\nTambién se esperan avances en la IA explicable que aumentarían la confianza en los sistemas ML y permitirían su adopción más generalizada.\nPor último, es probable que las aplicaciones locales de aprendizaje automático hagan que las aplicaciones de IoT sean más inteligentes y rápidas al llevar la toma de decisiones a los dispositivos periféricos. ¿Cuáles son las actividades típicas de consultoría de ML? Comprender las necesidades comerciales\nComo en toda consultoría, todo comienza con la necesidad empresarial. Ya sea que se trate de predecir dónde instalar estaciones base de telecomunicaciones o a quién mostrar anuncios, malinterpretar los requisitos comerciales sigue siendo una de las principales razones de la falta de éxito de los proyectos de consultoría y software. La consultoría ML, en la intersección de la consultoría y el software, es especialmente propensa a este problema. Configurar el equipo y el proceso\nNo todos los problemas necesitan aprendizaje automático. El aprendizaje automático y otros enfoques heurísticos tienen sentido en problemas que no pueden reducirse a un conjunto de reglas. Si las reglas son bien conocidas y simples, los sistemas basados ​​en reglas superan al aprendizaje automático y son más simples de mantener.\nSi ML es un buen ajuste fo es necesario delinear un problema, el equipo del proyecto, las partes interesadas y los objetivos de alto nivel. Recolección y exploración de datos\nSi la empresa tiene los datos, este es un paso relativamente sencillo. Los consultores deben trabajar con las empresas para validar que los datos estén correctamente etiquetados y no sean contradictorios.\nSi los datos no están disponibles, se deben considerar las técnicas descritas anteriormente, como aprovechar los datos en línea, pagar por el etiquetado de datos y enfoques novedosos de ML, como el aprendizaje de una sola vez. Modelo de desarrollo\nSe necesitan miles de experimentos para desarrollar un modelo de aprendizaje automático de alto rendimiento. Este es un proceso iterativo que tiene en cuenta las últimas investigaciones, comprende la dinámica empresarial y la exploración de datos.\nEn última instancia, todos los modelos se evalúan con el mismo conjunto de datos de prueba para evaluar su precisión. Desarrollo de aplicaciones de pila completa\nLlevar un modelo a producción requiere trabajo adicional de desarrollo e integración de software.\nLa mayoría de las veces, los modelos ML están encapsulados en API que son fáciles de integrar con cualquier aplicación. El desarrollo de la aplicación que operacionalizará el modelo ML y lo hará parte del proceso de toma de decisiones puede ser más difícil que construir el modelo. El desarrollo de aplicaciones puede requerir la integración a los sistemas empresariales existentes, lo que requiere trabajar con desarrolladores externos.\nLos problemas de escalabilidad y seguridad de los datos también deben abordarse como parte de la puesta en funcionamiento del modelo.\n¿Está interesado en consultar o capacitarse en Aprendizaje automático? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información en Machine Learning\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"3b805cc510b51df9bbc716e10a07b5af","permalink":"https://www.marcusrb.com/consultoria-freelance-machine-learning/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/consultoria-freelance-machine-learning/","section":"servicios","summary":"Servicio que ofrece creación de algoritmos de aprendizaje automático.","tags":null,"title":"Consultor freelance Machine Learning \u0026 AI","type":"page"},{"authors":null,"categories":null,"content":" Servicios freelance de Google Data Studio Google Data Studio es una herramienta gratuita de visualización e informes de datos basada en la nube que se conecta a muchas fuentes de datos diferentes y convierte esos datos en paneles informativos e informes que son fáciles de entender y compartir, y son totalmente personalizables.\nVea algunos de nuestros paneles en vivo, por ejemplo, informes de paneles de datos de Google Data Studio accionables\nCaracterísticas clave Google Data Studio es intuitivo, rápido, flexible y permite una gran cantidad de opciones de diseño y presentación.\nAmplia gama de conectores de datos. Data Studio tiene 17 conectores de datos internos y alrededor de 108 de terceros para elegir\nFunciones fáciles de usar Data Studio proporciona docenas de funciones matemáticas, de cadena, de fecha y otras para transformar sus datos en valores más útiles.\nVariedad de formas, imágenes y texto. Data Studio le permite agregar formas, imágenes y texto a sus informes y paneles para que sean más fáciles de leer.\nNiveles de permisos Aprovechando la tecnología Google Drive, puede administrar fácilmente a todos sus usuarios y su nivel de acceso\nMezcla de datos, ahora una realidad Data Studio le permite agregar datos de múltiples fuentes para tener una vista comparativa de ellos a la vez\n¿Está interesado en servicios o capacitarse en visualización en Google Data Studio ? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para la consultoría freelance en Data Studio\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"90500802117351ea8576c016559c6dc4","permalink":"https://www.marcusrb.com/servicios-freelance-google-data-studio/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/servicios-freelance-google-data-studio/","section":"servicios","summary":"Servicio de consultoría, implementación y mantenimiento cuadros de mando, dashboard y analítica avanzada con Google Data Studio | BigQuery","tags":null,"title":"Consultoría freelance reportes Google Data Studio","type":"page"},{"authors":null,"categories":null,"content":" ¿Qué es Microsoft Power BI? Ha pasado algún tiempo desde que Microsoft lanzó Power BI y la forma en que las cosas habían progresado para esta herramienta excepcional de Business Intelligence and Analytics, parecía que solo sería cuestión de tiempo antes de que se convirtiera en la plataforma preferida para BI y análisis con el mayoría de las empresas con visión de futuro. Power BI es una herramienta poderosa en manos de las empresas que desean extraer y convertir datos de múltiples fuentes dispares para obtener información significativa. Ofrece una experiencia de usuario sin precedentes con oportunidades de visualización interactiva junto con verdaderas capacidades de análisis de autoservicio. Todo esto ayuda a ver los mismos datos desde una variedad de ángulos, sin mencionar que los informes y paneles pueden ser creados por cualquier persona en la organización sin la ayuda de los administradores de TI. Algunos de los beneficios únicos de Power BI son:\n* Potentes gráficos y visualizaciones del tablero que se actualizan continuamente. * Función de análisis en memoria y base de datos en columna que admite datos tabulares. * Lo mejor de ambos mundos cuando se trata de facilidad de uso y rendimiento en una sola herramienta de BI. * Geo-mapping interactivo con Bing Maps. * Secuencias de comandos de expresiones de análisis de datos (DAX) para crear medidas y columnas.  Vea algunos de nuestros paneles en vivo, por ejemplo, informes de panel de control de Power BI procesables\n¿Por qué necesita Power BI? Permita que sus empleados y tomadores de decisiones analicen los datos más rápido con una mejor eficiencia y comprensión utilizando el servicio de análisis basado en la nube Power BI proporciona a los usuarios una amplia gama de información a través de paneles simplificados pero efectivos, informes precisos y visualizaciones atractivas, convirtiéndose así en una herramienta perfecta para dar vida a los datos.\nPower BI se conecta a cualquier fuente de datos y ofrece información empresarial convincente a un costo muy bajo, brindando análisis en tiempo real utilizando un tablero efectivo en varios dispositivos como computadoras de escritorio, dispositivos móviles, tabletas, etc. Microsoft Power BI está diseñado de una manera que no requiere finalización que los usuarios tengan habilidades de programación para explorar, analizar y procesar los datos para tomar decisiones comerciales mejor informadas.\n¿Está interesado en consultoría o capacitarse en visualización en Power BI ? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para la consultoría en Power BI\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"e2435bfae9e5dfe4745f17328e2c8bb4","permalink":"https://www.marcusrb.com/servicios-freelance-analisis-dax-power-bi/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/servicios-freelance-analisis-dax-power-bi/","section":"servicios","summary":"Consultor de inteligencia de negocio, implementación y optimización de DAX, Power Query y modelo de datos en Power BI | SQL Server | Azure","tags":null,"title":"Consultoría optimización y análisis en Microsoft Power BI","type":"page"},{"authors":null,"categories":null,"content":" Consultor freelance en Data Analytics \u0026amp; Business Intelligence Es un concepto de suma total que incluye el análisis de datos, para las empresas y decirle que estos son los puntos absolutos que faltan en el crecimiento de su empresa y con la ayuda de estos podemos registrar un enorme crecimiento, le proporcionaré un resumen general de sus datos y generaremos un número anónimo de informes que serán un aspecto clave en el crecimiento de nuestros clientes.\n¿Está interesado en servicios freelance o capacitarse en Analítica de datos e Inteligencia de negocio? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información en Data analytics, Business Intelligence\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"8910630d96d2bc6d2c4d33a66cf2d394","permalink":"https://www.marcusrb.com/servicio-freelance-data-analytics-business-intelligence/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/servicio-freelance-data-analytics-business-intelligence/","section":"servicios","summary":"Los servicios de análisis de datos ayudan a capturar esta esencia de Big Data Analytics y administración para ayudarle a comprender nuevas oportunidades, amenazas ocultas, sus competidores, etc.","tags":null,"title":"Freelance Data Analytics y Business Intelligence","type":"page"},{"authors":null,"categories":null,"content":" Servicio de consultoría freelance en Tableau Creemos en el futuro del análisis visual del mundo empresarial. Si aterrizaste aquí, probablemente estés en algún lugar del mismo camino.\nPara ayudarlo en su viaje, ofrecemos ayuda y consultoría de Tableau.\nSomos un socio de Tableau que trabajamos para ayudar a las personas a aprovechar al máximo el software y responder las preguntas correctas con datos.\n¿Por qué necesita consultar Tableau? ¡Aceptar asistencia no es una debilidad! Realmente creemos que un buen líder sabe cuándo pedir ayuda. Probablemente todos hemos aprendido que en algún momento podríamos usar algo de soporte.\n¡La velocidad es vital en el análisis de autoservicio! Permítanme repetirlo: ¡la velocidad es el juego!\nSi tiene la oportunidad de encontrar un consultor de Tableau que obtenga sus puntos débiles, es posible que pueda acelerar su progreso de una manera que no creía posible.\nEntonces, ¿cómo podemos ayudar? Un enfoque creativo para la resolución de problemas. Ayuda diligente de Tableau, comunicación clara, hacer las cosas de la manera correcta.\n¡Simple como eso!\nUna pasión por la visualización de datos Estamos trabajando con Tableau todos los días. ¡Alimentamos incansablemente a la bestia que es nuestro apetito por responder preguntas con datos! Nuestra experiencia profesional en análisis web nos brinda una vanguardia. Sobre todo entendemos la importancia de la corrección y la calidad de los datos.\nCreemos que cuando te apasiona algo, el trabajo se vuelve mucho más fácil. Para nosotros, ¡ni siquiera parece trabajo! El amor por trabajar con datos es lo que nos impulsa. Tenacidad, dedicación y creatividad son lo que obtendrá de nosotros.\nBusiness Analytics vs. Business Intelligence Creemos que puede beneficiarse de nuestro trabajo en ambas áreas. Al combinar la eficiencia (BI) con la creatividad (BA), estamos tratando de ayudar a las empresas a responder preguntas urgentes y formular otras nuevas.\nHemos estado utilizando un amplio espectro de fuentes de datos: desde Big Data como Exasol o Vertica hasta bases de datos clásicas como MySQL o Postgres, y otras menos tradicionales como Google Analytics y conectores de datos web.\nDiseño e implementación del tablero de instrumentos Para nosotros, un Tableau Dashboard es solo la punta del iceberg. La regla de pasar el 80% del tiempo trabajando con los datos y el 20% en la visualización en sí se aplica a nuestro caso.\nNo solo eso, sino que el principio de Pareto también es relevante para la cantidad de hojas que creamos versus cuántas guardamos en la versión final del tablero.\n¡La velocidad de probar diferentes enfoques en Tableau es fantástica! Sería una pena si no lo usáramos.\nPor lo general, comenzamos construyendo muchos gráficos diferentes (a veces cientos), y conservamos solo los que consideramos relevantes para la historia.\nPuede encontrar algunos ejemplos de visualizaciones de datos que hemos creado al final de esta página. Y algunos paneles más creativos en el perfil de Tableau Public de Dorian (nuestro jefe de Tableau).\nOptimización del rendimiento de Tableau Tenemos un estudio de caso sobre el ajuste del rendimiento de Tableau, que debe leer si tiene ~ 15 minutos disponibles.\nLa idea principal es que no nos centremos únicamente en la rapidez con la que podemos hacer un tablero, sino también en lo rápido que puede responder a sus preguntas.\nAdemás, trabajar con grandes cantidades de datos es complicado. Sin embargo, estamos aquí para romper esos muros.\nLicencias y soporte ¿Desea tomar la decisión correcta con respecto a la configuración de Tableau dentro de su organización?\nPodemos ayudarlo a descubrir cuál es la combinación correcta de:\n Productos de Tableau: escritorio, preparación, servidor, en línea Roles de usuario: (Creador, Explorador, Visor) Licencias: licencias básicas basadas en el usuario, análisis integrados, gestión de datos  También estamos disponibles para ofrecer soporte técnico para su servidor si lo necesita.\nUno de nuestros objetivos es ayudar a los clientes a dormir mejor. Esta parte cae directamente en esa área.\nExperiencia complementaria con R y Python Tecnologías como R y Python nos permiten trabajar con los datos en el nivel más cercano.\nPor nombrar algunas cosas para las que hemos utilizado modelos estadísticos creados con R junto con Tableau para duplicar el análisis:\n Modelos de atribución de Markov: atribución de canal de marketing más inteligente que el promedio para tiendas de comercio electrónico Pronosticar usando algoritmos que son un poco más avanzados (por ejemplo, el Profeta de Facebook) que los básicos (como el que usa Tableau) Cálculos bayesianos: los desarrollamos para cortar y cortar los resultados de las pruebas A / B dentro de Tableau Comprobaciones de impacto causal: estamos trabajando con este algoritmo para evaluar si los crecimientos o las caídas son el resultado de algo que hicimos o simplemente ruido aleatorio en los datos Flujos de trabajo de Business Intelligence: programación de informes usando Python junto con tabcmd (la interfaz de línea de comando para Tableau Server)  ¿Está interesado en consultoría o capacitarse en visualización en Tableau ? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para la consultoría en Tableau\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"af1ceaeed906ae70b1c9fca21d5e65b9","permalink":"https://www.marcusrb.com/servicios-freelance-tableau/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/servicios-freelance-tableau/","section":"servicios","summary":"Servicio de consultoría, implementación y mantenimiento cuadros de mando, dashboard y analítica avanzada con Tableau Desktop, Tableau Prep.","tags":null,"title":"Freelance en gestión y mantenimiento dashboard en Tableau","type":"page"},{"authors":["marcusRB"],"categories":["Tag Manager"],"content":" Cuando hablamos de Google Tag Manager pensamos en la gestión organizada de etiquetas, pixel de conversiones, mediciones de eventos para Google Analytics, etc., pero casi nunca nos paramos a pensar como está creado, y cuales son sus secretos más allá de ser el core de GTM.\n# Aquí está parte de sus secretos, la declaración inicial en su script del \u0026lt;head\u0026gt; \u0026lt;script\u0026gt; ... dl=l!='dataLayer'?'\u0026amp;l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-XXXX'); \u0026lt;/script\u0026gt;  Bien, después de muchos años que he estado utilizando esta fantástica herramienta de gestión de etiquetas, hemos llegado a poder \u0026ldquo;dominarlo\u0026rdquo; en parte, existen muchos tutoriales, pero cuando se complican las cosas, siempre necesitaremos gran parte del trabajo de un desarrollador front / back-end, con lo cuál, tendremos que considerar: - JavaScript - jQuery - CSS - HTML - analítica digital - CRO y testing AB - fundamentos de desarrollo de POO (Programación Orientado a Objetos) - etc.\npero hay otra parte que también un analista experimentado podrá realizar directamente en la interfaz de GTM, jugar con el dataLayer, y sacar todos su potencial.\nQué es el dataLayer? Si queremos buscar una definición, yo me quedaría con la mía:\n la variable de dataLayer es uno script en lenguaje JSON declarado dentro del contenedor principal de Google Tag Manager que nos ayuda a comunicar directamente desde el sitio web / aplicación móvil hacía la interfaz o viceversa, con la herramienta de análisis o medición. El formato JSON está compuesto de un objeto o listado de objetos, de par parámetro y valor.\n dataLayer({ \u0026quot;nombre_variable_x\u0026quot; : \u0026quot;cadena texto\u0026quot;, \u0026quot;numérico_y\u0026quot;: 50, \u0026quot;booleano\u0026quot;: true, \u0026quot;listado\u0026quot;: [{ \u0026quot;obj1\u0026quot;: 5, \u0026quot;obj2\u0026quot;: \u0026quot;hola\u0026quot;, \u0026quot;obj3\u0026quot;: false }] });  Prácticamente almacena la información, sea un evento de un botón, de un formulario, acción etc, y para su uso podemos o bien llamar la variable en GTM, o en caso específico de Google Analytics, enviarlas directamente si está en el formato adecuado, como el comercio electrónico mejorado.\nEste es el ejemplo del comercio electrónico mejorado de Google Analytics:\n# evento de click en productos dataLayer.push({ \u0026quot;event\u0026quot;: \u0026quot;productClick\u0026quot;, \u0026quot;ecommerce\u0026quot;: { \u0026quot;click\u0026quot;: { \u0026quot;actionField\u0026quot;: { \u0026quot;list\u0026quot;: \u0026quot;homepage\u0026quot; }, \u0026quot;products\u0026quot;: [{ \u0026quot;id\u0026quot;: \u0026quot;b55da\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Flexigen T-Shirt\u0026quot;, \u0026quot;price\u0026quot;: \u0026quot;16.00\u0026quot;, \u0026quot;brand\u0026quot;: \u0026quot;Flexigen\u0026quot;, \u0026quot;category\u0026quot;: \u0026quot;T-Shirts\u0026quot;, \u0026quot;position\u0026quot;: \u0026quot;3\u0026quot; }] } } });  Si has llegado hasta aquí, pero no conoces mucho de GTM, entonces te recomiendo esta pequeña recopilación que he redactado, son pequeños tutoriales y fundamentos de Google Tag Manager:\n tag manager  Cómo puedo crear mi propio dataLayer? La pregunta no es sencilla. Primero has de tener el acceso a tus archivos o servidor para poder implementar el dataLayer personalizado. Es muy importante que el dataLayer esté por encima del script principal de GTM.\n# Así debería ser tu estructura con el dataLayer personalizado \u0026lt;HTML\u0026gt; \u0026lt;HEAD\u0026gt; ... \u0026lt;script\u0026gt; var dataLayer = window.dataLayer || []; dataLayer.push({ 'event' : 'productData', 'name': '', 'brand': '', 'category':'{$category}', 'id': '', 'sku': '', 'stock':'', 'amountPrice' : '', 'regularPrice' : '', 'quantity' : '1' }); \u0026lt;/script\u0026gt; \u0026lt;!-- Google Tag Manager --\u0026gt; \u0026lt;script\u0026gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'\u0026amp;l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-XXXXXX');\u0026lt;/script\u0026gt; \u0026lt;!-- End Google Tag Manager --\u0026gt; ... \u0026lt;/HEAD\u0026gt; \u0026lt;BODY\u0026gt; \u0026lt;/BODY\u0026gt;  Si te fijas, viene primero la declaración de la variable dataLayer, sucesivamente hay un estado push donde desde el sitio web estamos enviando la información a quién quiera disponerla. Sucesivamente hay el script principal de GTM.\nCómo puedo llamar las variables del dataLayer Esta parte ya es más práctica. Si conocemos nuestra interfaz, ya sabemos que su composición es: - etiquetas - activadores - variables\nEntonces para poder crear la variable de capa de datos tenemos que seguir estos pasos:\ngraph TD; variables-.-\u0026gt;var_definida_usuario; var_definida_usuario-.-\u0026gt;nueva; nueva-.-\u0026gt;VAR_CAPA_DE_DATOS;  Aquí ya tenemos nuestra libertad de llamar nuestra variable del dataLayer, por ejemplo si introducimos:\ncategory\nnos dará la información de la categoría, siempre y cuando esta será populada con un valor, en caso contrario tendríamos: undefined\nIMAGEN DATALAYER category\nPuedo crear mi propio dataLayer desde la interfaz? Sí y no. Podemos siempre manipular la información del dataLayer principal, o hasta incluso crear nuestro dataLayer personalizado desde la interfaz, y para ello, tenemos que tener cuidado de no hacer crear ni bucles, ni errores de servidor, 5xx o de página 4xx.\nTenemos solo que ir en:\ngraph LR; etiquetas-.-\u0026gt;nueva; nueva-.-\u0026gt;HTML_personalizado;  y introducimos este pequeño script que a continuación es declaración del dataLayer:\n\u0026lt;script\u0026gt; var dataLayer = window.dataLayer || []; dataLayer.push({ 'event': 'fireDataLayer', }); \u0026lt;/script\u0026gt;  Si pongamos como ACTIVADOR una página vista cualquiera, tendremos la ejecucción del evento fireDataLayer que a su vez podemos incluir más informaciones y más variables.\nEjemplo Google Remarketing Tag Un ejemplo podría ser un pixel de conversión a medida o por ejemplo personalizar la etiqueta de remarketing de Google Ads. Este es solo un ejemplo que a la vez necesitaremos de más variables y más detalles.\n# Exactamente la continuación del ejemplo de arriba \u0026lt;script\u0026gt; var dataLayer = window.dataLayer || []; dataLayer.push({ 'event': 'fireRemarketingTag', 'google_tag_params': { 'ecomm_prodid': '{{dlv - productId}}', 'ecomm_pagetype' : '{{dlv - productPageType}}', 'ecomm_totalvalue' : {{dlv - productPrice}} } }); \u0026lt;/script\u0026gt;  Te has fijado en las tres variables dlv ? Buenos podemos también crear funciones CUSTOM JS como variables y recoger las informaciones y pasarla al dataLAyer, y estas pasarlas a Google Ads.\nEsté atento al próximo tutorial\n","date":1565283174,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"9cce3d4100f9f66fbcc17fe1574fa13a","permalink":"https://www.marcusrb.com/explotar-maximo-datalayer-google-tag-manager-parte-1/","publishdate":"2019-08-08T17:52:54+01:00","relpermalink":"/explotar-maximo-datalayer-google-tag-manager-parte-1/","section":"post","summary":"Hablaremos del core de google tag manager, el dataLayer o variable de capa de datos más importante para declarar las variables y/o escalar nuestras bases de esta herramienta de medición.","tags":["formacion big data","escuelas negocio","big data"],"title":"Cómo explotar al máximo el dataLayer de Google Tag Manager: la declaración","type":"post"},{"authors":["marcusRB"],"categories":["Google Analytics"],"content":"Despúes de crear la entrada de las respuestas de la certificación oficial de Google AdWords fundamentales y avanzado, no me imaginaba que iba a tener tantas visitas y repercusión mediática, y esto fue desde el año 2017 cuando grabé varios vídeos de las certificaciones en mi canal de YouTube. Es por este motivo que aquí os facilitaré algunas de las preguntas del examen de la certificación oficial de Google Analytics.\nNo va ni a favorecer mi blog, ni mi web ni nada, ya honestamente no vendo los exámenes, ni las hago para los demás a cambio de dinero, simplemente es una demostración que Google necesita comerciales y cuanto más somos mejor\u0026#8230;era broma. A final si las empresas piden unas certificaciones oficiales, creo que más que un TITULIS, mejor tener experiencia. Gran palabra poco valorada, creo yo. En fin, yo durante muchas entrevistas (personales y para mi agencia), me he encontrado de todo, y la última pregunta y no discriminatoria era sobre la certificación, y aconsejo que sacan este examen o varios exámenes para el simple hecho de estudiar PDF de más de 200 páginas, ver tutoriales, consumir buenas prácticas, ejemplos, consultar el foro de ayuda en inglés o español, y obviamente, en el caso de Analytics, vincular la cuenta DEMO del MerchandiseStore en una cuenta creada para ir tocando y probando la analítica web, en este caso para un ecommerce, de una manera GRATUITA.\nEsta \u0026#8220;gratuitidad\u0026#8221; obviamente tiene un precio, y esto se llama EXPERIENCIA.\nDe momento pueden tener acceso a estas preguntas\u0026#8230;y para las respuestas puedes seguir uno de mis vídeos a esta lista y subscribirte.\nGracias, y Good Luck!\n\u0026nbsp;\n Listado de preguntas examen Google Analytics \u0026#8211; parte 1 Listado de preguntas examen Google Analytics \u0026#8211; parte 2 Listado de preguntas examen Google Analytics \u0026#8211; parte 3 Listado de preguntas examen Google Analytics \u0026#8211; parte 4  ","date":1564675284,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"de8b2c8d2ca8d06d2c97144978bc57d1","permalink":"https://www.marcusrb.com/buscas-las-respuestas-del-examen-de-google-analytics/","publishdate":"2019-08-01T16:01:24Z","relpermalink":"/buscas-las-respuestas-del-examen-de-google-analytics/","section":"post","summary":"Despúes de crear la entrada de las respuestas de la certificación oficial de Google AdWords fundamentales y avanzado, no me imaginaba que tantas visitas iba a tener, y esto fue desde el año pasado cuando grabé varios vídeos de las certificaciones","tags":["certificación analytics 2019","examen google academy","preguntas respuestas analytics"],"title":"¿Buscas las respuestas del examen de Google Analytics?","type":"post"},{"authors":["marcusRB"],"categories":["Formación"],"content":"Han pasado casi 10 años cuando el término big data se puso tan popular en España, que no solo el área de tecnologías, sino en todos los sectores y principalmente hubo un auge en banca y finanza, campo médico, marketing y operaciones, logística llegando al sector educativo.\nCuando hablamos de big data nos referimos principalmente a ingestar grandes cantidades de datos, y por grande nos referimos no a gigas, sino a teras o petabytes de informaciones, llegando a que la cantidad almacenada, principalmente en servidores cloud cuáles Amazon AWS, Google GCP y Microsoft Azure, las empresas se encontraron con un gran obstáculo, como poder explotarlos, quiénes son los roles y profesionales capaces de leer, limpiar, preparar y realizar gestiones con estos datos.\nSi por Big Data nos referimos al tipo de dato, referente a los roles tenemos a 3 principales, junto con otros perfiles IT (cuáles podrían ser de seguridad, gobierno del dato, desarrolladores):\n Data Engineer: el arquitecto / ingeniero de los datos, este profesional es responsable de preparar la infraestructura, plataforma e ingestar los datos, verificar su integridad, seguridad y además, en la forma más rápida posible.\n Data Analyst / Business Analyst: son los profesionales que gracias a sus técnicas de minería de datos, dan una visión general del negocio, generarán un primer insight con los datos, hasta incluso dan muchas de las respuestas que las demás figuras internas de negocio no obtienen siempre. Hay que decir que luchan constantemente con la preparación previa, transformarlos y sucesivamente integrarlos en tools de visualización para la creación de cuadro de mandos y dashboard listo para ser publicados en los comité directivos.\n Data Scientist: la figura más buscada en los últimos años, este rol tiene un perfil más matemático y estadístico de una organización. El realizará “magia” con los datos, creará nuevos algoritmos, aplicará modelos de predicciones y de aprendizaje automático. En línea con los stakeholders tomará las decisiones necesarias a los objetivos empresariales designados a principio.\n  Estas figuras realmente siempre han existido, y desde hace más de 50 años. Pero mientras las nuevas tecnologías han ido evolucionando con el tiempo, se presentaron nuevas herramientas y nuevos desafíos que gracias a la aplicación de estadística y matemática, combinando los algoritmos y álgebra, estos roles han cobrado vida nuevamente, con otro nombre y más fuerte que nunca. Se definen como los empleados más sexy del siglo XXI.\nA ellos se suman las nuevas figuras de Business Intelligence, o aquellos con una visión más comercial y de negocio, otras que provienen principalmente de Telecos e IT, se han ido convirtiendo en estos nuevos roles demandados para las empresas. Se estiman que para 2020 la demanda de estas posiciones que sepan interpretar y crear nuevos modelos, que dan una vida propia a los datos, supera el millón en todo el mundo.\nEn España, por ejemplo, el crecimiento del término “master big data” y “master data science”, ha crecido en los últimos 5 años en un 250% (fuente Google Trends). Esto porque las ofertas de empleos han ido creciendo con un ritmo muy alto, y solo en los últimos 30 días a fecha de este post, a un ritmo de 1200 ofertas (fuente Indeed).\nEs por este motivo que la oferta educativa también juega un papel muy importante. La formación profesional, carreras y grados, máster y posgrados en universidades y centros privados, han crecido y mejorado en los últimos meses, y adaptadas a lo que realmente las empresas están demandando. Gracias a una formación en big data, los profesionales pueden desarrollar las habilidades solicitadas cubriendo todas las necesidades que el mismo mercado está requiriendo, desde una visión global de negocio, a las técnicas de análisis de datos avanzada, creando nuevos escenarios y ser capaces de transmitir estos resultados.\nEl mercado laboral ha crecido mucho gracias a estas nuevas posiciones, y la formación es una pieza muy importante para que siga generando empleo y pueda cubrir estas demandas. Incluso las empresas pueden aprovechar de estos centros educativos convirtiendo su papel en promotores, así participando para el crecimiento de los futuros profesionales y asegurando la calidad necesaria para que puedan estar a la altura del mercado laboral y siempre actualizados a las nuevas tecnologías.\n","date":1563295974,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"61aa071f148f83e4cb553c95f604844a","permalink":"https://www.marcusrb.com/importancia-big-data-empresas/","publishdate":"2019-07-16T17:52:54+01:00","relpermalink":"/importancia-big-data-empresas/","section":"post","summary":"Han pasado casi 10 años cuando el término big data se puso tan popular en España, que no solo el área de tecnologías, sino en todos los sectores y principalmente hubo un auge en banca y finanza, campo médico, marketing y operaciones, logística llegando al sector educativo.\nCuando hablamos de big data nos referimos principalmente a ingestar grandes cantidades de datos, y por grande nos referimos no a gigas, sino a teras o petabytes de informaciones, llegando a que la cantidad almacenada, principalmente en servidores cloud cuáles Amazon AWS, Google GCP y Microsoft Azure, las empresas se encontraron con un gran obstáculo, como poder explotarlos, quiénes son los roles y profesionales capaces de leer, limpiar, preparar y realizar gestiones con estos datos.","tags":["formacion big data","escuelas negocio","big data"],"title":"La importancia del big data en las empresas","type":"post"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Destripando Google Tag Manager Nivel 1.\n30 octubre 2018\nDestripando Google Tag Manager Después del primer Meetup sobre los fundamentos de Google Tag Manager, que está disponible en nuestro canal de YouTube, en este webinar vamos a realizar todo tipo de prácticas que nos permitirán conectar la interfaz más utilizada con el ecosistema de Google: Analytics, Marketing Platform como también Facebook Ads y herramientas de CRO. Pasaremos sucesivamente a las configuraciones avanzadas que nos permitirán subir de nivel y poder disponer de las métricas esenciales sin miedo de perder datos a la hora de analizar tu negocio.\n¿Quién es el ponente? Marco Russo\nConsultor y Especialista en Data \u0026amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.\nVideo   ","date":1561485600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"2d6a7fe39685a5303c3dc500f026fbb8","permalink":"https://www.marcusrb.com/talk/google_tag_manager_2/","publishdate":"2019-06-25T20:00:00Z","relpermalink":"/talk/google_tag_manager_2/","section":"talk","summary":"[Meetup] Escalando de nivel con Google Tag Manager","tags":["analytics","googletagmanager"],"title":"Meetup - Escalando de nivel con Google Tag Manager","type":"talk"},{"authors":["marcusRB"],"categories":["Tag Manager","Webinar"],"content":"En este primer webinar de nuestro grupo HTML5 Spain vamos a realizar todo tipo de prácticas que nos permitan conectar la interfaz más utilizada del ecosistema de Google: Analytics, Marketing Platform como también Facebook Ads y herramientas de CRO.\nEl contenido será eminentemente práctico y podrá seguirse online por streaming. Además podréis interactuar con el ponente y plantearle vuestras dudas o comentarios via chat.\nAquí el extracto del video, no dude de comentar y compartir!\n  ","date":1561481574,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"66ac2444c00dfd74a7a7c76d75dfd36a","permalink":"https://www.marcusrb.com/webinar-escalando-nivel-google-tag-manager/","publishdate":"2019-06-25T17:52:54+01:00","relpermalink":"/webinar-escalando-nivel-google-tag-manager/","section":"post","summary":"En este primer webinar de nuestro grupo HTML5 Spain vamos a realizar todo tipo de prácticas que nos permitan conectar la interfaz más utilizada del ecosistema de Google: Analytics, Marketing Platform como también Facebook Ads y herramientas de CRO.\nEl contenido será eminentemente práctico y podrá seguirse online por streaming. Además podréis interactuar con el ponente y plantearle vuestras dudas o comentarios via chat.\nAquí el extracto del video, no dude de comentar y compartir!","tags":["google tag manager","facebook ads","script"],"title":"[Webinar] - Escalando nivel con Google Tag Manager.","type":"post"},{"authors":["marcusRB"],"categories":["Analítica Datos"],"content":" Uno de los retos de un analista digital es pasar por diferentes procesos de limpieza, exploración y análisis de contenido para sacar buenas conclusiones y detectar nuevos escenarios e insights. Además, tiene que comprobar si las fuentes de datos son de dudosa calidad o no.\nY si hablamos de un analista digital, es muy probable que toda esa exploración pase por la herramienta más utilizada en el mundo de la analítica: Google Analytics. Pero seguro que también estará conectado a otras fuentes como Google Sheet, un CRM o base de datos, u otros ficheros de texto, en formato csv, y más.\nHoy en día, un analista digital no solo tendrá que tener conocimientos solo de usabilidad de la web, también de negocios, estadística, programación, base de datos, visualización de datos, además estar actualizado con las nuevas tecnologías.\nEntre las herramientas que normalmente podemos incluir en nuestro \u0026ldquo;maletín\u0026rdquo; de explorador de los datos, sería la programación. Y algunos de nuestros aliados deben ser el lenguaje R o Python, siendo \u0026ldquo;sencillos\u0026rdquo; en su aprendizaje y muy útiles a la hora de ejecutar pequeños script, ya que tienen muchas librerías open-source.\nVeamos qué ofrecen cada uno de ellos y cómo pueden ayudarnos al análisis y visualización de datos.\n¿Por qué R? R es un lenguaje generalista, con diversas librerías de análisis estadístico bastante potentes que pueden suplir el campo de aplicación R, cosa que no sucede con otros lenguajes como Python, por ejemplo.\nR está pensado para explotar su potencial que es la \u0026ldquo;estadística\u0026rdquo;. Este fantástico lenguaje nos permite una primera toma de contacto con los datos debido a su flexibilidad por la exploración, limpieza y análisis a diferentes fuentes de datos, así como aplicar modelos y algoritmos predictivos puede ser de gran ayuda en el mundo de la análisis de datos.\nIntro de R Studio R Studio es un entorno gráfico para el lenguaje de programación R que facilita la creación y ejecución de scripts. También simplifica la instalación de los paquetes necesarios para la ejecución de aquellos scripts que los requieran.\nR Studio utiliza una partición de la pantalla en diferentes secciones, de forma que todos los elementos necesarios se encuentran disponibles a un solo clic, incluidos el código fuente, los datos cargados y generados por dicho código, los resultados obtenidos, los gráficos generados, etc.\nTambién facilita la integración con otros sistemas para la creación de informes en diferentes formatos (principalmente HTML o PDF).\nLibrería de Google Analytics en R Entre las diferentes librerías en R para poder conectar y explorar los datos desde Google Analytics, hay dos en particular de lo que hablaré hoy.\nAmbos están en el repositorio oficial del CRAN (googleAuthR y googleAnalyticsR). Su función es que se necesitan en una el token Google Analytics, mientras el otro habilitar Google Cloud y su API, con lo cual necesitaríamos tener activados:\n Google Cloud, habilitar el proyecto. Permiso de edición de Google Analytics.  Aspectos a considerar de Google Analytics\nDurante la fase de exploración, consultaremos dimensiones y métricas de Google Analytics. SI no estás familiarizado con estas dos partes más importantes de analítica web, mi consejo es que consultes la guía oficial para conocer las más representativas.\nSi, por el contrario, ya conoces la interfaz de Analytics y quieres ir más allá, puedes consultar la tool externa de exploración de estos datos a través de la otra API de Google Analytics, Query Explorer,** **y la extensión o complemento para Google Sheet que permite tener datos directamente en una hoja de cálculo.\nFase de Instalación y Autorización de GA\nEn esta fase cargaremos los paquetes necesarios, previamente necesitaremos una cuenta de Google Cloud (que nos vendrá bien también si en un futuro queremos utilizar Big Query).\nEs importante tener una cuenta de Google Analytics que no sea ni demo, ni solo de lectura, ya que podría tener problemas con los permisos.\n Cargamos las librerías y configurar los valores opcionales:\ninstall.packages(\u0026ldquo;googleAuthR\u0026rdquo;) install.packages(\u0026ldquo;googleAnalyticsR\u0026rdquo;) library(googleAnalyticsR) library(RGoogleAnalytics) library(ggplot2) # para representar gráficamente los datos library(forecast) # para las predicciones seriales library(\u0026ldquo;tidyverse\u0026rdquo;)\n Autorización GA con Google Cloud y ejecutamos\nAutorizamos a través del token con nuestro account Google ga_auth()\n  Comenzamos con la primera query de Google Analytics in R\nVeamos el listado de los account de GA y la guardamos en una nueva variable:\naccount_list Lo que estamos realizando aquí es simplemente a través del token generado por Google Analytics para habilitar desde nuestro account de Google Analytics el listado de Cuentas, Propiedades y Vistas para poder así trabajar con una cuenta específica. ## EDA (Exploratory Data Analysis) El trabajo de exploración del dataset en R requiere de varios pasos para comprobar si existen valores nulos o vacíos, discrepancias que podemos arreglar, sustituir o eliminar. A este proceso se le llama EDA. # Create a list of the parameters to be used in the Google Analytics query # Get the Sessions by Month in 2018 gadata Podéis ver el código [en este repositorio de GitHub][7]. Con nuestra pequeña query, que guardaremos en una nueva variable **gadata**, almacenaremos las sesiones en un periodo de un año, por ejemplo. Paralelamente podemos observar si existen sesiones igual a cero. Comprobado que efectivamente no existen valores nulos ni ceros, procederemos a una representación gráfica con el librería **ggplot** del paquete instalado _ggplot2._   Representación gráfica de la dimensión date y métrica sessions\ngadata %\u0026gt;% ggplot(aes(x=date, y=sessions)) + geom_point()\n   Y si queremos visualizar cada valor por tamaño según la sesión, entonces tendríamos este:\ngadata %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(x=date, y=sessions, size = sessions)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  Y, si además le añadimos gradación por color de más oscuro a más claro, según el tamaño de las sesiones, obtendremos esto:\ngadata %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(x=date, y=sessions, size = sessions, color = sessions)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  Siguiendo la misma lógica, podemos añadir otras métricas de tráfico importantes (duración media, usuarios, páginas vistas, transacciones, eventos, etc), así que para tener una idea de la evolución o tendencia por periodo, podemos representar el gráfico de líneas:\ngadata %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(x=date,y=sessions,group=1)) + geom_line() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) # some styles to rotate x-axis labels  Ahora sí que se nota el pico máximo entre octubre y diciembre, nos hace pensar que este tráfico puede deberse al periodo entre el pre-Black Friday y durante Navidad (si es un retail tiene lógica). Y si queremos representar la tendencia, añadimos la línea de tendencia para que vayamos viendo la evolución del tráfico a lo largo del periodo observado:\ngadata %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(x = date, y = sessions) ) + geom_point() + geom_smooth() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  Con los primeros datos podemos observar que ha habido un periodo estable, con acciones puntuales (serán promociones) y el pico hasta navidad.\nAhora nos interesa conocer algo más de nuestros usuarios, segmentando así el tráfico según periodos más cortos.\nPasamos a detectar si existen diferencia durante los días de la semana y hora del día. Creamos una nuevo dataset con las métricas sesiones y duración media de sesión, por día de la semana.\n# Añadimos la dimensión día de la semana y fecha - solo 1er semestre gadata_2 ## Segmentos de sesiones por día de la semana, franja horaria y categorías de dispositivos ![][13] Podemos observar que los boxplot indicados son muy relevantes, aunque necesitamos profundizar más sobre temas de cuartiles, min y max, media y mediana. Los puntos son nuestro \u0026quot;amigos\u0026quot; **outliers, **así que en casos puntuales estos tendrán que ser excluidos en algunos modelos y análisis. Lunes, martes y miércoles a primera vista tienen el mismo impacto, al igual que viernes y sábado. ## Duración media de sesión por día de la semana ![][14] Los datos en la segunda métrica son expresados en segundos, y los días de la semana están según el formato anglosajón (0 = Domingo , 6 = Sábado). Nos interesa ahora conocer la duración media de sesión por hora del día y el día de la semana. Una representación gráfica podría ser una matriz, con dimensiones día de la semana y hora, veamos un periodo de tiempo más corto, por ejemplo 6 meses.   Cargamos las librería correspondientes y guardamos en una nueva variable el dataframe:\nlibrary(\u0026ldquo;RColorBrewer\u0026rdquo;)\ngadata_3\nPodría ejecutarlo para verlo, pero para una correcta lectura de los días de la semana, sustituyamos los números por nombres, ordenando así los días de la semana:\ngadata_3$dayOfWeekName\nCon esta matriz se puede observar que las franjas horarias de 8:00 a 12:00 de lunes a viernes tienen mayor impacto por duración promedio, aunque se observan picos desde las 21:00 a las 00:00 los martes, miércoles y jueves.\nSábado es el día más tranquilo comenzando desde el viernes por la tarde, siguiendo hasta el domingo donde se observa un pico a partir de las 17:00 hasta las 22:00. Interesante para preparar campañas publicitarias o remarketing en estos horarios.\nSeguimos ahora con la segmentación por categoría de dispositivos. No es lo mismo con equipo de escritorio que con móvil o tablet. Así que vamos con la creación de una nueva variable y con dimensión **deviceCategory. **\nOtra visualización a realizar e interesante, sería la comparación por dispositivo. gadata_4 % ggplot(aes(deviceCategory, sessions)) +\ngeom_bar(aes(fill = deviceCategory), stat=\u0026ldquo;identity\u0026rdquo;)\n  # plot avgSessionDuration with `deviceCategory` gadata_4 %\u0026amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;% ggplot(aes(deviceCategory, avgSessionDuration)) + geom_bar(aes(fill = deviceCategory), stat=\u0026quot;identity\u0026quot;)  Con la segmentación por categoría de dispositivos podemos observar que tenemos mucho tráfico entrante por móvil, pero la duración promedio es muy baja.\nDato interesante si queremos utilizar como medio de \u0026ldquo;prospecting\u0026rdquo; el dispositivo móvil, pero también mejorar la conversión en desktop. El objetivo es crear finalmente un proyecto CRO y analizar muchos otros aspectos de usabilidad.\nEn este primer post hemos realizado una exploración de los datos integrando la API de Google Analytics en RStudio y como resultado unos cuantos ejemplos útiles a la hora de generar Insights.\nEn la segunda parte veremos algo más de inferencia estadística y aplicaremos algún modelo de predicción, que nos será de utilidad a la hora de buscar patrones y tendencias.\nSi quieres tener acceso al repositorio, puedes acceder a él a través de este link.\n(FUENTE ORIGINAL)[https://www.paradigmadigital.com/dev/analitica-web-r-analisis-visualizacion-datos/]\n","date":1556643174,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"ac612dfe8419e3b4f52f68326d5258b3","permalink":"https://www.marcusrb.com/analitica-web-rstudio-analisis-visualizacion-datos/","publishdate":"2019-04-30T17:52:54+01:00","relpermalink":"/analitica-web-rstudio-analisis-visualizacion-datos/","section":"post","summary":"Uno de los retos de un analista digital es pasar por diferentes procesos de limpieza, exploración y análisis de contenido para sacar buenas conclusiones y detectar nuevos escenarios e insights. Además, tiene que comprobar si las fuentes de datos son de dudosa calidad o no.\nY si hablamos de un analista digital, es muy probable que toda esa exploración pase por la herramienta más utilizada en el mundo de la analítica: Google Analytics.","tags":["google analytics","estadistica descriptiva","r studio"],"title":"Analítica web con R: análisis y visualización de datos","type":"post"},{"authors":["marcusRB"],"categories":["Analítica Digital"],"content":" Los datos procesables son la clave del éxito para una tienda de comercio electrónico. Si has estado ejecutando un negocio de eCommerce, es posible que ya sepas la importancia del marketing de comercio electrónico basado en datos.\nEn este post vamos a ver los beneficios del proceso de configuración del seguimiento de comercio electrónico en Google Analytics. Un proceso que también se puede aplicar incluso si hablamos de una tienda en su estado más inicial. ¡Empezamos!\nSeguimiento de datos, clave en la optimización de un eCommerce La configuración del seguimiento de comercio electrónico es una necesidad para mantener un control del rendimiento de cualquier tienda.\nMientras que el seguimiento de comercio electrónico básico (o Classic Ecommerce) en Google Analytics nos muestra qué productos están teniendo éxito y cuáles no, en realidad, no proporciona información clave sobre el \u0026ldquo;viaje real\u0026rdquo; del cliente, o como estamos acostumbrados a definirlo: \u0026ldquo;Customer Journey\u0026rdquo;.\nPor lo tanto, el misterio de por qué algunos productos tienen más éxito que otros sigue sin resolverse.\nAquí es donde el análisis de comercio electrónico mejorado, o Enhanced Ecommerce de Google, nos permite sumergirnos en el mar de datos de una tienda y extraer todo lo que necesitamos para analizar el comportamiento en nuestro sitio web.\n¿Por qué es tan importante el Enhanced Ecommerce? Si nos ponemos en el lugar del cliente, vemos que hay varias situaciones que pueden darse antes de realizar una compra. El cliente puede agregar elementos a una lista de deseos antes de comprar e incluso visitar la página del producto 10 veces para revisar los detalles del producto a fondo.\nSus acciones podrían ser impulsadas por diversas tendencias, como la comparación de precios de productos en sitios web de la competencia, la espera de un precio con descuento, la espera de que un producto vuelva a estar en stock, etc.\nPara los comercializadores o desarrolladores de negocios es necesario obtener información sobre el Customer Journey (o \u0026ldquo;viaje del cliente\u0026rdquo;) para determinar qué área del negocio necesita optimización.\n El comercio electrónico mejorado nos permite obtener datos procesables que se generan al rastrear todo el recorrido del cliente.\n ¿Qué nos aporta el seguimiento de comercio electrónico mejorado? La analítica tradicional de comercio electrónico se basaba en un objetivo que debía cumplirse sí o sí (en la mayoría de los casos, una compra realizada) para generar datos como:\n La tasa de conversión de un producto. Valor total de ventas del producto. Cantidad vendida. Impresiones totales.  Con el comercio electrónico mejorado no tenemos que estar limitados en términos de su seguimiento. Nos permite rastrear los detalles más pequeños que nos ayudarán a optimizar nuestra tienda.\nEsto incluye:\n Comportamiento del cliente. Compromiso con el cliente. Informes de rendimiento del producto. Tasas de abandono del carro y causas. Cupón y descuento de rendimiento. Informes detallados de afiliados. Informes de reembolso. Informes de marketing de contenidos.  El comercio electrónico mejorado no solo nos permite controlar el rendimiento de los productos y analizar el embudo de ventas; también tiene como objetivo proporcionarnos información mucho más profunda sobre el comportamiento del cliente para poder optimizar ese embudo mejorando así la experiencia del usuario y nuestros planes de marketing.\n¿Qué datos se recogen? Para ayudar a los desarrolladores y analistas a identificar qué tipo de datos queremos rastrear, Google los ha agrupado en cuatro categorías:\n Datos de impresión: información completa sobre un producto que ha sido visto por un cliente. Datos del producto: información sobre los productos individuales que se visualizan. Datos de promoción: información sobre los elementos promocionales/banners que ha clicado en tu sitio web. Datos de acción: información sobre todas las acciones de comercio electrónico que tienen lugar en el sitio web.  Podemos consultar la guía de seguimiento de Google Analytics para obtener una lista detallada de todos los tipos de datos individuales que se pueden rastrear en cada una de estas categorías.\nAhora que ya sabemos qué podemos rastrear con Google Analytics, el siguiente paso es implementar Enhanced Ecommerce Analytics y usarlo para enviar los datos de análisis a Google Analytics desde nuestra tienda.\nA partir de aquí puede que el proceso empiece a ser un poco «tedioso», especialmente cuando se está ejecutando el análisis de una tienda con una enorme base de datos de productos y categorías.\nImplementación del comercio electrónico mejorado A día de hoy hay innumerables tiendas que aún consideran el comercio electrónico mejorado como una opción secundaria, por lo que pierden numerosos datos valiosos que pueden llevar su estrategia de marketing al siguiente nivel.\nLo más probable es que la razón sea que el proceso de configuración es un poco complejo, y sí, también exige conocimientos básicos de JavaScript o jQuery.\nRequisitos El análisis de comercio electrónico mejorado solo se puede activar utilizando Universal Analytics o Google Tag Manager (GTM).\n NOTA: la nueva versión de Universal Analytics sigue existiendo, solo que para una implementación manual ha pasado a hacerse con otro plugin, global tag o gtag.js, que es similar a la de Google Tag Manager, pero directamente vía código. Aquí podrás obtener más información sobre la Global tag de Analytics.\n Si bien la mayor parte de tiendas online ya se ha movido a Universal Analytics (si no es el Administrador de etiquetas de Google), algunos todavía están usando la analítica clásica. Si tú eres uno de ellos, es hora de que te actualices y descubras lo que te has estado perdiendo.\nActivamos el EEC (Enhanced Ecommerce) para Universal Analytics Si utilizamos Universal Analytics, tendremos que activar el EC.js o el complemento de comercio electrónico mejorado manualmente.\nEl código del complemento que lo activa se especifica justo después de nuestro código UA de Google Analytics, seguido de comandos de complemento para rastrear los datos y enviarlos a Google Analytics.\nDurante la fase de implantación, el código quedaría de la siguiente manera:\nPASO 1. El primer paso es definir la propiedad, como ya tenemos el script principal ya tenemos definida esta parte:\nga('create', 'UA-XXXXX-Y', 'auto');  PASO 2. Definimos el plugin ecommerce.js:\nga('require', 'ec');  PASO 3. Ahora podemos seguir con las demás mediciones (ejemplos):\nga('ec:addImpression', { ... }); ga('ec:addProduct', { ... }); // seguido por la acción ga('ec:setAction', 'click', { // click action. 'list': 'Search Results' // Product list (string). });  PASO 4. Sucesivamente se enviarán las acciones con el comando send:\nga(\u0026quot;send\u0026quot;, ...);  Este ejemplo es para enviar eventos con Categoría: homepage y Acción: click\nga(\u0026quot;send\u0026quot;, \u0026quot;event\u0026quot;, \u0026quot;homepage\u0026quot;, \u0026quot;click\u0026quot;, \u0026quot;\u0026quot;);  En este enlace podrás obtener más información sobre la estructura de los eventos de Google Analytics.\nEs necesario implementar los códigos exactamente en el mismo orden, de lo contrario, el seguimiento fallará.\nEste es un ejemplo del script del plugin ecommerce.js extraído directamente en la web de testing\nActivamos el EEC (Enhanced Ecommerce) para Google Tag Manager Si hemos configurado la analítica con Google Tag Manager, entonces el proceso para configurar el comercio electrónico mejorado se vuelve un poco más complejo y al que hay que agregarle algunos pasos más.\nLa implementación del código será a través de dataLayer, al igual que la globalTag.js, pero en la interfaz es mucho más sencilla de gestionar.\nLa idea fundamental de usar de GTM para un seguimiento mejorado es configurar varias capas de datos (dataLayers) en nuestro sitio web que almacenen temporalmente los datos representados en cada página de la tienda.\nEstas capas de datos están codificadas en varios elementos de la página en toda la tienda, por ejemplo: clic del producto, agregar al carrito, eliminar del carrito, agregar a la lista de deseos, etc.\nEjemplo de dataLayer en la web de testing anterior: vemos que después del evento addToCart tenemos un array o listado de objetos, en formato JSON, par de claves y valor, específico de productos, título, precio, categoría, etc.\nEstos datos luego son recuperados por las variables predefinidas en nuestra cuenta GTM usando etiquetas que se activan en función de los desencadenantes que definimos en nuestra cuenta de Google Tag Manager.\nA partir de estas variables, los datos de seguimiento se envían de forma acumulativa a Google Analytics, donde se convierten en métricas procesables.\nLa guía para desarrolladores de GTM resume las capas de datos específicas requeridas para las diferentes páginas de un sitio web, algo que todos deben seguir para configurar las capas de datos relevantes.\n NOTA: Es importante tener claro que existen una variables de Tag Manager así como de Analytics que son asignadas, con lo cual no debemos alterar su nomenclatura.\n Una vez que hayamos codificado las capas relevantes para nuestra tienda de comercio electrónico y hayamos configurado los activadores y etiquetas relevantes para recuperar datos de esas capas de datos, habilitamos los informes mejorados en nuestro GA o cuenta de administrador de etiquetas y configuramos las etiquetas de verificación relevantes para rastrear los datos.\nDebemos asegurarnos de agregar etiquetas de pago significativas, ya que eventualmente se utilizarán para determinar la tasa de éxito del pago.\nCon estas etiquetas, podemos hacer un seguimiento del comportamiento de nuestros clientes en diferentes pasos de un proceso de pago.\nEstos datos se recopilan y se muestran en la opción de comportamiento de pago como se muestra a continuación:\nSi seguimos el enfoque de Universal Analytics o el de Google Tag Manager para habilitar el seguimiento de comercio electrónico mejorado, terminaremos codificando los fragmentos de seguimiento (ya sea para GA o GTM).\nHacemos hincapié, una vez más, en el almacén de demostración de Google para resaltar cómo las diferentes áreas requieren que las capas de datos se configuren para recibir información de la tienda de comercio electrónico en GA.\nListado de productos o categorías Página de detalle del producto Gestión de una tienda online Si bien la tienda de demostración anterior solo consistía en una pequeña cantidad de datos que era fácil de comprender, tratar con una tienda de comercio electrónico compleja incrementa la dificultad en muchos pliegues manuales.\nComo todo el proceso implica la implementación del fragmento de seguimiento correcto o la capa de datos en toda la tienda, puede ser muy difícil configurarlo de manera impecable. Hay dos maneras de hacer esto:\n1. Uso de un complemento de terceros para su CMS de comercio electrónico Los principales sistemas CMS ya cuentan con complementos en sus mercados que pueden hacer el trabajo automáticamente. Simplemente instalando el complemento e integrándolo con tu cuenta de Google Analytics, puedes habilitar o inhabilitar las impresiones, acciones o elementos que deseas rastrear.\nAlgunos complementos confiables con los que hemos trabajado en el pasado reciente incluyen:\n2. Implementando manualmente las capas de datos A pesar de que las plataformas principales brindan soporte para complementos, es posible que encontremos un escenario en el que nuestro CMS aún no tenga un complemento (por ejemplo, BigCommerce o OpenCart), o que haya creado una tienda de comercio electrónico personalizada utilizando un Framework como Codeigniter o YII.\nEn tales casos, recomendamos contratar desarrolladores profesionales para implementar los fragmentos de código o las capas de datos en ubicaciones precisas para la tienda.\nEs el momento de testear con Google Tag Manager Es esencial probar su implementación antes y después de implementarla en el sitio web en vivo, ya que siempre existe la posibilidad de un error subyacente con la implementación de comercio electrónico mejorado.\nRealizar un testing con el modo Vista Previa en Tag Manager: El Administrador de etiquetas de Google presenta un modo de vista previa y depuración, que permite tener un editor de consola en la parte superior del sitio web en el que queremos publicar nuestro contenedor GTM.\nLa consola solo es visible en el navegador en el que hayamos habilitado el modo Vista previa.\nCon el modo Vista previa y Depuración, obtienes acceso a los 4 elementos de una cuenta GTM:  Línea de tiempo del evento: enumera todos los eventos de carga de página que se producen hasta que la página del punto se representa dentro del navegador. Etiquetas: enumera todas las etiquetas que se agregaron en la página, las que se dispararon y las que fallaron. Variables: muestra información detallada sobre las diferentes variables que recopilan datos en el evento seleccionado, incluido el tipo de variable, el tipo de datos devueltos y el valor resuelto. Capa de datos: ofrece una vista previa de la capa de datos exacta junto con todos los datos que se generaron para el evento específico. Errores: eventualmente se visualizarán los errores globales o locales tanto de código JS que de nuestro complemento.  Probando el seguimiento mejorado del comercio electrónico después de publicarlo El seguimiento de las pruebas, después de publicar la configuración de Google Tag, también es tan importante como probarlo antes de activarlo.\nLa mejor manera de probar la configuración del Administrador de etiquetas de Google y de verificar si sigue suministrando datos a Google Analytics de manera efectiva o no es mediante la instalación de la extensión Google Tag Assistant Chrome.\nLa extensión permite grabar la sesión en el sitio web y genera un informe completo sobre todos los eventos que se activaron en Google Analytics.\nUna vez que nuestros casos de prueba generen resultados positivos, estamos listo para publicar nuestro contenedor GTM. Ahora todo lo que necesitamos hacer es eliminar el código de seguimiento de comercio electrónico tradicional de la página de pago y publicar sus configuraciones de etiquetas.\nConclusión Como punto de partida de este blog, siempre sugeriríamos un complemento si tu CMS lo admite. La implementación del comercio electrónico mejorado de Google en una tienda que comprende múltiples categorías y productos definitivamente dará algún que otro dolor de cabeza incluso a los desarrolladores más experimentados.\nEs todo un reto. Sin embargo, vale la pena todo el esfuerzo, ya que los resultados traerán a la luz algunas ideas clave que ayudarán a tomar decisiones comerciales de manera más efectiva.\n(FUENTE ORIGINAL)[https://www.paradigmadigital.com/dev/enhanced-ecommerce-google/]\n","date":1554915174,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"5f84883bf9d0f55780b08b02b0057333","permalink":"https://www.marcusrb.com/impulsa-tu-tienda-online-enhanced-ecommerce-google/","publishdate":"2019-04-10T17:52:54+01:00","relpermalink":"/impulsa-tu-tienda-online-enhanced-ecommerce-google/","section":"post","summary":"Los datos procesables son la clave del éxito para una tienda de comercio electrónico. Si has estado ejecutando un negocio de eCommerce, es posible que ya sepas la importancia del marketing de comercio electrónico basado en datos.\nEn este post vamos a ver los beneficios del proceso de configuración del seguimiento de comercio electrónico en Google Analytics. Un proceso que también se puede aplicar incluso si hablamos de una tienda en su estado más inicial.","tags":["google analytics","medición de comercio electronico","enhanced ecommerce"],"title":"Impulsa tu tienda online con el Enhanced Ecommerce de Google","type":"post"},{"authors":["Marco Russo"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://www.marcusrb.com/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["marcusRB"],"categories":["Analítica Digital"],"content":" Google Analytics lleva años siendo la herramienta de referencia en marketing no solo a nivel de extracción de métricas, sino también a la hora de comprender el comportamiento de los usuarios de nuestro sitio web.\nPero Google Analytics no deja de ser un mastodonte enorme donde obtener determinados datos es, a veces, casi una misión imposible, sobre todo si estamos empezando a trabajar con la plataforma.\nYa la semana pasada recopilamos algunos consejos que nos facilitan el uso de la herramienta. Hoy continuamos con otras 5 recomendaciones que seguro que te ayudarán a exprimir al máximo Google Analytics.\n6. Aumenta la tasa de conversión con la métrica de la tasa de rebote La tasa de rebote es una métrica importante porque proviene del compromiso del usuario con tu página web.\nCuriosamente, la tasa de rebote es comúnmente mal entendida y mal interpretada por muchos webmasters, bloggers y profesionales de SEO.\n La tasa de rebote es básicamente el porcentaje de visitas de una sola página o sesiones web. En términos de palabras, el porcentaje de rebote representa el porcentaje de visitantes del sitio web que llegaron a una página en particular y se alejaron después de ver una página (por ejemplo, la página de «servicios»).\n La tasa de rebote es esa métrica que revela cuánto de útil es realmente tu contenido.\nCuanto menor sea la tasa de rebote, en la mayoría de los casos, mejor. Lo que significa que un porcentaje de rebote del 34% es, evidentemente, mucho mejor que el 87%. Debido a que el porcentaje más bajo es una indicación de que tu contenido es útil, los usuarios quedaron satisfechos y se mantuvieron cerca.\n NOTA: De todas formas, tampoco es útil tomar como referencia solo esta métrica y decir que 34% es mejor que 87% si no lo ponemos en contexto, valoramos los objetivos que tenemos establecidos, como por ejemplo interacciones, tiempo de visita, canal de captación…\n Al final, nuestro objetivo debe ser monitorear la tasa de rebote y tratar de reducirla. Si lo que quieres reducir es la tasa de rebote de tus páginas de destino, puedes hacerlo siguiendo esta ruta:\n Comportamiento \u0026gt; Contenido del sitio \u0026gt; Páginas de destino\n Cada pequeña mejora en la tasa de rebote probablemente producirá una mayor conversión.\nVeamos un ejemplo Un sitio web de automatización de marketing y CRM notó que su tasa de rebote había aumentado un 42% durante un período de 3 meses.\nTomaron los pasos correctos para arreglarlo. Cambiaron el formato del texto, mejoraron el enlace interno y agregaron un vídeo a la página de inicio. Como resultado, el sitio web registró un aumento del 15% en el envío de formularios.\nSi observas en Google Analytics una tasa de rebote muy alta en algunas de tus páginas de mayor rendimiento con respecto a las otras, pregúntate cuál podría ser la razón. Audita las páginas afectadas y optimízalas para aumentar las conversiones.\n7. Utilizar consultas de búsqueda en el sitio Hay muchos sitios web demasiado complicados de usar. La información o contenido importante está oculto y en muchos casos las páginas se rompen cuando los visitamos a través de dispositivos móviles. Este escenario molesta a los usuarios.\nSin embargo, una pista de que los visitantes que llegan a nuestra web son nuestro target, es cuando usan el buscador interno.\n¿Sabes los términos de búsqueda que están buscando? Desde Google Analytics es bastante fácil descubrir estos términos. Por lo tanto, es necesario configurar un informe para obtener esa información.\nDesde la configuración de vista, activamos la opción de \u0026lsquo;Seguimiento de la búsqueda en el sitio\u0026rsquo; (tal como indica la imagen), y tendríamos que añadir el parámetro de consulta:\n NOTA: para añadir el/los parámetros de consulta, en la mayoría de los casos, simplemente añade lo que sucede después del carácter ? en la ruta:\n Ej.\nFíjate en este parámetro q , este será el valor que tendrás que añadir.\nWordPress por defecto tiene q (está por query), algún otro CMS del mercado tendrá otro similar, aquí os menciono unos cuantos: query, search, searchword, searchquery, consulta, s, q, qs, string, searchq…\nQuizás la manera más sencilla es que realices tú mismo una búsqueda y te fijes en cómo cambia la ruta después de una búsqueda. Hay casos que mostrará el valor y hay otros casos que simplemente no devolverá nada (páginas dinámicas).\nAl analizar este informe, puedes ver las palabras clave exactas que las personas escribieron en tu buscador interno, directamente en tu sitio web.\nAquí es cómo localizar este informe importante:\n Comportamiento \u0026gt; Búsqueda en el sitio \u0026gt; Términos de búsqueda\n Una vez que esté el informe cumplimentado con estos términos de búsqueda en el sitio, puedes mejorar tus conversiones de varias maneras. Puedes recomendar productos y servicios relacionados con las palabras clave.\nIncluso puedes utilizar la información que recopiles en tus campañas de correo electrónico. ¡O mejor aún! Investiga cuál es la palabra clave más frecuente y crea páginas de destino específicas de alto valor para atender a este segmento de visitantes del sitio web.\n8. Utilice las expresiones regulares en filtros y reportes Obtendrás mayor potencia y resultados de Google Analytics si automatizas algunas de las tareas. Entonces, si bien la mayoría de estos consejos han sido sobre el uso de la interfaz, este es un poco diferente y es aplicable en muchos lugares diferentes dentro de Google Analytics (y el Administrador de etiquetas de Google).\n¿Qué son las expresiones regulares? Las expresiones regulares son una forma de diferenciarse del usuario casual de Google Analytics.\nUna breve definición: son una forma de describir patrones en texto usando caracteres especiales. Expliquemos un poco más en detalle en qué consisten.\nSupongamos que estás viendo tu informe de \u0026lsquo;Todas las páginas\u0026rsquo; dentro de Google Analytics y que quieres definir algunas páginas específicas. Tal vez las páginas de Inicio, Servicios y Acerca de nosotros.\nPuedes extraer todo el informe, desplazarte para encontrar las páginas que necesitas y anotar los números. O puedes cargar el informe, buscar Inicio, realizar una nueva búsqueda de Servicios y después realizar una nueva búsqueda de Quiénes somos.\n Inicio Or Servicios Or Quiénes somos.\n Se vería algo como esto: ^ (/ | / services / | / about-us /) $\nEl carácter de expresión regular más fácil de aprender es el pipe o \u0026ldquo;palito\u0026rdquo; |, que simplemente significa OR. Hay otros caracteres para indicar caracteres opcionales, «comienza con» y «termina con», ¡y muchos más!\nSi quieres investigar más sobre expresiones regulares, te dejo las siguientes guías:\n9. Sigue el camino de la generación de leads antes de que conviertan Existe una obsesión con la generación de leads. Muchos olvidan que alimentar a los clientes potenciales existentes es la mejor manera de hacer crecer un negocio. Probablemente tengas un representante de ventas o un departamento responsable de nutrir a tus clientes potenciales, pero ¿les delegas responsabilidad?\nPreviamente, debemos calificar a nuestros clientes potenciales y comprender su trayectoria por nuestro sitio web. Según las estadísticas de HubSpot, «solo el 25% de los clientes potenciales son legítimos y deben ser enviados a un representante de ventas».\nLos clientes potenciales suelen pasar por tres etapas básicas para confiar**, **creer y gustar. Así es como funciona el marketing online. El proceso de 3 pasos implica:\n Conciencia. Consideración. Decisión.  En realidad, hay una manera muy fácil de hacer esto con una búsqueda en el mismo informe. En todos los informes de Google Analytics, el cuadro de filtro de la tabla acepta expresiones regulares de forma predeterminada, lo que significa que solo puedes escribir en ese cuadro de búsqueda una frase que represente:\nPara comprender los diversos puntos de contacto que atraviesan los usuarios antes de comprar y las páginas a las que acceden en tu sitio web podemos consultarlo en Google Analytics siguiendo esta ruta:\n Comportamiento \u0026gt; Flujo del Comportamiento\n Para obtener resultados imparciales, céntrate en los nuevos usuarios de tu sitio web que no tengan experiencia previa con tu marca. Por lo tanto, ordénalos por nuevos usuarios o nuevas visitas. Luego, podrás ver cómo los nuevos usuarios interactúan con tus páginas, desde dónde comenzaron, cómo se fueron, etc.\n NOTA**: **También sería útil profundizar un poco más, quizás con segmentos por dispositivo o por canal de adquisición.\n Conocer las secuencias comunes del historial de páginas y cómo los nuevos usuarios se involucran, navegan y utilizan tus páginas web ayudarán a crear una mejor experiencia de usuario en el sitio.\n10. Analizar el compromiso de la página entre el móvil y el escritorio Como el uso de dispositivos móviles ha superado el tráfico de escritorio, según comScore, no debe descuidar a los usuarios de escritorio. Porque no a todos les gusta usar dispositivos móviles. Y ciertas tareas se realizan más fácilmente en una pantalla de escritorio más grande que en una pantalla más pequeña.\nComo propietario de un negocio, comercial digital o propietario de un sitio web, no gastes todo tu tiempo y recursos optimizando las conversiones del sitio web solo para usuarios de dispositivos móviles, hazlo también para usuarios de escritorio.\nObviamente, los dispositivos móviles pueden generar la mayor cantidad de tráfico para tu sitio web, lo que es bueno. Aquí puedes encontrar y analizar los resultados entre los canales móviles y de escritorio.\n Audiencia \u0026gt; Dispositivos móviles \u0026gt; Visión general\n En el momento en que ingreses al informe, segmenta sus datos tabulados seleccionando tu objetivo de conversión/ventas principal o principal.\nEn la imagen, si miramos en términos de ingresos, se puede ver que el móvil (37%) y el tablet (9%) generaron menos conversiones en comparación con el escritorio (54%).\nComparando el ratio de conversión, estamos a un 0,80% en tablet; 0,42% escritorio y solo un 0,09% en dispositivos móviles. Parece que genera muchas más sesiones el móvil por tener un ratio más bajo.\n Consejo profesional: si tu segmento móvil no genera suficiente tráfico o ventas, hay una tendencia a suponer que a los usuarios no les resulta fácil navegar por tu sitio con un dispositivo móvil. Asegúrate de analizar las diferencias en el tráfico, el comportamiento y las conversiones (ventas).\n Conclusión Google Analytics es su apuesta segura para descubrir insights u oportunidades que pueden mejorar también el posicionamiento orgánico en Google.\nSin embargo, la optimización del motor de búsqueda no es una estrategia de marketing digital que se establece y se olvida. Debe abordarse con un enfoque holístico e impulsado por datos, al mismo tiempo que se aprovecha el contenido atractivo de un sitio web para ganar tracción, atraer clientes potenciales calificados a un sitio web, aumentar su posicionamiento y aumentar sus ventas.\nCuando entiendes lo que motiva a las personas a usar tu sitio web, es más fácil hablar su idioma y hacer que participen con contenido bien pensado.\n(FUENTE ORIGINAL)[https://www.paradigmadigital.com/dev/guia-de-google-analytics-10-consejos-utiles-para-aumentar-su-trafico-y-ranking-2-2/]\n","date":1553100774,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"4f20c999da30f28b548f8cadcd727ed1","permalink":"https://www.marcusrb.com/guia-google-analytics-10-consejos-utiles-aumentar-trafico-ranking-2-2/","publishdate":"2019-03-20T17:52:54+01:00","relpermalink":"/guia-google-analytics-10-consejos-utiles-aumentar-trafico-ranking-2-2/","section":"post","summary":"Google Analytics lleva años siendo la herramienta de referencia en marketing no solo a nivel de extracción de métricas, sino también a la hora de comprender el comportamiento de los usuarios de nuestro sitio web.\nPero Google Analytics no deja de ser un mastodonte enorme donde obtener determinados datos es, a veces, casi una misión imposible, sobre todo si estamos empezando a trabajar con la plataforma.\nYa la semana pasada recopilamos algunos consejos que nos facilitan el uso de la herramienta.","tags":["google analytics","consejos configuración","medicion analitica web"],"title":"Guía de Google Analytics: 10 consejos útiles para aumentar tu tráfico y ranking (2/2)","type":"post"},{"authors":["marcusRB"],"categories":["Analítica Digital"],"content":" ¿Qué empresa que tenga parte de su negocio online no desea tener más tráfico? Da igual el sector al que pertenezca, su tamaño, su facturación… la respuesta siempre será sí. Y si el tráfico es de calidad, será un plus mayor.\nGoogle Analytics nos proporciona una enorme cantidad de datos que podemos aprovechar para aumentar nuestro tráfico orgánico y mejorar el ranking en buscadores.\nSin embargo, debemos ser conscientes de que no podemos lograr resultados de la noche a la mañana. El proceso de conseguir más tráfico nos va a llevar tiempo, pero la espera hará que los resultados merezcan la pena.\nCon la enorme información que proporciona Google Analytics, saber cómo \u0026ldquo;moverse\u0026rdquo; por su compleja plataforma y superar los informes básicos entre métricas y ajustes, es fundamental para su éxito.\nEn esta guía, detallamos cada una de las perspectivas de Google Analytics y cómo puedes implementarlo de inmediato en tu sitio web para aumentar la visibilidad de tu búsqueda.\n1. Aprovecha las notificaciones y alertas de Google Analytics por correo electrónico Los informes por correo electrónico son la forma más fácil de pasar menos tiempo investigando entre datos. La pregunta es, ¿cómo configurar un informe de correo electrónico de Google Analytics?\nEste es un ejemplo de una alerta de correo electrónico que recibí el 26 de diciembre de Google Analytics:\nEste simple truco nos ahorrará muchísimo tiempo. Google Analytics genera demasiada información y la clave es saber obtener los datos que nos interesan. Y si pueden llegar a nuestro email, mejor.\nSin duda, uno de los desafíos para las empresas es extraer datos útiles y Google Analytics es una buena herramienta para ello.\nMi primera experiencia con Google Analytics no fue nada agradable, entré en mi cuenta y me quedé sentado durante horas, sin saber qué hacer, cómo obtener los datos y cómo interpretar incluso los datos en el panel.\nPara configurar las alertas personalizadas, inicia sesión en tu cuenta y dirígete a la sección de Administrar:\nA continuación, encontrarás la pestaña «Alertas personalizadas«:\nPuedes configurar fácilmente alertas personalizadas para recibir actualizaciones por correo electrónico y notificaciones de mensajes de texto cada vez que ocurra un evento (o cualquier acción que consideres relevante para tu sitio web).\nCuando utilizamos las \u0026lsquo;Condiciones de alerta\u0026rsquo; es fácil personalizar escenarios específicos para recibir alertas por correo electrónico. Por ejemplo, si tu sitio web experimenta un aumento o disminución en el tráfico, o si un objetivo específico se completa en los últimos días.\nIndependientemente de las notificaciones que desees, puedes configurarlas y Google Analytics te las enviará por correo electrónico.\nPuedes crear todas las alertas que quieras para monitorear (diaria, semanalmente…) visitas únicas, páginas vistas, tasa de rebote, datos demográficos, etc.\n2. Optimizar y mejorar las rutas a través de su sitio web. En principio, tu página de inicio es el punto clave de tu sitio web y desde donde se conectan tus páginas internas. Por lo tanto, debes tener muy en cuenta el diseño y la experiencia que proporciona a los usuarios.\nCuando rediseñamos nuestro sitio web, debemos aseguramos que el diseño esté claramente trazado. Esto es importante porque va a determinar cómo se verá y funcionará todo el sitio web en su conjunto.\nPero Google Analytics nos dice algo completamente diferente. Cuando consultes tus páginas o páginas de destino más populares, notarás que, en la mayoría de los casos, hay páginas distintas a la página de inicio que reciben más tráfico.\nEsto pasa independientemente del tamaño de tu sitio web. Si tienes un blog adjunto y lo actualizas con frecuencia, verás que aproximadamente el 30% del tráfico entrante proviene de la página de inicio y, el resto, de las páginas de tu blog (gracias a las palabras clave de larga cola), que son las que generan tráfico y conversación en las consultas de búsqueda.\nEntramos en la interfaz y seguimos estos pasos:\n Comportamiento \u0026gt; Informe de flujo de comportamiento:\n  Comportamiento \u0026gt; Contenido del sitio \u0026gt; Todas las páginas:\n Haremos clic en la pestaña del \u0026lsquo;Resumen de navegación\u0026rsquo; en el panel de Google Analytics. Ahí podrás ver de dónde viene el tráfico y qué páginas visitaron tus usuarios.\nEl panel de la izquierda es la ruta de la página anterior, y el de la derecha el de las páginas siguientes. Hay que tener en cuenta que está filtrado por página de destino, aunque podríamos filtrarlo también por: agrupación de páginas (marcas, categorías superiores, género, autores, etc.), previa configuración en la vista principal…\nCómo se mueven las personas en tu sitio Por último, pero no menos importante, puedes ver las páginas a las que van los visitantes después de interactuar con la primera página de tu sitio web.\nCon esta información podemos (y debemos) mejorar la navegación de nuestros visitantes, haciendo llamadas a la acción más claras, intuitivas y atractivas.\nAhora que conocemos la ruta que hacen estos usuarios por nuestra web, podemos llevar a cabo algunas acciones que nos ayuden a generar leads en las páginas más visitadas. Por ejemplo, incluir formularios de suscripción.\nSi ignoramos esta información, dejaremos de prestar atención a clientes potenciales y desaprovecharemos un tráfico orgánico de calidad que puede aportarnos muchas ventajas.\n «Las empresas que adoptan un enfoque estructurado hacia la optimización de la conversión tienen dos veces más probabilidades de ver un gran aumento en las ventas».\n 3. Aprovecha las páginas de \u0026lsquo;bajo ratios\u0026rsquo; para mejorar el rendimiento de búsqueda orgánica ¿Conoces los términos de búsqueda que actualmente te están enviando tráfico? Puedes profundizar en esto en el \u0026lsquo;Informe de consultas\u0026rsquo; de tu cuenta de Google Analytics.\nPara ello, deberíamos tener vinculada y verificada la cuenta de Search Console**, **para consultar las mediciones de performance de nuestro site, así como reportes de consultas de búsquedas directamente de nuestro buscador Google.\nUna vez vinculada la cuenta de Search Console en nuestro Google Analytics, habilitaremos el informe aquí indicado desde:\n Adquisición \u0026gt; Search Console \u0026gt; Páginas de destino o Consultas\n Si tienes las alertas correctamente configuradas, podrás recibir la información que hayas seleccionado en tu email. Sin embargo, para obtener las consultas de bajo rendimiento, tendrás que hacerlo manualmente.\nPor razones de privacidad, Google dejó de pasar los datos de referencia a los propietarios de sitios web. Si ves el mensaje \u0026lsquo;Not provided\u0026rsquo; esa es la razón.\nSin embargo, los anunciantes de Google Ads sí pueden ver estos datos en su panel.\nPor otro lado, las páginas «ocultas» rara vez se mostrarán. Algunos estudios aseguran que el 75% de los usuarios de Google no se molestan en hacer clic más allá de la primera página de resultados.\nSi quieres optimizar las páginas de tu site que no funcionen, esto son algunos consejos que te ayudarán a conseguirlo:\nPaso 1: una vez que localices tus consultas de búsqueda y tus posiciones promedio, es hora de optimizarlas para obtener mejores clasificaciones y buscar tráfico.\nPaso 2: Ordena los datos de tu consulta de búsqueda para mostrar resultados con una posición promedio por encima de 10.\n Nota: Ya que hay 10 resultados en la primera página de SERP, significa que la 11ª posición es el primer resultado del motor de búsqueda en la segunda página, una buena página para encontrar consultas de búsqueda que se pueden enviar a la primera.\n\u0026ldquo;El mejor sitio donde esconder un cadáver en los buscadores es desde la segunda página en adelante…\u0026rdquo;.\n Paso 3**: **Reúne todos los resultados que se encuentran actualmente en la segunda página. Toma nota de los «términos de búsqueda de bajo rendimiento» (generalmente, con oportunidades de palabras clave de cola larga) que puede crear contenido atractivo y más rico para obtener ganancias de SEO rápidas.\nEstas son algunas de las formas de mejorar estas páginas:\n Disminuye el tiempo de carga de la página: Google se toma la velocidad del sitio web muy en serio. Y los usuarios no perderán el tiempo esperando a que un sitio web se cargue. Un retraso de 1 segundo en el tiempo de carga de la página se traduce en un 11% menos de visitas y un 7% de pérdida en las conversiones de sitios web. Agrega más palabras y valor a la página: si tus páginas de bajo rendimiento tienen menos de 2.000 palabras, añade alguna más que proporcione valor. Así podrás aumentar tu ranking y tráfico de búsqueda. Página responsive: si crees que todos los visitantes de su sitio web provienen de dispositivos de escritorio, te equivocas. A través de Google Analytics podrás ver el tráfico que proviene de canales móviles (utiliza los segmentos avanzados o simplemente el \u0026lsquo;Informe de Dispositivos\u0026rsquo;).  Por lo tanto, hacer que tu página sea móvil y responsive es vital, ya que puede aumentar tu ranking y tráfico.\nGoogle tomó esta decisión porque los usuarios de dispositivos móviles esperan que un sitio web y sus páginas estén optimizados para sus dispositivos. Alrededor del 48% de los consumidores dicen que no volverían a un sitio web si no se carga correctamente en sus dispositivos móviles.\n Incluye más datos, imágenes, vídeos, elementos interactivos: datos de origen de Google, sitios confiables, plataformas de investigación, organismos educativos y expertos de la industria. Incluye tantos datos que harán que las personas confíen más en tu contenido.  Para aumentar el valor real y percibido de tu contenido, incluye elementos interactivos (por ejemplo, calculadoras, herramienta de estimación de tráfico, herramienta de conversión) y no olvides crear imágenes personalizadas (por ejemplo, infografías, gráficos, tablas, ilustraciones) para tu publicación.\n Formatea correctamente el contenido: no importa lo útil sea tu contenido. El formato muy importante. Debes tener en cuenta:  El titular: hazlo irresistible. Escribe frases cortas y directas. Cada párrafo, 2 – 4 líneas máximo. Un solo párrafo de línea podría incluso funcionar. Usa viñetas y subtítulos para dividir grandes cantidades de información. Haz el contenido práctico (si es posible, conviértelo en una guía paso a paso).   4. Repara las \u0026ldquo;goteras\u0026rdquo; de tus páginas de búsqueda principales Hace unos meses, haciendo una auditoría de un sitio web, detecté que una de las página que más tráfico enviaba al sitio era en realidad una página con fugas.\nEn otras palabras, aunque la página estaba atrayendo muchas vistas únicas, no vi conversiones, en términos de suscripciones de correo electrónico, llamadas telefónicas, contacto de correo electrónico, ventas de productos, aumento de ingresos y reconocimiento general de la marca.\nPuede que tus páginas de búsqueda principales tengan fugas, si se así podrías perder oportunidades para el tráfico de búsqueda, clientes potenciales y ventas en la mesa.\nConseguir más de un objetivo con una página de destino es una técnica que casi nunca funciona. No puedes esperar que una publicación de blog determinada te consiga suscriptores, cree tu marca y anime a la gente a comprar tu producto. No funciona de esa manera. Define tu objetivo y alinea su contenido en esa dirección.\nAnalytics probablemente te mostrará las páginas populares, pero si ese no es el caso, haz clic en:\n Comportamiento \u0026gt; Contenido del sitio \u0026gt; Todas las páginas\n Haz clic en la pestaña \u0026lsquo;Visitas de página únicas\u0026rsquo; para mostrar primero las páginas más populares. Identifica las que tienen un alto porcentaje de rebote, (un ratio superior a 70% ya puede ser un claro indicador).\n¿Pero por qué los visitantes abandonan nuestra web? Muchas veces, los usuarios no encuentran lo que esperan y se van provocando una alta tasa de rebote. Para evitarlo, debemos optimizar nuestras páginas. Estas son algunas sugerencias:\n Utiliza herramientas cualitativas como Crazy Egg, Google Optimize… Ayúdate de mapas de calor. Realiza test. Consigue la opinión de tus visitantes, pídeles que te ayuden a identificar qué esperan ver en tu página y observa la ruta de conversión que toman.  5. Mejorar los enlaces internos con contenido popular Una vez que hayas identificado las páginas de destino (donde las personas ingresaron a tu sitio web desde una búsqueda o una campaña publicitaria) y las páginas de salida (donde se detuvieron después de interactuar con su página), es hora de mejorar la vinculación interna y la optimización general de su sitio web en la página.\nEsto es vital porque cuando implementas las tácticas correctas de optimización en la página, puede aumentar tu tráfico de búsqueda y tu clasificación, lo que evitará que tus páginas tengan fugas.\nSi tu página de inicio recibe mayor cantidad de tráfico, lo ideal es enlazar algunas de tus páginas internas. De esa manera, cuando se obtiene un enlace de confianza a la página de inicio, parte del contenido o el valor del enlace se transfiere a las páginas internas.\nFuente: Neil Patel\nHay un informe en Google Analytics que nos muestra cuáles son las páginas internas a las que podemos redireccionar contenido de valor:\n Conversiones \u0026gt; Ruta de objetivo invertida\n A continuación, selecciona el objetivo principal que deseas ver y analizar (yo he seleccionado entered checkout). Verás las páginas anteriores con las que interactuaron los visitantes del sitio web antes de lograr este objetivo en tu página. Esa es la página con la que pasaron más tiempo interactuando antes de la conversión.\nA partir de la información obtenida de Analytics, entendemos que estas páginas están convirtiendo a la mayoría de los visitantes en tu sitio web. Ahora el siguiente paso es canalizar todo ese tráfico a las páginas adecuadas.\nComo regla general, cada vez que publiques contenido nuevo, enlaza tus páginas de mayor clasificación para que los visitantes puedan descubrirlas fácilmente. Los rastreadores de motores de búsqueda también siguen estos enlaces en la página.\nY lo más importante, Moz hizo un estudio (abajo comparto los links) y descubrió que los enlaces de páginas nuevas y sitios web nuevos transmiten \u0026ldquo;link juice\u0026rdquo;, uno de los factores de posicionamiento que más le gusta a Google.\nConclusiones Para dar los primeros pasos para mejorar nuestro tráfico y nuestro ranking, empecemos con estos 5 consejos. En la segunda parte de este post, veremos los 5 restantes. ¡Espero que te sean útiles!\nReferencias (FUENTE ORIGINAL)[https://www.paradigmadigital.com/techbiz/guia-de-google-analytics-10-consejos-utiles-para-aumentar-tu-trafico-y-ranking-1-2/]\n","date":1551804774,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"97a231f385bf80c35067d300fbf7b8a1","permalink":"https://www.marcusrb.com/guia-google-analytics-10-consejos-utiles-aumentar-trafico-ranking-1-2/","publishdate":"2019-03-05T17:52:54+01:00","relpermalink":"/guia-google-analytics-10-consejos-utiles-aumentar-trafico-ranking-1-2/","section":"post","summary":"¿Qué empresa que tenga parte de su negocio online no desea tener más tráfico? Da igual el sector al que pertenezca, su tamaño, su facturación… la respuesta siempre será sí. Y si el tráfico es de calidad, será un plus mayor.\nGoogle Analytics nos proporciona una enorme cantidad de datos que podemos aprovechar para aumentar nuestro tráfico orgánico y mejorar el ranking en buscadores.\nSin embargo, debemos ser conscientes de que no podemos lograr resultados de la noche a la mañana.","tags":["google analytics","consejos configuración","medicion analitica web"],"title":"Guía de Google Analytics: 10 consejos útiles para aumentar tu tráfico y ranking (1/2)","type":"post"},{"authors":null,"categories":null,"content":" Se creará una sección especial con varios ejemplos del lenguaje R, para aprender la análisis de datos. R es un lenguaje de programación para la gestión y la análisis de datos, además de visualización de gráficos. Es un software libre y disponible en diferentes entornos (Unix, Linux, MacOS, Windows).\nEsta primera sección se especificará como instalar y como utilizarlas. Además de contribuir a añadir varios ejemplos de script para la exploración de datos, limpieza, uso de funciones matemáticas y estadísticas, aprendizaje automático y casos de uso como solución de negocio.\nUnos de los primeros proyectos realizados será la exploración de los datos, o EDA (Explorationa Data Analysis), pero con datos de Google Analytics, es decir exploraremos los datos de un sitio web y que conclusiones podemos sacar con esto.\n","date":1550530800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"78e70a446500553ae54824df2902463a","permalink":"https://www.marcusrb.com/projects/analisis-datos-con-r/","publishdate":"2019-02-19T00:00:00+01:00","relpermalink":"/projects/analisis-datos-con-r/","section":"projects","summary":"Se pondrán ejemplos de lenguaje R para Data Mining","tags":["machine learning","lenguaje r","data mining","exploración datos"],"title":"Análisis de datos con R","type":"projects"},{"authors":["marcusRB"],"categories":["Analítica Digital"],"content":" Hay muchísimos sitios web que gastan una gran cantidad de tiempo y energía para conseguir tráfico. Además, existe una creencia (demasiado extendida) de que obteniendo más tráfico de visitantes obtenemos también más conversiones.\n¡Error! Una web puede recibir muchísimo tráfico y no obtener conversiones. ¿Qué podemos hacer si no se cumplen nuestras expectativas y necesitamos aumentar nuestros leads? ¡Veamos algunas ideas!\n![][1]\nCuando hablamos de ratio de conversión hablamos de obtener la mayor cantidad de clientes posible, es decir, no es cuestión de visitas, sino de crear en ellas una necesidad y deseo de convertir; bien sea a través de una compra, contacto, lead, suscriptor, etc.\nSe trata de eliminar los baches que existen desde la parte más superior de nuestro embudo o funnel y hacer que lleguen hasta el final la mayor cantidad posible de potenciales clientes.\n¿Cómo puedo optimizar mi ratio de conversión? El término CRO viene del inglés \u0026ldquo;Conversion Rate Optimization\u0026rdquo;. Es el proceso que define la optimización del ratio de conversión, se trata de un proceso que busca las respuestas de \u0026ldquo;por qué\u0026rdquo; los visitantes toman unas decisiones o acciones que finalmente no los lleva a ser clientes, motivo por el cual no tenemos este ratio de conversión esperado.\nInvertir erróneamente en un canal de marketing o hacer una mala segmentación son otros motivos por los cuales no estamos recibiendo las conversiones que necesitamos/queremos. Este proceso nos ayudará a definir mejor nuestro plan táctico de captación de clientes potenciales.\n¡Y eso no es todo! Una vez que tengamos nuestro \u0026ldquo;cliente\u0026rdquo;, el proceso de mejora tiene que ser constante. Para que nuestra optimización sea aún más efectiva, debemos apoyarnos en Customer Experience.\nLos 5 pasos de mejora del CRO Una vez definido el CRO, veamos cómo llegar a la optimización de la conversión. Aquí mostramos los 5 pasos:\n Track. Creación del plan de medición, implementación de métricas e identificar las partes del embudo que debemos mejorar. Analyze. Analizar segmentos de clientes que no están cumpliendo con los objetivos. Plan. Creación del plan de hipótesis para realizar testing. Test. Realizar testing A/B con la versión actual del site VS nuevas secciones. Target. Una vez que vemos qué test ha funcionado mejor, aplicarlo en la web y ver qué ha fallado en el proceso de testing.  A nivel estratégico, el CRO es una práctica continua de aprendizaje y optimización. Desafortunadamente, el aspecto «continuo» a menudo se ignora.\nDesgranemos estos pasos uno a uno para ver cómo podemos optimizar nuestra tasa de conversión.\n1. Track: analizar las páginas existentes La forma más fácil de encontrar posibles mejoras para nuestras páginas de destino (y cualquier parte de su sitio web) es a través del análisis de los datos actuales.\nAntes de empezar con un plan de mejora y de optimización, podemos utilizar los datos de las herramientas de analítica digital que dispongamos.\nMuchas veces podemos incluso cruzar datos de otras fuentes externas, por ejemplo, CRM, base de datos, performance… para corroborar la exactitud del dato.\nSegún [BuiltWith][2], más del 80% de los sitios web utiliza la herramienta Google Analytics. Y se calcula que existen otras 300 herramientas diferentes de análisis web. Por lo tanto, si no estás midiendo, ¡no es por falta de recursos!\n¿Qué conclusiones sacamos con la analítica? Para sacar conclusiones debemos definir objetivos. Para ello, nos apoyaremos en preguntas como estas:\n ¿Por qué estoy probando esta página? ¿Quién es mi público objetivo? ¿Y por qué? ¿Cuál es un buen indicador KPI para esta página? ¿Qué quiere esta pagina que hagan los usuarios? ¿Cuáles son las acciones que está intentando promover? …  Identificar los objetivos del test sirve para dar peso al resultado esperado. No hablamos solamente de \u0026ldquo;objetivos pasivos\u0026rdquo; como el tráfico, que aunque seguramente serán más fáciles de lograr, en realidad no crean acciones. Porque tener un incremento de sesiones es bueno; solo si este convierte.\nLo que necesitamos es medir acciones, o mejor dicho \u0026ldquo;interacciones\u0026rdquo;, de los usuarios de acuerdo a los objetivo de nuestra página. En ocasiones, basta con tener una landing page (página de aterrizaje) para allanar el camino al consumidor, evitando así las distracciones, lo que nos daría más oportunidades de conversión.\n2a: Analyze: entender el comportamiento del usuario En este paso, lo que buscamos es saber y profundizar en cómo se comportan los usuarios.\nEl análisis visual emplea herramientas como mapas de calor, grabaciones de visitantes, mapas de desplazamiento y análisis de formularios, así como otros análisis de tipo cualitativo, que muestran exactamente cómo los usuarios interactúan con los diferentes elementos de nuestros site durante la fase de navegación e incluso cuando consultan las páginas de la competencia y comparan, por ejemplo, los precios.\n[Estudios realizados por Google y la Universidad Carnegie Mellon][3] han demostrado que los movimientos del ratón están relacionados con el movimiento ocular y se pueden utilizar para identificar dónde enfocan los usuarios su atención.\nEl análisis de datos puede indicarnos si los visitantes salen de nuestro producto o página a una velocidad extremadamente alta.\nLas grabaciones de los visitantes van un paso más allá, nos muestran la actividad del visitante en la página. Esto nos ayuda a visualizar dónde el usuario pasa la mayor parte del tiempo, las áreas en las que se detienen y prestan más atención…\n2b: Entender el Porqué de una determinada acción del usuario La excesiva confianza en los datos tampoco es tan buena. Hay momentos en que los datos no son concluyentes: el «cuándo», el «qué» y el «cómo» no brindan suficientes datos para saber el \u0026ldquo;por qué\u0026rdquo;.\nSi estás en proceso de definir quién es tu cliente, las respuestas de una encuesta pueden ayudarte a identificarlo y a definir de una forma más efectiva tu comunicación y estrategias de marketing hacia ese target objetivo.\nLos datos cuantitativos, si bien son exactos, no pueden captar la naturaleza humana del comportamiento del consumidor. Es errático, a veces, incluso irracional.\nLas encuestas, siendo por naturaleza datos cualitativos, aportan números que no podemos decodificar. Cuando la investigación esté completa, podremos identificar las páginas que necesiten una mejora. Lo que debemos hacer es tomar una decisión utilizando estudios de benchmarking y datos analíticos.\nAdemás de los datos, también es importante darle valor al contexto. Imagina que la tasa de conversión de una de tus páginas se ha incrementado en un 1%. Sin contexto es muy difícil saber por qué se ha obtenido ese resultado.\n3. Plan: creación de un plan de hipótesis Después de recoger los datos y analizarlos, además de tener un seguimiento y entender mejor las métricas, a través de la [creación de cuadros de mando o dashboard][4], el siguiente paso es construir hipótesis.\nPero, ¿qué es una hipótesis? Aplicado a CRO, una hipótesis es una declaración que consta de 3 partes:\n Basándonos en ideas recogidas de datos cuantitativos y datos cualitativos buscamos un cambio concreto. Para que ese cambio se lleve a cabo, buscamos un objetivo. Para ello, para lograr esa meta, nos centramos y apoyamos en las métricas que nos pueden ayudar a conseguirla. El siguiente paso es buscar el elemento (bien sea un gráfico, un texto…) que nos ayude a conseguir esa conversión.  Cuando llegamos a este punto en nuestra fase de investigación, deberíamos haber adquirido suficiente conocimiento para hacer una conjetura, generando insights sobre qué cambios en las páginas, elementos o funnel, podemos ir realizando.\nDesde aquí dividiremos el trabajo con la colaboración de varios \u0026ldquo;actores\u0026rdquo;, además de los analistas de datos, SEO, CRO…, entran en juego los diseñadores y profesionales de UX.\nPersonalmente, prefiero llamar a esta fase UX Analytics, porque mejora la experiencia de usuario, la usabilidad y el diseño, basado en datos (cualitativos y cuantitativos previamente analizados).\n4. Test: realizando test en plan de hipótesis Veamos un ejemplo de testing para una buena hipótesis:\n _\u0026ldquo;Debido a una bajada del ratio de conversión (manteniendo la misma inversión todos los meses en los canales de captación), hemos observado que a través de nuestra hipótesis la mejor manera de optimizar nuestros indicadores y métricas sería efectuar unos cambios en una de las landing pages utilizadas. _\nEsta incluye un formulario, un vídeo y una descripción. El objetivo es realizar un test y cambiar unas características\u0026rdquo;.\n Crearemos variaciones de la versión original y realizaremos cambios en:\n CTA (Call-To-Action) o Llamada a la acción. Mejora de los títulos. Cambio de elementos gráficos y/o colores. Mejora en alguna de las funcionalidades. …  Ahora tenemos diferentes alternativas:\n Realizaremos por un tiempo un test durante X semanas. O lanzaremos las versiones en test solo a una muestra de X% de los potenciales clientes. O lo probamos hasta que lleguemos a obtener un ratio de conversión del X%.  El objetivo de la prueba será averiguar cuál de estas variaciones convierten mejor.\nUna hipótesis bien estructurada también nos indicará la ruta para optimizar nuestros esfuerzos. Si la ruta falla, podemos volver sobre el punto que creemos que es conflictivo y corregir la ruta.\nPero sin este proceso estructurado, los esfuerzos de optimización pueden desviarse y perder su propósito.\nVeamos ahora un ejemplo que NO debemos seguir. Partamos del siguiente ejemplo:\n «Probemos cambiando el color de ese botón porque funcionó para las compañías X e Y».\n Debemos tener en cuenta que, aunque a una empresa le haya funcionado una acción o un elemento concreto, no significa que a nuestra compañía le vaya a funcionar.\nSi a una empresa le ha funcionado un determinado elemento es porque previamente ha tenido que realizar pruebas de mejora, de usabilidad y de optimización, pero con factores que a lo mejor son diferentes a los de nuestra web (características de los usuarios, idioma, cultural, categoría, etc).\n5: Target: the winner is…! En esta última etapa, crearemos y lanzaremos una nueva página y/o mejoraremos las funcionalidades de los elementos que mejor han funcionado durante la fase de testing y que, quizá, el usuario y cliente (segmentado) estaba pidiendo a \u0026ldquo;gritos\u0026rdquo;.\nGracias a los datos cualitativos y cuantitativos, un mejor diseño con usabilidad y buenas prácticas UX, llegamos a mejorar y optimizar nuestro Ratio de Conversión. Pero esto no termina aquí.\nVolveremos a medir y analizar nuevamente los procesos a lo largo de las semanas sucesivas al lanzamiento para seguir mejorando. Gracias al apoyo de muchas herramientas tomaremos mejores decisiones y estaremos un paso por delante del cliente.\n(FUENTE ORIGINAL)[https://www.paradigmadigital.com/techbiz/cual-es-el-papel-del-cro-en-un-proyecto-digital/]\n","date":1549817574,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"9d9cf002a842213ac8228085add0bd45","permalink":"https://www.marcusrb.com/cual-es-papel-cro-proyecto-digital/","publishdate":"2019-02-10T17:52:54+01:00","relpermalink":"/cual-es-papel-cro-proyecto-digital/","section":"post","summary":"Hay muchísimos sitios web que gastan una gran cantidad de tiempo y energía para conseguir tráfico. Además, existe una creencia (demasiado extendida) de que obteniendo más tráfico de visitantes obtenemos también más conversiones.\n¡Error! Una web puede recibir muchísimo tráfico y no obtener conversiones. ¿Qué podemos hacer si no se cumplen nuestras expectativas y necesitamos aumentar nuestros leads? ¡Veamos algunas ideas!\n![][1]\nCuando hablamos de ratio de conversión hablamos de obtener la mayor cantidad de clientes posible, es decir, no es cuestión de visitas, sino de crear en ellas una necesidad y deseo de convertir; bien sea a través de una compra, contacto, lead, suscriptor, etc.","tags":["CRO","optimizar conversiones","analitica web","posicionamiento seo"],"title":"¿Cuál es el papel del CRO en un proyecto digital?","type":"post"},{"authors":["marcusRB"],"categories":["Data Visualization"],"content":" Cada vez creo más firmemente que la gran mayoría de las empresas no supervisan el progreso de su negocio digital. Y no me refiero solo a pequeñas empresas, sino también a grandes compañías.\nUno de los problemas con los que me he encontrado en consultorías de analítica digital es que se piensa que es un tema relegado solo a técnicos o gente de marketing.\n La actividad digital, comenzando desde el marketing hasta los resultados finales de la conversión, es un elemento demasiado importante para NO ser monitoreado y controlado.\n Si tienes una inversión bancaria, ¿comprobarías si va bien o mal? Supongo que la respuesta es sí.\nLo mismo debemos hacer con nuestros proyectos digitales, por dos motivos obvios: porque es una parte integral de nuestro negocio y porque, sea cual sea el tamaño de la inversión, necesita que se verifique la efectividad para evaluar si continuar invirtiendo o mover el presupuesto a otros canales.\n¿Por qué medir? ¿Alguna vez has practicado deporte o tienes un pasatiempo donde intentas mejorar? Al igual que en la vida privada, también queremos “mejorar” en el negocio, queremos ser más eficientes: aumentar nuestro número de clientes, introducir nuevos productos o servicios en el mercado, o reducir tiempos de producción y los costos generales. Estas son cosas que solo se pueden mejorar si se miden.\n¿No eres un experto en Google Analytics? Estas son las excusas que escucho con más frecuencia: “No tengo tiempo”, “No tengo los conocimientos”, “Google Analytics siempre cambia de interfaz”, etc.\nEs cierto que Google Analytics es una herramienta que requiere un mínimo de conocimiento, pero existen soluciones para simplificar y facilitar la comprensión de todos los datos del negocio digital. ¡Estoy hablando de los dashboards!\n¿Qué es un dashboard o cuadro de mando? Un dashboard (o “cuadro de mando”) es una pantalla que te permite monitorizar en tiempo real el progreso de los informes y métricas comerciales más importantes de nuestro negocio.\nPermiten a los diferentes departamentos estar siempre informados de los datos más importantes. Y esto no afecta solo al equipo de marketing y ventas, sino también administración, finanzas, IT, comercial, logística, etc.\nObviamente, cada empresa debería tener un dashboard personalizado que le permita:\n Medir el rendimiento, especialmente en términos de marketing y ventas. Tomar decisiones rápidamente y actuar en consecuencia. Hacer que los datos sean accesibles de un vistazo. Alinear los objetivos de los diversos equipos.  ¿Cómo hacer un cuadro de mando? Lo sé, lo sé… lo ideal sería tener una herramienta mágica que cree automáticamente un fantástico tablero que puede resolver todos tus problemas.\nDesafortunadamente, es un gran error empezar a utilizar una herramienta y que, inmediatamente, comience a insertar indicadores, gráficos y tablas al azar. ¡Es inútil! El secreto para configurar un tablero es crear primero un measurement plan o un plan de medición.\nEl plan de medición En pocas palabras, un plan de medición es un documento que traduce tus principales objetivos empresariales en métricas que pueden medir en tu sitio web.\nProporciona un marco, no solo para una configuración personalizada de tu análisis web, sino que también forma parte vital de una estrategia de marketing digital más amplia. Esto luego determina cómo tus canales digitales se adaptan mejor para lograr tus KPI.\nSin un plan de medición sería como ir a ciegas en tu negocio. Sin él no podríamos saber:\n Los objetivos de negocio y estrategia. El desempeño y los compromisos de las estrategias (KPIs). Las métricas de medición.  Entonces, la forma correcta de implementar paneles es, en primer lugar, crear un plan de medición, implementarlo tanto en web como en Google Analytics, y luego extraer/vincular información usando herramientas para crear dashboards.\nComo dijo Avinash Kaushik, evangelista de Google, un plan de medición consta de 5 pasos:\n Objetivos. Estrategias y tácticas. KPI. Segmentos. Target.  #### 1. Objetivos Siempre debemos partir de los objetivos, que deben ser, según las pautas originales de Avinash, DUMP (Doable, Understandable, Manageable, Beneficial). Es decir:\n Realizables. Comprensibles. Manejables. Beneficiosos.  Estos son buenos ejemplos de ello:\n Incrementar las ventas. Aumentar el tráfico de marca o brand awareness. Captación de leads cualificados. Maximizar el ratio de fidelización, social engagement o mejorar el impacto socialmente.  2. Estrategias y tácticas Una vez definidos los objetivos a alcanzar, necesitaremos definir las estrategias para alcanzarlos a través unas tácticas, donde sucesivamente se monitorizarán a través de acciones.\nVeamos un ejemplo: si el objetivo es incrementar las transacciones online en un trimestre de un 10%, una estrategia podría ser la de buscar un segmento de potenciales clientes en un nicho de mercado X, y la táctica será a través de un canal de marketing o un conjunto de canales (content marketing, publicidad de banner, etc).\n3. KPI (Key Performance Indicator) Aquí están los famosos indicadores clave de desempeño (del inglés “Key Performance Indicators”), a través de las cuales medimos nuestras estrategias y tácticas.\nHay que saber diferenciar entre los distintos tipos de KPIs:\n KPI de marketing: ROAS (Return On-Ad Spend), retorno de cada 1€ gastado de publicidad. KPI de ventas: ROI (Return On Investment), retorno de la inversión. KPI offline: ratios financieros/económicos, indicadores logísticos o de productividad, comerciales. KPI de micro-conversión: ratio de consultas de detalles de fichas de productos visualizadas, ratio de acciones referente a un post de un blog, ratio de scrolling x% totales que se han recibido en un determinado post. KPI finales: Tasa de conversión del canal de marketing X, CPL (Coste per Lead), CPA (Coste por Adquisición).  4. Segmentos No solo necesitamos monitorizar lo que las personas hacen en nuestros proyectos digitales, sino que necesitamos saber a qué segmento pertenecen, qué subconjunto o condiciones coinciden.\nComo por ejemplo: rango de edad, fuente de tráfico, ubicaciones geográficas, dispositivo móvil o de escritorio, audiencia masculina o femenina…\nLos segmentos son más efectivos si se desarrollan como parte de la fase de KPI del plan de medición, por lo que es recomendable establecer los KPI relativos a los diferentes segmentos.\n5. Target Esta es una de las partes más difíciles. Definimos Target como una meta a alcanzar, que temporalmente se encuentra entre la medición de su desempeño hasta que conseguimos aumentar los niveles de rendimiento en la empresa. En definitiva, es cuantificar en valores numéricos los indicadores KPIs.\nPara ello, los objetivos de nuestro Target deben estar bien definidos, factibles y, sobre todo, delimitados dentro de un marco de tiempo.\nPor ejemplo: “Quiero aumentar la facturación en un 10% en 6 meses”, no es lo mismo que “Quiero aumentar la venta de 100.000 unidades para este finde”.\nImplementar el plan de medición Lamentablemente a menudo se descuida la parte “técnica”, porque los desarrolladores no tienen tiempo, porque no se le da importancia a esta parte, porque no hay personas que tengan estas habilidades, porque no se ha puesto un presupuesto o simplemente no se sabe qué hacer y cómo hacerlo.\nEs importante que el sitio web haya sido creado correctamente. Para ello es necesario verificar que los diversos elementos de la micro conversión o conversión sean rastreables: botones, llamadas a la acción, formularios, páginas de agradecimiento, descargas, vídeos, desplazamiento de las páginas para saber si su artículo ha sido “leído”, así como muchos eventos personalizados de comportamiento para la mejora de la UX.\nPor lo tanto, necesitamos un “analista-desarrollador” que se encargue de nuestro sitio web, que pueda configurar el seguimiento y configurar todo en Google Analytics, utilizando a la vez un gestor de administración de etiquetas, como Google Tag Manager, Tealium, etc.\nDiseñamos cuadros de mando Ahora que ya tenemos el plan de medición, vamos con el Dashboard. Cuando hacemos un dashboard, es recomendable seguir estos pasos:\n Identificar el tipo de negocio Cada empresa tiene sus propias características y necesita su propia adaptación, por eso es importante identificar el tipo de negocio que tenemos.  Nos ayudará hacernos preguntas tipo: ¿qué tipo de proyecto tenemos? ¿Hay un sitio web de la empresa, multilingüe, dispone de comercio electrónico o un catálogo de productos, landing pages, un proyecto de adquisición de clientes potenciales, un CRM, una app móvil, un meetup anunciando un evento o un curso online?\n Definir los destinatarios Es muy importante pensar en quién debe consultar los paneles, ya que cada usuario tendrá funciones específicas y habilidades diferentes.  Posibles usuarios:\n Stakeholders. Analista de datos. SEO o roles en digital marketing. Comerciales. Social media. Finanzas. Departamento IT. Etc.  Un director o los stakeholders, así como los CEO’s o propietarios de un negocio, necesitarán una visión general del progreso de su proyecto web o digital, sin entrar en demasiados detalles técnicos a diferencia del analista de datos que tendrá todo disponible con información sobre las diversas áreas.\nAquellos que hacen Marketing Digital necesitarán tener todos los detalles sobre el tráfico, las mejores palabras clave y el contenido más efectivo en los motores de búsqueda.\nAquí, es recomendable tener paneles de control conectados directamente a Google Search Console o herramientas de adquisición de tráfico de pago, como por ejemplo Google Adwords.\nFinalmente, los desarrolladores IT necesitarán también tener un seguimiento de errores, eventuales bugs, controles incluso de seguimiento de procesos, donde se necesitará un tablero dedicado.\n Tipos de tablero de instrumentos En función de los destinatarios y le tipo de negocio, estos son, por ejemplo, algunos de los tipos de paneles que se podrían crear:   Visión de conjunto. Tráfico y contenido. Idiomas / Extranjero. Conversiones. Adwords. Social. …  Maquetación de un dashboard Aún nos falta un pequeño paso antes de usar las herramientas. Mi consejo es coger lápiz y papel y ¡dibujar! Pintar un mockup nos ayudará a tenerlo todo organizado previamente.\nLos primeros elementos que tenemos que tener en cuenta, aunque alguno pueda parecer trivial, son:\n Título del tablero (por ejemplo, visitas de resumen). Logotipo de empresa/marca (es importante, ¡no lo olvides!). Rango de fechas (seleccionable).  Las herramientas para crear tableros Finalmente llegamos a las herramientas. Para empezar hay que diferenciar en cuadro de mandos de Business Intelligence o de tipo analítico; de pago, gratuito o freemium…\nGoogle Data Studio Una buena opción puede ser Google Data Studio:\n Es un instrumento en constante evolución, actualmente la versión Beta tiene muchos conectores para su integración. Está perfectamente integrado con el ecosistema de Google. Tiene muchas plantillas preparadas. Es gratis.  Data Studio básicamente permite hacer dos cosas:\n Conectar datos de fuentes como Google Analytics. Representar cualquier dato de una manera visual. Para empezar a utilizarlo, configuramos segmentos, eventos y conversiones para el seguimiento en Google Analytics:  [IMG]\nVinculamos la fuente de datos de Data Studio con nuestra cuenta de Analytics y después seleccionamos los indicadores gráficos apropiados vinculados a las métricas que queremos monitorizar.\nVeamos los paneles de una página de destino para anunciar un libro electrónico sobre una investigación científica:\n[IMG PANEL DE TRAFICO]\n[IMG PANEL DE conversiones]\nAdemás, nos permite ser bastante creativos al poder utilizar imágenes de fondo y elementos gráficos para hacer que nuestros paneles sean más impactantes y asegurarnos de que nuestro destinatario final sea más hábil para controlar sus “negocios”.\nRecursos gratuitos para Google Data Studio\nData Studio Report Gallery.\nFree Google Analytics (Lead Generation) Data Studio.\nPlantilla para la plantilla de Adwords.\nPlantilla para tablero de e-commerce.\nPower BI Otra herramienta alternativa a Google Data Studio totalmente recomendable es Power BI, más enfocada a negocio.\nTiene características interesantes como un gran número de conectores, muchas plantillas para utilizar y la capacidad de personalizar la representación de indicadores tanto a nivel gráfico como a nivel de fórmulas matemáticas. De hecho, se complementa perfectamente con Excel y su PowerQuery (integrado).\nOtras solucioens de Business Intelligence Entre las diferentes herramientas de pago, disponemos de algunas que nos ayudan en nuestra visión de negocio. Entre ellas encontramos: Klipfolio, Tableau, QliK, Microstrategy y las propias de IBM, Salesforce, SAP.\nTienen, además, la ventaja de integrarse con software de gran importancia a nivel empresarial, aunque en algunos casos necesitemos profesionales cualificados para su configuración y despliegue (no sucede esto en todas las herramientas).\nSegún el informe de la consultora americana Gartner, este tipo de herramientas son las soluciones de business intelligence y visualización de datos más destacadas.\n[IMG GARTNER]\nCon este artículo he querido transmitir, con un resumen muy general, las consideraciones para crear un cuadro de mando, ya sea para uso interno o para una visión de negocio global.\nTeniendo en cuenta que existen muchas soluciones hoy en día, tenemos que adaptar la herramienta a nuestro negocio y preguntarnos qué respuestas nos puede proporcionar, descubriendo nuevos insights y aumentar los beneficios empresariales.\nY tú, ¿qué herramientas utilizas para monitorizar tus objetivos empresariales?\n(FUENTE ORIGINAL)[https://www.paradigmadigital.com/techbiz/como-crear-dashboard-para-poner-en-orden-tus-metricas/]\n","date":1547311974,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"c005016aecffd33ee010dc9b8752086b","permalink":"https://www.marcusrb.com/crear-dashboard-para-poner-orden-tus-metricas/","publishdate":"2019-01-12T17:52:54+01:00","relpermalink":"/crear-dashboard-para-poner-orden-tus-metricas/","section":"post","summary":"Cada vez creo más firmemente que la gran mayoría de las empresas no supervisan el progreso de su negocio digital. Y no me refiero solo a pequeñas empresas, sino también a grandes compañías.\nUno de los problemas con los que me he encontrado en consultorías de analítica digital es que se piensa que es un tema relegado solo a técnicos o gente de marketing.\n La actividad digital, comenzando desde el marketing hasta los resultados finales de la conversión, es un elemento demasiado importante para NO ser monitoreado y controlado.","tags":["visualización de datos","soluciones business intelligence","cuadros de mando","crear dashboards"],"title":"Como Crear Dashboard Para Poner en Orden Tus Métricas","type":"post"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Destripando Google Tag Manager Nivel 1.\n30 octubre 2018\nDestripando Google Tag Manager Hemos puesto este título un poco en honor a la fiesta de Halloween, pero realmente hay cierto \u0026ldquo;miedo\u0026rdquo; al utilizar la herramienta de medición más utilizada del mundo por parte de muchos usuarios.\nEn esta ocasión le toca el turno a \u0026ldquo;Google Tag Manager\u0026rdquo;, vamos a conocer los casos típicos de uso, y no solo de la forma sencilla, gracias a nuestro experto Marco Russo.\nAunque generalmente es utilizada por profesionales de digital marketing y analítica digital, es una herramienta todo-terreno, válida también para realizar pruebas de performance, testing front y back-end, interactuando con el DOM, jugar con script de Google AppScript y Google Spreadsheet, etc.\nPretendemos dar una visión general sobre las variadas opciones de GTM, y de cómo explotarlas al máximo mostrando ideas y casos prácticos.\nMarco Russo es consultor y especialista en Digital Data Analytics a nivel internacional trabajando para diferentes sectores industriales. Además, desde hace 6 años compagina su trabajo con la formación in-company y en diferentes escuelas de negocios y Cámara de Comercio, realizando módulos y cursos de Analytics, DataViz, CRO y Google Tag Manager. Cuando no piensa en los datos, además de compaginar su tiempo con la familia, puedes encontrarlo escalando montañas en la sierra de Madrid con su bici de carretera o en una cancha de basket.\n¿Quién es el ponente? Marco Russo\nConsultor y Especialista en Data \u0026amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.\nVideo   ","date":1540922400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"e72d14c48c8178dc191814eb5b2e24ca","permalink":"https://www.marcusrb.com/talk/google_tag_manager_1/","publishdate":"2018-10-30T00:00:00Z","relpermalink":"/talk/google_tag_manager_1/","section":"talk","summary":"[Meetup] Destripando Google Tag Manager","tags":["analytics","googletagmanager"],"title":"Meetup - Destripando Google Tag Manager","type":"talk"},{"authors":null,"categories":["Tag Manager"],"content":" Índice HAHAHUGOSHORTCODE-TOC0-HBHB\n Este contenido se actualizará en la sección Tutoriales   #moocGTM - nuevo seminario online para profundizar los conocimientos de Tag Manager Comenzamos la nueva temporada de cursos y seminarios online, y antes de acabar el 2018 he preparado un nuevo seminario titulado #moocGTM, y hablará principalmente de los fundamentos de Google Tag Manager.\nEsta herramienta que está comenzando a gustar no solamente a profesionales del marketing, sino a figuras profesionales de otros departamentos de una organización, aunque la verdad que sigue habiendo mucho desconocimiento sobre sus principales funcionalidades.\nObjetivos de este seminario Come parte de este seminario explicaré los siguientes puntos:\n Entender el sistema de etiquetado Comprensión de la interfaz de Google Tag Manager Implementación de micro-objetivos y conversiones Creación de las etiquetas personalizadas Integrar con eficiencia Google Tag Manager y Google Analytics Lograr nuevos insights dentro de la analítica digital  A quién va dirigido El seminario es de nivel básico-intermedio, con lo que el alumno necei conocimientos de analítica de base y conoce la terminología de marketing, negocio, diseño web. En el caso no tengas conocimientos, se intentará derivar a un curso básico ya creado anteriormente en mi plataforma de Udemy titulado Fundamentos de Analítica digital, Analytics y Tag Manager, disponible gratuitamente para su visión y aprendizaje.\nEn particular este seminario es apto a todos los niveles, y principalmente para:\n profesionales de marketing en sus inicios con la analítica web community manager, social media y otras figuras de digital marketing estudiantes de máster y cursos de posgrados del sector consultores SEO y de diseño web agencias, startups y empresarios gestores de sitios web y comercio electrónico aquellos que quieren comenzar con Tag Manager e ir avanzando  6 Módulos principales más casos prácticos y Tutoriales Está compuesto de 6 módulos principales de más de 30 vídeos, no más largos de 5 minutos de duración, que directamente explicaré las diferentes funcionalidades. Estará disponible en la plataforma YouTube cada semana dos vídeos más casos prácticos.\n Introducción al Administrador de Etiquetas de Google: Google Tag Manager\n Interfaz de Google Tag Manager\n Etiquetas, Variables, Activadores de Google Tag Manager\n Objetivos y Conversiones de Google Analytics\n Medición de eventos importantes con Google Tag Manager\n Conclusiones y recomendaciones\n  Lista de reproducciones en YouTube Como siempre se subirán los vídeos de los módulos en mi canal YouTube. Puedes seguirme y subscribirte para no perderte las actualizaciones y novedades del mundo digital.\n ","date":1540724375,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"c490019aba3d4dc108964f0d5fc60b4b","permalink":"https://www.marcusrb.com/moocgtm-seminario-online-google-tag-manager/","publishdate":"2018-10-28T11:59:35+01:00","relpermalink":"/moocgtm-seminario-online-google-tag-manager/","section":"post","summary":"Comenzamos la nueva temporada de cursos y seminarios online, y antes de acabar el 2018 he preparado un nuevo seminario titulado #moocGTM, y hablará principalmente de los fundamentos de Google Tag Manager","tags":["seminario google tag manager","aprender fundamentos tag manager","webinar"],"title":"#MoocGTM Seminario Online Google Tag Manager","type":"post"},{"authors":["marcusRB"],"categories":["Desarrollo"],"content":" Ya sé, no es un título sensacionalista. Con esto quiero decir que yo he abandonado Wordpress, para dedicarme al desarrollo de un blog a medida con HUGO, creado con lenguaje GO, o GOLANG, sí el mismo desarrollado por Google. Hugo de momento es junto con Next que más rápido está creciendo dentro de los generadores estático de sitos web, aunque lejos de CMS cuál Wordpress, o algo más profesional cómo Drupal, ya cada vez hay aficionados que se acercan a este creador de sitios web estáticos.\nPor qué he decidido migrar Wordpress a Hugo? Después de unos cuantos años con mi blog personal en la plataforma Wordpress, utilizando un hosting no compartido, y queriendo optimizar al máximo los recursos, muy similar a un VPS, he decidido migrar hace unos meses a Hugo, una generador de sitios estático realizado con código abierto. El motivo, muy sencillo: estaba harto de estar actualizando Wordpress cada dos por tres, y la plantilla tenía que descolocarse cada x tiempo, dependencias absurdas del resto de módulos y plugin, agotar memoria y afectar a la velocidad, no obstante el CDN. Base de datos no muy lógica la verdad, wordpress nunca me ha gustado como estructura, pero, si, sencilla de montar, pero no me gustaba por el tema de personalización. También añado el hecho de querer algo fácil de crear post y recursos, sin tener que depender de plugin.\nDocumentación de Hugo Antes de nada, la documentación, hay bastante además su página oficial, muchos aficionados de códigos open-source y frameworks, materiales y tutoriales, tampoco hace falta aprender demasiado, con pocas reglas, se entiende perfectamente el fin de este generador. La idea es leer atentamente cada paso de la documentación oficial, porque me ha costado más entender como arrancar y publicar que el resto de acciones. La curva de aprendizaje es muy corta, yo he tardado dos meses en entender buena parte de la estructura y su documentación, aún así, me gusta.\nPlantillas o themes Aunque el punto positivo que más me ha motivado ha sido por no tener que diseñar yo mismo un tema, hay la posibilidad de crear una ad hoc, pero la verdad se hacía demasiado largo el tiempo de desarrollo, y maquetar, en mi caso iba a ser complicado. Así que decidí Hugo por tener unas cuantas plantillas de editar, montar, y listo. Con atribuciones y obviamente GRATUITAS. La mayoría de themes puedes encontrarlas aquí, y cada vez hay muchas más.\nEstructura de Hugo Lo que diferencia Hugo del resto de \u0026lsquo;CMS\u0026rsquo; por decirlo de una manera, es que trabaja sin servidor de apoyo, es decir no necesitas un alojamiento y una base de datos, también puedes alojarla en Firebase, GitHub, Bitbucket, en una VPS y el hosting de toda la vida, incluso en tu servidor (obviamente teniendo recursos para que sea rápida la carga), esto porque el generador, que es la base de Hugo, crea tantas carpetas y archivos html que puedes alojarlos donde quieras, muchos JS van en otra carpeta y si hay llamada a Bootstrap o JS externos, ya no hace ni falta. Yo por ejemplo tengo alojado en GitHub y he creado una redirección a mi dominio, así que de momento cero preocupaciones.\nLenguaje de marcado o Markdown Pues sí, si nunca has oido hablar de marcado, ten en cuenta que HTML es un marcador también, pero redactar post, crear los esqueletos de Hugo, está interamente creado con el lenguaje Markdown, gracias a los fundadores, Aaron Swartz y John Gruber que en 2004 hicieron lo posible para crear algo tan sencillo, fácil de interpretar, y sobre todo escribir, en cualquier editor de textos (bloc notas, sublime, atom), y teniendo en cuenta que GitHub y el mismo programa estadístico que utilizo R, tiene para editar sus textos.\nEl lenguaje de marcado tiene tres tipologías de extensiones, cada una difiere por el estilo de escritura y debería respetar como regla general para el resto de archivos:\n YAML\ncategories: - Development - VIM date: \u0026quot;2012-04-06\u0026quot; description: spf13-vim is a cross platform distribution of vim plugins and resources for Vim. slug: spf13-vim-3-0-release-and-new-website tags: - .vimrc - plugins - spf13-vim - vim title: spf13-vim 3.0 release and new website  TOML\ncategories = [\u0026quot;Development\u0026quot;, \u0026quot;VIM\u0026quot;] date = \u0026quot;2012-04-06\u0026quot; description = \u0026quot;spf13-vim is a cross platform distribution of vim plugins and resources for Vim.\u0026quot; slug = \u0026quot;spf13-vim-3-0-release-and-new-website\u0026quot; tags = [\u0026quot;.vimrc\u0026quot;, \u0026quot;plugins\u0026quot;, \u0026quot;spf13-vim\u0026quot;, \u0026quot;vim\u0026quot;] title = \u0026quot;spf13-vim 3.0 release and new website\u0026quot;  JSON\n{ \u0026quot;categories\u0026quot;: [ \u0026quot;Development\u0026quot;, \u0026quot;VIM\u0026quot; ], \u0026quot;date\u0026quot;: \u0026quot;2012-04-06\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;spf13-vim is a cross platform distribution of vim plugins and resources for Vim.\u0026quot;, \u0026quot;slug\u0026quot;: \u0026quot;spf13-vim-3-0-release-and-new-website\u0026quot;, \u0026quot;tags\u0026quot;: [ \u0026quot;.vimrc\u0026quot;, \u0026quot;plugins\u0026quot;, \u0026quot;spf13-vim\u0026quot;, \u0026quot;vim\u0026quot; ], \u0026quot;title\u0026quot;: \u0026quot;spf13-vim 3.0 release and new website\u0026quot; }   Como crear un post Una vez que instalemos hugo en nuestro ordenador local, en una carpeta al estilo wordpress, accedemos directamente a un editor de texto del contenido de la carpeta y listo, ya tenemos de frente a unas series de directrices: CONTENT, STATIC, THEMES. Si bien en la última tenemos alojada nuestra plantilla y los archivos más importantes a la hora de realizar script, funciones y llamadas de variables, la primera y la segunda donde estarán situados nuestros archivos más importantes: post, páginas, recursos, imágenes, otros archivos, etc. Crear un post desde cero, es fácil \u0026ldquo;entre - comillas\u0026rdquo;, ya que tenemos que darle un encabezado, un estilo y luego redactar el post con el lenguaje de marcado.\nEjemplo de como se redactaría un post El estilo de formato para un post, donde hay título, negritas, cursivas etc, el markdow es muy fácil de utilizar, aunque aquí hay que decir que está bastante limitado. Así que muchas veces, tengo que combinar elementos de HTML y CSS.\n Encabezados\n# Encabezado h1 ## Encabezado h2 ### Encabezado h3 #### Encabezado h4 ##### Encabezado h5 ###### Encabezado h6  Saltos de líneas\n\u0026quot;Quien fue a Santiago, perdió su clase de redes\u0026quot;  Inyección de un código como este:\n``` [language] Código en varias líneas  Citas\n\u0026gt; La vida es muy corta para aprender Alemán. -Tad Marburg  Listas sin ordenación\n* Un elemento en una lista no ordenada * Otro elemento en una lista - Un elemento más - Otro del listado  Listas ordenadas\n1. Elemento en una lista enumerada u ordenada. 2. Otro elemento  Enlaces con hipervínculo\n[Texto del enlace aquí](URL \u0026quot;Título del enlace\u0026quot;)  Enlaces con referencia\n[Texto del enlace aquí][1]  Imágenes\n![Texto alternativo](URL \u0026quot;Título de la imagen\u0026quot;)   Y para dar colores, formatos al texto, ya se creará específicamente un hoja de estilo CSS para el resto. La idea es simplificar al máximo, código limpio y sobre todo, ágil.\nPublicaciones Entre la estructura, crear la lógica SEO onPage (metadatos, encabezados, titulares, etc), ha tardado un poco. Pero nunca comparado con la publicación, ya que hay dos opciones: o lo subes manualmente cada vez que creas un post, y con esto digo, no solamente este post, sino toda la carpeta auto-generada aunque cuando modificas una sola línea\u0026hellip;y con esto digo que Hugo no es adapto, o bien creas uno script en lenguaje Perl (un bash por decirlo de una manera), que ejecuta unas series de lineas de código entre tu máquina local y tu servidor remoto, y automáticamente sube los contenidos modificados.\nExactamente que significa?\nEsto me preguntaba yo también. Si creas un post con Wordpress, abres el back-end, nuevo post, redactas, guardas, ajustas, sube imágenes, y listo. Publicado! Cada vez tienes que hacer esto y ten en cuenta, que cada vez estás interrogando la base de datos, cierto?\nHugo es diferente. No tiene base de datos, la única es tu ordenador, o compilador. Una vez que redactas el post, desde tu misma maquina local, normalmente la visualización en local es con el comando:  hugo server \ny con esto se abre la ruta siguiente:\nServing pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop  De aquí vas a ver siempre en una pestaña del navegador tu vista previa y como queda tu página. Y cuando finalmente quieres publicar, se crea una carpeta llamada PUBLIC donde estarán tus contenidos para la parta pública, apunto carpetas y archivos index.html, categorizados o no, con un solo comando:\n hugo  Con esto, tenemos que ejecutar un comando de envío de esta carpeta al hosting que sea (GitHub, Bitbucket, Firebase, el tuyo que sea, VPS, etc.).\nVentajas de Hugo  Velocidad de ejecución ausencia de una base de datos menor estrés del servidor huésped tiempo de carga por debajo de los 2 segundos !!! mejora del lado SEO (siempre si no le pones imágenes de gran tamaño) ejecución del resto de script y dependencias, fuera de tu servidor principal aprendes mucho de programación y de algoritmos (en mi caso viene super Bien!) personalización sin depender de una plantilla y de un diseñador de un CMS particular pocas actualizaciones del core (está solo en local, nadie puede tener acceso al back-end) SEGURIDAD (no te pueden atacar porque quieren, no tienes base de datos, ni accesos de admin)  y cuando hablo de velocidad, menciono lo siguiente: - no tienes que instalar plugin ni módulos, ni cargas pesadas de estos últimos - tu creas dependencias que quieras y bajo tu mismo control - un framework es autogestionable, no necesitas ser un experto, aunque de desarrollo front-end un poco más. - existen muchos elementos, llamados ESQUELETOS, que te permiten automatizar procesos en la publicación de páginas. - Wordpress tiene que cargar más de 10 segundos elementos X que ni tú sabes que existen, e instalas un CDN para optimizar estos procesos. - Hugo tiene solo archivos ESTÁTICOS creados en HTML pero con un generador muy potente.\nConclusiones En mi caso, el theme Academic que cito por su simplicidad, y propiamente para mi uso diario de tutoriales, proyectos, post, vídeos algo relacionado con mi trabajo actual y mis estudios, lo veo super eficiente, rápido en la ejecución y ante los buscadores y la usabilidad, limpio y ordenado.\nExisten muchos más frameworks: - Jekyll - Django - Flask - Next - Gatsby\nmuchos de ellos similares a Hugo, otros dependen de lenguajes Python, PHP, Javascript, Ruby, etc. y puede que la curva de aprendizaje sea más largo (si no conoces Python, por ejemplo), excepto el citado Jekyll, muy similar a Hugo, son generadores que cada vez buscan cuota de mercado (las tendencias llevan a un aumento de búsquedas de estos), y honestamente no me hace retroceder a Wordpress nuevamente.\nDedicaré más tiempo en profundizar algunos aspectos para ayudar al resto de usuarios que decidan acercarse a la revolución de los CMS de blog (limpios y rápidos), pero cada vez más veo también páginas corporativas y comercio electrónico (aunque aquí hablamos de pequeñas tiendas, muy pequeñas la verdad).\nEspero poder compartir al máximo en las próximas semanas, y si tienes dudas, ya sabes, pregunta lo que quieras e estaré actualizado este post sobre HUGO.\n","date":1539538337,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"04f1c0cf895bba72dec098a68db5964b","permalink":"https://www.marcusrb.com/adios-wordpress-bienvenido-hugo/","publishdate":"2018-10-14T19:32:17+02:00","relpermalink":"/adios-wordpress-bienvenido-hugo/","section":"post","summary":"Dedicaré más tiempo en profundizar algunos aspectos para ayudar al resto de usuarios que decidan acercarse a la revolución de los generadores estáticos de sitios web.","tags":["programación","hugo","wordpress","markdown"],"title":"Adiós Wordpress, Bienvenido Hugo","type":"post"},{"authors":["MarcusRB"],"categories":["Tag Manager"],"content":" Índice HAHAHUGOSHORTCODE-TOC0-HBHB\n¿Qué es y a que sirve el píxel de Facebook? Tal como indicando en la web oficial de Facebook:\n El píxel de Facebook es una herramienta de análisis con la que podrás medir la eficacia de tu publicidad. Puedes usar el píxel de Facebook para conocer las acciones que las personas realizan en el sitio web y llegar a los públicos que te interesan\n Si pensamos que normalmente se utilizan diferentes píxeles de conversiones, para medir la eficacia de acciones de marketing (Tradedoubler, Doubleclick, Google Ads, Remarketing), acciones de experiencia de usuario y CRO (Hotjar, CrazyEgg, Mouseflow, VWO), además de pixel de seguimiento de otras interacciones y de performance, su implementación podría ser un obstáculo, entre la solicitud al departamento de IT, o su personalización constante, entraríamos en un campo minado.\nComo siempre digo y lo repito, por suerte que los administradores de etiquetas, cómo Google Tag Manager, fueron creados para realizar nuestras tareas más sencillas*, ahorrar tiempo y manejar con total autonomía tareas también más complejas de medición y de seguimiento. No estoy hablando solo de conversiones, también gestión más avanzadas de estudio de datos de experiencia de usuario y mejora del CRO.\n*Hablando de \u0026ldquo;sencillas\u0026rdquo; obviamente daré por hecho que se necesita conocimiento más que un nivel básico de diseño web, hoja de estilo CSS, y JavaScript, en caso contrario va a ser complicado. No se trata solo de copiar y pegar códigos, sino entender su funcionamiento.\nMás informaciones sobre el píxel de Facebook Ads Configurando el píxel de Facebook es colocar su código en el encabezado del sitio web. Cuando alguien visita tu sitio web y realiza una acción (por ejemplo, completar una compra), el píxel de Facebook se activa y registra esta acción. De esta forma, sabrás cuándo un cliente realiza una acción y podrás llegar a él de nuevo a través de futuros anuncios de Facebook.\nLas ventajas de utilizar el pixel de conversiones Existen varias formas de usar los datos obtenidos con el seguimiento del píxel de Facebook para perfeccionar la estrategia publicitaria en Facebook.\nEl píxel de Facebook permite hacer lo siguiente:\n Llegar a las personas adecuadas Encuentra nuevos clientes o personas que visitaron una página específica o realizaron una acción que te interesa en tu sitio web. Además, crea públicos similares para llegar a más personas que son parecidas a tus mejores clientes. Genera más ventas Configura pujas automáticas para dirigirte a personas con mayor probabilidad de realizar alguna acción relevante para ti, por ejemplo, hacer una compra. Medir los resultados de los anuncios Mira cuánto éxito tuvo el anuncio según los resultados que generó. Puedes consultar información como las conversiones y las ventas obtenidas.  Vamos con la implementación del pixel principal de evento Page Views Para que todo funcione correctamente, y con esto digo que recoge las variables dinámicamente y no dejando a medias el potencial que pueda tener el pixel, pero incluso su peligro de proporcionar muchos datos a Facebook, vamos por pasos.\nNecesitamos el pixel base que será la principal de páginas vistas y su implementación recomendada es como se muestra.\nSiempre optamos por una implementación a medida, y no guiada, no queremos bajo ningún concepto ser ayudados por Facebook, ya que compartir inclusos más datos desde Tag Manager, no es una buena idea.\nUna vez en nuestro contenedor de Tag Manager comienza la parte más divertida:\n Creamos una etiqueta tipo HTML personalizado y añadimos nuestro pixel, ya sabes porque, verdad? De momento FacebookAds no tiene ningún acuerdo con Google sobre tag de 3rd party, así que tenemos que personalizarlo cómo uno script se tratara.    Y no olvidar de realizar dos tareas más, sacar el noscript en una etiqueta Imágen personalizada y la otra indicar que solo se podrá realizar cuando el javascript está deshabilitado. Esta parte la explicaré con calma en otra ocasión.  También podemos personalizar nuestro pixel ID con una variable del tipo constante y así tener más dinamismo en nuestras personalizaciones.\nTRIGGER o Activador Todas las páginas vistas\nEventos estándar Tal como nos sugiere la misma guía de Facebook Developers, ahora podemos utilizar nuestras personalizaciones con la función\n fbq('track')  y nuestros eventos tendrán un nombre ya asignados según como se muestra en esta tabla que mostraré a continuación.\nCada uno de los eventos tendrá propiedades tipo obligatorias y otras opcionales, sirven basicamente para recoger informaciones de nuestros usuarios (comportamientos, usabilidad, conversiones, comercio electrónico, etc), obviamente si lo mismo lo recogemos desde el Enhancend Ecommerce (Comercio Electrónico mejorado) para Google Analytics, las mismas variables serán enviadas a Facebook Analytics. Lógico, no?\nAlgo muy importante Cada una de las siguientes etiquetas personalizadas, tendrá que tener una lógica de secuencialidad: - Cada etiqueta tendrá que cargarse DESPUÉS de la principal, ya que el core del código sigue teniendola la anterior, mientras en la personalizada solo tendrá una porción del código y sus variables.\n\n\u0026nbsp;\nCreación de variables personalizadas Una cosa no meno importante es crear ahoras las variables personalizadas, en mi caso de comercio electrónico, que para utilizarlas en todas las etiquetas con los parámetros, más o menos se reciclarán las mismas una y otra vez para el resto de eventos.\n value: será el valor del producto / servicio. será tratado como FLOAT (con decimales), y no tendrá comillas. Su código tendrá un aspecto similar (depende de nuestro dataLayer):    currency: por defecto será la moneda utilizada en el comercio electrónico, y en caso de multidivisa, entonces podemos crear una regla, normalmente este valor variable retornará un valor único, EUR, USD, GBP, JPY, etc. Podemos crear algo así:  1a  var dlv = window.google_tag_manager[\u0026ldquo;{{Container ID}}\u0026rdquo;].dataLayer.get(\u0026lsquo;ecommerce\u0026rsquo;); return (dlv.currencyCode); \no con la captura de exepciones sería:\n try{ var dlv = window.google_tag_manager[\"{{Container ID}}\"].dataLayer.get('ecommerce'); return (dlv.currencyCode); }catch(e){ window.console(e); } }  2 o mejor todavía con la variable dataLayer (variable de capa de datos):\n ecommerce.currencyCode\n Como puedes ver existen muchas formas de extraer las variables, recuerdas de nombrarlas correctamente\n cJS - currencyCode DLV - currencyCode  Fácil verdad? :-)\n productName: Capturar el nombre de producto, será sencillo con la regla de dataLayer, pero cuando se trata de recoger más valores y inserirlo en un listado podemos utilizar un algoritmo. No todas los sitios web son iguales, ni las estructuras de URLs son iguales, así que en mi caso he utilizado esta función:   La explicación sobre el cómo está construido depende también de otros factores, por decirlo de una manera si queremos utilizar la misma función para una tarea o varias (yo he optado para la segunda opción y una regla para el resto, para no triplicar el trabajo.)\n categoyName: Al igual que el anterior, aquí recogemos los nombres de las categorías:   Al igual que en productos, depende como se tiene la estructura del dataLayer, las variables de URLs o la estructura del sitio web para recoger unos valores u optar por otra manera de scraping\n productID: repitiendo lo mismo concepto, utilizando otro parámetro, esta vez extraemos el valor ID del producto, y sucesivamente para la categoría.    categoryList: esta variable tendrá un formato con separador por parentesis angular \u0026lsquo;\u0026gt;\u0026rsquo;, en lugar de comas \u0026lsquo;,\u0026rsquo;, así que en este caso creamos un listado y al final juntamos las categorías de varios niveles (en mi caso hasta 6), con este símbolo.   El resto de variables personalizadas serán del tipo: - addToCart - removeFromCart - checkoutEvents (AddPaymentInfo, DeliveryMethod, checkoutStep, DiscountCode, etc.)\nExisten otras variables, las que definen reglas para nuestro Analytics de Facebook.\nLa primera será: - product_catalog_id, esta será creada en nuestro Administrador de publicidad de FacebookAds como repositorio de varios pixel y creatividades. Directamente cremos una variable del tipo constante y le añadimos el valor correspondiente, será una alfanumerico, y nada más.\nLa otra: - content_type: será por defecto marcado por Facebook cuál product o product_group\n\u0026nbsp;\nEvento estándar ProductList Cómo el principal que muestra todas las páginas vistas, apunto donde está el pixel principal, este evento mostrará los elementos visualizados en una página de servicios o de productos. Al igual que las impresiones del comercio electrónico utilizaremos exactamente el mismo trigger, aunque tendríamos que trabajar con las variables anteriormente creadas.\n TRIGGER o Activador Todas las páginas que incluyen listados de productos y/o categorías. También podemos utilizar una evento personalizado de impressions pero sería crear más reglas y condiciones. Recuerdas que tienes que adaptar la etiqueta y variables, así como los activadores a tu sitio web, y en este último caso, si ti es imposible realizarlo, hay que utilizar el sentido común de un buen desarrollador: NO LO HAGAS complicado.\n\u0026nbsp;\nEvento estándar ViewContent En este caso será la ficha de nuestro producto o servicio seleccionado, al tener ya un evento personalizado en Tag Manager que está recogiendo esta acción, utilizaremos la misma.\n TRIGGER o Activador El activador será el evento personalizado ya declarado del comercio electrónico mejorado productClick. En caso de no tenerlo correctamente configurado desde el dataLayer, tenemos que crear una regla del tipo clickEvent o linkClick. En todos casos recuerdes que tener el dataLayer del enhanced ecommerce bien implementado te ahorras mucho trabajo.\n\u0026nbsp;\nEvento estándar AddToCart Este será un evento personalizado ya creado en Comercio Electrónico Mejorado, addToCart, así que recuperaremos este y lo enviaremos a Facebook Ads en su activador. Puede verificarse tanto en Página inicial, de categorías, de productos, de búsqueda, incluso en páginas de MyAccount (listado de favoritos, compras recientes, etc.)\n TRIGGER o Activador Reutilizar el Custom Event addToCart, este se producirá siempre y cuando hay un evento en un botón en \u0026lsquo;Añadir al Carrito\u0026rsquo;. Activadores \u0026gt; Eventos personalizados \u0026gt; addToCart (fijarse en las letras capitalizadas desde tu consola)\nEn caso no lo tengas, podrías utilizar un método de scraping de evento clickEvent, buscarías la .class o #id del botón, siempre y cuando este no sea diferente por cada tipología de acción, y recogerías su VALUE.\n\u0026nbsp;\nEvento estándar AddToWishlist Este evento no existe como tal en Comercio Electrónico mejorado, así que será un evento de captura de acciones, a través de linkClick o clickEvent. Las condiciones pueden ser diferentes, las reglas y variables dependen de como esté tu botón, así que utilizando el metodo scraping, podemos realizarlo.\n TRIGGER o Activador En caso que no tengas un evento personalizado, podrías utilizar un método de scraping de clickEvent, buscarías la .class o #id del botón, siempre y cuando este no sea diferente por cada tipología de acción, y recogerías su VALUE. Es importante bypassare también la información del nombre producto, categoría, ID, precio, variación\u0026hellip;así que esta tarea te llevará un buen rato para recoger todas las informaciones. Además es reutilizable para el resto de etiquetas de conversiones de terceros, además para trabajos de CRO y UX.\n\u0026nbsp;\nEvento estándar Search Este evento que tampoco existe en Google Tag Manager ya heredado del comercio eletrónico mejorado, será un evento personalizado sobre el uso del buscador interno.\n TRIGGER o Activador El activador será sencillo. Si tenemos la URL con ?q=texto_introducido_buscador_interno o similar, donde el parámetros de consulta será en mi caso la \u0026lsquo;q\u0026rsquo;, u otro parecido, tenemos que crear primero la variable personalizada del tipo URL y con parámetro CONSULTA. Y si este parámetro se encontrará en la URL, devolverá el valor, en este caso el término buscado. Listo!\n\u0026nbsp;\nEvento estándar InitiateCheckout Este evento se produce en el momento exacto que pasamos en la fase 1 del checkout. Si está implementado desde el comercio electrónico mejorado, tendríamos que tener un evento personalizado desde el dataLayer checkout. Sucesivamente, el primer paso será recogido de la siguiente manera con la variable dataLayer:  ecommerce.checkout.actionField.step \n TRIGGER o Activador Evento personalizado checkout y que la condición de la variable dataLayer STEP sea 1.\n\u0026nbsp;\nEvento estándar AddPaymentInfo El evento de recogida del método de pago seguramente estará también recogido en el dataLayer, al igual que el anterior, este podrá ser stepX, dependiendo si estará situado al punto 3, 4 o lo que fuera. En caso de no tener pasos durante la fase de checkout, podemos también adoptar sistemas de recogidas a través de páginas virtuales, con elementos .css o #id.\n TRIGGER o Activador Evento personalizado checkoutOption y que la condición de la variable dataLayer STEP sea X. Donde por X será el paso que lo corresponde. En caso de no tener este evento declarado en el dataLayer, podemos crear un activador de tipo linkClick o de clickEvent.\n\u0026nbsp;\nEvento estándar Purchase El evento de transacción finalizada será nuestra páginas de gracias o lo que corresponda. En caso de tener el evento de comercio electrónico transaction o purchase, o crear uno a medida. Muchas tiendas online o páginas de servicios del tipo ecommerce, también puede estar recogiendo las informaciones de transacciones finalizadas a través del Protocolo de Medición y podemos nosotros crear un dataLayer personalizado.\n TRIGGER o Activador Evento personalizado transaction o purchase o donde se verifique la página thankYouPage. Cuidado con esta página, tiene que tener la información de transacción y sus parámetros configurados.\n\u0026nbsp;\nExisten otros eventos estándares para los registros, login, leads, contactos, pero estamos siempre hablando de etiquetas personalizadas que tendrán su activador un evento personalizado. Seguramente si estás recogiendo eventos en Google Analytics u otra plataforma, entonces simplemente puedes replicar el mismo trigger.\nEn caso de dudas, problemas con los activadores, condiciones, variables personalizadas, puedes consultarme o directamente hablarlo con el desarrollador para que pueda implementar la mayoría en un dataLayer por tí.\n","date":1539514819,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"9a2d67837ed4945a4e898c1a8a7d4168","permalink":"https://www.marcusrb.com/guia-personalizar-pixel-facebook-ads-google-tag-manager/","publishdate":"2018-10-14T13:00:19+02:00","relpermalink":"/guia-personalizar-pixel-facebook-ads-google-tag-manager/","section":"post","summary":"La guía completa de como personalizar el pixel de Facebook Ads a través de Google Tag Manager y sus variables de comercio electrónico mejorado.","tags":["pixel facebook ads","google tag manager","creación eventos"],"title":"[Guía] Personalizar el pixel de Facebook Ads en Google Tag Manager","type":"post"},{"authors":["MarcusRB"],"categories":["Digital Marketing"],"content":" ¿Sabes cuánto cuesta una campaña de marketing influyente ? ¿Cuánto deberías pagarle a un influencer? Si no estás seguro, no te preocupes, hay más de uno que no conoce la respuesta, y yo entre ellos! No sabía mucho antes de hacer la investigación para esta publicación en el blog, y exactamente hace una semana cuando compartí este post que me llamó mucho la atención:   \nY, en verdad, no hay una ciencia exacta para calcular cuánto se debe pagar a influyentes, como le explicó un ejecutivo de medios sociales a Digiday\n No tenemos idea de qué pagarles. Ese es el problema.\n Y con esto estamos todos de acuerdo.\nSi bien no hay una respuesta clara a esta pregunta, hay algunas pautas utilizadas por los comerciales, agencias y personas influyentes. Incluso hay herramientas para ayudarnos a calcular cuánto pagarle a personas influyentes específicas. Vamos a compartir todo esto en esta publicación de blog. Si estás interesado, vamos a ello.\n6 factores que afectan el costo de marketing de influencers Antes de profundizar en las pautas utilizadas por los especialistas en marketing, las agencias y las personas influyentes, me gustaría cubrir brevemente los factores que afectan el costo de cada campaña de marketing para influencer.\nEsto se debe a que las pautas a continuación no siempre se pueden aplicar a su campaña de marketing de influencer. Comprender estos factores le permite ajustar sus tarifas en consecuencia para que pueda encontrar un precio que funcione tanto para usted como para el influencer.\n1. Plataforma de redes sociales La plataforma de redes sociales elegida para la campaña es uno de los factores clave que influyen en el costo. Cómo leerá a continuación , el costo del marketing influyente varía según los canales. Es como si cada plataforma de medios sociales tuviera su propia tasa de \u0026#8220;mercado\u0026#8221; .\nInstagram tiende a ser la mejor opción para el marketing de influyentes en redes sociales, seguido de YouTube y Snapchat. En general, el marketing influyente es menos común en Facebook y Twitter.\nMusical.ly , una plataforma de medios sociales aún menos conocida pero masiva, se está volviendo más popular para el marketing de influencers. Compañías como Coca-Cola se han asociado con influencers en musical.ly para sus campañas.\n2. Seguidores Tradicionalmente, las marcas miran el alcance potencial de un canal de publicidad para decidir cuánto pagar por un anuncio. Por lo tanto, el número de seguidores de un influencer se convirtió en una consideración al decidir cuánto pagarle a un influencer. La idea es que cuantos más seguidores tenga un influencer, más personas podrían alcanzar la marca. Por lo tanto, las personas influyentes con un mayor número de personas generalmente cobrarán más.\nSegún el estado del informe de marketing influyente de Digiday, \u0026#8220;El número de seguidores sigue siendo el estándar de oro para la \u0026#8216;influencia\u0026#8217; de una estrella social \u0026#8216; .\nPero cómo es posible que la gente compre seguidores tanto en redes como Instagram o comprar visitas en Youtube para inflar su conteo de seguidores, las marcas también están buscando otras métricas para valorar su campaña de mercadeo de influenciadores.\n3. Compromiso El compromiso es una de las métricas alternativas que las marcas han estado usando. Si bien es fácil comprar seguidores, es más difícil comprar un compromiso falso (aunque no es imposible). Cuando los influencers pueden lograr que sus seguidores interactúen con sus publicaciones en las redes sociales, la campaña de marketing influyente se vuelve más efectiva para la marca, ya que estos seguidores están esencialmente comprometidos con la marca.\nAdemás, los algoritmos de las redes sociales están dando prioridad al compromiso . Mientras más participación positiva tenga una publicación, más personas verán la publicación. Los influencers que tienen una mayor tasa de interacción tienen más probabilidades de tener un mayor alcance.\nPor lo tanto, cuanto mayor sea la tasa de participación que obtiene el influencer, más cara será la campaña .\n4. Producto El producto que está vendiendo (o la industria en la que se encuentra) también puede afectar el costo de su campaña de marketing de influencias. Por lo general, contratar a un influencer para promocionar un auto deportivo costará más que contratar a un influencer para promocionar un producto de gaming, por ejemplo.\nUna buena regla empírica es que cuanto más caro sea su producto, más cara será la campaña.\n5. Asociación directa o a través de una agencia Chelsea Naftelberg, directora asociada de contenido y asociaciones para la agencia de medios sociales, Attention, compartió con Digiday que \u0026#8220;si trabajas con un agente de talentos en lugar de directamente con un influencer, espera pagar un poco más para tener en cuenta sus honorarios\u0026#8221;.\n Trabajar con una agencia de talentos es más costoso que trabajar directamente con un influencer._\n Esto se debe a que la agencia de talentos suele cobrar una comisión por conectarte con los influenciadores adecuados. Además, aquellos que forman parte de una agencia de talentos suelen tener más experiencia con este tipo de marketing de influencer y cobrarían una tarifa más alta.\n Si desea ahorrar algo de dinero aquí, aquí hay dos formas de encontrar a los influenciadores adecuados para su campaña usted mismo.\n 6. Campaña Finalmente, el alcance de la campaña también afectará el costo. Aquí hay algunas preguntas para pensar:\n¿Cuántas publicaciones quieres del influencer? ¿Quién crea el contenido? Tú o el influencer? ¿Quieres que el influencer se publique en sus otros perfiles de redes sociales? ¿Quieres que el influencer mantenga la publicación en su perfil de forma permanente?\n ¿Algunos influencers borran sus publicaciones patrocinadas después de la campaña?\n En pocas palabras, mientras más trabajo tenga el influenciador, más costosa será la campaña.\n¿Cuánto cuesta el marketing de influencers? Una estimación estaría entre 5.000€ a 10.000€\nSegún HYPR Hyprbrands.com, una plataforma de marketing influyente, ese es el precio que puede esperar pagar por una publicación de un influencer con 500,000 a un millón de seguidores en sus redes sociales.\n Pero, ¿cuáles son las tasas de \u0026#8220;mercado\u0026#8221; para cada plataforma de redes sociales?\n ¿O qué pasa si quieres trabajar con influencers más pequeños, o lo que vienen denominados, y que siguen aumentando, los micro-influencers ?\n  De mi investigación (mejor dicho mi estudio de datos analíticos, cualitativos y cuantitativos por vía del Máster de Data Science que estoy realizando), descubrí pautas intuitivas de diversas fuentes, como Digiday, Quora y numerosos blogs de diferentes sectores. En algunos casos, los vendedores y las agencias compartieron cuánto pagan por las personas influyentes. En otros casos, los propios influencers revelaron cuánto cobran las marcas.\n\u0026nbsp;\nInfluencers en Instagram Estimación de unos 10€ por 1.000 seguidores\nSegún los hallazgos de Digiday, una buena guía para el marketing de influencia de Instagram es de 1.000€ por cada 100,000 seguidores (o simplificados a 10€ por cada 1.000 seguidores).\nChelsea Naftelberg, directora asociada de contenido y asociaciones de la agencia de medios sociales Attention, calcula cuánto debe pagar su equipo a un influencer de Instagram basado en 1.000€ por cada 100,000 seguidores .\nLanger cree que las marcas pueden comenzar con 250€ por publicación de Instagram para estrellas sociales con menos de 50,000 seguidores, luego agregar aproximadamente 1,000€ por cada 100,000 seguidores por publicación .\nEsto coincide con lo que el blogger de lifestyle, Lee Anne, cobra por sus publicaciones patrocinadas por Instagram. La fórmula que usa y recomienda es de 5€ a 10€ por cada 1,000 seguidores.\n250€ a 750€ por cada 1,000 interacciones\nOtra directriz es observar el compromiso promedio que obtiene el influencer.\nComo la mayoría de las personas tiene más seguidores que la cantidad de engagement que reciben por publicación, generalmente pagará más por cada participación que cada seguidor. Tony Tran, CEO y cofundador de la plataforma de marketing de influencer, Lumanu, sugiere pagar alrededor de 0.25€ a €0.75 por interacción (o 250€ a 750€ por cada 1000 interacciones). Eso significa que si el influencer generalmente obtiene unas 10.000 interacciones para cada publicación, querrá pagarle de 2500€ a 7500€ :O\nUna herramienta que puede usar para ayudarlo es la Calculadora de dinero de Instagram de Influencer Marketing Hub.\nSimplemente ingrese el nombre de usuario del influencer de Instagram que le interesa y calculará las ganancias por publicación (o el costo por publicación para usted).\nOtra herramienta es InfluencerDB que sugiere un valor de medios por publicación (o costo por publicación para usted) según el compromiso y el alcance de la cuenta de Instagram.\nInfluencers en Youtube Estimación de 20€ por 1,000 suscriptores\nEs común que los influencers de YouTube cobren más que influencers de Instagram, ya que crear un video de YouTube puede requerir mucho más esfuerzo que crear una publicación de Instagram. Además, los videos de YouTube suelen ser más largos que los videos de Instagram.\nHenry Langer, un gerente de cuenta líder de la plataforma de marketing influencer, HYPR, compartió que los influencers de YouTube generalmente cobran 2,000 € por cada 100,000 suscriptores que tienen (o simplificado a 20€ por cada 1,000€ suscriptores).\nPara los usuarios de YouTube con más de 50,000 suscriptores, los especialistas en marketing pueden agregar aproximadamente 2,000€ por cada 100,000 seguidores por video hasta aproximadamente 1 millón de suscriptores, momento en el que un video dedicado podría costar entre 25,000 € y 50.000€ según Langer.\n\u0026nbsp;\nEstimación de 50€ a 100€ por 1,000 visualizaciones de videos\nGracias a los análisis proporcionados en YouTube, también puede ver las vistas de video promedio del influencer.\nNo todos los suscriptores mirarían cada video del influencer, por lo que cada vista de video generalmente costaría más que cada suscriptor. Tony Tran de Lumanu recomienda pagar alrededor de 0,05€ a 0,10€ por visualización de video promedio (o 50€ a 100€ por cada 1,000 vistas de video). Sugiere mirar los 30 videos más recientes del influencer para obtener un buen promedio de sus reproducciones de video 9 .\nSi no desea tomarse el trabajo de fijar el precio de su campaña, podría utilizar una plataforma como FameBit (de YouTube). En FameBit, puede establecer un presupuesto y hacer que los influencers de YouTube se comuniquen con usted con su propuesta. Aquí hay un gran respuesta de Quora sobre cómo configurar su presupuesto según métricas como el valor de vida del cliente y la tasa de conversión.\nInfluencers en Snapchat Estimación de unos 10€ por 1,000 seguidores\nHay menos datos de coste de marketing de influencers para Snapchat que Instagram y YouTube, pero aquí hay algunos puntos de datos que podrían ser útiles.\nSegún Captiv8, una startup de marketing influyente, un influencer de Snapchat con tres millones a siete millones de seguidores puede cobrar, en promedio, 75,000 € por una instantánea, mientras que una con 50,000 a 500,000 seguidores puede cobrar, en promedio, 1.000 € por un snap. Eso es alrededor de 11€ a 25€ por 1,000 seguidores para influencers y de 2 € a 20 € por 1.000 seguidores para los micro-influencers o una cifra aproximada de 10€ por cada 1.000 seguidores.\nLos marketers en el Reino Unido parecen pagar mucho más a los influencers de Snapchat: alrededor de 70 € a 100 € por cada 1.000 seguidores.\n\u0026nbsp;\nEstimación de unos 100 € por 1.000 visualizaciones\nComo Snapchat no revela el conteo de seguidores, ese número se estima a menudo usando el número de vistas de la primera historia de Snapchat. Algunas marcas y agencias prefieren ver el número de visitas directamente. Después de recopilar información de más de 30 influencers de Snapchat, la influencia de Snapchat, Cyrene Quiamco, compartió las siguientes tarifas con Digiday:\n 500 € por 1,000-5,000 vistas 1.000 € \u0026#8211; 3.000 € para 5,000-10,000 vistas 3.000 € \u0026#8211; 5.000 € por 10,000-20,000 visitas 5.000 € \u0026#8211; 10.000 € por 30,000-50,000 vistas 10.000 € \u0026#8211; 30.000 € por 50,000-100,000 vistas  Eso es alrededor de 100 € por cada 1,000 vistas .\n¿Has trabajado anteriormente con un influencer de Snapchat? Será genial si te gusta compartir algunos de tus números.\nInfluencers en Facebook y Twitter, tal vez no Según mi investigación, el marketing de influencer parece ser menos común en Facebook y Twitter.\nTony Tran de Lumanu en realidad desaconseja.\n Facebook: nunca deberías pagar por un influencer solo en Facebook. Facebook es un canal para la amplificación de contenido; por lo tanto, tiene sentido pagarle a un influencer para que publique en Facebook si ya le está pagando por la creación de contenido en uno de sus otros canales. El alcance orgánico de Facebook es notoriamente malo, así que no tiene ningún valor pagarle a un influencer de Facebook sin importar cuán grande sea su alcance, a menos que puedas obtener permiso para aumentar el contenido (¡y de todos modos es tu dinero!)\n Y sigue hablando de Twitter:\n Twitter: olvídalo, detente, date la vuelta y CORRE. Twitter es un sumidero para arrojar dinero de marketing, mucho menos marketing de influencia. A menos que reciba un mensaje gratuito en Twitter, ni siquiera piense en gastar dinero de influencia aquí.\n Esto coincide con mi experiencia personal. No recuerdo haber visto publicaciones de marketing influyentes en Facebook y Twitter. ¿Has visto alguno? Tal vez los micro-influencers para poder dar empuje a las demás redes sociales, y de hecho lo veo cada vez más en LinkedIn (que se parece más a un Facebook pijo!).\nSi has hecho marketing de influencias en Facebook o Twitter, ¡me encantaría escuchar tus experiencias! ¿Lo recomendarías?\n\u0026nbsp;\nY aquí mi pregunta: ¿Cuánto pagarías por el marketing de influencers? De mi investigación, una cosa está clara. No existe una respuesta clara al costo del marketing de influencers. Se ve influenciado por muchos factores, como la plataforma de redes sociales elegida, el número de seguidores, la tasa de participación y más.\nAfortunadamente, hay algunas pautas que puede consultar y ajustar según su situación particular.\n Instagram : 10€ por cada 1,000 seguidores o 250€ a 750€ por cada 1,000 participaciones YouTube : 20€ por cada 1,000 suscriptores o 50€ a 100€ por cada 1,000 visualizaciones de video Snapchat : 10€ por cada 1,000 seguidores o 100€ por cada 1,000 visualización  Además, si eres una marca que tiene mucho budget, el todo obviamente tendrá que ser analizado con un KPI muy importante, el ROAS (Return on Ad-Spend). Aunque con los grandes dudo que esto sea un problema, para los micro-influencers (que comprarán seguidores y visitas), dudo que tengas números positivos.\n¿Estos números coinciden con tus experiencias? ¿Cuánto ha pagado o cuánto pagaría por marketing de influencers? Compartamos lo que sabemos y ayudémonos unos a otros.\n","date":1534346004,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"3bb882815d22149803eb0d56ee4f21c7","permalink":"https://www.marcusrb.com/cuanto-cuesta-el-marketing-de-los-medios-sociales/","publishdate":"2018-08-15T15:13:24Z","relpermalink":"/cuanto-cuesta-el-marketing-de-los-medios-sociales/","section":"post","summary":"¿Sabes cuánto cuesta una campaña de marketing influyente ? ¿O cuánto deberías pagarle a un influencer?","tags":["comprar seguidores instagram","suscriptores redes sociales","visualizaciones youtube","influencers en snapchat","marketing influencer"],"title":"¿Cuánto cuesta el marketing de los medios sociales?","type":"post"},{"authors":["marcusRB"],"categories":["Tag Manager"],"content":" Un post sobre la medición de eventos, cuales clicks, conversiones, formularios, transacciones desde un iframe con Google Tag Manager. Seguramente ya te puesto a buscar y rebuscar sin éxito, o directamente pedir ayuda a un desarrollador.\nEs lo mismo que me ha pasado a mí en más de una ocasión, hasta encontrar con varias respuestas y opciones, y gracias a Lunametrics, Simo Avaha y Measureschool, finalmente yo también puedo redactar un post sobre la medición de eventos, cuales clicks, conversiones, formularios, transacciones desde un iframe con Google Tag Manager a Google Analytics o pixel de terceros.\nOJO! Esta guía funciona solo si tienes acceso al sitio web tanto propietario que del iframe, en tal caso, no es posible realizarlo (de momento!)\nCheck 1. No tengo acceso al módulo o al sitio web IFRAME. Lo siento, esta guía no es para ti.\nCheck 2. Tengo acceso al módulo o al sitio web IFRAME (sigues adelante)\n\u0026nbsp;\nComprobaciones del contenedor de Google Tag Manager Para la prueba de su correcto funcionamiento, estoy realizando el test desde un sitio web propio, un iframe interno para el mismo dominio. Más adelante veremos con uno externo y tener en consideración USER-id y CrossDomain. La idea es de registrar click o eventos: enlaces, llamadas, formularios o todo lo que podría registrar como tal y enviarlo a Google Analytics.\nDiferenciamos así el iframeParent (el padre o el principal) y el iframeChild (el hijo o el huésped).\nAquí una imagen del Parent:\n\ny para comprobar el iframe Child, tecla de ratón derecho y como la imagen:\n\n\u0026nbsp;\nAhora necesitaremos los accesos a los dos contenedores de Google Tag Manager:\n Contenedor GTM Parent Contenedor GTM Child  Este último en particular va a ser especial, porque realmente necesitaremos darle unas directrices y unos códigos diferentes al resto que ya tenemos en nuestro principal.\n\u0026nbsp;\nPasos para el contenedor Google Tag Manager CHILD Una vez creado el contenedor de Google Tag Manager para el iframe, el Child, lo implementamos como siempre solo en el iframe, sin que le pongamos etiquetas de terceros, ni Google Analytics, ya tendrá el suyo probablemente, luego, subiremos este archivo .json que a continuación explicaré a que sirve.\nARCHIVO JSON\nEste archivo, como el resto de \u0026#8220;recetas\u0026#8221; de Google Tag Manager, está en formato JSON, o resumido, un contenedor con elementos, para importar directamente en nuestro Google Tag Manager, sin tener que reescribir nuevamente todas las etiquetas y códigos. Con este en particular, para un contenedor nuevo no tenemos que sobrescribir nada (si el contenedor es nuevo, no hace falta). Si ya tenemos elementos en el contenedor, entonces no tenemos que seleccionar la opción de sobrescribir.\nUna vez importado el archivo entramos en la etiqueta SENDER \u0026#8211; postMessage para sus comprobaciones.\n\n\u0026nbsp;\nVamos a explicar en detalle esta etiqueta personalizada:\n ejecuta una declaración try\u0026#8230;catch , tal como indica el nombre, es un bloque de instrucciones para intentar (try) y en caso de excepción (catch) devuelve el error en la consola de desarrollador de nuestro navegador. Más info Asignamos la variable postObject del tipo JSON.stringify, este método convierte un valor dado en javascript a una cadena JSON, opcionalmente reemplaza valores si es especificada la función \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;de reemplazo\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;, o si se especifican las propiedades mediante un array de reemplazo. de un lado tenemos el event y lanza el propio iframe, así que como nombre he puesto iframeACCION (sustituye ACCION por el tipo de evento, form, link, click, video, transaction, etc), y del otro lado tengo link en mi caso estoy lanzado eventos de tipo enlace y le asigno un nombre, una variable, cualquier cosa que quieras que mida.  Ej. {{CLICK URL}} \u0026#8211; {{CLICK TEXT}}, devolverá la ruta completa del click sobre este enlace y el nombre del texto ancla.  mandamos el todo a la ventana principal con el método parent.postMessage() donde activa la comunicación entre la pantalla origen y destino, y que le enviamos los siguientes datos:  la variable postObject la ruta y dominio huésped   \u0026nbsp;\nConfiguración Eventos Child Ahora bien. Podemos proceder con la configuración de los eventos en este contenedor iframeChild, yo voy a seleccionar por ejemplo, el vinculo a la llamada telefónica, mejor dicho aquellos usuarios que hará click en el enlace TELÉFONO\nActivamos las variables integradas (en el caso sea nuevo el contenedor es importante tenerlos activos):\n\n\u0026nbsp;\nModifico algunos parámetros de la etiqueta personalizada anterior dejándola así con su activador correspondiente para las llamadas telefónicas:\n\nLos cambios efectuados son solamente descriptivos, además de asignarle la ruta donde tengo alojado el iframe.\nGuardamos, creamos versión y Publicamos. Hasta aquí finalizado el proceso de configuración del iframe mismo.\n\u0026nbsp;\nPasos para el contenedor Google Tag Manager PARENT Ahora toca al contenedor padre tener que realizar unas cuántas configuraciones. Primero vamos a importar este archivo json:\nRecuerda d no modificar el resto de etiquetas o elementos de nuestro contenedor, con lo cuál le damos a NO SOBRESCRIBIR el resto de elementos.\nLo que vamos a tener son dos elementos importados:\n etiqueta personalizada con uno script de llamadas de parámetros LISTENER una variable tipo dataLayer que importa el valor del iframe EVENT iframePhoneClick, justo para seguir el ejemplo anterior  El script es un poco largo de detallar pero básicamente tiene unos puntos importantes a subrayar:\n\n\u0026nbsp;\n con el addEvent enviamos el mensaje desde el iFrame hacía el Parent declaramos la variable data con el _JSON.parse, _un comunicador entre las dos partes, y así el dataLayer Ejecutamos el dataLayer en el caso existan eventos definición de eventos con el addEventListener  \u0026nbsp;\nConfiguración Eventos Parent Así creado no tenemos que realizar nada más que otras dos acciones.\n Asignarle un activador, una simple Página Vista de la página de donde se ejecuta el evento, en mi caso /event.html  \n\u0026nbsp;\n2. Creamos el evento para Google Analytics de este modo: \n\u0026nbsp;\nAsignamos:\n Categoria: lo que queramos desde el iframe, en mi caso childPhone _Acción: _yo he puesto la página donde se ha ejecutado la acción, o poner lo que sigue en Etiqueta _Etiqueta: _alternativamente a la anterior, utilizar una variable dataLayer del mensaje desde el iframe creado anteriormente, postMessageData  OJO, porque la variable ahora tiene dos partes, la primera _postMessageData _y el segundo parámetro del script del child, adivinista cuál? sí, es Link   \nEsta variable nos sirve como \u0026#8220;comunicador\u0026#8221; de la información que teníamos creada desde el CHILD con el tipo de click, form o transaction, lo que sea, básicamente si dentro del Child teníamos una variable de {{Click URL}} aquí me enviará la respuesta.\n3. Ahora toca al activador del evento de Google Analytics: \nEste activador va a ser una llamada al Evento Personalizado del _iframeChild, _sí sí, lo que teníamos declarado en el otro, exactamente cuando se ejecuta el evento, entonces DISPARA nuestra etiqueta.\nGuardamos, creamos versión y Publicamos.\nFIN!\n\u0026nbsp;\nTesting de eventos en iFrame Vamos a realizar la dinámica y ver como queda el todo con las siguientes imágenes de vista previa y debug:\n\n\u0026nbsp;\nVerificación del evento ejecutado y dispara la etiqueta a Google Analytics\n\u0026nbsp;\n\n\u0026nbsp;\nTiempo real en Google Analytics y Prueba Terminada!\n\nConclusiones Hemos realizado el primer test sencillo de como enviar eventos desde un marco (iframe) dentro de un sitio web, estamos hablando de uno básico, mismo dominio y pocos eventos de click en enlaces o un formulario.\nImagina ahora de tener que implantar uno snippet externo de booking, sí, propio para las reservas hoteleras que tanto se utilizan o utilizar EventBrite, o compras de tickets de eventos.\nBien esta parte llevaría exactamente los mismos pasos anteriores más dos configuraciones más que actualizar más adelante:\n CLIENT-ID y COOKIE: la idea es tener la misma sesión abierta en ambos sitios sin perder el usuario logueado, ni tener que a contabilizar más sesiones por usuario (y reconocerlo sobre todo) CROSS DOMAIN o multidominio: el tener que pasar por dos sitios web diferentes, parece raro pero realmente son dos páginas enmarcadas en una sola, pero con dos dominios distintas, entonces necesitaríamos utilizar la configuración también de esta, así como su exclusión como referral en Google Analytics.  Hasta aquí es todo, cuéntame tu experiencia que tal fue o se has buscado algún otro atajo con los iframe\n\u0026nbsp;\n","date":1530347193,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"c0258571d610c066fea807e56d56ed62","permalink":"https://www.marcusrb.com/guia-mediciones-de-click-desde-un-iframe-con-google-tag-manager/","publishdate":"2018-06-30T08:26:33Z","relpermalink":"/guia-mediciones-de-click-desde-un-iframe-con-google-tag-manager/","section":"post","summary":"Un post sobre la medición de eventos, cuales clicks, conversiones, formularios, transacciones desde un iframe con Google Tag Manager.","tags":["eventos iframe gtm","google tag manager","creación eventos"],"title":"[Guía] Mediciones de click desde un iframe con Google Tag Manager","type":"post"},{"authors":["marcusRB"],"categories":["Tag Manager","Digital Marketing"],"content":" Aquí la configuración optima para estar protegido desde el 25 de Mayo 2018 con la nueva ley de protección de datos y no tener problemas. Si estás aquí significa que ya has oído hablar del servicio CookieBot para administrar el GDPR en su sitio y le gustaría instalarlo con el Administrador de etiquetas de Google Tag Manager.\nTengo que darte dos noticias, una buena (¡hurra!) y una mala (¡doh!).\nLa buena noticia es que también hay una guía oficial de CookieBot sobre cómo instalarlo con Google Tag Manager (busque el enlace al final de esta guía). Esto es bueno porque los propietarios del servicio han entendido que Google Tag Manager ahora es esencial para administrar el seguimiento en los sitios.\nPero desafortunadamente hay malas noticias. La mala noticia es que la guía indicada por ellos no funciona exactamente al 100%, ya que no tiene en cuenta algunos detalles del funcionamiento técnico de Google Tag Manager, por lo que pierde uno de los principales beneficios.\nEn otras palabras, la guía oficial va a crear fallas de funcionamiento que también afectarían a las otras etiquetas de su contenedor.\n¿Qué detalle? Te lo contaré despúes de su guía oficial. En pocas palabras, probando la guía, pensando, probando, pensando y probando, a final no me gustó. Y decidí poner mi mano para resolver este problema.\nTengo que decir que gracias a otras guías de otros usuarios, finalmente pude con el, solo no hubiera logrado solucionarlo del todo. Así que logré hacerlo más poderoso sin perder el potencial de Google Tag Manager.\nAhora voy a enumerar lo que dice la guía oficial y luego lo que he cambiado y cómo lo he optimizado. PASO 1: INSTALE EL SCRIPT CON UNA ETIQUETA HTML PERSONALIZADA Lo primero que la guía dice que haga, es instalar el script de JavaScript para que funcione.\nATENCIÓN \u0026#8211;  deberá cambiar el código de identificación con el que tiene dentro del panel de CookieBot en la pestaña \u0026#8220;Sus Scripts\u0026#8221;:\nAhora que ha encontrado el código de su CookieBot, tendrá que insertarlo en una secuencia de comandos dentro de una etiqueta HTML personalizada.\nEl script es el siguiente:\n\u0026lt;script id=\"Cookiebot\" src=\"https://consent.cookiebot.com/uc.js?cbid=00000000-0000-0000-0000-000000000000\" type=\"text/javascript\"\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;script\u0026gt; function CookiebotCallback_OnAccept() { if (Cookiebot.consent.preferences) dataLayer.push({'event':'cookieconsent_preferences'}); if (Cookiebot.consent.statistics) dataLayer.push({'event':'cookieconsent_statistics'}); if (Cookiebot.consent.marketing) dataLayer.push({'event':'cookieconsent_marketing'}); } \u0026lt;/script\u0026gt; ETIQUETAS \u0026gt; Nuevo.\nTipo de etiqueta: HTML personalizado\nAgregar ACTIVADORES \u0026gt; \u0026#8220;Todas las páginas\u0026#8221;.\nGuarde la etiqueta como: \u0026#8220;Consentimiento de GDPR de CookieBot\u0026#8221;.\n¿Qué hace este script? sencillo\nCree tres eventos personalizados según la elección del usuario en las cookies:\n_preferencias (cookieconsentpreferences)\n_estadísticos (cookieconsentstatistics)\n_marketing (cookieconsentmarketing)\nProbablemente ya hayas adivinado el siguiente paso, ¿verdad?\n\u0026nbsp;\nPASO 2 \u0026#8211; CREAR ACTIVADORES Ahora que el script genera estos tres eventos personalizados, debemos capturarlos con tantos activadores de tipo de EVENTO PERSONALIZADO:\nACTIVADORES \u0026gt; Nuevo\nTipo de activador: Evento Personalizado\nNombre del evento: cookieconsent_preferences.\nGuarde con el nombre: \u0026#8220;cookieconsent_preferences\u0026#8221;.\nHaremos lo mismo para los otros dos eventos:\n_cookieconsentstatistics y _cookieconsentmarketing\n\u0026nbsp;\nPASO 3 \u0026#8211; CONECTE LOS ACTIVADORES A SUS ETIQUETAS Ahora que hemos creado tres eventos, tenemos que eliminar los activadores de tipo \u0026#8220;Página vista\u0026#8221; (es decir, Window Loaded, DOM Ready y PageView) encontrado previamente en la etiqueta, y reemplazarlos con tres eventos que hemos creado .\nPor ejemplo, puede vincularlos a estas etiquetas:\nEtiqueta principal de Google Analytics de páginas vistas\nFacebook Pixel\nHotjar\nRemarketing de AdWords\ny así para el resto de etiquetas que afecten a las cookie de marketing y de social, por ejemplo.\nA continuación se muestra un ejemplo de la página de Google Analytics.\n\nPASO 4: ACTUALIZA ACTIVADORES PERSONALIZADOS Pero, ¿cómo gestionamos todos los demás activadores, cómo un Activador en la página de privacidad (en su guía, haga este ejemplo, pero para hacerle entender que podría ser un Activador de la página de agradecimiento)?\nLa guía oficial nos dice que agreguemos el evento como un parámetro de filtro de activador:\n\nPASO 5 \u0026#8211; ACTUALIZA TODOS LOS ACTIVADORES Pero si tengo que administrar otros activadores, como clics o un enlace, ¿entonces, cómo lo hago?\nLa guía sugiere que creemos variables de JavaScript personalizadas que detectarán las opciones de los usuarios sobre los tipos de cookies que desean recibir.\nVariables \u0026gt; Variables definidas por el usuario \u0026gt; Nuevo\nTipo de variable: JavaScript personalizado\nNombre de la variable: Cookiebot.consent.marketing\nIngrese el siguiente código y luego guarde\nfunction()\u0026lt;br /\u0026gt; {\u0026lt;br /\u0026gt; return Cookiebot.consent.marketing.toString()\u0026lt;br /\u0026gt; } Lo mismo puede ser replicado por las preferencias y estadísticas\nCookiebot.consent.preferences.toString()\nCookiebot.consent.statistics.toString()\nLa función devolverá verdadero o falso TRUE o FALSE\nEl problema con esta versión es que JavaScript recibirá el valor solo después del evento Window Loaded (ventana cargada) y luego todas las etiquetas se deben administrar DESPUÉS del evento Window Loaded.\nComo puede ver en la Vista de página y DOM Ready aún no está definido.\n\u0026nbsp;\nEn la práctica, perdemos todos los eventos nativos de Google Tag Manager (Vista de página y DOM Ready) porque tendremos que activar las etiquetas solo DESPUÉS de COMPROBAR la variable de tipo JavaScript.\n###\nPor qué esta guía no está bien?\nMe parece absurdo tener que renunciar a Page View y DOM Ready, ya que van a estropear los eventos personalizados conectados a ellos.\nEn otras palabras, la guía propuesta por Cookiebot no está funcionando. Y esto no sería muy útil para aquellos que ya están utilizando desde hace años Google Tag Manager y tener que volver a re-escribir nuevamente códigos.\nVéamos como funciona el atajo, gracias a Matteo Zambon donde se puede ver desde su post en inglés de GDPR, el aporte a esta solución para los usuarios de Cookiebot y Tag Manager.\nGracias Matteo!\n\u0026nbsp;\nAhora voy a enumerar los nuevos pasos optimizados nuevo PASO 1 \u0026#8211; CREAR VARIABLES DE COOKIEBOT Antes que nada, crea una variable que contenga el código de tu CookieBot Id que hemos tomado con el PASO 1 anterior.\n\u0026nbsp;\nVariables \u0026gt; Variables definidas por el usuario\u0026gt; Nuevo\nTipo de variable: constante\nNombre de la variable: CookieBotId\nValor: ingrese el valor de su identificación\nGuardar\nAhora necesitaremos cuatro variables personalizadas más:\nCookieConsent\nCookieConsent.marketing\nCookieConsent.preferences\nCookieConsent.statistics\nnuevo PASO 2: CREAR LA VARIBLE COOKIE La CookieBot dispone de una cookie que contiene los valores de las opciones del usuario: CookieConsent\nEn este punto, sólo tenemos que tomar esa \u0026#8220;galleta\u0026#8221; con Google Administrador de etiquetas a través de una variable. Hacerlo es lo suficientemente simple\nVariables\u0026gt; Variables definidas por el usuario\u0026gt; Nuevo.\nTipo de variable: cookie propietaria\nNombre de la variable: CookieConsent\nNombre de la cookie: \u0026#8220;CookieConsent\u0026#8221;\nGuardar\nnuevo PASO 3: CREAR VARIABLES DE CONSIDERACIÓN Ahora que tenemos una variable que contiene las preferencias de cookies podemos crear tres variables para manejar los valores. Para hacerlo, usaremos un poco de astucia y expresiones regulares\nVariables\u0026gt; Variables definidas por el usuario\u0026gt; Nuevo.\nTipo de variable: tabla de expresiones regulares\nValor de entrada: {{CookieConsent}}\nPatrón: marketing: verdadero ► Salida: verdadero\nPatrón: marketing: falso ► Salida: falso\nConfiguración avanzada: solo marca ignorar mayúsculas y minúsculas\nNombre de la variable: CookieConsent.marketing\nGuardar\nVamos a repetir el mismo procedimiento para las otras dos CookieConsent.statistics y variables CookieConsent.preferences, teniendo cuidado incluso cambiar los valores en el campo \u0026#8220;Patrón\u0026#8221;.\nPero ¿Por qué estas variables?\nUhhh \u0026#8230; espera, ahora te lo diré.\nTendremos el valor de la preferencia del usuario mucho antes de la ventana cargada. En otras palabras, no habrá problemas en la gestión de Google Administrador de etiquetas de eventos predefinidos.\nYuppieeeeeeee!\n\u0026nbsp;\nnuevo PASO 4 \u0026#8211; Instala el guión con una etiqueta HTML personalizado El procedimiento es similar al primer mal paso, pero con algunos cambios para instalar la secuencia de comandos es la siguiente:\nTipo de etiqueta: HTML personalizado\nConfiguración avanzada: opciones de activación de etiqueta: \u0026#8220;Una vez por página\u0026#8221;\nActivadores: \u0026#8220;Todas las páginas\u0026#8221;.\nGuarde la etiqueta como \u0026#8220;CookieBot GDPR consentimiento\u0026#8221;.\nVEMOS EN EL DETALLE LO QUE HA SIDO CAMBIADO.\nPrimero me puse esta etiqueta se activa sólo una vez por página (por lo que se evita que se carguen dos veces).\nPasemos al segundo cambio. Si ha notado añadí otra condición antes de cada impulso en la capa de datos. De esta manera, ahora hay dos condiciones:\nla primera condición comprueba la existencia de la función de JavaScript\nla segunda condición ocurre que ya ha sido registrada y el CookieConsent que el valor es diferente de verdaderos (gracias a las Tablas expresiones regulares ya creados).\nEn otras palabras, el evento personalizado se inserta en la pila sólo si el usuario ya posee CookieConsent (y por lo tanto ya ha expresado su preferencia sobre qué tipo de cookies para aceptar) y si ese tipo de cookie (preferencias, estadísticas o comercialización) no fue aceptado.\nPor qué esto? Lo explicaré en los siguientes pasos.\n\u0026nbsp;\nnuevo PASO 5 \u0026#8211; CREATE el activador (igual a la etapa 2) Este fue el único paso realmente útil en la guía anterior.\nAhora que la secuencia de comandos genera estos tres eventos personalizados tenemos que cogerlos con la mayor cantidad de activadores de tipo Evento personalizado.\nActivadores\u0026gt; Nuevo.\nTipo de activador: evento personalizado.\nNombre del evento: cookieconsent_preferences.\nGuarde con el nombre: \u0026#8220;cookieconsent_preferences\u0026#8221;.\nVamos a hacer lo mismo para los otros dos eventos: cookieconsent_statistics y cookieconsent_marketing\n####\nPASO 6: CREAR ACTIVADORES NEGATIVOS Además de los PASO 5 Activadores, debemos crear activadores con lógica inversa.\nGeneralmente los llamo activadores negativos. Tendremos que ponerlos como los activadores de tipo de excepción en las etiquetas más comunes.\nTipo de activador: Visualización de página.\nEste activador se activa en: algunas vistas de página.\n_Activar este activado_r: CookieConsent.marketing no es igual a TRUE\nGuardar como: \u0026#8220;Fail cookieconsent_marketing\u0026#8217;.\nHaremos lo mismo para las estadísticas y preferencias.\n¿Cómo generar universal negativa ACTIVADOR DE válidas para ningún EN TODO MOMENTO\nEl ejemplo que acabamos de ver se aplica solo a la vista de página. Pero si tuviera las etiquetas que se activan al hacer clic o cualquier otro tipo de activador como lo hacemos?\nAhora te lo explicaré\nActivadores\u0026gt; Nuevo.\nTipo de activador: evento personalizado\nNombre del evento :. *\nIndicador Utiliza coincidencia de expresión regular: verdadero\nEste disparador se activa en CookieConsent.preference no es igual a la verdadera\nGuardar con el nombre: \u0026#8220;CookieConsente.preference\u0026#8221;\n","date":1527629720,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"8c243aeb4e193a0747837ffcad32c3d9","permalink":"https://www.marcusrb.com/guia-cookiebot-en-gtm-y-estar-protegido-de-la-nueva-rgpd/","publishdate":"2018-05-29T21:35:20Z","relpermalink":"/guia-cookiebot-en-gtm-y-estar-protegido-de-la-nueva-rgpd/","section":"post","summary":"Aquí la configuración optima con Google Tag Manager y CookieBot para estar protegido desde el 25 de Mayo con la nueva ley de protección de datos GDPR","tags":["rgpd","cookies","cookiebot","alerta cookie tag manager"],"title":"[Guía] Cookiebot en GTM y estar protegido de la nueva RGPD","type":"post"},{"authors":["marcusRB"],"categories":["Analítica Web"],"content":" Un sitio web de comercio electrónico o la presencia de una empresa en Internet no es un juego. Los principales objetivos siguen siendo el incremento de ventas, la reducción de costes y la fidelización de clientes. Comprender la importancia y seriedad que conlleva un proyecto en Internet es vital para alcanzar el éxito, se requiere que intervenga varias disciplinas, entre ellas la analítica web.\nAlgunas de las clásicas preguntas de clientes o propietarios de sitios web:\n ¿Por qué tengo pocas visitas? Tengo muchas visitas, pero muy pocos usuarios realizan compras. ¿Por qué? ¿Por qué hay tiendas online que sí venden y la mía no? Estoy haciendo publicidad en Internet. ¿Está bien hecha? ¿Es eficiente? ¿Cómo puedo conseguir que más usuarios se registren o me envíen consultas?  Para conocer el problema existente y dar una respuesta se recurre a la analítica web, igual que, salvando las diferencias, cuando una persona tiene un problema de salud o un deportista necesita mejorar su rendimiento recurre a una serie de pruebas y análisis para conocer su estado y determinar qué necesita para mejorar.\nVolviendo al campo empresarial, veamos algunas diferencias entre el mundo offline y el mundo online que nos ayudarán a comprender la necesidad de la analítica web y saber más sobre ella.\n\u0026nbsp;\nPublicidad en el mundo offline: Ejemplo 1 \u0026#8211; La clínica veterinaria Imaginemos una clínica veterinaria y para promocionarla se ha decidido contratar una valla publicitaria en un emplazamiento de una ciudad por el que transitan muchos vehículos y personas. Le han estimado que la valla será vista por más de 1000 personas al día.\n la contratación de esa valla es de 700 euros al mes per un periodo de 12 meses producción gráfica y fijación es de 1000 euros  En total el primer año se invertirá 9400 euros por este anuncio. Estimando unas 1000 personas que verán este anuncio cada día, podría tener muchos clientes en poco tiempo. Pero tendríamos que cuestionarnos relacionadas con la efectividad de esa publicidad.\n1 \u0026#8211; ¿Cuántas de esas personas pasan delante del anuncio lo ven?\nLa manera de saberlo es realizar algún tipo de encuesta en la calle para preguntar a las personas que se fijaron en el anuncio. Periódicamente durante los próximos 12 meses para conocer su efectividad.\n2 \u0026#8211; De las personas que ven el anuncio, cuántas tenían mascotas o están interesadas en este tema?\nPodemos basarnos en informes sobre el % de mascotas en la zona para hacer una estimación. Pero saber con exactitud cuántas personas que pasan por la valla están interesadas en mascotas, puede ser complicado.\n3 \u0026#8211; ¿Cuántas personas han ido a su clínica gracias al anuncio?\nPara este dato la única manera es preguntar directamente a cada uno de los clientes que acudan a la clínica si llegaron allí gracias a la valla publicitaria\u0026#8230;un poco incómoda cómo pregunta.\n\u0026nbsp;\nLas respuestas a estas preguntas nos permiten saber cómo de eficiente o rentable está siendo dicho anuncio y si es necesario hacer algún cambio.\nNo es lo mismo invertir 9400 euros en un año para 3000 clientes potenciales y nos proporcionaría solo 56 clientes reales, que sólo nos vean 700 clientes potenciales y nos proporcione 12 clientes. ¿Cómo podríamos sacar este dato con tanta precisión?\n\u0026nbsp;\nPublicidad en el mundo offline: Ejemplo 2 \u0026#8211; Un supermercado Sara es propietaria de un gran supermercado. En él vende productos de alimentación, higiene, cosmética, etc.\nSara puede saber qué productos son más o menos vendidos gracias a los resultados de las ventas, pero lo que no puede saber es cuántos clientes tenían interés por un producto de la marca A y finalmente eligieron la marca B. Para averiguar esto hay dos opciones: realizar encuestas o colocar sensores en una zona de los estantes de los productos para observar constantemente qué hace el cliente frente a éste. Pero ¿hacemos encuestas de todos los productos para saber qué piensan los clientes de cada uno de ellos? o ¿invertimos algo en tecnología para sensores, extraer datos, analizarlos como científicos de datos y crear modelos matemáticos de cada unos de los productos?\nSería interesante conocer qué hacen los clientes en el supermercado, en qué productos se fijaron y cuál compraron finalmente o qué secciones son las que más visitan. Con estos datos podríamos descubrir los intereses de los clientes, detectar posibles problemas que hacen que elijan un producto en lugar de otro o, incluso, lanzar ofertas relacionadas con esas secciones que visitan mucho pero en las que no suelen comprar.\nSin embargo, no hay manera de tener todos esos datos de forma precisa y menos de forma individualizada.\n\u0026nbsp;\nPublicidad en el mundo online: Datos y mediciones En el mundo offline no es fácil obtener datos precios de todo lo que ocurre, Sin embargo, en el mundo online todo lo que hacemos puede quedar registrado, así que la cantidad de datos disponible llega a ser desbordante, veáse también el problema de la privacidad. Internet es el mundo perfecto para conocer todo acerca de los usuarios y disponer de datos suficientes para analizar qué está ocurriendo y qué hacer para mejorar.\nVéamos que podemos registrar y conocer:\n dónde se hace clic qué palabras se escribieron en el buscador para llegar a nuestro sitio web país o ubicación geográfica el que se conecta tiempo que permanece viendo uan página web páginas web o productos que visitó número de usuarios que visitaron una página web porcentaje de usuarios que hicieron una compra, solicitud o reserva número de usuarios que provienen de anuncios de pago páginas de sitios más visitadas zonas de páginas web que más o menos captan la atención datos demográficos por rango de edad o sexo y más datos  \u0026nbsp;\n¿Qué podemos determinar con esto?  qué páginas debemos mejorar la usabilidad si tenemos que realizar el sitio web en otros idiomas si el proceso de compra debe ser modificado mejorar el ratio de conversión y optimizarlo la efectividad o rentabilidad de campañas de pago evolución del tráfico de un buscador o orgánico en caso de productos, cuáles son lo que no interesan o presentan problemas determinar acciones que no proporcionan más visitas, ventas o solicitudes repercusión de nuestra marca y reputación online comportamiento de los usuarios según datos demográficos y más  A tener en cuenta que para contestar a cada una de las posibles preguntas o más, es necesario registrar los datos correctamente y analizarlos, así como descubrir nuevas oportunidades, o insight dentro de la análitica web y mejorar la rentabilidad y el objetivo empresarial.\n","date":1526390427,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"bb93f81b864346cd3b8a86903aecc15d","permalink":"https://www.marcusrb.com/por-que-tengo-pocas-visitas/","publishdate":"2018-05-15T13:20:27Z","relpermalink":"/por-que-tengo-pocas-visitas/","section":"post","summary":"Un sitio web de comercio electrónico o la presencia de una empresa en Internet no es un juego. Los principales objetivos siguen siendo el incremento de ventas, la reducción de costes y la fidelización de clientes.","tags":["sesiones web","metricas visitas","aumentar trafico"],"title":"¿Por qué tengo pocas visitas?","type":"post"},{"authors":["marcusRB"],"categories":["Tag Manager"],"content":" Web Analytics: 10 razones para usar Tag Manager en su análisis de marketing Google Tag Manager es una de las herramientas más innovadoras en la web para una gestión avanzada de nuestro análisis de marketing . Para aquellos que nunca han oído hablar de él, es una herramienta principalmente dedicada a los analistas de marketing y gerentes de marketing, que le permite llevar la Optimización de la tasa de conversión o CRO (Conversion Rate Optimization), la optimización de los procesos de conversión dentro de un sitio web en un nivel que de otro modo solo se puede alcanzar mediante el uso de herramientas a menudo bastante costosas.\nCon Google Tag Manager es posible, entre otras cosas, rastrear y transmitir fácilmente a su análisis o herramientas de administración de marketing digital (incluyendo Google Analytics, por supuesto, pero también Facebook Analytics) cualquier tipo de comportamiento, desde ver una página web hasta haga clic en una llamada a la acción, desde compartir en redes sociales hasta el embudo completo de compra de comercio electrónico.\nTodo lo que muchas herramientas de analítica , apropiadamente programadas, hacen de forma independiente, por supuesto.\n¿Por qué, entonces vale la pena cambiar a Google Tag Manager? Aquí están unas 10 razones, y hay más! \u0026nbsp;\n1 # La solución ideal para el hombre (o la mujer) que no debe preguntar, nunca \nLa primera, la más importante de las ventajas que descubre de inmediato quién pasa a Google Tag Manager. Aquí está: libérate de la esclavitud de preguntar.\nEs un clásico, ¿no?\nSu cliente le confía la gestión de los procesos de marketing y optimización dentro de su sitio web. Sin embargo, aunque siempre hay uno, el mantenimiento técnico del sitio se confía a una empresa externa , que es una de las miles de responsabilidades del departamento interno de TI de la compañía, que (como siempre) tiene mucho que hacer y (como siempre) no él tiene algo de tiempo para perder con \u0026#8220;los de marketing\u0026#8221;.\nSituación clásica, por decir lo menos: levante la mano que no la haya experimentado al menos una vez, de un lado o del otro de la barricada. En resumen, imagine al pobre analista que enfrenta esta situación: decenas de llamados a la acción para rastrear , uno o más embudos de conversión para analizar, quizás (en un comercio electrónico), todo el proceso de compra que se gestionará y un administrador de TI reticente eso no te permite trabajar al ritmo y con la precisión con la que fueras prefijado. ¿Qué hacer?\nLa solución es: cambiar a Google Tag Manager. De esta forma, finalmente podrá trabajar libremente si cree que debe etiquetar cualquier elemento del sitio web de su cliente (siempre que se realice al menos decentemente, por supuesto) sin tener que recurrir a las intervenciones del desarrollador o del webmaster.\nMejor que eso!\n###\n2 # Puedes planificar mejor tu trabajo ¿Qué implica todo esto?\nEn primer lugar, implica la posibilidad de planificar su trabajo de análisis de datos mucho más fácilmente, explotando el potencial de Google Tag Manager para supervisar sistemáticamente las rutas internas al sitio web que desea analizar, decidiendo de vez en cuando el nivel de detalle del análisis.\nPor ejemplo?\nPiensa en una llamada a la acción . Obviamente, querrá ser monitoreada de una manera muy específica, separándolo de otra llamada a la acción en la misma página. Ahora, en cambio, piense en los contenidos relacionados de un blog. En este caso, no le interesará saber, probablemente, qué contenido en particular ha sido objeto de clics, pero solo necesita saber el número total de clics en los correlatos tomados en general. Aquí: Tag Manager le permite diversificar el nivel de análisis en relación con sus objetivos .\nY para hacerlo fácilmente (lo que nunca duele).\n###\n3 # Puede hablar con Google Analytics en su \u0026#8220;lengua\u0026#8221; Otra ventaja considerable: a través del Tag Manager puede usar fácilmente todos los scripts que le permiten enviar datos y parámetros a Google Analytics sin escribir una línea de código. En su lugar, encontrará interfaces convenientes que le permitirán trabajar sin problemas y sin errores, tanto para mejorar y adaptar a sus necesidades el código de seguimiento de Analytics, tanto para rastrear eventos y eventos sociales con simplicidad, utilizando las variables proporcionadas por Google Tag Manager para hacer los parámetros y variables dinámicas.\n\n###\n4 # Puedes probar nuevas soluciones sin dañarlas. Además, puedes regresar cuando quieras Y dime que esto no es una ventaja.\nGoogle Tag Manager funciona con dos herramientas extraordinarias. El primero es la vista previa: le permite probar cualquier solución y verificar la viabilidad y cualquier defecto antes de entrar en producción. Una ventaja no poco, que comprenderá fácilmente a cualquiera que tenga sus manos al menos una vez en su vida en una línea de código.\nEl segundo, igualmente importante, es el versionado . En la práctica, Tag Manager mantiene todas las instancias que hemos publicado de nuestra cuenta en su base de datos. De esta forma, si nos damos cuenta demasiado tarde de que hemos hecho algunas tonterías, podemos volver fácilmente a la última versión operativa .\n\n###\n###\n5 # Puedes centralizar todas las etiquetas ¿Alguna vez intentó dibujar una serie de eventos que pueden suceder en cualquier lugar de su sitio web , por ejemplo, hacer clic en una Llamada a la acción? Hay muchas formas de hacer esto. Si no tiene experiencia en programación, probablemente deba buscar todas Llamadas a la acción presente en su sitio y asociarlo con un evento que se transmitirá a Google Analytics. Si, en cambio, tiene alguna experiencia en programación, tal vez no pueda hacer este esfuerzo, tratando de configurar una función que presida automáticamente esta tarea.\nO puede usar Google Tag Manager. De esta forma, la tarea de reaccionar dependerá del clic listener . En su lugar, tendrá la tarea, mucho más sencilla, de explicar al Administrador de etiquetas qué evento debe transmitir a Google Analytics en relación con el clic en el sitio.\nMás fácil que eso!\n###\n6 # Puedes reutilizar los códigos y soluciones A veces sucede que tiene ideas brillantes. Gracias a la modularidad de Google Tag Manager, podrá utilizar estas ideas y soluciones una y otra vez. Sí has entendido bien, el mismo activador, por ejemplo el de formulario enviado, puedes utilizarlo para eventos de Google Analytics, para la etiqueta de conversiones de Google AdWords, para el pixel de Facebook u otro pixel de terceros. Y porque no, utilizarlo también para eventos dinámicos, ejemplo, comercio electrónico, donde alguna variable comparte etiqueta, o en Remarketing dinámico. Esta es una ventaja bien útil además, puesto que no tienes que meter mano al código nunca (bueno sí, a principio), pero al resto piensa todo GTM.\n###\n7 # Puede usar funciones, cálculos, variables y transmitir sus resultados a Google Analytics con facilidad Google Analytics, usted sabe, solo permite unos pocos meses, y solo en algunos casos, realizar cálculos con las métricas en su base de datos, o manipular su tamaño gracias al uso de filtros avanzados. Cualquier entrada que deseemos dar a Analytics, en resumen, esto probablemente debería haber sido procesado previamente.Imagine, por ejemplo, que desea importar en Analytics un determinado tamaño presente en el código de su sitio , quizás dentro de una meta etiqueta o en una ubicación específica de la URL.\n¿Cómo lo vas a realizar?\nUna solución es calcular esta cantidad a través de una o más funciones para procesarla dentro del código de su página e importar el resultado en Analytics gracias a una modificación del código de seguimiento.\nEl otro? Puedes delegar la tarea en Tag Manager , que proporciona toda su base de datos de variables personalizadas y dedicada para este fin.\n###\n8 # Finalmente puedes simplificar el trabajo de etiquetas en ecommerce ¿Quién ha intentado elaborar las funciones de Google Analytics que presiden las diversas etapas del proceso de compra? Nada fácil, ¿verdad? El comercio electrónico mejorado es una de las herramientas más complejas para configurar en Google Analytics, ya que requiere la activación de varias funciones que presiden la búsqueda y envío de datos en tiempo real, a medida que el usuario avanza en su Customer Journey .\n\nGoogle Tag Manager simplifica enormemente este proceso y, en algunos casos, lo hace posible incluso sin agregar una sola línea de código al sitio.\n\u0026nbsp;\n9 # Puede usar fácilmente los nuevos píxeles de conversión de Facebook en relación con los eventos Otro aspecto que afectará, creo, a muchos colegas. Con Google Tag Manager puede simplificar en gran medida la implementación de los nuevos píxeles de conversión de Facebook , e incluso puede configurarlos sin cambiar el código del sitio.\n\u0026nbsp;\n10 # Finalmente puede etiquetar su sitio para explorar el comportamiento del usuario con facilidad Por último, pero no menos importante: con el Administrador de etiquetas de Google, puede marcar cualquier botón , cualquier comportamiento, cualquier interacción de nuestros usuarios en su sitio, comenzando, por ejemplo, desde interacciones sociales , que a menudo permanecen indescifrables para Google Analytics. En resumen, tendrá la capacidad de rastrear toda la experiencia del usuario , haga clic para hacer clic: algo invaluable, especialmente para la determinación de casos de uso y perfiles de usuario en los sitios más complejos.\nUn engranaje adicional en análisis de marketing En resumen, se habrá te habrás dado cuenta que: con Tag Manager, el trabajo de rastrear y analizar un sitio web no solo es más sencillo, sino también más funcional, ya que le permite realizar prácticamente cualquier seguimiento que desee , manteniendo el nivel de análisis de datos una capa sobre la del código del sitio. Entonces, si normalmente logra involucrar el dpto TI en su proyecto de seguimiento, solo podrá ganar aún más eficiencia, un tema que hoy, en mi opinión, está en todos lados.\nSi también usa Google Tag Manager, ¿agregaría algunos puntos a esta lista? Cuéntame sobre tu experiencia con esta increíble herramienta en los comentarios.\n\u0026nbsp;\nFuente original:** https://it.semrush.com/blog/google-tag-manager-10-buoni-motivi-per-usarlo/\n**\n","date":1525792754,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"a7ae47b929021046b64873ded274a7cd","permalink":"https://www.marcusrb.com/google-tag-manager-10-buenas-razones-para-usarlo-bien/","publishdate":"2018-05-08T15:19:14Z","relpermalink":"/google-tag-manager-10-buenas-razones-para-usarlo-bien/","section":"post","summary":"Google Tag Manager es una de las herramientas más innovadoras en la gestión avanzada de nuestro análisis de marketing","tags":["configuración google tag manager","mejoras tag manager"],"title":"Google Tag Manager: 10 buenas razones para usarlo bien","type":"post"},{"authors":["marcusRB"],"categories":["Tag Manager"],"content":" ¿Quiéres aprender a crear un CRM con Google Sheet, pasar valores dinámicos con Google Tag Manager y tener un seguimiento de eventos? Será un tutorial dividido en dos partes, la primera la explicación de como crear en Tag Manager variables, personalizadas y dinámicas, ejemplo al enviar un formulario para Leads, y de otro lado enlazando Google Sheet y tener toda la información (excluyo la información sensible en virtud de la nueva ley GDPR no haré mención a datos sensibles), además a tener en cuenta que tampoco llevaré estos datos a Google Analytics para evitar penalizaciones (nunca me ha pasado y tampoco quiero arriesgarme). Pronto también el vídeo con el tutorial, esté atento en mi canal YouTube\nPreparando las variables en Google Tag Manager Antes de empezar, hay que tener en cuenta unos cuantos puntos, si eres nuevo en Google Tag Manager, seguramente llegarás a final de este párrafo y te irás llorando, porque hablaré de Tag Manager a nivel más avanzado. En caso de dudas sobre etiquetas, variables e incluso el mismo Tag Manager, te recomiendo hagas una primera visita aquí. \nSi ya estás acostumbrado a extraer informaciones a nivel más avanzado, y conoce de DOM, HTML y CSS, además de JavaScript, entonces, bienvenido.\nComenzaré hablar de como tener un cuadro de seguimiento, y podría ampliar más informaciones y campos, pero me conformo con este CRM básico\n\nSegundo paso tengo este formulario básico de momento justo para alinearme a mi CRM y luego explicaré como ampliar con más informaciones y detalles\n\nPara extraer informaciones desde un DOM, es necesario tener bien claro una cosa, realizar lo siguiente:\nconsola de desarrollador  pruebas con ID, clases o Selector CSS  aplicar a más de uno y hacer pruebas Una vez que tengamos los campos necesario para extraer los valores de los campos del formulario, podemos inclusive determinar otras KPI\u0026#8217;s importantes y útiles:\n abandono del formulario creación de un funnel y determinar los porcentajes de campos no rellenados vs completo tiempo de permanencia en el formulario y medición de duración en segundos desde el primer click hasta el envío test A/B en el mismo con diferentes opciones y determinar cuál de los formularios mejora el %ratio de conversión etc\u0026#8230;  \u0026nbsp;\nEntramos ahora con el concepto de extraer la información en una alerta de consola y ver los valores Una vez dentro de nuestro Tag Manager vamos con la creación de nuestras variables, primero ha de comprobar los campos del formulario (si existen ID, clases o selector CSS), y ver su activador de tipo bóton de envío formulario (tipo HTML, jQuery o Ajax, modal, iframe, etc).\nCreación de la variables Este es el código fuente de mi formulario:\n\nY este es como veo los campos DOM en la Consola de Google Chrome:\n\nAhora bien que tengo los campos de nombre, apellidos, dirección, género, email y comentarios, incluido botones de enviar y resetear, iré a lo práctico.\nCrearé las variables de tipo Javascript Personalizado y veámos como función la vista previa para el primer campo NAME.\nSi para extraer el valor en el campo, la query será: document.querySelectorAll('[id=classicForm]')[0][0].value Resultado:\n\nAhora bien, mi CRM puedo rellenar de valores cuáles, también datos personales y sensibles, así que me limitaré a asignarle solo valores booleanos: TRUE o FALSE, siempre y cuando la condición será \u0026gt; enviar el formulario rellenado y no vacío.\nel código así reescrito es:\n\n###\nEn práctica asigno una variable a mi selector, primer campo y lo confronto con un valor vacío \u0026#8221; \u0026#8220;, si no es así, entonces el valor de retorno será TRUE sino será FALSE.\nCuídado, el valor por defecto es UNDEFINED, porque no está asociado a ningún activador, así que siempre tendré en cuenta que el formulario será enviado (para su validación total).\n\u0026nbsp;\nMODO AVANZADO\nTambién puedo crear una variable que directamente hará el trabajo de validación y con este script le digo de sacarme el valor Verdadero:\nEn cada elemento tendré el parámetro Validity y según sea un campo obligatorio o no, tendré varias opciones\n\nAsí que resumiendo nuevamente tenemos:\ndocument.querySelectorAll('[id=classicForm]')[0] y siguiendo puedo extender con la validación\ndocument.querySelectorAll('[id=classicForm]')[0][0].validity.valid y esto me dará el resultado TRUE o FALSE\nAhora hacemos lo mismo para todos los campos interesados:\nNombre, Apellidos, Dirección, Género, Email, Comentarios.\nSolo necesitamos replicar al igual que el campo \u0026#8220;nombre\u0026#8221; los demás que quedan, ya que todos los campos de textos son de igual tipologia, serán exactamente iguales que el primer ejemplo, solo que tenemos que pasar de [0][0] a [0][1] y así hasta llegar al campo [0][5]\n\u0026nbsp;\nCreamos la etiqueta y activador Pasamos de crear las variables de los campos del formulario, y necesitaríamos el activador, en este ejemplo, mi formulario, tiene onsubmit=ValidationEvent() , en tu caso puede que el submit tenga una ID o una CLASS, o que el formulario esté creado con jQuery, así que vamos por lo más sencillo.\nMi ejemplo es un Evento Personalizado del tipo Submit con el valor ValidationEvent() y todo lo que necesito es enviar la información recogida hacía Google Sheets.\nSí, falta algo, crear un conector entre Google Tag Manager y Google Sheets, y para ello utilizo una etiqueta tipo PIXEL, que en su defecto sería la etiqueta Imagen\nEl conector que vamos a crear es con Google Apps Script. \nEntramos en Google Drive \u0026gt; Herramientas \u0026gt; Editor de secuencias de comandos y creamos el script personalizado:\nque pueden descargar aquí\nInsert your content here Muy importante cambiar los dos primeros valores: SHEET_NAME: nombre de la hoja donde tenemos los campos a rellenar (en mi ejemplo se llama Sheet1)  SHEET_KEY: El valor ID de la hoja de cálculo lo que sigue /d/XXXXXXXXXXXX sería a rellenar \nAhora solo hace falta Guardar \u0026gt; Publicar \u0026gt; Publicar como aplicación Web\nImportante seleccionar el nivel de acceso a todos. Copiamos el código del script y seguimos\n\u0026nbsp;\nCreamos la variable de Google Apps Script en Tag Manager Seguimos con la creación de la etiqueta Pixel, o imágen personalizada añadiendo la variable conector.\nDefinimos la variable de tipo Constante con este valor Google Apps Script, y volvemos a la etiqueta\n\n\nel código tendrá esta estructura\n{{var_SCRIPT_GOOGLE_APPS}}?nombre_campo1={{var_1}}\u0026nombre_campo_2={{var_2}} etc etc  Si nos fijamos, el primer campo es la constante con la url de Google Apps Script, separamos con un ? y seguimos con los mismos nombres de la primera linea de Google Sheets (si tenemos NAME será NAME y si será NOMBRE entonces pondríamos NOMBRE,) si fallamos esta parte no leerá la correspondencia entre los nombre de las primera file de Google Sheets con las variables donde hacer pasar los valores.\nEjemplo?\nPara comprobar su correcto funcionamiento podemos hacer lo siguiente:\nuna ruta nueva con la URL de Google Apps Script y la secuencia\n\nResultado?\n \n\u0026nbsp;\nVista previa y Publicación: Testing Vamos con la vista previa y si todo funciona correctamente tendremos valores TRUE y FALSE en nuestro CRM, o los valores correspondientes en caso de no tener que pasar por la validación Booleana.\nEl orden de los valores que siguen después \u0026amp; es indiferente, siempre será una concatenación y los valores se pondrán en su sitio correspondiente en la hoja de cálculo.\n\u0026nbsp;\n###\n###\n###\n###\n###\n###\n###\nAquí el webinar sobre como crear un CRM y pasar las variables dinámicas desde Google Tag Manager a nuestro Google Sheets  ","date":1525267936,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"e02d1962078d618020e54afaac2072d7","permalink":"https://www.marcusrb.com/quieres-aprender-a-crear-un-crm-con-google-sheet-pasar-valores-dinamicos-con-google-tag-manager-y-tener-un-seguimiento-de-eventos/","publishdate":"2018-05-02T13:32:16Z","relpermalink":"/quieres-aprender-a-crear-un-crm-con-google-sheet-pasar-valores-dinamicos-con-google-tag-manager-y-tener-un-seguimiento-de-eventos/","section":"post","summary":"Google Tag Manager es una de las herramientas más innovadoras en la gestión avanzada de nuestro análisis de marketing","tags":["integrar google script apps tag manager","extraer información de formularios a google tag manager","pasar valores dinámicos a google sheet","integrar google script apps tag manager"],"title":"Realizar un CRM con Google Spreadsheets con Google Tag Manager","type":"post"},{"authors":null,"categories":["Google Ads"],"content":" ¿Quieres ver como he realizado una optimización de una cuenta AdWords para el sector de reparaciones móviles y pantallas de smartphones? No te pierdas mi próximo webinar de una hora de duración el próximo\n Jueves 10 de Mayo 2018 a las 19:00h   . Se comentará en directo de cómo extraer datos de una cuenta de AdWords y desde Analytics, crear patrones para organizar mejor los grupos de anuncios, las campañas y las palabras claves. Dar un nuevo enfoque a los anuncios de textos y aprovechar del Remarketing y Shopping.\nSe ha logrado pasar de una inversión (mal aprovechada) de 3000 euros al mes , facturando lo justo para pagar el alquiler del local en Madrid, a convertirse en un negocio de nicho muy rentable y logrando invertir más de 15000 euros al mes, con un ROI x 3\nTe espero el próximo jueves en Hangout de YouTube y en Facebook\n","date":1525101437,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"033475dc334f1da8e012abd9663b446a","permalink":"https://www.marcusrb.com/realizando-auditoria-y-optimizacion-adwordsreparacion-moviles/","publishdate":"2018-04-30T15:17:17Z","relpermalink":"/realizando-auditoria-y-optimizacion-adwordsreparacion-moviles/","section":"post","summary":"¿Quieres ver como he realizado una optimización de una cuenta AdWords para el sector de reparaciones móviles y pantallas de smartphones?","tags":["auditoria adwords","gestión campañas adwords","optimización cuenta adwords","webinar optimización adwords"],"title":"Auditoría en Google Ads - Reparación Móviles","type":"post"},{"authors":["marcusRB"],"categories":["Tag Manager"],"content":" Durante más de 5 años he estado utilizando Google Tag Manager en auditorias de analítica web, a mis clientes y sobre todo actualizándome constantemente con el fin de impartir clases presenciales y a distancia, y siempre veo que hay una \u0026#8220;negación\u0026#8221; hacía el aprendizaje de este gestor de etiquetas o \u0026#8220;administrador de etiquetas\u0026#8220;.\nHoy hablaremos de como efectuar una migración correcta hacia Tag Manager sin arriesgar el trabajo de otras agencias o consultoras, o de nuestros compañeros de otros departamentos, que han realizado algunas tareas de \u0026#8220;etiquetado manual\u0026#8221;, y con esto quiero decir código \u0026#8220;a saco\u0026#8221; dentro del site, y por compañeros me refiero a internos y externos que sean, ya que pueden ser de diferentes departamentos:\n departamento IT: para sus pruebas en los sitios web o en partes para verificar el correcto funcionamiento de las aplicaciones departamento Marketing: que además de utilizarlo en la medición de campañas con \u0026#8220;pixel\u0026#8221; de diferentes canales (SEM, SEO, social, afiliados, referenciados etc) departamento Administrativo / Comercial: que podría estar midiendo campañas y pasarlo a sus CRM\u0026#8217;s departamento de Analytics: para etiquetar todos los eventos e interacciones de usuarios, usabilidad UX y CRO y más que podrían ser otros interesados en la creación de tareas personalizadas  Como puedes apreciar, el uso de Google Tag Manager podría abarcar todas estas tareas, y obviamente, para poder realizar una migración hacía a ella, tendríamos tener además de experiencia técnica, para que podamos realizar no solamente la \u0026#8220;medición de Analytics\u0026#8221; y stop, sino hay unas series de tareas que cualquier departamento puede realizar y por este motivo vamos por partes.\n\u0026nbsp;\n\u0026nbsp;\nDefinición de un plan de medición Ante de comenzar con un plan de migración, necesitaríamos ver y poder realizar de forma resumida, un plan de medición para poder realizar sucesivamente comprobaciones de implementaciones existentes o tener que crearlas.\n¿No sabes que es un plan de medición?\nPues, el plan de medición resumido, al igual que este pantallazo aquí abajo, representaría como alcanzar nuestro objetivo empresarial, siguiendo varios pasos y después de seguir unas series de configuraciones previas en el sitio web, herramienta a utilizar y medición de resultados.\n\n Definición del plan de medición Documentar los aspectos técnicos Creación de un plan de implementación Implementación Mantenimiento y mejora continua \u0026#8211; Optimización vuelve a principio  Entre un test y otro del punto 5 y 6, se crean diferentes dashboard, o cuadros de mandos con las KPI\u0026#8217;s a seguir y alcanzar. Aquí entrarían los factores para extrapolar las informaciones, detectar insights y mejorar con los test A/B, CRO, UX etc.\n\u0026nbsp;\n¿Cómo compruebo las implementaciones anteriores? Una vez que tengamos el plan de medición y verificada la parte técnica (dedicaré otro post para cada uno de los pasos), realizaremos las implementaciones oportunas (vía código, vía GTM, o lo que haga falta en las herramientas). Pero entraría aquí el aspecto de comprobar los trabajos anteriormente realizados.\nPodemos utilizar algunas de las herramientas aquí mencionadas:\n Google Analytics: Hablando de Google Analytics, sería nuestra primera herramienta a verificar si efectivamente existen unas implementaciones realizadas, porque, en tal caso ya estaríamos recogiendo estos datos y de una manera estarían llegando a nuestra herramienta de análisis. Plugin de terceros: Podríamos tener unos plugin instalados que estarían ya haciendo el trabajo por nosotros, en caso de migración podríamos o bien crear conflictos, o bien cargarnos el trabajo realizado. Aplicaciones para navegador: Con las aplicaciones o complementos para navegadores (Chrome, Firefox, principalmente), podríamos chequear unas series de aspectos cuáles implementaciones y recogidas de valores dentro del site Informes anteriores: Con los informes ya podríamos tener una idea de lo que se está realizando o lo que no, así alinearnos con las KPI\u0026#8217;s y el plan de medición.  \u0026nbsp;\nEn practica tenemos que fijarnos en algunos apartados dentro de Google Analytics, exactamente dentro de la interfaz podemos ver cuáles:\n conversiones comportamiento dimensiones o métricas personalizadas agrupación de contenidos eventos personalizados  \u0026nbsp;\nCon los plugin implementados en los gestores de contenidos: WordPress, Prestashop, Joomla, Magento, Shopify etc, pueden darnos mucha información, tenemos que ver que están realizando para no duplicar información\nCon las aplicaciones, tenemos que verificar principalmente que se está realizando, aquí comentaré 3 principales:\n Google Tag Assistant para comprobar las etiquetas existentes, eventos recogidos, incluso podemos realizar grabaciones y comprobaciones de lo que se está midiendo Google Analytics Debug: nos dará mucha información sobre la etiqueta de Google Analytics dentro del site y muchos objetivos creados. Analytics pros DataLayer: este último probará a extraer informaciones muy valiosa del site, es algo más avanzado a primera vista pero será útil a más de uno  \u0026nbsp;\nCon los informes anteriores realizados, panel de control o informes en Google Data Studio, podemos comprobar si existen dimensiones o métricas adicionales para poder ver si efectivamente tenemos eventos creados, interacciones o objetivos creados a medida.\n\u0026nbsp;\nRealizamos una práctica y ver si efectivamente hay etiqueta/s en un sitio web \n\u0026nbsp;\nYa que llegamos al final de post, hemos realizado todo tipo de comprobaciones para su correcta migración de etiquetas existentes, eventos o interacciones, conversiones y objetivos, hasta incluso del tipo transaccionales de tipo comercio electrónico.\nUna vez llegado a este punto, tenemos que crear un informe como este que se muestra a continuación, el que puedes realizar una copia para poder tener todo bajo control y una migración perfecta. Para su compilación ya he realizado un video explicativo donde cada una de las celdas tendremos que asignar:\n nombre de la etiqueta donde activarlo cuál variable añadir qué objetivo medir y donde migrarlo  ¿Dudas? Puedes comentar aquí mismo sobre su migración\n[https://docs.google.com/spreadsheets/d/e/2PACX-1vQ4z5jRtI-D7gjTcVAcKqP_Lf3Gdrp9a7-lTI6fer7gouhDWm1lLOMUu2KarRU7oqOUsG4KQbt-XmEp/pubhtml?widget=true\u0026amp;headers=false]\n","date":1525036683,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"81cb95f5a75c3cff6c720e69cdb70a7d","permalink":"https://www.marcusrb.com/como-efectuar-correctamente-una-migracion-a-google-tag-manager/","publishdate":"2018-04-29T21:18:03Z","relpermalink":"/como-efectuar-correctamente-una-migracion-a-google-tag-manager/","section":"post","summary":"Hoy hablaremos de como efectuar una migración correcta hacia Tag Manager sin arriesgar el trabajo de otras agencias o consultoras","tags":["etiquetas google tag manager","migración tag manager","plan implementación","plan tag manager"],"title":"Como efectuar correctamente una migración a Google Tag Manager","type":"post"},{"authors":null,"categories":["Google Ads"],"content":"Después del post con las preguntas del examen de Google AdWords fundamentos, en este hay algunos de los test con las preguntas de Google Adwords Avanzado.\nHace más de cinco años que realizo test para la certificación oficial de Google AdWords, además de otros cursos para Google Partners, que ahora se llama Academy For Ads. Durante todos estos años he visto diferentes programas de agencias para freelance, agencias y consultoras, pasando de Google Aqua, Google Engage a Google Partners y ahora cambiando completamente de plataforma para unificar cursos de digital marketingl, quedandose la mismas estrategias de AdWords, la de fomentar el uso de sus herramientas, ampliando de contenido para aprender y certificarse.\nDecidí en junio 2017 abrir una lista de reproducciones a mi canal de YouTube, hasta aquella fecha abierta hace años y ni utilizada, y puse online mis primeras grabaciones de los exámenes, para \u0026#8220;facilitar\u0026#8221; aquellos estudiantes de mis cursos a aprobarlos, obtener así la certificación, además de aprender la teoría, y muy bien aprendida, con mis cursos online de AdWords, efectivamente era, y sigue siendo una buena manera de obtener la certificación.\nNo pido dinero a cambio, ni lo hago por publicidad (encima con los cambios actuales de YouTube tampoco monetizo). Lo hacía por pura diversión con la idea de compartir recursos, lo aprendido y ser de una manera reconocido en este \u0026#8220;mundillo\u0026#8221;, que ya llevo bastante años la verdad, casi 10 y solo que he cambiado de dominios (muy mal para el SEO, ya sé).\nHe recibido amenazas, denuncias, me banearon\u0026#8230;pero sigo allí, y no me importa quien me reputa como \u0026#8220;estafador\u0026#8221;, justamente hablan muchos quien cobra por ello, o tiene agencias grande que se dedican a esto (a vender los exámenes, si, esto!). Y no me importa, luchar contra el grande Google, que gracias a estos exámenes está llevando a la ruina un sistema que ellos mismos crearon tantos comerciales, tantos vendedores de sus productos\u0026#8230;y con creces que sigan estafando a muchos pequeños clientes y no solo, que otro día lo explicaré.\nAsí comparto las preguntas de los exámenes oficiales, a través de páginas en Google Form, que normalmente realizo durante mis cursos. Son varios módulos, de momento las tres partes de Google Avanzado, y seguiré añadiendo Google Shopping, Mobile, YouTube, Display y las Preguntas de Google Analytics 2018.\nDe momento pueden tener acceso a las preguntas\u0026#8230;y para las respuestas puedes seguir uno de mis vídeos a esta lista y SUBSCRÍBETE.\nGracias y Good Luck!\n\u0026nbsp;\n Listado de preguntas examen Google AdWords Avanzado de Búsqueda 2018 \u0026#8211; 1 Listado de preguntas examen Google AdWords Avanzado de Búsqueda 2018 \u0026#8211; 2 Listado de preguntas examen Google AdWords Avanzado de Búsqueda 2018 \u0026#8211; 3  ","date":1524757661,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"481da8f4da3538b174e8d7103ffb14c7","permalink":"https://www.marcusrb.com/test-examen-de-google-ads-avanzado-de-busqueda/","publishdate":"2018-04-26T15:47:41Z","relpermalink":"/test-examen-de-google-ads-avanzado-de-busqueda/","section":"post","summary":"Después del post con las preguntas del examen de Google AdWords fundamentos, en este hay algunos de los test con las preguntas de Google Adwords Avanzado.","tags":["adwords certificación","examen de google ads fundamentales","preguntas certificación adwords","preguntas respuestas academy for ads"],"title":"Preguntas y test examen de Google Ads avanzado de Búsqueda","type":"post"},{"authors":null,"categories":["Tag Manager"],"content":" Este post sería un resumen en formato texto, de un vídeo anterior realizado en YouTube y está pensado como memoria para aquellos que quieren seguir todos los pasos de una creación de eventos dinámicos con Google Tag Manager. El nivel de este tutorial es intermedio. Así que cualquier duda pueden ver primero el vídeo explicativo Google Tag Manager en 30 minutos y luego este\n\u0026nbsp;\nDefinición de evento Vamos directo al grano, primero definimos que es un evento. Aquí tenemos una desde el RAE:\n 1. m. acaecimiento.\n2. m. Eventualidad, hecho imprevisto, o que puede acaecer.\n3. m. Suceso importante y programado, de índole social, académica, artística o deportiva. U. m. en Am.\n4. m. Ling. Situación descrita por un predicado, ya sea estática o dinámica.\n5. m. Ling. Predicado de naturaleza dinámica.\na cualquier, o a todo, evento\n1. locs. advs. En previsión de todo lo que pueda suceder.\n2. locs. advs. Sin reservas ni preocupaciones.\n Me quedo con dos y de hecho son previsiones, de lo que podría pasar en este caso aplicado a nuestro mundo, eventos o acciones de usuarios en nuestro sitios web, aplicaciones móviles.\nPerfecto, con lo cuál serían interacciones, hitos o hits, como definirlo mejor en Google encontramos esto:\n Información sobre los eventos Utilice los eventos para recopilar datos sobre las interacciones con su contenido.   Los eventos son interacciones del usuario con cuyo contenido se puede realizar independientemente a partir de una página web o una carga de pantalla.  Las descargas, los clics en anuncios para móviles, los gadgets, los elementos Flash, los elementos insertados AJAX y las reproducciones de vídeo son todos ejemplos de acciones de las que puede realizar un seguimiento como eventos.   Con lo cuál tenemos un elenco de algunos ejemplos de eventos, como la descarga de elementos, o click en diferentes botones o imágenes por ejemplo. Aquí mencionar que deberíamos tener un sitio web con muchos botones para diferenciar uno y otro clics en enlaces, sería ya suficiente para hacer una prueba. ¡Vamos con ello!\nAnatomía de los eventos Un evento tiene los siguientes componentes, un hit de evento incluye un valor para cada componente y estos valores se muestran en sus informes.\n Categoría Acción Etiqueta (opcional, pero recomendado) Valor (opcional)  Por ejemplo, puede configurar un botón de reproducción de vídeo en su sitio para que envíe un hit de evento con los valores siguientes:\n Categoría: \u0026#8220;Vídeos\u0026#8221; Acción: \u0026#8220;Reproducir\u0026#8221; Etiqueta: \u0026#8220;El primer cumpleaños del bebé\u0026#8221;  Directamente desde Google Analytics, desde su propia guía, vamos a comentar que necesitamos unas series de elementos para poder crear eventos con Google Analytics, así que mejor decir que sin esta información, no podemos tener nada.\nCategoría Una categoría es un nombre que se debe proporcionar para agrupar los objetos de los cuales quiera realizar un seguimiento. Normalmente, utilizará el mismo nombre de categoría varias veces sobre los elementos relacionados de la interfaz de usuario que quiera agrupar en una determinada categoría.\nSupongamos que también quiere realizar el seguimiento de las veces que se descarga el vídeo. Podría utilizar:\n Categoría: \u0026#8220;Vídeos\u0026#8221; Acción: \u0026#8220;Descargados\u0026#8221; Etiqueta: \u0026#8220;Lo que el viento se llevó\u0026#8221;  En este caso, solo habría una categoría (\u0026#8220;Vídeos\u0026#8221;) en sus informes y podría ver métricas agregadas para la interacción del usuario con el conjunto total de elementos para ese único objeto de vídeo.\nSin embargo, probablemente tenga más de un solo objeto del que desea realizar el seguimiento de eventos, por lo que merece la pena considerar cómo se van a clasificar los informes antes de implementar la invocación. Por ejemplo, tal vez prefiera hacer el seguimiento de todas las películas por separado en la misma categoría principal de vídeos de manera que obtenga números adicionales para cualquier interacción de vídeo, indistintamente del usuario con el que se interactúe.\nAdemás, podría crear categorías separadas según el tipo de vídeo, una para vídeos de películas y otra para vídeos musicales. También podría tener una categoría aparte para las descargas de vídeo:\n Vídeos: películas Vídeos: música Descargas  En este escenario, se podría ver el recuento total de eventos combinados de las tres categorías en sus informes. La métrica _Total de eventos_ muestra todos los recuentos de eventos de todas las categorías indicadas en la implementación del seguimiento de eventos. Sin embargo, no podrá ver las métricas combinadas de todos los vídeos aparte de las descargas, porque las métricas de eventos detalladas se combinan en sus respectivas categorías.\nAunque el modelo de objeto de seguimiento de eventos es totalmente flexible, es conveniente prever primero la estructura de informes que se quiera tener, antes de decidir las categorías de nombres. Si tiene previsto utilizar el mismo nombre de categoría en varias ubicaciones, procure hacer referencia correctamente a la categoría que desee por su nombre. Por ejemplo, si tiene previsto denominar a su categoría de seguimiento de vídeos \u0026#8220;Vídeo\u0026#8221; y posteriormente lo olvida y utiliza el plural \u0026#8220;Vídeos\u0026#8221;, tendrá dos categorías para el seguimiento de vídeos. Además, si decide cambiar el nombre de categoría de un objeto del que ya se ha realizado el seguimiento con otro nombre, el historial de datos de la categoría original no se volverá a procesar, por lo que tendrá métricas del mismo elemento de página web incluidas en dos categorías en la interfaz de informes.\nAcción Por lo general, el parámetro de acción se usa para asignar un nombre al tipo de interacción o evento del que se quiere realizar el seguimiento para un objeto concreto del sitio web. Por ejemplo, con una única categoría \u0026#8220;Vídeos\u0026#8221; puede hacer el seguimiento de una serie de eventos específicos con este parámetro, como los siguientes:\n hora a la que termina la carga del vídeo, clics del botón \u0026#8220;Reproducir\u0026#8221;, clics del botón \u0026#8220;Detener\u0026#8221;, clics del botón \u0026#8220;Pausa\u0026#8221;.  Como en el caso de las categorías, el nombre que indique para una acción lo decide usted, pero tenga en cuenta dos características importantes del modo en que se utiliza una acción de evento en los informes:\n Todas las acciones aparecen de forma independiente de sus categorías principales. Esto ofrece otra manera útil de segmentar los datos de evento en los informes. Un evento único viene determinado por un nombre de acción único. Puede utilizar nombres de acción duplicados en distintas categorías, pero este hecho puede repercutir en el modo de calcular los eventos únicos. Consulte las sugerencias que se indican a continuación y la sección \u0026#8220;Recuento implícito\u0026#8221; para obtener más detalles.  Etiqueta Las etiquetas permiten proporcionar información adicional para los eventos cuyo seguimiento desee realizar, como el título de la película en el caso de los ejemplos de vídeos anteriores, o el nombre de un archivo al realizar el seguimiento de las descargas.\n Categoría: \u0026#8220;Descargas\u0026#8221; Acción: \u0026#8220;PDF\u0026#8221; Etiqueta: \u0026#8220;/salesForms/orderForm1.pdf\u0026#8221;  Al igual que con las categorías y las acciones, hay un informe que muestra todas las etiquetas que haya creado. Imagínese las etiquetas como una manera de crear una dimensión adicional de los informes para la interacción del usuario con los objetos de la página. Por ejemplo, supongamos que dispone de cinco reproductores de vídeo en su página de cuyas interacciones desea realizar el seguimiento. Cada uno de estos reproductores puede utilizar la categoría \u0026#8220;Vídeos\u0026#8221; con la acción \u0026#8220;Reproducir\u0026#8221;, pero cada uno podría tener también una etiqueta aparte (como el nombre de la película) para que aparezcan como elementos diferenciados en el informe.\n Categoría: \u0026#8220;Vídeos\u0026#8221;, acción: \u0026#8220;Reproducir\u0026#8221;, etiqueta: \u0026#8220;Lo que el viento se llevó\u0026#8221; Categoría: \u0026#8220;Vídeos\u0026#8221;, acción: \u0026#8220;Reproducir\u0026#8221;, etiqueta: \u0026#8220;Huckleberry Finn\u0026#8221;  Como en el caso de las categorías, el nombre que indique para una etiqueta lo decide usted, pero tenga en cuenta dos características importantes del modo en que se utiliza una etiqueta de evento en los informes:\n Todas las etiquetas se enumeran de forma independiente a sus categorías principales y acciones. Esto ofrece otra manera útil de segmentar los datos de evento en los informes. Un evento único viene determinado por un nombre de etiqueta único. Puede utilizar nombres de etiquetas duplicados en distintas categorías, pero este hecho puede repercutir en el modo de calcular los eventos únicos. Consulte las sugerencias que se indican a continuación y la sección \u0026#8220;Recuento implícito\u0026#8221; para obtener más detalles.  ###\nValor El valor difiere de los otros componentes en que es un número entero en lugar de una secuencia, por lo que debe utilizarlo para asignar un valor numérico a un objeto de página de seguimiento. Por ejemplo, podría utilizarlo para proporcionar el tiempo en segundos que tarda en cargarse un reproductor o para activar un valor monetario cuando se alcance un determinado marcador de reproducción en un reproductor de vídeo.\nCategoría: \u0026#8220;Vídeos\u0026#8221;, acción: \u0026#8220;Tiempo de carga de vídeo\u0026#8221;, etiqueta: \u0026#8220;Lo que el viento se llevó\u0026#8221;, valor: downloadTime\nEl valor se interpreta como un número, y el informe agrega los valores totales sobre la base del recuento de cada evento (consulte \u0026#8220;Recuento implícito\u0026#8221; a continuación). El informe también determina el valor medio para la categoría. En el ejemplo anterior, se invoca al evento para la acción \u0026#8220;Tiempo de carga de vídeo\u0026#8221; al concluir la carga del vídeo. El nombre del vídeo se proporciona como una etiqueta y se acumula el tiempo de carga calculado para cada descarga de vídeo. A continuación, se podría determinar el tiempo medio de carga para todas las acciones \u0026#8220;Tiempo de carga de vídeo\u0026#8221; de la categoría \u0026#8220;Vídeos\u0026#8221;. Supongamos que ha habido 5 descargas únicas de los vídeos de su sitio web con los tiempos de descarga en segundos que se indican a continuación:\n 10 25 8 5 5  Sus informes se calcularían del siguiente modo (los números del ejemplo muestran el tiempo de descarga en segundos):\n Sesiones con eventos: 5 Valor: 53 Valor medio: 10,6  No se admiten números enteros negativos.\nEventos en Google Analytics Una vez explicado y definidos los elementos principales de los eventos, veamos un pantallazo de como se mostraría un evento en Google Analytics, que afecta toda la propiedad (y a todas las vistas o perfiles.\n  Desde nuestra interfaz vamos en: Comportamiento \u0026gt; Eventos\n\nAquí una muestra en Tiempo Real \u0026gt; Eventos\n\u0026nbsp;\n\u0026nbsp;\nCrear eventos en Google Tag Manager Y finalmente llegamos a crear los eventos con Google Tag Manager, tenemos una sección dedicada para poder crear los eventos directamente en la etiqueta de Google Analytics, así que sin ninguna dificultad veremos los mismos elementos arriba mencionados: Categoría, Acción, Etiqueta y Valor.\nVeamos un ejemplo aqui:\n\u0026nbsp;\ny su explicación de los eventos en Google Tag Manager aquí:\nhttps://support.google.com/tagmanager/answer/6106716?hl=es\n¿Cuál es la diferencia entre los eventos de Google Analytics y Google Tag Manager? Los eventos de Google Analytics son interacciones que se envían como resultado de las etiquetas de Analytics activadas desde Tag Manager. Los eventos del navegador de Tag Manager son interacciones de los usuarios con elementos de páginas web (elementos DOM) que registra el navegador y envía a la capa de datos de Tag Manager para que se puedan usar para configurar activadores.\nPues claro, verdad? La primera parte es la nos interesa ahora mismo, ya tendremos otro post explicando los grandes misterios del dataLayer, pero así como viene a ser este post tendríamos que utilizarlo también, sería la parte dinámica de nuestro Tag Manager, así que vamos por la creación de uno y del otro.\nNo iré explicando todo todo de Google Tag Manager, así asumo que ya conocen las variables, activadores y etiquetas. Asumo que ya conocen las diferentes variables existentes entre predefinidas y personalizadas, así como los activadores. En tal caso, os rimando a otras guías internas para realizar cada una de ella.\nEventos básicos: Clic en Enlaces Salientes Para crear un evento básico con Google Tag Manager integrando la etiqueta de Google Analytics seguiremos estos pasos:\n Elegimos y creamos la etiqueta de Google Analytics, del tipo Universal Tipo de seguimiento elegimos la opción Evento Entre los parámetros de seguimientos de eventos rellenamos la siguiente información:  CATEGORIA: ClickContactos ACCIÓN: clic ETIQUETA: {{Click URL}} VALOR: \u0026#8211; no marcamos nada \u0026#8211;  Seleccionaríamos o bien la variable de configuración de Google Analytics (creada anteriormente en el caso de tenerla), o marcamos la opción de Habilitar la anulación de configuración de esta etiqueta y añadimos nuestra ID Propiedad de GA Como activador seleccionaríamos: Todas las Páginas  ¿Listos?\nSi llegamos a leer esta linea significa que solamente tenemos dos opciones: o utilizar la Vista Previa y podemos probar antes de publicar nuestra versión creada, y para ello tenemos que ya tener un contenedor de Tag Manager implementado o bien utilizar el Tag Manager Injector para simular la creación del contenedor. No recuerdas como sería? Puedes echar un vistazo a este vídeo como hacerlo:\n  Como se mostraría en nuestra preview en Google Tag Manager sería así:\n\u0026nbsp;\ny en la interfaz de Tiempo Real en Google Analytics así:\nimg\nEventos dinámicos en Google Tag Manager Te imaginas una etiqueta que recoge dinámicamente todos los valores según la tipología de clic, formulario o lo que sea y lo podría diferenciar si el elemento tal y cuál?\nBueno vamos por partes, necesitamos primero definir la regla del dataLayer. \nEn este caso la etiqueta personalizada de HTML con esta lineas de código:\n\u0026lt;script\u0026gt;\u0026lt;br /\u0026gt; var dataLayer = window.dataLayer || [];\u0026lt;br /\u0026gt; dataLayer.push({\u0026lt;br /\u0026gt; \"event\" : \"linkContact\",\u0026lt;br /\u0026gt; \"eventCategory\" : \"\", //puedes crear un nombre fácil de recordar para esto tipo de acciones, en este caso optaría para ClickContactLink\u0026lt;br /\u0026gt; \"eventAction\" : \"\", //puedes crear una tabla donde recoge los valores del protocolo y lo transforma en Acción, te acuerdas la variable\u0026lt;br /\u0026gt; \"eventLabel\" : \"\" //aquí también es sencillo, puedes recoger un dato multiple, el valor (email, telefono, whatasapp y la página vista, ejemplo\u0026lt;br /\u0026gt; });\u0026lt;br /\u0026gt; \u0026lt;/script\u0026gt; Para ejemplo sería así:\n\u0026lt;br /\u0026gt; \"eventCategory\": \"ClickEvent\",\u0026lt;br /\u0026gt; \"eventAction\" : \"{{Click Text}}\",\u0026lt;br /\u0026gt; \"eventLabel\" : \"{{Click URL}}\"\u0026lt;br /\u0026gt;  La primera linea de código nos valida el dataLayer como variable, si existe o está vacía, así que la confirmamos nuevamente, y con la acción push enviamos nueva información.\nLa segunda linea es generar un evento en este caso asignamos un valor para luego recuperarlo linkContact, en mi caso.\nLas siguientes lineas asignamos los parámetros de eventos de GA, personalizados en su caso, y le asignamos los valores.\nEl Activador en este caso será nuestro click en url con protocolo: tel, mailto, whatsapp (son para eventos de llamadas), he realizado algo muy particular, puedes o bien seleccionar {{Click URL}} o bien crear una variable personalizada tipo URL \u0026gt; Protocolo \u0026gt; desde {{Click URL}}.\n\n\nde otro lado prepararé para cada uno de los parámetros del dataLayer, unas variables personalizadas tipo dataLayer:\neventCategory\n\neventAction\n\neventLabel\n\n\u0026nbsp;\nUna vez tengamos las variables creadas, nos falta solo rellenar las informaciones dentro de nuestra etiqueta de Google Analytics que teníamos antes, y aplicamos las personalizaciones dinámicas, así:\n\nSu activador será el evento personalizado linkContact que se lanzará desde el dataLayer anteriormente creado, con lo cuál solo en clic en enlaces de llamadas con unos determinados protocolos, nos dará este evento, así que la creación del activador será así:\n\nUna vez tengamos todo, realizamos nuevamente una actualización del nuestro workspace y actualizamos el sitio web y lo que se mostrará será lo siguiente:\n\n\n\nEsto es todo, una pequeña muestra de lo que podemos realizar con Google Tag Manager dinámicamente y con una buena dosis de planificación podemos inclusive crear funciones tipo if \u0026#8211; else con variables personalizadas, un ejemplo? Del tipo acciones dentro de una página que en base a unos activadores particulares, y reglas o condiciones activaríamos unos eventos dinámicos.\nCualquier duda que tengan, pueden enviarme un mensaje y lo miramos, juntos , gracias y hasta el próximo post.\n\u0026nbsp;\n","date":1524432891,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"c123371dd816b89a0dbf4510f8f28bfa","permalink":"https://www.marcusrb.com/como-crear-eventos-dinamicos-con-google-tag-manager/","publishdate":"2018-04-22T21:34:51Z","relpermalink":"/como-crear-eventos-dinamicos-con-google-tag-manager/","section":"post","summary":"Este post sería un resumen en formato texto, de un vídeo anterior realizado en YouTube, y está pensado como memoria para aquellos que quieren seguir todos los pasos de una creación de eventos dinámicos con Google Tag Manager","tags":["analytics eventos","crear eventos analytics","eventos dinámicos tag manager","tag manager google"],"title":"Como crear eventos dinámicos con Google Tag Manager","type":"post"},{"authors":null,"categories":["Google Analytics","Tag Manager"],"content":"## El próximo jueves 26 Abril 2018 se grabará un nuevo webinar con MarketingDirecto School, siguiendo el anterior con el plan de medición\ny descubrir nuevos insight dentro de la analítica web, esta vez se hablará de Google Tag Manager y como integrarlo dentro del plan de implementación.\nOs dejo con el post oficial a continuación y para su inscripción GRATUITA:\nCon este webinar vamos a aprender cómo efectuar correctamente un plan de implementación, integrando Google Analytics con Tag Manager para que podamos recolectar datos y enviarlos. Mediremos eventos de botones, enlaces, llamadas, email, descargas y vídeos.\nSe trata de una visión general del administrador de etiquetas más utilizado en este momento. Marco Russo nos dará un enfoque al plan de medición, para que en el próximo webinar pueda enseñarnos cada paso hacía la “medición ninja”.\nDía: jueves 26 de abril 2018\nHora: en diferido\nNivel: medio\n¿Te apuntas? También disponible otros webinar y próximamente casos prácticos. ¿Quieres que veamos en directo alguna vuestra petición? Comenta aquí abajo y Comparte!\nhttp://marketingdirectoschool.com/producto/aprende-a-integrar-google-analytics-con-tag-manager/ ","date":1523984908,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"bedb45f6325080fe03ae9a946b5397ec","permalink":"https://www.marcusrb.com/webinar-aprende-a-integrar-google-analytics-con-tag-manager/","publishdate":"2018-04-17T17:08:28Z","relpermalink":"/webinar-aprende-a-integrar-google-analytics-con-tag-manager/","section":"post","summary":"El próximo jueves 26 Abril 2018 se grabará un nuevo webinar con MarketingDirecto School, siguiendo el anterior con el plan de medición","tags":["integrar tag manager","plan implementación","webinar marketindirecto","webinar tag manager"],"title":"Webinar: Aprende a integrar Google Analytics con Tag Manager","type":"post"},{"authors":null,"categories":["Google Ads"],"content":" Pues sí, ya se había comentado hace muchos años, Google AdWords utiliza un sistema interno de Machine Learning o en castellano Aprendizaje Automático,  solo que cuando se puso de \u0026#8220;moda\u0026#8221; este término es donde ahora comenzamos a ver un poco más la luz a este tema tan complicado.\nEntre muchos \u0026#8220;profesionales\u0026#8221; del sector PPC, y quién ya trabaja con estas herramienta ya hace años, sabemos que entre muchas dificultades que AdWords conlleva, el auto-aprendizaje se venía realizando a través de script personalizados, autogestionales o bien con reglas automatizadas. Pero cuando el Big Data empezó fuerte y se comenzó hablar de Big Query e IA o Inteligencia Artificial, entonces veíamos nuevos sistemas de pujas, nuevos sistemas de ranking y calidad, nuevas posibilidad para alcanzar los resultados y tener siempre un ROI positivo, ya que la competencia irrumpió a lo grande.\nQue yo me acuerde en 2013 cuando comencé a trabajar con cuentas de AdWords de más de 2MM € al año de inversión, veía estas posibilidades factibles, jugaba mucho con los \u0026#8220;sistemas automatizados\u0026#8221; de Google y me gustaba la verdad, además veía los resultados demasiados buenos. Era el único que sabía de esto o los competidores no la estaban aprovechando?\nHasta que comencé a ver unos bajones, y luego subidas, y así altibajos durante meses y hasta semestres se veían buenas rachas y perdidas.\n¿Qué estaba pasando exactamente? Según leía entre foreros, la comunidad de AdWords oficial, libros técnicos, expertos de muchos años nacionales e internacionales, los cambios de las reglas en el mismo sistema de ranking y calidad comenzaban a llegar a quien no estaba acostumbrado, y sucesivamente llegarían las estrategia de pujas.\nVamos hablando un poco más de ellas. Por ejemplo en Búsquedas, Display, Shopping tenemos la posibilidad de elegir:\n Puja Manual o CPC manual Maximizar por Clics CPC Manual mejoradas Maximizar por Clics y opción CPC mejorado CPA objetivo  Las diferencias vienen a ser si es priorizar por clics o bien priorizar por conversiones, así que elegir una estrategia u otra, nos permite optimizar también el ROI, y ¿los costes?\nAquí la particularidad, los costes siempre y cuando alcancemos el objetivo fijado que suelen ser:\n branding o visibilidad de marca o per impresiones clics y tráfico clics hacía conversiones clics hacía transacciones  ¿Y cuando entra en juego la Inteligencia de Google o Machine Learning? Si volvemos un poco más atrás en los años, recordamos que ampliaron el abanico de estrategias de pujas ya por resultado, no empresarial, sino hacía los objetivos anteriormente hablado pero más enfocado a las pujas, entonces Google AdWords sacó lo que se llama ahora Smart Bidding o Pujas Inteligentes.\nVeamos cuales son, en el nivel de cartera, hay:\n CPA objetivo Retorno de la inversión publicitaria (ROAS) objetivo CPC mejorado Maximizar Clics Ranking superior objetivo.  En el nivel estándar de la campaña, hay:\n CPA objetivo ROAS objetivo CPC mejorado Maximizar Conversiones Maximizar Clics  Y aquí la definición oficial directamente de la guía de AdWords referente a las pujas inteligentes o smart bidding.\n AdWords Smart Bidding es un subconjunto de estrategias de pujas automáticas en las que se utiliza el aprendizaje automático para optimizar las conversiones o el valor de conversión en cada subasta, una función conocida como \u0026#8220;pujas en el momento de la subasta\u0026#8221;.\n Obviamente el requisito es tener habilitadas las conversiones, bueno, aquí era lógico ya que habla de objetivos de AdWords y es normal, aunque estábamos acostumbrados a recibir un mínimo de 15 conversiones en los últimos 30 días, y así creando estrategias de CPA objetivos con sinsentido alguno, y bajo los \u0026#8220;super-consejos\u0026#8221; de la campanillas de las Oportunidades de AdWords, nosotros puntualmente la activamos (y seguimos haciendo lo mismo).\n¿Por qué utilizar las pujas inteligentes de AdWords \u0026#8220;Smart Bidding\u0026#8221;? Según leyendo en la guía indica lo siguiente:\n AdWords Smart Bidding ofrece cuatro ventajas clave para ahorrar tiempo y mejorar el rendimiento.\n1. Aprendizaje automático avanzado Los algoritmos de aprendizaje automático basan la puja en datos a gran escala. De esta forma, puedes hacer predicciones más fiables sobre cómo diferentes importes de puja afectarían a las conversiones o a su valor. Estos algoritmos tienen en cuenta una gran cantidad de parámetros que afectan al rendimiento, muchos más de los que una sola persona o un equipo podría calcular.\n2. Amplia gama de señales contextuales Al definir las pujas en el momento de la subasta, es posible optimizarlas teniendo en cuenta una amplia variedad de señales. Las señales son atributos identificables de un usuario o de su contexto en el momento de una subasta en particular. Entre ellos se incluyen atributos como el dispositivo y la ubicación, que están disponibles como ajustes de la puja manuales, además de señales adicionales y combinaciones de señales exclusivas de AdWords Smart Bidding. A continuación encontrarás una lista con varias de estas señales.\n O sea habla de señales al igual que era antes, solo que con tanta competencia tenía que establecer unas \u0026#8220;reglas\u0026#8221; a este juego de pujas, y las señales ahora son factores de Ranking y creo que muy pocos conocerán, excepto quien trabaja codo a codo con agencias o con Google Partners.\nLas señales serán, bajo mi punto de vista, no del todo visible al ojo humano, pero si, teniendo en cuenta cuáles son, podemos establecer micro-objetivos y KPI\u0026#8217;s, métricas de seguimientos y ver de ir ganando\u0026#8230;o a través de herramientas externas via API\u0026#8217;s o scripts personalizados.\nAquí el listado de las señales visible, pero si hablar cada una de ella, tardaré mucho más de lo que tenía pensado para este post inicialmente, así que el listado está aquí, y poquito a poco hablaré de todas adjuntando mis experimentos realizados con ellas y ver de dar más luz.\nSeñales de pujas automáticas   Dispositivo  Ubicación física   Intención de ubicación   Día de la semana y hora   Lista de remarketing   Características de los anuncios   Idioma de interfaz   Navegador   Sistema operativo   Datos demográficos (Búsqueda y Display)   Consulta de búsqueda real (Búsqueda y Shopping)   Partner de la Red de Búsqueda (solo Búsqueda)   Emplazamiento web (solo Display)   Comportamiento en el sitio web (solo Display)   Atributos de producto (solo Shopping)   Valoraciones de aplicaciones móviles (próximamente)   Competitividad del precio (próximamente disponible para Shopping)   Estacionalidad (próximamente disponible para Shopping)     Bueno está la primera parte que es la fundamental de cada cuenta y campaña, no lo veo así de raro, o sí. Claro, tiene que tener un mínimo de dificultad sino resultaría demasiado básico para ellos. Tiene en cuenta de otros factores, y para ello entrará en juego el Modelo de Atribución, que seguramente más de uno lo tendrá configurado en último clic, o ya lo estará cambiando en estos días después de recibir llamadas de Google Partners para cambiarlo.  Controles de rendimiento flexibles   Con las pujas de Smart Bidding de AdWords se puede establecer objetivos de rendimiento y personalizar la configuración según los propios objetivos de negocio:   Optimiza las pujas de búsqueda en función de tu modelo de atribución, incluida la atribución basada en datos.   Define objetivos de rendimiento específicos para móviles, ordenadores y tablets con pujas de CPA objetivo (próximamente).    Se está poniendo difícil ahora, ya no es fácil controlar esto, puesto que el modelo asignado se refiere a las macro-conversiones, pero y a las micro-conversiones, que pasará con ellas?  Smart Goals, te suena de algo? Claro los Objetivos Inteligentes de Google Analytics que tiene también su importancia (ignotas por muchos), útiles en algunos casos y completamente ineficientes para otros. Una vez que se activan estos objetivos inteligentes e importandos en Google AdWords, el hechizo será cumplido.  Aquí explican los Objetivos Inteligentes que se activan en Analytics:   Medir las conversiones explícitas, tanto con el seguimiento de conversiones de AdWords como con las transacciones de comercio electrónico importadas de Analytics, es una forma excelente de optimizar las pujas, los anuncios y el sitio web. Sin embargo, si no va a medir las conversiones, la función Objetivos Inteligentes es una forma sencilla de utilizar las mejores sesiones como conversiones. Puede utilizar los objetivos inteligentes para optimizar el rendimiento de AdWords.   A quién están dirigidas estas funciones   Las pujas de AdWords Smart Bidding ofrece buenos resultados para grandes y pequeñas empresas (diría a todos). Para maximizar estos resultados y dar a los algoritmos de aprendizaje automático o Machine Learning ,muchos seguirán llamarlas Inteligencia Artificial, pero no aplica para mi y ya lo explico a continuación, serán suficientes datos como para poder tomar decisiones de puja fundamentadas.   Se recomienda que los anunciantes hayan registrado un mínimo de 30 conversiones en los últimos 30 días antes de usar el CPA objetivo. En el caso del ROAS objetivo, recomendamos 50 conversiones en los últimos 30 días.   Digo yo, incluso mejor esperar a un mínimo de 50 o incluso 100 en los últimos 30 días, ya que serán importantes para la estabilidad de las mismas en la campañas, y con todo el trabajo de optimización que conllevará.  ¿Por qué no es IA en lugar de Machine Learning?  Inteligencia Artificial no lo es porque Google en general no es capaz de hacer nada sin los \u0026#8220;humanos\u0026#8221;, el cuál será siempre el que enseñará como buen profesor a Google como actuar en cada situación, las query de búsquedas por ejemplos, o con las imágenes, o con las visualizaciones de los anuncios, interacciones en los sitios web taggados por Tag Manager y Google Analytics, o a través del utilizo de su herramienta de Adwords.  Te has fijado que cuando cambias algo en Adwords, el estado será \u0026#8220;En Aprendizaje\u0026#8221;, y tarda una semana en aprender de tus palabras, grupos, anuncios, estructura y luego irá caminando por sí solo? No? Lees a continuacióneste texto.  No ves nada raro? Y esta imagen?    Esta es una primera parte de mi post relacionado con el Machine Learning de Google, y aplicado al día a día de mi trabajo como consultor PPC. Una reflexión de que hay que ir aprendiendo, enseñando y sobre todo estudiar mucho para optimizar grandes cuentas (o medianas). Para rentabilizarlas hay que tener en cuenta tantos factores, hay mucha competencia y nuestro objetivo es ganarle a Google, o esto nos hace creer! Tú que opinas? Qué estrategia utilizas en Google AdWords y si el machine learning está siendo un beneficio para tu negocio.  Hasta el próximo post 😉    ","date":1523219116,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"adb85188be9d5b8adc73f20f14571140","permalink":"https://www.marcusrb.com/machine-learning-aplicado-a-google-adwords/","publishdate":"2018-04-08T20:25:16Z","relpermalink":"/machine-learning-aplicado-a-google-adwords/","section":"post","summary":"Pues sí, ya se había comentado hace muchos años, Google AdWords utiliza un sistema interno de Machine Learning o en castellano Aprendizaje Automático","tags":["estrategias pujas inteligentes","inteligencia artificial google","machine learning adwords","pujas big query","pujas inteligentes","smart bidding"],"title":"El Machine Learning aplicado a Google Ads","type":"post"},{"authors":null,"categories":["Analítica Web"],"content":"El pasado Jueves 22 de Marzo se emitió en directo el primer webinar de analítica web con la colaboración de MarketingDirectoSchool y por la ocasión he hablado de las oportunidades que una buena medición y una meticulosa configuración de la herramienta de Analytics, por ejemplo, podemos detectar muchas necesidades, o insights.\nAquí el vídeo que habla además de una checklist ideal para tenerlo siempre a la mano:\n\u0026nbsp;\n\u0026nbsp;\n  \u0026rdquo;\u0026lsquo;YouTube\u0026rdquo;\u0026rsquo; (pronunciación [[Alfabeto Fonético Internacional|AFI]] {{IPA|[ˈjuːtjuːb]}}) es un [[sitio web]] dedicado a compartir [[vídeo]]s. Presenta una variedad de [[CLIPS|clips]] de [[Película|películas]], [[programa de televisión|programas de televisión]] y [[vídeo musical|vídeos musicales]], así como contenidos amateur como [[videoblog|videoblogs]] y YouTube Gaming. A pesar de las reglas de YouTube contra subir vídeos con [[todos los derechos reservados]], este material existe en abundancia. Las personas que alojan sus vídeos en esta plataforma de manera habitual son conocidas como \u0026ldquo;[[youtuber]]s\u0026rdquo;.\n","date":1522939635,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"b41f95d8d37dd3fd5e2fe5db2ee0d740","permalink":"https://www.marcusrb.com/webinar-marketingdirectoschool-detectando-las-necesidades-de-la-analitica-web/","publishdate":"2018-04-05T14:47:15Z","relpermalink":"/webinar-marketingdirectoschool-detectando-las-necesidades-de-la-analitica-web/","section":"post","summary":"El pasado Jueves 22 de Marzo se emitió en directo el primer webinar de analítica web con la colaboración de MarketingDirectoSchool","tags":["insight","marketingdirecto","webinar","analitica web"],"title":"Webinar MarketingDirectoSchool – Detectando las necesidades de la analítica web","type":"post"},{"authors":null,"categories":["Google Adwords"],"content":"Hace más de cinco años que realizo test para la certificación oficial de Google AdWords fundamentales, además de Google Avanzado, y más de Google Partners que ahora se llama Academy For Ads. Durante todos estos años he visto diferentes programas de agencias para freelance, agencias y consultoras, pasando de Google Aqua, Google Engage a Google Partners y ahora cambiando completamente de plataforma para unificar cursos de digital marketingl, quedandose la mismas estrategias de AdWords, la de fomentar el uso de sus herramientas, ampliando de contenido para aprender y certificarse.\nDecidí en junio 2017 abrir una lista de reproducciones a mi canal de YouTube, hasta aquella fecha abierta hace años y ni utilizada, y puse online mis primeras grabaciones de los exámenes, para \u0026#8220;facilitar\u0026#8221; aquellos estudiantes de mis cursos a aprobarlos, obtener así la certificación, además de aprender la teoría, y muy bien aprendida, con mis cursos online de AdWords, efectivamente era, y sigue siendo una buena manera de obtener la certificación.\nNo pido dinero a cambio, ni lo hago por publicidad (encima con los cambios actuales de YouTube tampoco monetizo). Lo hacía por pura diversión con la idea de compartir recursos, lo aprendido y ser de una manera reconocido en este \u0026#8220;mundillo\u0026#8221;, que ya llevo bastante años la verdad, casi 10 y solo que he cambiado de dominios (muy mal para el SEO, ya sé).\nHe recibido amenazas, denuncias, me banearon\u0026#8230;pero sigo allí, y no me importa quien me reputa como \u0026#8220;estafador\u0026#8221;, justamente hablan muchos quien cobra por ello, o tiene agencias grande que se dedican a esto (a vender los exámenes, si, esto!). Y no me importa, luchar contra el grande Google, que gracias a estos exámenes está llevando a la ruina un sistema que ellos mismos crearon tantos comerciales, tantos vendedores de sus productos\u0026#8230;y con creces que sigan estafando a muchos pequeños clientes y no solo, que otro día lo explicaré.\nAsí comparto las preguntas de los exámenes oficiales, a través de páginas en Google Form, que normalmente realizo durante mis cursos. Son varios módulos, de momento las tres partes de Google Fundamentos, además de las preguntas de Google AdWords Avanzado, seguiré añadiendo Google Shopping, Mobile, YouTube, Display y las Preguntas de Google Analytics 2018.\nDe momento pueden tener acceso a las preguntas\u0026#8230;y para las respuestas puedes seguir uno de mis vídeos a esta lista y SUBSCRÍBETE.\n\u0026nbsp;\n Listado de preguntas examen Google AdWords Fundamentales 2018 \u0026#8211; 1 Listado de preguntas examen Google AdWords Fundamentales 2018 \u0026#8211; 2 Listado de preguntas examen Google AdWords Fundamentales 2018 \u0026#8211; 3  ","date":1522159373,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"5e1edb15f63da6d01d88eee135aa2d8d","permalink":"https://www.marcusrb.com/buscas-las-respuestas-del-examen-de-google-ads-fundamentales/","publishdate":"2018-03-27T14:02:53Z","relpermalink":"/buscas-las-respuestas-del-examen-de-google-ads-fundamentales/","section":"post","summary":"Hace más de cinco años que realizo test para la **certificación oficial de Google AdWords fundamentales, además de Google Avanzado","tags":["adwords certificación 2018","examen de adwords fundamentales","preguntas certificación adwords","preguntas respuestas academy for ads 2018"],"title":"¿Buscas las respuestas del examen de Google Ads fundamentales?","type":"post"},{"authors":null,"categories":["Google Analytics","Tag Manager"],"content":" Dentro del plan de medición, normalmente con el marcar un objetivo empresarial y crear diferentes estrategias llegamos al punto de la implementación técnica. Normalmente seguimos siempre unos pasos para hacerlo, pero casi siempre que nos encontramos con un teléfono o correo electrónico, enlaces salientes del tipo social, o eventos de interacciones del usuario en el sitio web, llegamos al punto más crítico, el hacer mediciones de interacciones de formulario.\nDonde normalmente sea por productos (en el caso de valoraciones, coupon descuento, contactos de soporte o atención al cliente, formulario de registro y finalización pedido, comentarios) o sea para servicios, el clásico formularios para capturar leads, no es siempre fácil su implementación ni manualmente ni con tag manager.\nBien, después de ilustrar varias fases de medición de formularios clásicos con el activador de Formularios que por defecto nos proporciona Google Tag Manager, el de HTML, hay unos cuantos realizados en otro framework, o en jQuery o en Ajax que nos siempre da el resultado esperado.\nCitando varias fuentes, LunaMetrics o AnalyticsMania que resumieron gracias a diferentes soluciones para ambientes Ajax como solventar el asunto, he realizado varios vídeos de momento, explicando primero las tipologías de formularios, segundo como interactuar con ellos, y tercero como extraer datos y convertirlos en eventos.\nLos pasos previos a seguir sería instalar uno script como código personalizado y unas cuantas variables del tipo capa de datos o dataLayer en nuestro contenedor.\nSegundo, sería leer e interpretar los parámetros del dataLayer del tipo ajaxComplete y luego sacar los valores.\nEl primer video está aquí y en otro post os contaré como extraer los datos.\n  ","date":1522009482,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"cf6f9362e71f75ebed24394fe6b63cb0","permalink":"https://www.marcusrb.com/medicion-formularios-contactform7-con-tag-manager/","publishdate":"2018-03-25T20:24:42Z","relpermalink":"/medicion-formularios-contactform7-con-tag-manager/","section":"post","summary":"Dentro del plan de medición, normalmente con el marcar un objetivo empresarial y crear diferentes estrategias llegamos al punto de la implementación técnica.","tags":["contact form 7 formulario","contactform wordpress","medición contact form 7","tag manager y formulario ajax"],"title":"Medición formularios Contact Form7 de WordPress con Tag Manager","type":"post"},{"authors":null,"categories":["Data Science","Digital Marketing"],"content":" Hace mucho tiempo que tenía gana de redactar un artículo sobre esta tema, y durante más de algunas clases impartidas de analítica web, se comentaba que requisitos ha de tener un analista web y que diferencia tiene con el analista de datos mencionando el rol que desempeña.\nEstá claro que por analista web no estamos hablando de realizar un curso de Google Analytics y fin, y que espero hoy en día todo el mundo conoce, pero era de ir profundizando el rol que tiene este perfil con el analista de datos. Hay muchas más herramientas, muchas más competencias a tener en cuenta, además de tener unos estudios universitarios o postgrados de rama científica.\nViendo el escenario actual hay que definir el analista digital y el analista de datos, como dos perfil por separado, teniendo en cuenta que el primero puede ser complementar al segundo y así viceversa.\nSi hablamos del analista digital, ha de tener unos requisitos bien definidos hacía:\n extraer datos de las herramientas de analítica web, o a través de mediciones off-line segmentar estos datos con el fin de alinearse a los objetivos empresariales, y de desempeño por el cuál fueron creados (hablamos aquí de las KPI\u0026#8217;s) filtrar los datos en tantos grupos, para que puedan ser utilizados de manera optima y re-utilizados, también para otras mediciones e ir comparando por patrones (benchmark) representar los datos a través de gráficos, informes dinámicos y reports periódicos, inclusive utilizando storytelling, o contar historias a través de los datos detectar nuevas oportunidades (insight), fallos o errores, probar diferentes escenarios con los test A/B aplicar las mejoras y así volver al punto de partida el dominio del idioma, es muy importante tanto para conversación y negociación con los clientes, así como participación a congresos y seminarios y networking  Dicho esto por supuesto puede que haya más, pero se debe tener en cuenta que el objetivo del analista web es pensar primero en el cliente (UX), y luego al retorno empresarial. Creo que esto es fundamental, sino pensamos primero a quién va dirigido el producto o servicio ofrecido, y lo medimos mal, el retorno será nulo, o incluso negativo\n¿Qué diferencia tiene con el analista de datos el primer rol?  Programación orientado a objetos PHP, JavaScript, PHP, Python, C/C++, (no hablamos de pequeños scripts solamente, además de aplicaciones de gestión) Diferentes sistemas operativos lado cliente que server, UNIX, LINUX, Debian, etc. Conocimiento y utilización de diferente base de datos relacionadas tipo SQL y no relacionadas tipo NoSql, (Cassandra, Spark, Hadoop, Hbase, Redis, MongoDB, etc) Uso de las herramientas de analítica cualitativa y cuantitativa a través de funciones matemáticas y estadísticas (como base solidas y creación de modelos) Conocer que soluciones existente puedan albergar la mayor cantidad de datos, para así estudiar la posibilidad de cloud server (Google Cloud, Amazon Cloud, Microsoft Azure, etc) Lo mismo de antes, uso del idioma inglés hablado casi a diario, tanto para comunicación con los clientes que para hacer networking, como para las diferentes soluciones informáticas y más.  Según el estadístico John Tukey, que definió en 1961 el análisis de datos así \u0026#8220;Procedimientos para analizar datos, técnicas para interpretar los resultados de dichos procedimientos, formas de planear la recolecta de datos para hacer el análisis más fácil, más preciso o más exacto\u0026#8221;. Se parece mucho a lo que viene a ser el analista digital o analista web, pero con pequeñas diferencias.\nAsí que mientras el Data Analyst o analista de datos parece ser un híbrido entre informatico y project manager sobre lo que crear diferentes soluciones, si tenemos en cuenta además funciones de negocio, entonces se convierte en Business Analyst\nEl Analista de Datos tiene un rol de rompedor de esquemas entre el mundo digital y mundo off-line, el suyo es ir sacar datos través de la minería de datos, técnica que permite a la exploración de datos no convencionales, o inclusive que ni sabemos como interpretarlos. Se buscarán formas de \u0026#8220;homogeneizar\u0026#8221; los datos brutos y transformarlos en datos más \u0026#8220;legibles\u0026#8221;.\n¿Puede un analista web ser analista de datos? Según leo en algunas revistas especializadas del sector, la respuesta es sí, pero con limitaciones y bastantes, ya lo hace con los datos cualitativos ya que estos serán \u0026#8220;interpretaciones\u0026#8221; del analista a detectar necesidades o mejoras. Y el resto a través de test A/B, pero siempre trabajará con la gran mayoría de veces con datos ya \u0026#8220;limpios\u0026#8221; y listo para su utilizo.\nAsí que el rol del analista de datos, o bien Data Analyst, o Big Data Analyst, es un rol más completo, que inclusive puede llegar a meter manos en las herramientas de análisis web aunque veo sus limitaciones de no poder sacar más de lo que ya está midiendo, podría limitar bastante sus conocimientos en algo más \u0026#8220;básico\u0026#8221;, que no es del todo verdad.\nEn analista web podría ser Business Analyst, ya que este rol podría abarca análisis, seguimientos de objetivos y alineando el todo con los objetivos empresariales, de negocio.\nPor cierto, no confundamos el rol del analista de datos con el Data Scientist, este será el responsable para la coordinación de más equipos y recursos para llevar a cabo proyectos de más envergadura, teniendo en cuenta que trabajarán más analistas de datos, informáticos, matemáticos, sociólogos, estadísticos, físicos y más profesionales.\nAsí que dedicaré más adelante un listado completo de recursos que se podrían encontrar en internet de centros educativos de pago y Mooc para complementar tus conocimientos, que más de uno querrá tenerlo en cuenta.\nAsí que de momento esto es todo, he querido aclarar a todos aquellos que me preguntan sobre los perfiles digitales que más se buscan pero con su pequeñas matices.\nTienes más duda sobre estos dos perfiles? Pueden ir comentando más abajo y así abrir un debate.\n\u0026nbsp;\n","date":1521400305,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"0e83726ff36c9ce5509a1d4b45fe7e46","permalink":"https://www.marcusrb.com/el-perfil-del-analista-de-datos-y-del-analista-digital-y-sus-requisitos/","publishdate":"2018-03-18T19:11:45Z","relpermalink":"/el-perfil-del-analista-de-datos-y-del-analista-digital-y-sus-requisitos/","section":"post","summary":"Dentro del plan de medición, normalmente con el marcar un objetivo empresarial y crear diferentes estrategias llegamos al punto de la implementación técnica.","tags":["diferencia entre big data analyst y business analyst","perfil analista de datos","rol analista digital"],"title":"El perfil del analista de datos y del analista digital y sus requisitos","type":"post"},{"authors":null,"categories":["Analítica Web","Tag Manager"],"content":" Hola, en este post, gracias a un comentario propuesto por vosotros a mi canal de YouTube https://www.youtube.com/c/Marcusrb, dedicaré a un tutorial de como realizar un etiquetado correcto de nuestra web gracias a Google Tag Manager para los eventos de Google Analytics. Hay que tener en cuenta muchos factores, y en esto lo tenemos tener claro a través de un diseño de un plan, el plan de medición.\nEste plan tiene que recoger los objetivos principales de la empresa, específicos, medibles, alcanzable por supuesto, relevantes y en un tiempo prefijado, así llamado también S.M.A.R.T. (Specific, Measurable, Achievable, Relevant, Timely) así que hay muchos ejemplos y plantillas de planes para poder tener todo bien controlado antes de meter en práctica todo que hay que realizar. Buscando plantillas objetivos SMART, plan de medición o en in inglés Measurement Plan, en Google os saldrán muchos.\n\u0026nbsp;\nComo medimos todo esto? Claro, a través de los indicadores de performance o K.P.I. (Key Performance Indicators) y podemos así tener un seguimiento de las métricas más habituales de todo lo realizado en este plan.\nEl Plan de medición resumido sería algo así:\n estrategia de plan de medición resumido \u0026#8211; desde el curso de Fundamentos y analítica de GA y GTM para KPIschool  \u0026nbsp;\n\u0026nbsp;\nBien, llegamos hasta aquí y aprendemos Plan de Medición, SMART y KPI. Que nos falta? Bueno, la implementación de Google Tag Manager o una buena dosis de programación Web, junto con hojas de estilos CSS, JavaScript y más .En sustitución, una buena base de diseño web básico podemos con todo esto, pero en un wordpress o prestashop no es lo mismo que con un diseño personalizado.\nCada página tiene su código en PHP o similar, a través de framework, desarrollos a medidas tipo Bootstrap, AngularJS (para que nuestras páginas lucen más bonitas por gráfica e imagen, hay un buen desarrollo por detrás o un motor que hagan que funcionen correctamente).\n\u0026nbsp;\n¿Por qué cuento todo esto para un vídeo de 30 minutos? Porque puedo realizarlo incluso en 5 minutos y nadie va a entender exactamente como funciona una cosa u otra, sino hay un plan, objetivos y llegar finalmente al plan de Implementación (punto 4 del gráfico arriba), no podemos tener ni datos, ni mejora continua, ni saber si estamos haciendo las cosas bien, o que nuestras campañas funcionen correctamente, o si la estrategia utilizada es la correcta y si las tácticas o técnicas utilizadas son las adecuadas.\n\u0026nbsp;\n¿Qué necesitamos para Google Tag Manager? En el vídeo se indicará cada cosa, cada elemento que necesitemos, buena visión y los comentarios abajo o en el mismo vídeo 😉 y recuerdas que tendré muchos más tutoriales, así que Subscribete a mi canal.\n\u0026nbsp;\n[MAn20Yi62z8\u0026amp;enablejsapi=1\u0026amp;origin=https://www.marcusrb.com]\n","date":1520509696,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"727b3d900df77db04c4bf0af4845f46b","permalink":"https://www.marcusrb.com/google-tag-manager-etiquetando-nuestra-web-en-menos-de-30-minutos/","publishdate":"2018-03-08T11:48:16Z","relpermalink":"/google-tag-manager-etiquetando-nuestra-web-en-menos-de-30-minutos/","section":"post","summary":"Este plan tiene que recoger los objetivos principales de la empresa, específicos, medibles, alcanzable por supuesto, relevantes y en un tiempo prefijado","tags":["administrador de etiquetas","eventos google analytics y tag manager","google tag manager","implementar tag manager","que es una etiqueta"],"title":"Google Tag Manager, etiquetando nuestra web en menos de 30 minutos","type":"post"},{"authors":null,"categories":["Formacion"],"content":"[Actualizado junio 2020] - Desde febrero 2019 ya imparto cursos a particulares desde la plataforma Superprof, con las 3 especializaciones en Google Analytics y Tag Manager, Google Ads avanzado y Data Analytics \u0026amp; Data Science además de las herramientas más utilizadas en Business Intelligence - Power BI, Tableau, Data Studio.\nPues sí, este mes (Junio 2018) he cumplido exactamente 6 años desde cuando he comenzado con la aventura de impartir mis primeros cursos y programas de posgrados, el Máster de SEO y SEM en Aula Creactiva y sigo teniendo muy buenos recuerdos.\nAquí mi resumen de todos los cursos realizados, másters y programas de postgrado en empresas de educación superior, escuelas de negocios y AA.PP. :  Cámara de Comercio de Madrid y delegación en Alcalá de Henares EAE Business School AdveiSchool KPI\u0026rsquo;s escuela digital IIM Instituto Internacional de Marketing IFM Business School DigitalBrain IEDGE Business School IEBS Business School FICTIZIA NEOLAND CICE Business School DATA School AI School  y formación a medida a empresas.\nNo ha sido fácil recolectar todos los datos, ya que me hubiera gustar aportar con fotos, vídeos y las recensiones de aquellos cursos impartidos a lo largo de estos años y que seguiré impartiendo. Formando a más de ~30000 estudiantes (entre profesionales y en búsqueda de mejora laboral).\n\n   Fecha Programa Duración/turno Instituto     Jun 2012 Curso de Posicionamiento SEO presencial 24 horas - mañana Aula Creactiva   Jun 2012 Curso de Posicionamiento SEO presencial 24 horas - tarde Aula Creactiva   Ene 2013 Curso de Posicionamiento SEO presencial 24 horas - mañana Aula Creactiva   Ene 2013 Curso de SEM presencial 8 horas - tarde Aula Creactiva   Feb 2013 Curso de SEM presencial 16 horas - mañana Aula Creactiva   Feb 2013 Curso de SEM presencial 24 horas - tarde Aula Creactiva   Mar 2013 Curso de SEM presencial 24 horas - mañana Aula Creactiva   Mar 2013 Curso de Posicionamiento SEO presencial 24 horas - tarde Aula Creactiva   May 2013 Curso de Posicionamiento SEO presencial 36 horas - tarde Cámara de Comercio Madrid   Jun 2013 Curso de SEM presencial 36 horas - tarde Cámara de Comercio Madrid   Sep 2013 Curso de Posicionamiento SEO presencial 36 horas - tarde Cámara de Comercio Madrid   Oct 2013 Curso de SEM presencial 36 horas - tarde Cámara de Comercio Madrid   Feb 2014 Curso de SEM presencial 21 horas - tarde Cámara de Comercio Madrid   Nov 2014 Curso de Google AdWords presencial 21 horas - tarde Cámara de Comercio Madrid   Feb 2015 Curso de Comercio Electrónico presencial 45 horas - tarde Cámara de Comercio Madrid   Mar 2015 Curso de Google Analytics presencial 36 horas - tarde Cámara de Comercio Madrid   Nov 2015 Curso de Posicionamiento SEO presencial 36 horas - tarde Cámara de Comercio Madrid   Dic 2015 Curso de Google Analytics presencial 36 horas - tarde Cámara de Comercio Madrid   Ene 2016 Curso de Google Analytics presencial 24 horas - tarde Cámara de Comercio Madrid   Feb 2016 Curso de Google Analytics presencial 36 horas - tarde Cámara de Comercio Madrid   Mar 2016 Curso de Posicionamiento SEO presencial 36 horas - tarde Cámara de Comercio Madrid   Abr 2016 Curso de Google Analytics presencial 36 horas - tarde Cámara de Comercio Madrid   May 2016 Curso de Google AdWords presencial 21 horas - sab EAE Business School - Madrid   Jun 2016 Curso de Analítica web presencial 21 horas - sab EAE Business School - Madrid   Jun 2016 Curso de Posicionamiento SEO presencial 36 horas - tarde Cámara de Comercio Madrid   Feb 2017 Curso de Google AdWords presencial 36 horas - tarde Cámara de Comercio Madrid   Feb 2017 Curso de Google AdWords presencial 21 horas - sab EAE Business School - Madrid   Mar 2017 Curso de Analítica web presencial 21 horas - sab EAE Business School - Madrid   Mar 2017 Webinar de Google Tag Manager 2 horas Instituto Internacional de Marketing - Barcelona   Abr 2017 Webinar de Analítica web 2 horas Instituto Internacional de Marketing - Barcelona   Abr 2017 Curso de Analítica web presencial 21 horas - sab EAE Business School - Madrid   Abr 2017 Curso de Google AdWords presencial 36 horas - tarde Cámara de Comercio - Madrid   Jul 2017 Curso de Google AdWords presencial 36 horas - tarde Cámara de Comercio - Madrid   Oct 2017 Curso de Google Analytics presencial 24 horas - tarde Adveischool - Madrid   Nov 2017 Curso de Google Tag Manager distancia 20 horas - online Adveischool - Madrid   Ene 2018 Curso de Google Tag Manager distancia 20 horas - online Adveischool - Madrid   Feb 2018 Curso de Google Tag Manager online 60 horas - online KPI\u0026rsquo;s digital school   Mar 2018 Curso de Google Tag Manager distancia 20 horas - online Adveischool - Madrid   Abr 2018 Curso de Google Tag Manager distancia 4 horas - online MD school - Madrid   Abr 2018 Curso de Google Tag Manager distancia 20 horas - online Adveischool - Madrid   May 2018 Curso de Analítica Web \u0026amp; Big Data online 40 horas - online KPI\u0026rsquo;s digital school   Jun 2018 Curso de Google Tag Manager distancia 20 horas - online Adveischool - Madrid   Jun 2018 Curso de Google AdWords online 40 horas - online KPI\u0026rsquo;s digital school   Jul 2018 Curso de Google Tag Manager distancia 10 horas - online IEBS - Madrid   Ago 2018 Curso de Google Tag Manager distancia 2 horas - online IEDGE - Madrid   Nov 2018 Webinar de Google Analytics avanzado 1 hora - online IEDGE - Madrid   Dic 2018 Webinar de Google Analytics avanzado 1 hora - online IEDGE - Madrid   Ene 2019 Webinar de Google Analytics avanzado 1 hora - online IEDGE - Madrid   Abr 2019 Webinar de Google Tag Manager avanzado 1 hora - online IEDGE - Madrid   May 2019 Webinar de Google Tag Manager avanzado 1 hora - online IEDGE - Madrid   Jun 2019 Webinar de Google Tag Manager avanzado 1 hora - online IEDGE - Madrid   Ago 2019 dataLayer avanzado I-II 2 horas - online IEDGE - Madrid   Ago 2019 Curso de visualización de datos y Google Data Studio 10 horas - online KPI\u0026rsquo;s digital school   Sep 2019 Máster y tutoría Data Science 198 horas - presencial NEOLAND   Oct 2019 Curso de Data Analytics 40 horas - presencial NEOLAND   Jun 2020 Curso de Data Visualization 120 horas - remoto DataSchool   Sep 2020 Curso de Business Intelligence 360 horas - remoto DataSchool    \n* actualizado septiembre 2020\nY si estás interesado en colaboración para seminarios presenciales o a distancia, grabados o mi presencia en eventos, meetups, podcasts o cualquier otro tipo, puedes ponerte en contacto conmigo.\n","date":1520290390,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"ffb47ad35f56a90be38dddac948db931","permalink":"https://www.marcusrb.com/cumpliendo-anyos-formador-docente-analitica-datos/","publishdate":"2018-03-05T22:53:10Z","relpermalink":"/cumpliendo-anyos-formador-docente-analitica-datos/","section":"post","summary":"Este plan tiene que recoger los objetivos principales de la empresa, específicos, medibles, alcanzable por supuesto, relevantes y en un tiempo prefijado","tags":["enseñanza analítica digital","formación escuela negocio","programa formación"],"title":"8 años como profesor de Data \u0026 Visualización","type":"post"},{"authors":null,"categories":["Digital Marketing"],"content":"Primero, para no confundir con el título y con el contenido voy primero aclarando unas cuantas cosas.\nA enero 2018 he cumplido 5 años y medio como formador y docente, aunque es verdad que en muchos puestos también como profesor. Buscando y rebuscando con el fin de interpretar mejor la información proporcionada en mis curriculum y presentación, realmente hay diferencia entre una cosa y otra.\nGoogleando me sale este primer resultado, que bien finalmente alguien que lo aclare:\nhttp://www.colfisiocv.com/node/15396\nY así que finalmente entiendo que mientras Docencia deriva del latín: docēre _(enseñar), _la terminología abarca mucho más elementos.\n El término docente es polisémico se usan como sinónimos del mismo las siguientes palabras: pedagogo, instructor, formador, educador, enseñante, adiestrador, maestro, didáctico, académico, normativo, purista, clásico, culto, asesor, consejero, facilitador, promotor, orientador, coordinador, consiliario, tutor, gestor, mentor, guía, gurú, mediador y conductor, entre otras\n En linkedin, veo con el término docente, exactamente hasta la fecha (04.03.2018) solo en España\nShowing 1,101,377 results\ncon el término **formador:\n** Showing 47,246 results**\n**\ny con **formadora:\n** Showing 40,770 results**\n**\nAsí que debe \u0026#8220;molar\u0026#8221; más el primero\u0026#8230;\nY con profesor cambia totalmente la cosa, es algo más que enseñar, formar o instruir, es algo que inclusive llega a ser como maestro (siempre si no comparamos este término con \u0026#8220;experto de\u0026#8221;).\n El profesor es la persona que enseña un conjunto de saberes sin embargo, el maestro es aquel al que se le reconoce una habilidad extraordinaria en la materia que instruye. De esta forma, un docente puede no ser un maestro (y recíprocamente). Más allá de esta diferencia, todos deben poseer pericias académicas para convertirse en agentes efectivos en el proceso de aprendizaje.\n Y siguiendo:\n El docente, en definitiva, reconoce que la enseñanza es su dedicación y profesión fundamental. Por lo tanto, sus habilidades consisten en enseñar de la mejor forma posible al alumno.\n Bueno entonces me veo muy identificado como docente.\nY sigo\n Profesor es u término que define, aquella la persona que enseña una cátedra o que está asignado a un Departamento, es decir que se enfoca a la enseñanza especializada en un tema además de estar calificado para ello.\n Efectivamente no soy profesor, pero si en algunos departamentos de marketing en la EAE Business School he sido profesor. He sido y sigo siendo profesor en Cámara de Comercio, pero, influyen también las horas acumuladas, los esfuerzos y los reconocimientos? Hay que verlo y ir sumando.\nEn Linkedin buscando con el término **profesor\n** Showing 640,673 results**\n**\ny tener datos también de **profesora\n** Showing 426,171 results\n\u0026nbsp;\nY llega el formador\n En pocas palabras un formador es una persona que instruye o enseña a otras. Cómo la formación de formadores en el ámbito de la educación no formal se dirige a la capacitación, acreditación, reciclaje y desarrollo profesional continuo de aquellos profesionales que trabajan para la formación de aquellas personas que han dejado atrás la etapa escolar y que se encuentran sin trabajar (formación ocupacional) o están trabajando (formación continua).\n Mmhh, he estado estudiando para esto también y la verdad también encaja.\nConclusión, no me meto en mérito de quien ejerce la docencia o la enseñanza hoy en día, muy a parte de que estas personas deber tener un conocimiento muy alto de lo que se imparte, de tener pasión por lo que se hace e ilusión cada día. Reciclarse y seguir aprendiendo, de los errores sobre todo, y no tener miedo en equivocarse. Profesar una materia, o más de una no te hace sentir \u0026#8220;gurú\u0026#8221;, o \u0026#8220;maestro\u0026#8221;, ya que sería innovar, mejorar o inclusive divulgar a gran escala por lo que se hace (en este sentido si te pagan bien, mejor). Conozco muchos que lo hacen ad-honorem y esto me causa un enorme placer de ver la total dedicación.\nJusto para exponer unos cuantos casos de pseudo-expertos y maestros super ilustres de pacotilla\u0026#8230;siempre en linkedin buscando post de muchos mis colegas, hablamos una y otra vez de los miles expertos de nada, de tener solamente la suerte de saber hablar (se comentaba el saber-hacer y el saber)\u0026#8230;creo que más del 90% son \u0026#8220;saber\u0026#8221;.\nEn linkedin los expertos son:\nShowing 35,380 results\ny las **expertas:\n** Showing 1,505 results\nLo triste de la historia no es tener tantos expertos (porque no estoy juzgando a nadie, ni conozco a todos), es ver en las búsquedas relacionadas que hay muchas ofertas de trabajos que contratan \u0026#8220;Expertos de SEO\u0026#8221;, \u0026#8220;Experto de Marketing\u0026#8221;, \u0026#8220;Experto de AdWords\u0026#8221;\u0026#8230;y ahí es todo veo el fracaso desde el principio, y no desde el individuo. Hay que ver porque están reclutando expertos y luego pagar lo que paguen, en fin.\n\u0026nbsp;\nY tu eres profesor, docente o formador? Me gustaría conocer tu opinión.\n","date":1520183713,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"8cb9f5bb9ea8c7cbf83f7a4511c13127","permalink":"https://www.marcusrb.com/profesor-docente-formador-marketing-digital/","publishdate":"2018-03-04T17:15:13Z","relpermalink":"/profesor-docente-formador-marketing-digital/","section":"post","summary":"A enero 2018 he cumplido 5 años y medio como formador y docente, aunque es verdad que en muchos puestos también como profesor.","tags":["diferencias formador y profesor","formador y docencia","profesor de digital marketing"],"title":"¿Profesor, docente o formador?","type":"post"},{"authors":null,"categories":["Generícos"],"content":"Cargando\u0026#8230; ","date":1519923599,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"017f88c0d507f6d2b3e866c6bbbe3e70","permalink":"https://www.marcusrb.com/encuesta-que-te-gustaria-aprender-mas-en-este-momento/","publishdate":"2018-03-01T16:59:59Z","relpermalink":"/encuesta-que-te-gustaria-aprender-mas-en-este-momento/","section":"post","summary":"Cargando\u0026#8230; ","tags":null,"title":"Encuesta: ¿Qué te gustaría aprender más en este momento?","type":"post"},{"authors":null,"categories":["Google Analytics","Tag Manager"],"content":" Hola a tod@s con un nuevo post, y esta semana hablaré de una gran herramienta gratuita para \u0026#8220;test A/B\u0026#8221; que anteriormente solo se podía gestionar a través de Google Analytics: \u0026#8220;Experimentos\u0026#8221;. Google Optimize no ha tardado en salir en formato BETA hace unos años y integrado con Google Tag Manager a través de una etiqueta que al parecer es fácil de implementar. Hay que mencionar que Google Tag Manager, el sistema de etiquetado más utilizado (y gratuito) del momento, y de la gran familia Google, nos permitía integrar en nuestros sitios web experimentos de landing page tipo:\n  Test A/B   Multivariantes   Redirecciones   Pero a través de su contenedor unificado NO permite gestionar dentro del lo que queríamos. Sí has leído bien, antes de utilizar el Google Optimize dentro de Tag Manager tienes que fijarte si aún conservas el antiguo contenedor de un solo bloque como este. porque no es compatible. El actual está separado en dos partes (en virtud de este cambio) nos permite gestionar tanto el \u0026#8220;encabezado\u0026#8221;, que el \u0026#8220;contenido\u0026#8221;. Como este:Vamos en parte para hablar de Google Optimize y los pasos para su implementación, ya que la guía oficial no es muy clara como siempre.\n\u0026lt;script\u0026gt; (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-XXXXX-Y', 'auto'); \u0026lt;strong\u0026gt; ga('require', 'GTM-XXXXXX');\u0026lt;/strong\u0026gt; ga('send', 'pageview'); \u0026lt;/script\u0026gt;  \nAquí el vídeo completo con la implementación con Tag Manager, unos 30 minutos de prácticas, buena visión!\n\u0026lt;iframe id=\u0026quot;ytplayer\u0026quot; src=\u0026quot;https://www.youtube.com/embed/VblE7FGyBbI\u0026quot; width=\u0026quot;720\u0026quot; height=\u0026quot;405\u0026quot; frameborder=\u0026quot;0\u0026quot; allowfullscreen=\u0026quot;allowfullscreen\u0026quot;\u0026gt;\u0026lt;span data-mce-type=\u0026quot;bookmark\u0026quot; style=\u0026quot;display: inline-block; width: 0px; overflow: hidden; line-height: 0;\u0026quot; class=\u0026quot;mce_SELRES_start\u0026quot;\u0026gt;﻿\u0026lt;/span\u0026gt;\u0026lt;/iframe\u0026gt;  ","date":1517832262,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"6d2bb6b8b015d6ae200d6e7d6f79dbd5","permalink":"https://www.marcusrb.com/integrar-google-optimize-con-tag-manager-experimentos-con-analytics-parte-1/","publishdate":"2018-02-05T12:04:22Z","relpermalink":"/integrar-google-optimize-con-tag-manager-experimentos-con-analytics-parte-1/","section":"post","summary":"Google Optimize no ha tardado en salir en formato BETA hace unos años y integrado con Google Tag Manager a través de una etiqueta que al parecer es fácil de implementar.","tags":["experimentos google analytics","google optimize","tag manager y optimize","test ab google optimize"],"title":"Integrar Google Optimize con Tag Manager – Experimentos con Analytics parte 1","type":"post"},{"authors":null,"categories":["Tag Manager"],"content":" Los eventos de Comercio Electrónico o e-commerce para Google Analytics, siempre ha dado problemas y confusión a la hora de configurarlo correctamente, especialmente si queremos realizarlo integramente en proyectos personalizados, sitios web ad-hoc donde no utilizan especificamente plugin o módulos free, ejemplo un híbrido de Magento, Prestashop o similar.\nLa cuestión es ir explicar paso a paso como grabar los eventos más utilizados de un comercio electrónico, y no solo transacciones. Además, las mismas podemos \u0026#8220;reciclar\u0026#8221; los mismos eventos para otras etiquetas de terceros: AdWords, Remarketing, Facebook, Doubleclick principalmente, además de otras más especificas como TradeDoubler etc.\nEsta guía proporcionará solo y exclusivamente los pasos a implementar en Google Tag Manager en sitios web, sin utilizar mucho código dentro de nuestro site, excepto el enviar informaciones con el dataLayer\nConceptos generales del comercio electrónico en Tag Manager Comercio electrónico clásico de Google Analytics Existen dos métodos principales para implementar el comercio electrónico de Google Analytics:\nEn los informes de comercio electrónico estándar de Google Analytics puede analizar la actividad de compra que se ha registrado en su aplicación o sitio web. Entre otros datos, puede ver información sobre productos y transacciones, el valor de pedido medio, la tasa de conversión de comercio electrónico y el tiempo hasta la compra.\nComercio electrónico mejorado de Google Analytics El comercio electrónico mejorado amplía las funciones de sus informes de Google Analytics. Con este método puede ver cuándo añadieron los clientes productos al carrito, cuándo iniciaron el proceso de tramitación de compra correspondiente y cuándo completaron la transacción. El comercio electrónico mejorado también se puede usar para identificar segmentos de clientes que se quedan fuera del embudo de compras.\nAmbos métodos pueden implementarse con Tag Manager:\nComercio electrónico en Google Analytics  Comercio electrónico estándar   \u0026lt;div class=\u0026quot;answer\u0026quot;\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Para crear una etiqueta de comercio electrónico estándar de Google Analytics: \u0026lt;/p\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt; Habilite el comercio electrónico en sus informes de Google Analytics. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Cree una etiqueta de Universal Analytics y seleccione \u0026lt;em\u0026gt;Transacción\u0026lt;/em\u0026gt; en \u0026lt;strong\u0026gt;Tipo de seguimiento\u0026lt;/strong\u0026gt;. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Configure la etiqueta con los campos obligatorios. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Especifique cuándo debe activarse la etiqueta. \u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;p\u0026gt; Toda la información de la transacción debe transferirse a través de la capa de datos, con los nombres de variable que se muestran a continuación: \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Datos de la transacción\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;table class=\u0026quot;nice-table\u0026quot;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; Nombre de la variable \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Descripción \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Tipo \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; transactionId (obligatorio) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Identificador de transacción único \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; cadena \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; transactionAffiliation (opcional) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Partner o tienda \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; cadena \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; transactionTotal (obligatorio) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Valor total de la transacción \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; numérico \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; transactionShipping (opcional) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Gastos de envío correspondientes a la transacción \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; numérico \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; transactionTax (opcional) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Impuestos correspondientes a la transacción \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; numérico \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; transactionProducts (opcional) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Lista de artículos comprados en la transacción \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; matriz de productos \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Datos del producto\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;table class=\u0026quot;nice-table\u0026quot;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; Nombre de la variable \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Descripción \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Tipo \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; name (obligatorio) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Nombre del producto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; cadena \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; sku (obligatorio) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Código SKU de producto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; cadena \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; category (opcional) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Categoría del producto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; cadena \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; price (obligatorio) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Precio unitario \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; numérico \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; quantity (obligatorio) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Número de elementos \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; numérico \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;p\u0026gt; A continuación, se muestra un ejemplo de cómo implementar el código de la capa de datos en JavaScript: \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Incluya este código \u0026lt;em\u0026gt;encima\u0026lt;/em\u0026gt; del fragmento de contenedor de Tag Manager, de modo que la capa de datos esté lista cuando Tag Manager active la etiqueta de Google Analytics. \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Si, por algún motivo, no puede incluir los datos de comercio electrónico encima del fragmento de contenedor de Tag Manager, también puede probar una de las siguientes soluciones: \u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; Haga que la etiqueta se active según un evento personalizado más abajo en la página (p. ej., combine el envío de eventos y datos, tal como se describe en la documentación para desarrolladores, \u0026lt;strong\u0026gt;esta parte se explicará a continuación de este post.\u0026lt;/strong\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Seleccione el tipo de activador \u0026amp;#8220;DOM preparado\u0026amp;#8221;. \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;div class=\u0026quot;question\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;title\u0026quot;\u0026gt; \u0026lt;i class=\u0026quot;icon-plus acc-icon-plus\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;i class=\u0026quot;icon-minus acc-icon-minus\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;Comercio electrónico mejorado \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;answer\u0026quot;\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Existen dos métodos para implementar el comercio electrónico mejorado con Tag Manager: \u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;b\u0026gt;Capa de datos:\u0026lt;/b\u0026gt; es el método recomendado. Envíe los datos de comercio electrónico a la capa de datos desde el código. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;b\u0026gt;JavaScript personalizado:\u0026lt;/b\u0026gt; cree una variable de JavaScript personalizada en Tag Manager. Esta variable debería devolver un objeto con la información que, en el método anterior, se enviaría a la capa de datos. \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;div class=\u0026quot;alert\u0026quot; role=\u0026quot;alert\u0026quot;\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Importante:\u0026lt;/strong\u0026gt; Al implementar el comercio electrónico mejorado, puede utilizar varios tipos de información específicos. Consulte la \u0026lt;a href=\u0026quot;https://developers.google.com/analytics/devguides/collection/gtagjs/enhanced-ecommerce\u0026quot; target=\u0026quot;_blank\u0026quot; rel=\u0026quot;noopener\u0026quot;\u0026gt;documentación para desarrolladores\u0026lt;/a\u0026gt; si quiere obtener una lista completa de los tipos de datos admitidos. \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Implementación de capa de datos\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt; En el código, añada la información de comercio electrónico a un objeto de capa de datos denominado \u0026amp;#8220;ecommerce\u0026amp;#8221;. Consulte la documentación para desarrolladores de Tag Manager si quiere obtener información sobre cómo implementar una capa de datos. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; En Tag Manager, cree una etiqueta de Universal Analytics y seleccione \u0026lt;strong\u0026gt;Página vista\u0026lt;/strong\u0026gt; o \u0026lt;strong\u0026gt;Evento\u0026lt;/strong\u0026gt; en \u0026lt;strong\u0026gt;Tipo de seguimiento\u0026lt;/strong\u0026gt;. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; En \u0026lt;strong\u0026gt;Más opciones\u0026lt;/strong\u0026gt; \u0026lt;img title=\u0026quot;a continuación\u0026quot; src=\u0026quot;https://lh3.googleusercontent.com/sDpfETHk7K0ryVo50RvXGzPtfrDQ2W0xK4sJdOKYqerlc79U0MaNKQggC7nI6gJY0A=w13-h18\u0026quot; alt=\u0026quot;a continuación\u0026quot; width=\u0026quot;13\u0026quot; height=\u0026quot;18\u0026quot; /\u0026gt; \u0026lt;strong\u0026gt;Publicidad\u0026lt;/strong\u0026gt;, seleccione \u0026lt;em\u0026gt;True\u0026lt;/em\u0026gt; en \u0026lt;strong\u0026gt;Habilitar funciones de comercio electrónico mejorado\u0026lt;/strong\u0026gt;. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Seleccione \u0026lt;strong\u0026gt;Usar capa de datos\u0026lt;/strong\u0026gt;. \u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Usar variables\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt; Cree una variable de JavaScript personalizada. Esta variable debe devolver un objeto que contenga el objeto de comercio electrónico, de un modo similar al ejemplo de código siguiente: \u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt; En esta variable, use la misma sintaxis que usaría para enviar estos datos a la capa de datos. Consulte la documentación para desarrolladores si quiere obtener más información. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Cree una etiqueta de Universal Analytics y seleccione \u0026lt;strong\u0026gt;Página vista\u0026lt;/strong\u0026gt; o \u0026lt;strong\u0026gt;Evento\u0026lt;/strong\u0026gt; en \u0026lt;strong\u0026gt;Tipo de seguimiento\u0026lt;/strong\u0026gt;. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; En \u0026lt;strong\u0026gt;Más opciones\u0026lt;/strong\u0026gt; \u0026lt;img title=\u0026quot;a continuación\u0026quot; src=\u0026quot;https://lh3.googleusercontent.com/sDpfETHk7K0ryVo50RvXGzPtfrDQ2W0xK4sJdOKYqerlc79U0MaNKQggC7nI6gJY0A=w13-h18\u0026quot; alt=\u0026quot;a continuación\u0026quot; width=\u0026quot;13\u0026quot; height=\u0026quot;18\u0026quot; /\u0026gt; \u0026lt;strong\u0026gt;Publicidad\u0026lt;/strong\u0026gt;, seleccione \u0026lt;em\u0026gt;True\u0026lt;/em\u0026gt; en \u0026lt;strong\u0026gt;Habilitar funciones de comercio electrónico mejorado\u0026lt;/strong\u0026gt;. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Seleccione \u0026lt;strong\u0026gt;Usar capa de datos\u0026lt;/strong\u0026gt;. \u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;h3\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;h3\u0026gt; Comercio electrónico mejorado: conceptos y fundamentos \u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt; El comercio electrónico mejorado ofrece una serie de\u0026lt;strong\u0026gt; informes detallados y útiles\u0026lt;/strong\u0026gt;. A continuación se indican algunos de los informes disponibles cuando se habilita el comercio electrónico mejorado de una vista. \u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; Requisitos previos \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Ver los informes de comercio electrónico mejorado \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Datos disponibles \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Visión general de comercio electrónico \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Análisis del comportamiento de compra \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Análisis del comportamiento del proceso de pago \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Rendimiento del producto \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Rendimiento de las ventas \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Rendimiento de la lista de productos \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Promoción interna \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Cupón de pedido \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Cupón de producto \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Código de afiliado \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Recursos relacionados \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;h3\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;h3 id=\u0026quot;ecommerce-data\u0026quot;\u0026gt; Tipos de datos de comercio electrónico mejorado y acciones \u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt; Hay varios tipos de datos de comercio electrónico que puedes enviar usando analytics.js (gtag.js también): \u0026lt;strong\u0026gt;datos de impresión\u0026lt;/strong\u0026gt;, \u0026lt;strong\u0026gt;datos de producto\u0026lt;/strong\u0026gt;, \u0026lt;strong\u0026gt;datos de promoción\u0026lt;/strong\u0026gt; y \u0026lt;strong\u0026gt;datos de acción\u0026lt;/strong\u0026gt;. \u0026lt;/p\u0026gt; \u0026lt;h3\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;h4 id=\u0026quot;impression-data\u0026quot;\u0026gt; Datos de impresión \u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt; Representan la información sobre el producto que se ha visto. Se hace referencia a ellos mediante \u0026lt;code\u0026gt;impressionFieldObject\u0026lt;/code\u0026gt; y contienen los siguientes valores: \u0026lt;/p\u0026gt; \u0026lt;div class=\u0026quot;devsite-table-wrapper\u0026quot;\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; Clave \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Tipo de valor \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Obligatorio \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Descripción \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; id \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;strong\u0026gt;Sí*\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; ID de producto o SKU (por ejemplo, P67890). \u0026lt;strong\u0026gt;*Es necesario definir este campo o el de nombre.\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; name \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;strong\u0026gt;Sí*\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Nombre del producto (por ejemplo, camiseta de Android). \u0026lt;strong\u0026gt;*Es necesario definir este campo o el de ID.\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; list \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Lista o colección a la que pertenece el producto (por ejemplo, Resultados de búsqueda) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; brand \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Marca asociada al producto (por ejemplo, Google) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; category \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Categoría a la que pertenece el producto (por ejemplo, ropa). Usa \u0026lt;code\u0026gt;/\u0026lt;/code\u0026gt; como delimitador para especificar hasta 5 niveles de jerarquía (por ejemplo, Ropa/Hombre/Camisetas). \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; variant \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Variante del producto (por ejemplo, Negro) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; position \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; entero \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Posición del producto en una lista o colección (por ejemplo, 2) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; price \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; moneda \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Precio de un producto (por ejemplo, 29,20) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h3\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;h4 id=\u0026quot;product-data\u0026quot;\u0026gt; Datos de producto \u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt; Los datos de producto representan los productos individuales que se han visto, que se han agregado al carrito, etc. Se hace referencia a ellos mediante el objeto \u0026lt;code\u0026gt;productFieldObject\u0026lt;/code\u0026gt; y contienen los siguientes valores: \u0026lt;/p\u0026gt; \u0026lt;div class=\u0026quot;devsite-table-wrapper\u0026quot;\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; Clave \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Tipo de valor \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Obligatorio \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Descripción \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; id \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;strong\u0026gt;Sí*\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; ID de producto o SKU (por ejemplo, P67890). \u0026lt;strong\u0026gt;*Es necesario definir este campo o el de nombre.\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; name \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;strong\u0026gt;Sí*\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Nombre del producto (por ejemplo, camiseta de Android). \u0026lt;strong\u0026gt;*Es necesario definir este campo o el de ID.\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; brand \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Marca asociada al producto (por ejemplo, Google) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; category \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Categoría a la que pertenece el producto (por ejemplo, ropa). Usa \u0026lt;code\u0026gt;/\u0026lt;/code\u0026gt; como delimitador para especificar hasta 5 niveles de jerarquía (por ejemplo, Ropa/Hombre/Camisetas). \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; variant \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Variante del producto (por ejemplo, Negro) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; price \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; moneda \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Precio de un producto (por ejemplo, 29,20) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; quantity \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; entero \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Cantidad de un producto (por ejemplo, 2) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; coupon \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Código de cupón asociado a un producto (por ejemplo, COMPRA_VERANO13) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; position \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; entero \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Posición del producto en una lista o colección (por ejemplo, 2) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h3\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;h4 id=\u0026quot;promotion-data\u0026quot;\u0026gt; Datos de promoción \u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt; Representan la información sobre una promoción que se ha visto. Se hace referencia a ellos mediante el objeto \u0026lt;code\u0026gt;promoFieldObject\u0026lt;/code\u0026gt; y contienen los siguientes valores: \u0026lt;/p\u0026gt; \u0026lt;div class=\u0026quot;devsite-table-wrapper\u0026quot;\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; Clave \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Tipo de valor \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Obligatorio \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Descripción \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; id \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;strong\u0026gt;Sí*\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; ID de promoción (por ejemplo, PROMO_1234). \u0026lt;strong\u0026gt;*Es necesario definir este campo o el de nombre.\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; name \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;strong\u0026gt;Sí*\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Nombre de la promoción (por ejemplo, Compra de verano) \u0026lt;strong\u0026gt;*Es necesario definir este campo o el de ID.\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; creative \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Creatividad asociada a la promoción (por ejemplo, banner_verano2) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; position \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Posición de la creatividad (por ejemplo, banner_slot_1). \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h3\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;h4 id=\u0026quot;action-data\u0026quot;\u0026gt; Datos de acción \u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt; Representan la información sobre una acción que se lleva a cabo relacionada con el comercio electrónico. Se hace referencia a ellos mediante el objeto \u0026lt;code\u0026gt;actionFieldObject\u0026lt;/code\u0026gt; y contienen los siguientes valores: \u0026lt;/p\u0026gt; \u0026lt;div class=\u0026quot;devsite-table-wrapper\u0026quot;\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; Clave \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Tipo de valor \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Obligatorio \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Descripción \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; id \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;strong\u0026gt;Sí*\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; ID de la transacción (por ejemplo, T1234). \u0026lt;strong\u0026gt;*Es obligatorio si el tipo de acción es \u0026lt;code\u0026gt;purchase\u0026lt;/code\u0026gt; o \u0026lt;code\u0026gt;refund\u0026lt;/code\u0026gt;.\u0026lt;/strong\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; affiliation \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Tienda o afiliación donde se ha originado esta transacción (por ejemplo, Google Store). \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; revenue \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; moneda \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Especifica los ingresos totales o la suma total asociados a la transacción (por ejemplo, 11,99). Este valor puede incluir los gastos de envío, los impuestos u otros ajustes de los ingresos totales que quieras incluir para calcular tus ingresos. \u0026lt;strong\u0026gt;Nota:\u0026lt;/strong\u0026gt; Si no se definen los ingresos, se calculará el valor automáticamente usando los campos de cantidad y de precio de todos los productos pertenecientes al mismo hit. \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; tax \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; moneda \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Impuestos totales asociados a la transacción \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; shipping \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; moneda \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Gastos de envío asociados a la transacción \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; coupon \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Cupón canjeado con la transacción. \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; list \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Lista a la que pertenecen los productos asociados. Opcional. \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; step \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; entero \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Número que representa un paso en el proceso de pago. Es opcional para acciones definidas como \u0026lt;code\u0026gt;checkout\u0026lt;/code\u0026gt;. \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; option \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; texto \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; No \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Campo adicional para acciones de \u0026lt;code\u0026gt;checkout\u0026lt;/code\u0026gt; y \u0026lt;code\u0026gt;checkout_option\u0026lt;/code\u0026gt;que pueden describir la información de opción en la página de pago, como la forma de pago seleccionada. \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h3\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;h4 id=\u0026quot;action-types\u0026quot;\u0026gt; Acciones de producto y de promoción \u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt; Las acciones especifican cómo interpretar los datos de producto y de promoción enviados a Google Analytics. \u0026lt;/p\u0026gt; \u0026lt;div class=\u0026quot;devsite-table-wrapper\u0026quot;\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; Acción \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Descripción \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; click \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Clic en un producto o en un enlace de producto de uno o varios productos \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; detail \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Visualización de los detalles de un producto \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; add \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Uno o varios productos agregados a un carrito de compra \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; remove \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Uno o varios productos suprimidos de un carrito de compra \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; checkout \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Inicio del proceso de compra para uno o varios productos \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; checkout_option \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Envío del valor de opción de un paso de compra determinado \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; purchase \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Venta de uno o varios productos \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; refund \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Devolución de uno o varios productos \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; promo_click \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Clic en una promoción interna \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h2\u0026gt; \u0026lt;/h2\u0026gt; \u0026lt;h2\u0026gt; Implementación del dataLayer \u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt; En las siguientes secciones se describe cómo implementar el complemento de comercio electrónico mejorado para medir la actividad de comercio electrónico en un sitio web con la biblioteca analytics.js. \u0026lt;/p\u0026gt; \u0026lt;h3 id=\u0026quot;sending-data\u0026quot;\u0026gt; Envío de datos de comercio electrónico mejorado \u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt; Una vez cargado, se agregará un par de comandos nuevos específicos del seguimiento de comercio electrónico mejorado al objeto de seguimiento predeterminado, y podrás empezar a enviar datos de comercio electrónico. \u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;#measuring-activities\u0026quot;\u0026gt;Medición de actividades de comercio electrónico\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;#measuring-checkout\u0026quot;\u0026gt;Medir el proceso de pago\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Medición de transacciones \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Medición de devoluciones \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Medición de promociones internas \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt;\u0026lt;aside class=\u0026quot;special\u0026quot;\u0026gt; \u0026lt;strong\u0026gt;Nota:\u0026lt;/strong\u0026gt; Los datos de comercio electrónico solo pueden enviarse con un hit, por ejemplo con un hit de página vista (\u0026lt;code\u0026gt;pageview\u0026lt;/code\u0026gt;) o de evento (\u0026lt;code\u0026gt;event\u0026lt;/code\u0026gt;). Si usas comandos de comercio electrónico, pero no envías hits, o si el hit se ha enviado antes del comando de comercio electrónico, los datos de comercio electrónico no se enviarán.\u0026lt;/aside\u0026gt; \u0026lt;aside\u0026gt;\u0026lt;/aside\u0026gt; \u0026lt;aside\u0026gt;\u0026lt;/aside\u0026gt; \u0026lt;aside\u0026gt; \u0026lt;h3 id=\u0026quot;measuring-activities\u0026quot;\u0026gt; Medición de actividades de comercio electrónico \u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt; Una implementación de comercio electrónico mejorado normal medirá las impresiones de producto y cualquiera de las siguientes acciones: \u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; Clics en un enlace de producto. \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; visualización de los detalles del producto, \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; impresiones y clics de promociones internas, \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; productos agregados o suprimidos de un carrito de compra, \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; inicio del proceso de compra de un producto, \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; compras y devoluciones. \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;h4\u0026gt; \u0026lt;/h4\u0026gt; \u0026lt;h4\u0026gt; Medición de Impresiones de productos \u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Configuración en Google Tag Manager:\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;em\u0026gt;Tipo de etiqueta:\u0026lt;/em\u0026gt; Universal Analytics\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Tipo de medición:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;Páginas Vistas\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activación del comercio electrónico mejorado:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Uso del dataLayer:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activador:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;event\u0026lt;/strong\u0026gt; igual \u0026lt;strong\u0026gt;gtm.dom\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt;\u0026lt;/aside\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;h4\u0026gt; Medición de click en productos \u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Configuración en Google Tag Manager:\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;em\u0026gt;Tipo de etiqueta:\u0026lt;/em\u0026gt; Universal Analytics\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Tipo de medición:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;Event\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Categoria\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;Ecommerce\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Acción\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;Product Click\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activación del comercio electrónico mejorado:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Uso del dataLayer:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activador\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;event\u0026lt;/strong\u0026gt; igual \u0026lt;strong\u0026gt;productClick\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;h4\u0026gt; Medición de detalles en productos \u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Configuración en Google Tag Manager:\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;em\u0026gt;Tipo de etiqueta:\u0026lt;/em\u0026gt; Universal Analytics\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Tipo de medición:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;Páginas Vistas\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activación del comercio electrónico mejorado:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Uso del dataLayer: \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activador:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;event\u0026lt;/strong\u0026gt; igual \u0026lt;strong\u0026gt;gtm.dom\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;h4 id=\u0026quot;track_additions_to_and_removals_from_shopping_cart\u0026quot;\u0026gt; Seguimiento de los productos que se añaden y se retiran en el carrito de la compra: \u0026lt;/h4\u0026gt; \u0026lt;h5\u0026gt; \u0026lt;strong\u0026gt;Medición añadir al carrito\u0026lt;/strong\u0026gt; \u0026lt;/h5\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Configuración en Google Tag Manager:\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;em\u0026gt;Tipo de etiqueta:\u0026lt;/em\u0026gt; Universal Analytics\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Tipo de medición:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;Event\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Categoria\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;Ecommerce\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Acción\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;Add to Cart\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activación del comercio electrónico mejorado:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Uso del dataLayer:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activador\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;event\u0026lt;/strong\u0026gt; igual \u0026lt;strong\u0026gt;addToCart\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;h5\u0026gt; \u0026lt;strong\u0026gt;Medición retirar del carrito\u0026lt;/strong\u0026gt; \u0026lt;/h5\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Configuración en Google Tag Manager:\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;em\u0026gt;Tipo de etiqueta:\u0026lt;/em\u0026gt; Universal Analytics\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Tipo de medición:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;Event\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Categoria\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;Ecommerce\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Acción\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;Remove from Cart\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activación del comercio electrónico mejorado:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Uso del dataLayer:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activador\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;event\u0026lt;/strong\u0026gt; igual \u0026lt;strong\u0026gt;removeFromCart\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;h3 id=\u0026quot;measuring-checkout\u0026quot;\u0026gt; Medición de los procesos de pago \u0026lt;/h3\u0026gt; \u0026lt;h4 id=\u0026quot;checkout-steps\u0026quot;\u0026gt; 1. Medición de los pasos de pago \u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt; En cada paso del proceso de pago, tenemos que implementar el código de seguimiento correspondiente para enviar datos a Google Analytics. \u0026lt;/p\u0026gt; \u0026lt;h5\u0026gt; Campo \u0026lt;code\u0026gt;step\u0026lt;/code\u0026gt; \u0026lt;/h5\u0026gt; \u0026lt;p\u0026gt; En cada paso del pago que midas, debes incluir un valor de \u0026lt;code\u0026gt;step\u0026lt;/code\u0026gt;. Este valor se usa para asignar tus acciones de pago a las etiquetas configuradas en cada paso en \u0026lt;strong\u0026gt;Configuración de comercio electrónico\u0026lt;/strong\u0026gt;. \u0026lt;/p\u0026gt;\u0026lt;aside class=\u0026quot;note\u0026quot;\u0026gt; \u0026lt;strong\u0026gt;Nota\u0026lt;/strong\u0026gt;: Si tienes un proceso de pago de un solo paso o si no has configurado un embudo de compra en la \u0026lt;strong\u0026gt;configuración del comercio electrónico\u0026lt;/strong\u0026gt;, el campo \u0026lt;code\u0026gt;step\u0026lt;/code\u0026gt; es opcional.\u0026lt;/aside\u0026gt; \u0026lt;h5\u0026gt; Campo \u0026lt;code\u0026gt;option\u0026lt;/code\u0026gt; \u0026lt;/h5\u0026gt; \u0026lt;p\u0026gt; Si tienes información adicional sobre un paso de pago determinado en el momento en que se mide, puedes configurar el campo \u0026lt;code\u0026gt;option\u0026lt;/code\u0026gt; con una acción \u0026lt;code\u0026gt;checkout\u0026lt;/code\u0026gt; para capturar esta información. Se podría tratar del tipo de pago predeterminado para el usuario (por ejemplo, \u0026amp;#8220;Visa\u0026amp;#8221;). \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;Configuración en Google Tag Manager:\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;em\u0026gt;Tipo de etiqueta:\u0026lt;/em\u0026gt; Universal Analytics\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Tipo de medición:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;Event\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Categoria\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;Ecommerce\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Acción\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;Checkout\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activación del comercio electrónico mejorado:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Uso del dataLayer:\u0026lt;/em\u0026gt; \u0026lt;strong\u0026gt;true\u0026lt;/strong\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;em\u0026gt;Activador\u0026lt;/em\u0026gt;: \u0026lt;strong\u0026gt;event\u0026lt;/strong\u0026gt; igual \u0026lt;strong\u0026gt;checkout\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt;  ","date":1515164304,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"19bbd3102baa077514730e3189db8e1d","permalink":"https://www.marcusrb.com/guia-implementar-comercio-electronico-en-google-tag-manager-web/","publishdate":"2018-01-05T14:58:24Z","relpermalink":"/guia-implementar-comercio-electronico-en-google-tag-manager-web/","section":"post","summary":"Los eventos de Comercio Electrónico o e-commerce para Google Analytics, siempre ha dado problemas y confusión a la hora de configurarlo correctamente","tags":["comercio electrónico mejorado","google analytics ecommerce","pasos comercio electronico tag manager","tag manager"],"title":"[Guía] Implementar Comercio electrónico en Google Tag Manager para sitios web","type":"post"},{"authors":null,"categories":["Google Analytics","Tag Manager"],"content":" Como se venía anunciando hace muchos meses, finalmente Google Tag Manager activó la posibilidad de medir los eventos del tipo \u0026#8220;videos\u0026#8221; directamente con variables propias.\nDesde que comencé a enamorarme de Tag Manager, fue en 2013, y gracias a la comunidad de Analytics de habla inglesa, la única posibilidad era a través de customHTML, es decir Javascript personalizados con jQuery. Hace mención particular a los desarrolladores de Lunametrics y sus fantásticos repositorios que sigo utilizando y personalizando. No he dejado de utilizarlos, al contrario, gracias a ellos es posible medir muchas más cosas, pero las variables nativas de GTM hace la vida más fácil a los analistas y desarrolladores menos experimentados.\n\u0026nbsp;\nVamos a presentar la interfaz separado por las diferentes partes. Entramos en Variables, , desde Variables Integradas, Configurar -\u0026gt; activaríamos todas las indicada como la imagen de arriba\n \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;div id=\u0026quot;attachment_100\u0026quot; style=\u0026quot;width: 658px\u0026quot; class=\u0026quot;wp-caption aligncenter\u0026quot;\u0026gt; \u0026lt;a href=\u0026quot;https://www.marcusrb.com/img/2017/10/Captura-de-pantalla-2017-10-24-a-las-16.29.04.png\u0026quot;\u0026gt;\u0026lt;img class=\u0026quot;wp-image-100 size-full\u0026quot; src=\u0026quot;https://www.marcusrb.com/img/2017/10/Captura-de-pantalla-2017-10-24-a-las-16.29.04.png\u0026quot; alt=\u0026quot;tracking variables Videos tag manager\u0026quot; width=\u0026quot;648\u0026quot; height=\u0026quot;373\u0026quot; srcset=\u0026quot;https://www.marcusrb.com/img/2017/10/Captura-de-pantalla-2017-10-24-a-las-16.29.04.png 648w, https://www.marcusrb.com/img/2017/10/Captura-de-pantalla-2017-10-24-a-las-16.29.04-300x173.png 300w, https://www.marcusrb.com/img/2017/10/Captura-de-pantalla-2017-10-24-a-las-16.29.04-254x146.png 254w, https://www.marcusrb.com/img/2017/10/Captura-de-pantalla-2017-10-24-a-las-16.29.04-50x29.png 50w, https://www.marcusrb.com/img/2017/10/Captura-de-pantalla-2017-10-24-a-las-16.29.04-130x75.png 130w\u0026quot; sizes=\u0026quot;(max-width: 648px) 100vw, 648px\u0026quot; /\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;p class=\u0026quot;wp-caption-text\u0026quot;\u0026gt; Variables integradas de tipo videos en Tag Manager \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;accordion\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;mfn-acc accordion_wrapper \u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;question\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;title\u0026quot;\u0026gt; \u0026lt;i class=\u0026quot;icon-plus acc-icon-plus\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;i class=\u0026quot;icon-minus acc-icon-minus\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;Activadores \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;answer\u0026quot;\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Referente a los activadores, \u0026lt;img class=\u0026quot;alignnone wp-image-104\u0026quot; src=\u0026quot;https://www.marcusrb.com/img/2017/10/trigger_activador_icon_tagManager_marcusRB-150x150.png\u0026quot; alt=\u0026quot;trigger_activador_icon_tagManager_marcusRB\u0026quot; width=\u0026quot;52\u0026quot; height=\u0026quot;52\u0026quot; srcset=\u0026quot;https://www.marcusrb.com/img/2017/10/trigger_activador_icon_tagManager_marcusRB-150x150.png 150w, https://www.marcusrb.com/img/2017/10/trigger_activador_icon_tagManager_marcusRB-85x85.png 85w, https://www.marcusrb.com/img/2017/10/trigger_activador_icon_tagManager_marcusRB-80x80.png 80w\u0026quot; sizes=\u0026quot;(max-width: 52px) 100vw, 52px\u0026quot; /\u0026gt;tenemos una nueva para activar, en este caso solo para YouTube \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;div id=\u0026quot;attachment_322\u0026quot; style=\u0026quot;width: 1034px\u0026quot; class=\u0026quot;wp-caption aligncenter\u0026quot;\u0026gt; \u0026lt;a href=\u0026quot;https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.41.03.png\u0026quot;\u0026gt;\u0026lt;img class=\u0026quot;wp-image-322 size-large\u0026quot; src=\u0026quot;https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.41.03-1024x673.png\u0026quot; alt=\u0026quot;\u0026quot; width=\u0026quot;1024\u0026quot; height=\u0026quot;673\u0026quot; srcset=\u0026quot;https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.41.03-1024x673.png 1024w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.41.03-300x197.png 300w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.41.03-768x505.png 768w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.41.03-222x146.png 222w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.41.03-50x33.png 50w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.41.03-114x75.png 114w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.41.03.png 1132w\u0026quot; sizes=\u0026quot;(max-width: 1024px) 100vw, 1024px\u0026quot; /\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;p class=\u0026quot;wp-caption-text\u0026quot;\u0026gt; activadores de videos youtube en Tag Manager \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;accordion\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;mfn-acc accordion_wrapper \u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;question\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;title\u0026quot;\u0026gt; \u0026lt;i class=\u0026quot;icon-plus acc-icon-plus\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;i class=\u0026quot;icon-minus acc-icon-minus\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;Etiquetas \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;answer\u0026quot;\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Referente a las etiquetas , \u0026lt;a href=\u0026quot;https://www.marcusrb.com/img/2017/10/etiquetaTag_icon_tagManager_marcusRB.png\u0026quot;\u0026gt;\u0026lt;img class=\u0026quot;wp-image-103 alignnone\u0026quot; src=\u0026quot;https://www.marcusrb.com/img/2017/10/etiquetaTag_icon_tagManager_marcusRB.png\u0026quot; alt=\u0026quot;etiquetaTag_icon_tagManager_marcusRB\u0026quot; width=\u0026quot;53\u0026quot; height=\u0026quot;50\u0026quot; srcset=\u0026quot;https://www.marcusrb.com/img/2017/10/etiquetaTag_icon_tagManager_marcusRB.png 252w, https://www.marcusrb.com/img/2017/10/etiquetaTag_icon_tagManager_marcusRB-153x146.png 153w, https://www.marcusrb.com/img/2017/10/etiquetaTag_icon_tagManager_marcusRB-50x48.png 50w, https://www.marcusrb.com/img/2017/10/etiquetaTag_icon_tagManager_marcusRB-79x75.png 79w\u0026quot; sizes=\u0026quot;(max-width: 53px) 100vw, 53px\u0026quot; /\u0026gt;\u0026lt;/a\u0026gt;utilizamos del tipo Google Analytics, seleccionando como siempre, EVENTOS que para YouTube podemos una estructura como esta: \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;div id=\u0026quot;attachment_323\u0026quot; style=\u0026quot;width: 1034px\u0026quot; class=\u0026quot;wp-caption aligncenter\u0026quot;\u0026gt; \u0026lt;a href=\u0026quot;https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.40.36.png\u0026quot;\u0026gt;\u0026lt;img class=\u0026quot;wp-image-323 size-large\u0026quot; src=\u0026quot;https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.40.36-1024x747.png\u0026quot; alt=\u0026quot;\u0026quot; width=\u0026quot;1024\u0026quot; height=\u0026quot;747\u0026quot; srcset=\u0026quot;https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.40.36-1024x747.png 1024w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.40.36-300x219.png 300w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.40.36-768x560.png 768w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.40.36-200x146.png 200w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.40.36-50x36.png 50w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.40.36-103x75.png 103w, https://www.marcusrb.com/img/2017/09/Captura-de-pantalla-2018-03-11-a-las-15.40.36.png 1137w\u0026quot; sizes=\u0026quot;(max-width: 1024px) 100vw, 1024px\u0026quot; /\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;p class=\u0026quot;wp-caption-text\u0026quot;\u0026gt; ejemplo etiqueta de tag Manager para videos YouTube. Son clásicos eventos de Analytics con variables integradas y jugando un poco con Categoría, Acción, Etiquetas. \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;p\u0026gt; Podemos jugar un poco con el tema de Categorías, Acción y Etiquetas con las variables integradas, inclusive añadir el valor, en este caso podría ser el tiempo consumido, aunque lo interprete como valor monetario. \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Como siempre, hagamos un test en una contenedor prueba para ver su efectividad. Aquí el vídeo, buena visión a tod@s 😉 \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Comentarios y sugerencias, más abajo. \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;iframe width=\u0026quot;1200\u0026quot; height=\u0026quot;675\u0026quot; src=\u0026quot;https://www.youtube.com/embed/yvfB5vFVtgk?feature=oembed\u0026amp;#038;enablejsapi=1\u0026amp;#038;origin=https://www.marcusrb.com\u0026quot; frameborder=\u0026quot;0\u0026quot; allow=\u0026quot;autoplay; encrypted-media\u0026quot; allowfullscreen\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026amp;nbsp; \u0026lt;/p\u0026gt;  ","date":1506266109,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"8b59798510d24c0d3146fc0d753ae181","permalink":"https://www.marcusrb.com/medir-videos-tag-manager-parte-1/","publishdate":"2017-09-24T15:15:09Z","relpermalink":"/medir-videos-tag-manager-parte-1/","section":"post","summary":"Como se venía anunciando hace muchos meses, finalmente Google Tag Manager activó  la posibilidad de medir los eventos del tipo videos; directamente con variables propias.","tags":["videos tag manager","youtube analytics","tracking tag manager"],"title":"Nuevas variables y activadores para medir los {{videos}} en Tag Manager • Parte I","type":"post"},{"authors":null,"categories":["Tag Manager"],"content":" Mi primer post para Tag Manager es hablar sobre ella de manera sencilla, cual es su utilidad y porque todos marketero necesita formarse. Se habla mucho de analítica web, de objetivos, de kpi, experiencia de usuario y siempre caemos en lo de siempre, muchos datos y no tenemos maneras de separar los datos buenos de los malos, muchas veces datos que ni sirven, o interpretaciones mal de los datos.\nHe realizado varios videos tutoriales hablando de ello, explicando el porque se elige Google Tag Manager. Así que iré elencando mis razones de utilizar Google Tag Manager.\nAquí unas ventajas de tenerlo en nuestro plan de medición:\nDefinición  Un Tag Managment System o Sistema de etiquetado , (Tag Manager de manera más coloquial) es un sistema que permite a las empresas mejorar la forma de gestionar las etiquetaso “tags” que requieren incluir en sus sitios web o apps móviles. \nLas empresas con un TMS pueden unificar todas los tags de las diferentes herramientas y plataformas que utilizan (que van desde la analítica al search, social, marketing de afiliados, pasando por el testing, personalización o las herramientas de remarketing, etc.) y realizar su gestión desde una única interfaz.  \u0026nbsp;\n¿Por qué Google Tag Manager y no otro administrador de etiquetas? \u0026nbsp;\n Poner orden al caos —\u0026gt; crear un estándar y pautas de actuación en el si de la organización (roles, departamentos, países) Acelerar tu marketing —\u0026gt; etiquetado más fácil para lanzar campañas más rápido Eficiencia y menor dependencia de IT —\u0026gt; permitir a los desarrolladores centrarse en otras prioridades estratégicas Mayor velocidad de carga de la página —\u0026gt; más ventas Mayor organización de los tags y control sobre las agencias —\u0026gt; mayor seguridad  Orígen TagMan (ahora de Ensighten) fue el primer tag manager que apareció en el año 2007. Desde entonces la madurez del sector digital y el crecimiento de la publicidad de manera exponencial han multiplicado las empresas que proporcionan servicios de tag management. Hasta el punto que al cerrar el 2014 hablamos de entre 12-15 TMS y centenares de posibilidades de tags.\u0026nbsp;  Estas empresas de gestión de tags pueden ser especialistas, aunque muchas ya han diversificado para proporcionar datos de atribución, y algunos (como Google) han entrado en el juego más tarde, después de haber creado otras soluciones estrella. \nExperiencia de Usuario para el TMS  Es importante tener en cuenta las habilidades disponibles en tu equipo de marketing y la experiencia del usuario proporcionada por el TMS.\u0026nbsp;  Si la independencia de IT es un objetivo importante para contratar un TMS, deberás seleccionar una herramienta que lo permita. La interfaz deberá ser friendly para los usuarios no “técnicos”, permitiéndoles añadir tags de manera fácil. \nNúmeros de etiquetas y proveedores  Si nos encontramos ante un sitio web que requiere de una implementación compleja, con un volumen de visitas elevado y con numerosas herramientas es aconsejable usar un Tag Manager potente y dedicado que permite mayor flexibilidad en la identificación de elementos y en la definición de reglas. Tealiumpor ejemplo tiene la posibilidad de integrarse con más de 500 herramientas. \n¿Qué es Google Tag Manager?  TMS: Tag Management System (sistema de gestión de etiquetas)  Objetivo es crear y modificar fragmentos de códigos JavaScript \u0026#8211; HTML en un sitio web de forma remota   Lanzamiento Oct. 2012 \u0026#8211; 2ª vers Oct. 2014 \u0026#8211; actualmente (HTML, SDK, AMP)  Principalmente para elementos de Google (Analytics, AdWords, Doubleclick, GDN)  Continua actualización del repositorio de etiquetas de terceros \nBeneficios de Google Tag Manager  Centraliza las etiquetas en un solo lugar  Reducción de tiempo de implementación  Elementos Reutilizables  Mejoras la colaboración entre dptos Mkt\u0026lt;\u0026gt;IT  Debug y Testing en tiempo real  Cualquiera puede subir una etiqueta*  Plan de Medición y Reporting más exhaustivo  Gratuitos \u0026#8211; Premium (Google 360, Adobe, Tealium, etc)  Foro de ayuda, Community\n*se necesita siempre un conocimiento mínimo previo de JS, CSS, HTML \n\u0026nbsp;\nAquí más datos sobre Google Tag Manager\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n¿Y tú, utilizas Google Tag Manager u otro gestor?\n","date":1503419053,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"561573037002874503e82bd2cff7a1b1","permalink":"https://www.marcusrb.com/que-es-google-tag-manager-y-sus-beneficios/","publishdate":"2017-08-22T16:24:13Z","relpermalink":"/que-es-google-tag-manager-y-sus-beneficios/","section":"post","summary":"Mi primer post para Tag Manager es hablar sobre ella de manera sencilla, cual es su utilidad y porque todos marketero necesita formarse.","tags":["beneficios tag manager","tag management system utilidades","ventajas de google tag manager"],"title":"Qué es Google Tag Manager y sus beneficios","type":"post"},{"authors":null,"categories":["Google Adwords"],"content":"Hola, seguramente habrás leído mucho sobre AdWords y quizá si estás aquí es porqué estabas buscando trucos o consejos de Google AdWords, bueno esta era la idea de crear esta categoría, “Consejos\u0026Trucos de Adwords” Sobre AdWords hay mucho, pero muchisimo material online, así que pongamos un poco de orden al todo. Los consejos, trucos y técnicas van en orden del básico a las más avanzadas, así que no te preocupes.  Sabías que existe además de la cuenta clásica de Adwords, una cuenta especial denominada MCC o Mi Centro clientes?\nY explicamos cosa es. Cuando abres una cuenta de Adwords, normalmente te pide de indicar una cuenta correo electrónico, bien las que ya usas de gmail (o de otro servicio de Google), o la que siempre has estado utilizando, las tuya personal, de empresa, de yahoo, telefónica, vodafone, hotmail, etc…\nVas introduciendo los datos de email y contraseña, confirmas. Bueno ya está, no? Puedes ya comenzar con utilizar Adwords y crear campañas y ir configurando la cuenta.\nY que pasa si quieres abrir otra cuenta para otra empresa o bien a tu primo o como freelance?…Pues , el sistema ya no te deja con este email, ya que solo permite vincular una sola cuenta adwords, así que ya comienzan los problemas de nuestra día a día….cuantos mails y contraseñas estamos manejando cada día? uff…yo he contado que manejo diariamente personales y laborales, unas 65 cuentas de correo, al día!, pues sí….además de los seudónimos para otros fines SEO y Social Media….puedo llegar a 300 cuentas de email y no todas tienen la misma contraseña…así que ya tengo que armar un excel con todos bien organizado.\nNo hay mucho problemas en abrir y cerrar si queremos gestionar por ejemplo, 15 cuentas de adwords …no? Pero si fueran 150 ?…peor aún…y más si estamos trabajando en una agencia de publicidad, o como freelance no?\nY aquí llega Adwords con la cuenta del Mi Centro Clientes, denominada MCC es decir es una cuenta global (bien para uso empresarial o bien para uso personal), donde en su interior hay todas las cuentas que vayamos metiendo, mediante invitación o que vayamos creando.\nAsí que más sencillo, con 1 sola cuenta de correo electrónico, podemos gestionar muchas cuentas de Adwords, y además otras MCC, y así sucesivamente…\nSi quieres abrir una cuenta MCC en lugar de una personal, siempre puedes hacerlo desde link oficial aquí, MCC Adwords pero siempre y cuando tu cuenta de correo electrónico no esté vinculada con un account de adwords… \nCuales son las demás ventajas además de gestionar multiples cuentas? Hay muchas ventajas además de gestionar más cuentas a la vez, hay ya posibilidad de crear informes globales , mensuales, semanales, o diarios de cuenta, campañas, grupos de anuncios , palabras claves o de rendimiento de todas las cuentas o que queremos seleccionar, así programando su envío por mail o bien en formatos excel o pdf.\nY no termina aquí, también podemos programar tareas personalizadas a nivel de cuenta, campaña, grupo de anuncios, palabras claves o texto de anuncios y incluso manejar la facturación y para los más “manitas” comando personalizados o script a nivel API.\nEs decir, imaginemos que trabaja para una multinacional, empresa que vende en 30 países de Europa y quiere en cada país administrar sus cuentas de AdWords, pues, ya sabes, abrir una MCC principal, donde vayamos invitando la MCC de esta empresa, y en esa las 30 cuentas de AdWords. Recuerdas que en la principal puedes invitar más MCC (más ordenado lo tengas mejor para su ubicación y fácil para su control de la herramienta).\nEspero te haya servido este consejo, y cualquier duda comente sobre este mismo post. Hasta el próximo consejo. \nPuja Automática  Por puja automática definimos siempre base al budget y/o bien un coste máximo por click, automaticamente a todas las palabras de la campaña (en otra ocasión veremos como hacerlo de manera separada) y Google trabajará por nosotros. Pues sí, practicamente él se encarga de “optimizar” en la mejor forma el cpc destinado a la puja, siempre teniendo en cuenta de la competencia de la keyword, de nuestra QS (quality score = nivel de calidad), CTR% historico, extensiones de anuncios y la relevancia del anuncio y de la página de destino.\nNo siempre funciona para todas las cuentas, de hecho no siempre es aconsejable utilizarlo para quien dispone de presupuesto”ilimitado”. Para los pequeños budget (1, 5, 10 al día), quizás es una buena opción al principio para estabilizar la primera toma de contacto con la herramienta sea Google o Bing, para que vayamonos acostumbrando a la herramienta y vamos a tener más control sobre ella, luego optaríamos con una puja manual.\nA veces he estado utilizando el sistema de pujas autómatica en campañas de dificil contro, por ejemplo los mercados americanos, inglés y alemán los CPC por regla general de grandes sectores son los más cotizados y sus CPC pueden llegar hasta 40euros. Pués a través de una buena segmentación / optimización de la cuenta es posible a principio trabajar con pujas automáticas para estabilizar nuestra cuenta y luego jugar con ellas. Sí, he logrado bajar CPC de 25eur a 2,50, y también aumentaron conversiones…todo es posible pero habrá que ir con cuidado.\n","date":1500802822,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"ebceef0631c7487b0a2d023b0e29219b","permalink":"https://www.marcusrb.com/mcc-vs-cuenta-personal-tecnicas-y-consejos-de-google-adwords/","publishdate":"2017-07-23T09:40:22Z","relpermalink":"/mcc-vs-cuenta-personal-tecnicas-y-consejos-de-google-adwords/","section":"post","summary":"Hola, seguramente habrás leído mucho sobre AdWords y quizá si estás aquí es porqué estabas buscando trucos o consejos de Google AdWords, bueno esta era la idea de crear esta categoría, “Consejos\u0026Trucos de Adwords” Sobre AdWords hay mucho, pero muchisimo material online, así que pongamos un poco de orden al todo. Los consejos, trucos y técnicas van en orden del básico a las más avanzadas, así que no te preocupes.  Sabías que existe además de la cuenta clásica de Adwords, una cuenta especial denominada MCC o Mi Centro clientes?","tags":["cuenta adwords agencia","cuenta adwords mcc","cuenta adwords personal"],"title":"MCC vs cuenta personal – técnicas y consejos de google AdWords","type":"post"},{"authors":null,"categories":["Google Adwords"],"content":" Si hablamos tanto de Google AdWords o Microsoft Bing AdCenter, después estudiar la competencia, nuestro target, palabras claves , crear las diferentes landing page y pensar al budget diario y/o mensual, hay una parte muy interesante relacionado con la configuración de la puja de nuestra palabras claves: automática o manual. Cómo funciona una puja?  Realmente es interesante saber como funciona, porqué es desde momento que entran en juego mucho elementos para que nuestros anuncios sean visualizados al usuario y que hagan clic, pero sobretodo aqui se decide cuanto finalmente pagamos para cada clic. El sistema de CPC, pago por clic, funciona mediante subasta, ya como la bolsa o las subastas del Estado, más o menos es lo mismo. Pero entran en juego muchos factores a la hora de entrar en la subasta, y todo esto comienza en el momento que el usuario teclea la palabra clave en el buscador.  Puja Manual  Supongamos que somos una tienda online de bicicletas, y queremos solamente anunciar la categoría MTB. Elaboramos la campaña , grupos de anuncios y comenzamos a seleccionar las palabras claves. Cuando se eligen las palabras claves, muchas de ellas ya han tenido tráfico historicamente y conservan también una calidad, si es buena o mala. La calidad de esta palabra con que comenzamos a trabjar casi siempre es un 5/10, también podría ser más alta o más baja. Además de esto, también se calcula un coste en base a los competidores que pujaron historícamente y los actuales pujadores. Su precio se divide en dos partes: Estimación de la primera página y Estimación del top de página. Es decir tiene un coste inicial solo para aparecer en la página (desde la primera posición hasta la última, en total son 11, – 3 arriba y 8 laterales), pero también calcula la estimación para aparecer en el top 3, obviamente más cara. Así que tenemos una palabra clave (concordancia amplia, frase, exacta o amplia modificada tendrá sus diferencias), una QS (nivel de calidad), un coste inicial por click. Ahora bien, si queremos comenzar a “jugar”, solo tenemos que elegir el budget diario y el precio máximo que queremos pagar por cada click. Es lógico pensar que al igual de condiciones y factores, si disponemos de 10 euros al día en una campaña, y destinamos por 10 palabras en un grupo de anuncios y cada uno tiene seleccionado un CPC máximo de 0,10eur, si tuvieramos clic, el todo llegaría hasta agotar los 10 euros (durante el día / franja horaria seleccionada u otras condiciones). Tot: 10eur/0,10eur = 100 clics  Puja Automática (Maximizar Clics)  Por puja automática definimos siempre base al budget y/o bien un coste máximo por click, automaticamente a todas las palabras de la campaña (en otra ocasión veremos como hacerlo de manera separada) y Google trabajará por nosotros. Pues sí, practicamente él se encarga de “optimizar” en la mejor forma el cpc destinado a la puja, siempre teniendo en cuenta de la competencia de la keyword, de nuestra QS (quality score = nivel de calidad), CTR% historico, extensiones de anuncios y la relevancia del anuncio y de la página de destino. No siempre funciona para todas las cuentas, de hecho no siempre es aconsejable utilizarlo para quien dispone de presupuesto”ilimitado”. Para los pequeños budget (1, 5, 10 al día), quizás es una buena opción al principio para estabilizar la primera toma de contacto con la herramienta sea Google o Bing, para que vayamonos acostumbrando a la herramienta y vamos a tener más control sobre ella, luego optaríamos con una puja manual. A veces he estado utilizando el sistema de pujas autómatica en campañas de dificil contro, por ejemplo los mercados americanos, inglés y alemán los CPC por regla general de grandes sectores son los más cotizados y sus CPC pueden llegar hasta 40euros. Pués a través de una buena segmentación / optimización de la cuenta es posible a principio trabajar con pujas automáticas para estabilizar nuestra cuenta y luego jugar con ellas. Sí, he logrado bajar CPC de 25eur a 2,50, y también aumentaron conversiones…todo es posible pero habrá que ir con cuidado.  ","date":1498210680,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"f93305e747ae8dac0e0216329cb1ec7e","permalink":"https://www.marcusrb.com/puja-cpc-manual-automatica-maximizar-clics/","publishdate":"2017-06-23T09:38:00Z","relpermalink":"/puja-cpc-manual-automatica-maximizar-clics/","section":"post","summary":"Si hablamos tanto de Google AdWords o Microsoft Bing AdCenter, después estudiar la competencia, nuestro target, palabras claves , crear las diferentes landing page y pensar al budget diario y/o mensual, hay una parte muy interesante relacionado con la configuración de la puja de nuestra palabras claves: automática o manual. Cómo funciona una puja?  Realmente es interesante saber como funciona, porqué es desde momento que entran en juego mucho elementos para que nuestros anuncios sean visualizados al usuario y que hagan clic, pero sobretodo aqui se decide cuanto finalmente pagamos para cada clic.","tags":["pujas adwords","pujas automatica","pujas manuales","smart bidding"],"title":"Puja CPC Manual o Automática – maximizar clics?","type":"post"},{"authors":null,"categories":["Analítica Web"],"content":" Heading \u0026lt;h2\u0026gt;Si estás leyendo esta entrada seguramente eres uno de ellos. Noo? Mejor, pero seguramente quieres saber algo más sobre Analítica Web y conversiones. \u0026lt;/h2\u0026gt; Es una de las preguntas que más oigo a muchos posibles clientes, que me llaman o me envian mail y alumnos. Que pasa cuando no hay conversiones? Que más tengo que hacer, o estrategia tengo que seguir? Que estoy haciendo mal? Todas a estas y más preguntas, existen unas respuestas estándar y otras de sugerencias para detectar el mejor camino. no-repeat;center top;; auto 30 default Heading \u0026lt;h3\u0026gt;Definición de conversión\u0026lt;/h3\u0026gt; Primero vamos a hablar de conversiones u objetivos. Si bien recordamos de algún curso de Analytics, de Adwords, técnicas de marketing digital, que lo has leído en algún foro o por ahí, la conversión es realmente interesante que se cumpla en nuestra web para que vayamos entendiendo mejor la estrategía de marketing implementada. Si por ejemplo tenemos un comercio electrónico de bicicletas, nuestra conversión principal será la adquisición de una bici y/o componentes. Si tenemos una web de servicios, nuestra conversión principal será la contratación de unas de nuestras prestaciones y si tenemos un blog, nuestra conversión principal será “el consumo” de nuestro materiales, o compartir nuestros posts, o que se subscriban en algún enlace de nuestros afiliados. Existen diferentes conversiones y todas tienen que llegar a cumplir unas etapas hacía la meta, “goal” en inglés. Cuantas conversiones podemos tener? Los más expertos en marketing digital han logrado separar las conversiones en dos grupos : micro conversión e macro conversión. no-repeat;center top;; auto 20px Heading \u0026lt;h3\u0026gt;Micro conversiones, los pequeños pasos que te llevan al éxito\u0026lt;/h3\u0026gt; De todos los clientes que conozco, no he conocido a ninguno que tenía clara como funciona la Analítica Web y la configuración de las conversiones. Si hablamos de micro-conversiones, son todas aquellos “eventos” que vamos a implementar en nuestra web. Van de pequeños objetivos a comportamientos de los usuarios que tienen que llevar a cabo en nuestras páginas claves, categorías, productos, o en banner, fotos, post, carrito de la compra, etc etc, Todo lo que tenemos que hacer es “medir todo lo medible”, ya que no hay otra forma de espiar nuestros clientes. A través de Google Analytics, pero sobre todo con herramienta más complejas como Google Tag Manager, es posible crear tantos eventos para poder trackear los pasos de los usuarios y llevarlo Conversiones en un segundo momento. no-repeat;center top;; auto 20px Heading \u0026lt;h3\u0026gt;Macro conversiones, la segmentación de nuestro público objetivo. \u0026lt;/h3\u0026gt; Si realmente las micro-conversiones nos ayuda a crear un lugar más “ameno” para que nuestros clientes se sienten a gusto con nuestra web, las macro-conversiones además de segmentar el público objetivo, nuestro target, nos permite capturar, fidelizar y aumentar seguidores. Cómo? Las conversiones que llevan a rellenar formularios, newsletter, descargar ebooks, carrito de la compra y adquisición, llamada telefónica, son todas acciones que llevan a conocer el cliente, quien es, de donde viene, que hace, que demanda, etc etc no-repeat;center top;; auto 20px Heading \u0026lt;h3\u0026gt;Y ahora? \u0026lt;/h3\u0026gt; Ahora hace falta solo estudiar bien tus objetivos principales y empezar a crear una estrategia de marketing para que ya no tengas excusas de decir “no tengo conversiones” porqué si vas a empezar a obtener las micro-conversiones que hará que lleguen a las macro-conversiones, y si no funciona la primera estrategia.  ","date":1495532083,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"af90baf8eb752caa944518c00fa2e60e","permalink":"https://www.marcusrb.com/no-conversiones-hago/","publishdate":"2017-05-23T09:34:43Z","relpermalink":"/no-conversiones-hago/","section":"post","summary":"Heading \u0026lt;h2\u0026gt;Si estás leyendo esta entrada seguramente eres uno de ellos. Noo? Mejor, pero seguramente quieres saber algo más sobre Analítica Web y conversiones. \u0026lt;/h2\u0026gt; Es una de las preguntas que más oigo a muchos posibles clientes, que me llaman o me envian mail y alumnos. Que pasa cuando no hay conversiones? Que más tengo que hacer, o estrategia tengo que seguir? Que estoy haciendo mal? Todas a estas y más preguntas, existen unas respuestas estándar y otras de sugerencias para detectar el mejor camino.","tags":["conversiones adwords","macro conversiones","micro conversiones","objetivos a trackear"],"title":"No Tengo Conversiones, Que Hago?","type":"post"},{"authors":null,"categories":["seo-sem"],"content":" A través de ésta infografia, se muestra de forma básica qué aspectos on site y off site pueden ser considerados en un primer análisis a un sitio web, con ésto, va a permitir identificar carencias y puntos de mejora De forma más elaborada Geoff Keyton en “Technical Site Audit Checklist” propone un checklist más elaborado para llevar a cabo una auditoría técnica sobre el sitio web, los principales puntos de control son:  1. **Visibilidad de la indexación que tienen los buscadores sobre el sitio web** Indexación de las páginas Búsqueda por marca y términos vinculados a la marca Comprobar la caché 2. **Contenido** Contenido de la página de inicio Landing pages Contenido de relevancia Uso apropiado de keywords target Canibalización de keywords Correcto del marcado semántico a través del etiquetado Cantidad de contenido vs Publicidad (Ads) 3. **Contenido Duplicado** Comprobar la existencia de URLs por contenido único Comprobar la existencia de contenido duplicado a través de la búsqueda (ej. scraped) Contenido duplicado en subdominios Chequear la versión segura del sitio Chequear otros sitios de la compañía 4. **Accesibilidad del sitio** Comprobar el fichero robots.txt Deshabilitar Javascript, cookies y CSS Comprobar si se está haciendo uso de cloacking Comprobar la existencia de las páginas de error 4xx y 5xx 5. **Arquitectura del sitio** Analizar la arquitectura Landing pages Número de páginas por categoría Paginación existente en la navegación Número de clicks que se hacen desde diferentes contenidos en el sitio (entre 2 y 5 clicks) Priorización del contenido de relevancia 6. **Aspectos técnicos** Uso de redireccionamientos (ej. 301, 302) Uso de JavaScript Uso de iframes Uso de flash Velocidad de la página Uso de los Alt en el texto Chequear errores que arroja la herramienta Google Webmaster Tools XML Sitemaps 7. **Canonicalización** Gestión a través de 301′s, Google Webmaster Tools, etiqueta Rel canonical Uso de URLs absolutas frente a URLs relativas 8. **URLs** Existencia de URLS claras, evitando en exceso el uso de parámetros o sesiones ID URLs cortas (a ser posible menos de 115 caracteres, favorece sobre todo aspectos de usabilidad) URLs descriptivas 9. **Linking Interno** Número de enlaces en una página (ej. 100 es un buen número, pero no una regla) Enlaces verticales Enlaces horizontales Enlaces que están en el contenido Enlaces en el pie Uso del anchor text interno Comprobar enlaces rotos 10. **Etiquetado: Etiqueta Title** Contenido únicas en el Title por página Keywords de calidad Las Keywords principales deberían estar al inicio de la etiqueta Title Uno del nombre de la marca dentro de la etiqueta Que tenga una longitud entre 65-75 caracteres 11. **Etiquetado: Etiquetas Meta** Uso de la etiqueta Keywords Uso de la etiquetas description Uso de la etiqueta Meta robots - [See more at](https://web.archive.org/web/20160509065512/http://es.marcusrb.com:80/seo/auditoria-seo-de-un-sitio-web-puntos-de-analisis/#sthash.K1muySj5.dpuf):  De forma más elaborada Geoff Keyton en “Technical Site Audit Checklist” propone un checklist más elaborado para llevar a cabo una auditoría técnica sobre el sitio web, los principales puntos de control son: ","date":1490258312,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"d62e6459bbe2f2bff7f3b8d385d4a09e","permalink":"https://www.marcusrb.com/auditoria-seo-sitio-web-puntos-analisis/","publishdate":"2017-03-23T08:38:32Z","relpermalink":"/auditoria-seo-sitio-web-puntos-analisis/","section":"post","summary":"A través de ésta infografia, se muestra de forma básica qué aspectos on site y off site pueden ser considerados en un primer análisis a un sitio web, con ésto, va a permitir identificar carencias y puntos de mejora De forma más elaborada Geoff Keyton en “Technical Site Audit Checklist” propone un checklist más elaborado para llevar a cabo una auditoría técnica sobre el sitio web, los principales puntos de control son:  1.","tags":["auditoria seo","checklist seo"],"title":"Auditoría SEO de un Sitio Web: Puntos de Análisis","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"6f8c6b33263a59bd4e4de690decef4a9","permalink":"https://www.marcusrb.com/projects/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/projects/external-project/","section":"projects","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"projects"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"79cfc0c3ddcfb9255d2520d5f850143e","permalink":"https://www.marcusrb.com/projects/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/projects/internal-project/","section":"projects","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"projects"},{"authors":["Marco Russo","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://www.marcusrb.com/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Marco Russo","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609845561,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://www.marcusrb.com/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]