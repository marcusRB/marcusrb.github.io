[{"authors":["admin"],"categories":null,"content":"  “If you torture the data long enough, it will confess.” – Ronald Coase\n Data Specialist \u0026amp; Machine Learning Engineer  Data collection Data prep and processing Data mining \u0026amp; Data modeling Business Intelligence \u0026amp; Big Data Analyst AI: Machine Learning \u0026amp; Deep Learning Data Visualization: Power BI - Tableau - Qlik - Google Data Studio Big Data environment AWS • GCP • Azure • Databricks Robotics \u0026amp; IoT  Digital Analyst  Digital Analytics strategic planning Measurement plan and Tag management implementation Data Analysis \u0026amp; Visualization Consultant SEO \u0026amp; SEM (Search, Display, YouTube, Shopping, Mobile)  Long experience in project management for large accounts in retail, bank-finance, insurance, energy \u0026amp; power, industry and corporate. National industrial and commercial sectors in Spain, Europe and LATAM.\nProfessor and in-company trainer: In the last 8 años he impartido +3500 hours in:\n data science, artificial intelligence, machine learning, deep learning, material creation and tutoring in undergraduate, postgraduate and master\u0026rsquo;s programs, as well as courses in data science and data analytics. big data, infrastructure in cloud AWS, GCP, Azure, tutorials for machine learning and I use a pipeline for ETL, Storage, Database, exploitation and visualization. Hadoop and Spark. Knime, RapidMiner, H2O, Databricks mining tools, among many. data analytics, fundamentals and Data mining in R-Studio, Weka, Python business intelligence, ETL: Talend, Pentaho, SSIS - Data Warehouse, SQL, data modeling: SSAS, DAX data visualization, data visualization and interpretation with control panel and dashboard in Google Data Studio, PowerBI, Tableau, Qlik Sense, Grafana. digital marketing, SEO, Google Ads (ex AdWords), FacebookAds, Google Analytics, Tag Manager, Optimize. ecommerce, SEO, Prestashop, Wordpress, Electronic Commerce and Mobile  In class training and full remote: business school:\n EAE Business School IIM Instituto Internacional de Marketing IEDGE Business School IMF Business School IEBS Business School CICE escuela profesional  universities:\n Colaborador/Consultor en la UOC Universitat Oberta Catalunya MIT Massachusetts Institute Technology  other:\n Neoland Fictizia AdveiSchool Digital Brain Aula Creactiva  eLearning platform:\n Udemy YouTube Google Scholar EdX / Coursera vTutor  Other collaborations and in-company training in Machine Learning, Data Analytics and Digital Analysis-Tag Management and Data Visualization.\nInClass training:  Master in Business Intelligence / course on Business Intelligence, ETL, Power BI, SSAS, Data Integration at CICE Business School\n Bootcamp of Data Science and Data Analytics, Machine Learning, Deep Learning and Big Data in ** Master in Data Science Neoland ** face-to-face and online in Madrid.\n Data Science and Deep Learning Bootcamp in ARTIFICIAL INTELLINGE SCHOOL\n Data Analysis, Google Tag Manager and Digital Analytics classroom / distance courses at KPIschool.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1609845561,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.marcusrb.com/en/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/authors/admin/","section":"authors","summary":"“If you torture the data long enough, it will confess.” – Ronald Coase\n Data Specialist \u0026amp; Machine Learning Engineer  Data collection Data prep and processing Data mining \u0026amp; Data modeling Business Intelligence \u0026amp; Big Data Analyst AI: Machine Learning \u0026amp; Deep Learning Data Visualization: Power BI - Tableau - Qlik - Google Data Studio Big Data environment AWS • GCP • Azure • Databricks Robotics \u0026amp; IoT  Digital Analyst  Digital Analytics strategic planning Measurement plan and Tag management implementation Data Analysis \u0026amp; Visualization Consultant SEO \u0026amp; SEM (Search, Display, YouTube, Shopping, Mobile)  Long experience in project management for large accounts in retail, bank-finance, insurance, energy \u0026amp; power, industry and corporate.","tags":null,"title":"Marco Russo","type":"authors"},{"authors":null,"categories":null,"content":" Guias, recursos y tutoriales de Microsoft Power BI, con ejemplos a descargar con Power Query, Power M, DAX y visualización de datos.\nEstructura recursos Power BI Intro Import data Data transformation strategies  ","date":1609718400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609850196,"objectID":"81333248adf93795b4740df2296c25f1","permalink":"https://www.marcusrb.com/en/power-bi-resources/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/en/power-bi-resources/","section":"resources","summary":"Microsoft Power BI tutorials.","tags":null,"title":"Tutoriales de Power BI, DAX, Power Query, Power M","type":"docs"},{"authors":null,"categories":null,"content":" Recursos de Cloud Computing especificamente de los entornos de los proveedores más importantes del mercado: Amazon Web Services AWS, Google Cloud Platform GCP, Microsoft Azure, Databricks.\n [Online courses]() Project or software documentation Tutorials Cheatsheets  Índice Learning Path Estructura programación de Cloud computing Introducción Cloud computing GCP - Google Cloud Platform AWS - Amazon Web Services Azure - Microsoft Azure [ ] [Databricks - ]  ","date":1607385600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"97efdd488bfee045261ddb56512f9a3d","permalink":"https://www.marcusrb.com/en/courses/cloud-computing/","publishdate":"2020-12-08T00:00:00Z","relpermalink":"/en/courses/cloud-computing/","section":"courses","summary":"Learning Path de Cloud Computing. Material, tutoriales, documentos técnicos ingenería de datos, data pipeline, scripting e implementación de servicios en la nube.","tags":null,"title":"Aprendizaje en Cloud computing","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1607382000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"65050c12220534faa7a2f3280e15cd5a","permalink":"https://www.marcusrb.com/en/aws-amazon-web-services/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/en/aws-amazon-web-services/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción AWS","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1607382000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"3e8a3d8844fcfe73902c7df7d057deea","permalink":"https://www.marcusrb.com/fundamentos-cloud-computing/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/fundamentos-cloud-computing/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción Cloud computing","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Cheatsheets GIT  Aprender GIT  Cheatsheets R Studio  Base R Rstudio IDE Data Transformation in R Datatable in R Regex in R String in R Shiny Data Visualization in R  Cheatsheets Maths para Data Science  Matemática para Machine Learning Álgebra lineal para Machine Learning Data Science Cheatsheet example tex doc  ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"44b196b46131409cb10727bbc84cba89","permalink":"https://www.marcusrb.com/en/resources/cheatsheets/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/resources/cheatsheets/","section":"resources","summary":"Se presentan diferentes cheatsheets de programación Python, Pandas, Numpy, Machine Learning.","tags":null,"title":"Recursos de Cheatsheets","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Los recursos de este curso están disponibles en:\n Cursos online fundamentos de analítica digital Cursos online Avanzado con analítica digital Video-Tutoriales Google Analytics Test examen Certificación Google Analytics Examénes Google Academy  Índice Learning Path Google Analytics ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"52431789d4fe56f32b529fd1ecd63443","permalink":"https://www.marcusrb.com/curso-google-analytics/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/curso-google-analytics/","section":"courses","summary":"Learning Path de Google Analytics. Aprende a como realizar informes, analizar e implementar eventos en Google Analytics.","tags":null,"title":"Learning Path de Google Analyitcs","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Introducción a Unix Sistema de ficheros [Control de procesos] Flujos de información Procesamiento de ficheros de texto Expresiones regulares Scripts de Bash Administrador Administración de software 1 Variables de entorno Administración de software 2 Redes  ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"2891a4ce7c14b5b0de4e0c4cbd7eb14f","permalink":"https://www.marcusrb.com/tutorial-unix/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/tutorial-unix/","section":"resources","summary":"Tutorial de Unix.","tags":null,"title":"Tutorial de Unix, Shell y Bash","type":"docs"},{"authors":null,"categories":null,"content":"  [Online courses]() Project or software documentation Tutorials Cheatsheets  Índice Learning Path Estructura programación de Business Analytics Introducción de SQL Advanced SQL [SQL Server]() SQL Server Integration Services SQL Server Analysis Services SQL Server Reporting Services  ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"1f58ddac04fd2a061a88d66414e8cda1","permalink":"https://www.marcusrb.com/en/courses/business-analytics/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/business-analytics/","section":"courses","summary":"Learning Path de Business Intelligence, Business y Data Analytics. Material, tutoriales, documentos técnicos sobre minería de datos, exploración estadísticos.","tags":null,"title":"Aprendizaje en Business Analytics","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"d59f424b4cd53277c87ebb50085b3f26","permalink":"https://www.marcusrb.com/gcp-google-cloud-platform/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/gcp-google-cloud-platform/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción GCP","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"d01ea0b2b0f7120296a27691f6a6adac","permalink":"https://www.marcusrb.com/en/courses/business-analytics/intro-sql/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/business-analytics/intro-sql/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción SQL","type":"docs"},{"authors":null,"categories":null,"content":" TBD\nGetting Started Aprenda el flujo de trabajo para manejar grandes conjuntos de datos con BigQuery y SQL\nSelect, From \u0026amp; Where Los componentes principales para cualquier query de SQL\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"79c6be3ff774dac5fed33564fb40a0e0","permalink":"https://www.marcusrb.com/microsoft-azure/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/microsoft-azure/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en lenguaje SQL. Herramientas cuál BigQuery de Google.","tags":null,"title":"Introducción Azure","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 2 millones de usuarios a nivel mundial, R se convierte rápidamente en el lenguaje de programación líder en estadística y ciencia de datos. Cada año, el número de usuarios de R crece en 40%, y cada vez más organizaciones lo están usando para sus actividades cotidianas. En esta introducción a R, vas a dominar los elementos básicos de este bello lenguaje de programación: vectores, factores, listas y data frames. Con el conocimiento obtenido en este curso serás capaz de llevar a cabo tus propios análisis de datos.\nIntroducción En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!\nVectores Vamos a ir a Las Vegas! donde aprenderemos como analizar los resultados de los juegos usando vectores en R! Después de completar este capítulo serás capaz de crear vectores en R, nombrarlos, seleccionar elementos de ellos y comparar diferentes vectores.\nMatrices En este capítulo vamos a aprender a trabajar con matrices en R. Al finalizar el capítulo serás capaz de crear matrices y hacer operaciones básicas con éstas. Vamos a analizar cuánto dinero hizo la película Star Wars para ilustrar el uso de matrices en R. Que la fuerza te acompañe!\nFactores Existen un tipo de variables llamadas variables categóricas. Por ejemplo, el género puede ser femenino o masculino. En R las variables categóricas son llamadas factores. Dada la importancia de los factores en el analisis de datos, vamos a aprender a crearlos y a todo lo relacionado con su manejo. Empecemos!\nData frames La mayoría de los datos con los que trabajarás en R van a ser guardados en data frames. Al finalizar este capítulo vas a ser capaz de crear data frames, seleccionar partes del mismo y ordenar los datos que contiene de acuerdo a cierta variable.\nListas Las listas, en contraste con los vectores, pueden tener elementos de diferentes tipos. En este capítulo aprenderemos a crear, nombrar y extraer elementos de las listas. ¿Listo? ;-)\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"902e93500041e1b33816f91262b7ce36","permalink":"https://www.marcusrb.com/en/courses/r-studio/intro-r/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/r-studio/intro-r/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"Introducción de R","type":"docs"},{"authors":null,"categories":null,"content":" Machine Learning con Python [Python Web Scraping Tutorials] [Linear Regression in Python] [Practical Text Classification With Python and Keras]  ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"47f68833bd7241e86982e41910ee6ea1","permalink":"https://www.marcusrb.com/en/resources/tutorials/python/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/resources/tutorials/python/","section":"resources","summary":"En la sección de tutoriales, se vincularán los diferentes proyectos realizados, así como los tutoriales de IoT, Raspberry, Python, R, Machine Learning, además de analítica avanzada de datos.","tags":null,"title":"Python","type":"docs"},{"authors":null,"categories":null,"content":"En esta sección se publicarán los diferentes formatos de cursos y tutoriales, de rutas de aprendizajes de la plataforma de Google Marketing, Data Science, Data y Business Analytics, Data Visualization. Además de las documentaciones técnicas, manuales, recursos y actualizaciones. Los repositorios serán accesibles en GitHub o Bitbucket.\n Documentación técnica de proyectos Tutoriales  [ ] Servidores [ ] Cloud Computing UNIX [ ] Lenguajes de programación [ ] Big Data [ ] Inteligencia Artificial Business Intelligence [ ] Docker [ ] Map Reduce [ ] Apache Spark [ ] Apache PySpark [ ] Pandas [ ] NumPy   ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"d3768b0fe27625f255b6605a91cda7ea","permalink":"https://www.marcusrb.com/en/tutorials/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/tutorials/","section":"resources","summary":"En la sección de tutoriales, se vincularán los diferentes proyectos realizados, así como los tutoriales de IoT, Raspberry, Python, R, Machine Learning, además de analítica avanzada de datos.","tags":null,"title":"Tutorials","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Los recursos de este curso están disponibles en:\n Cursos online fundamentos de analítica digital Cursos online Avanzado con analítica digital Video-Tutoriales Google Analytics Test examen Certificación Google Analytics Examénes Google Academy  Índice Learning Path Estructura programación de GTM 1. Introducción de GTM 2. GTM 2  ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"3224bfa9b604f56cf03dd313d47714e4","permalink":"https://www.marcusrb.com/curso-google-tag-manager/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/curso-google-tag-manager/","section":"courses","summary":"Learning Path de Google Tag Manager. Aprende cómo implementar etiquetas de Google y de terceros en una interfaz muy intuitiva.","tags":null,"title":"Learning Path de Google Tag Manager","type":"docs"},{"authors":null,"categories":null,"content":" 1.1. ¿Qué es Google Tag Manager? Google Tag Manager es un sistema de administración de etiquetas que permite recoger datos relacionados con campañas de marketing, tráfico web y el comportamiento del usuario en el sitio web, actualizando de forma fácil y rápida las etiquetas y los fragmentos de código de un sitio web. Es decir, permite editar HTML y JavaScript directamente sin tener que acceder al código de seguimiento. Se pueden añadir y actualizar etiquetas personalizadas, de terceros o de Ads, Google Analytics, Firebase Analytics y Floodlight desde la interfaz de usuario de Tag Manager en lugar de cambiar el código de los sitios web. De este modo, se reduce el número de errores y se evita recurrir a un desarrollador o programador para configurar las etiquetas.\nEn definitiva, una vez conozcamos el funcionamiento de Google Tag Manager, podremos administrar las etiquetas de nuestro sitio web de forma fácil y rápida sin necesidad de depender de un departamento IT o de desarrollo. Además, este software facilita mucho la tarea en una campaña de marketing, porque supone un acceso a la web en tiempo real (recordemos que al insertar manualmente los códigos de seguimiento los datos pueden tardar hasta 24 horas en aparecer en Google Analytics) y nos permite unificar todos los códigos en uno solo, lo que significa trabajar con un solo ID de Google Tag Manager y una sola interfaz de usuario\n1.2. ¿Por qué Google Tag Manager? Cuando se trata de una campaña de publicidad digital o un E-Commerce, si no medimos los resultados y tomamos decisiones estratégicas a través de ellos, no podremos mejorar. Como hemos visto hasta ahora, para poder medir el rendimiento de todas las campañas, visitas y los funcionamientos de los procesos de una página web, necesitamos insertar píxeles o códigos de seguimiento. Éstos sirven para enviar un recuento de conversiones o de visitas y atribuírselo a las campañas adecuadas. Normalmente, quien se encarga de gestionarlo son programadores expertos.\nPongámonos en la situación de un experto y profesional de Marketing Online trabajando en una campaña. Casi todo está listo: la creatividad de los anuncios, los banners están contratados para aparecer en los medios, los enlaces están etiquetados, las redes sociales están actualizadas, etc. ¿Qué pasa cuando llega la hora de trabajar con Google Analytics, donde se tienen que insertar píxeles y códigos de seguimiento referidos a cada página de interés? No todas las empresas disponen de los medios necesarios para tener operativo un departamento de desarrollo de forma constante ni contratar programadores para esta constante tarea de inserción de códigos.\nSi desde el departamento de desarrollo se pueden ahorrar todo este tiempo y este riesgo de cometer errores al insertar códigos, y si esta tarea se convierte en una tarea fácil y rápida de hacer para cualquier profesional del sector de Marketing, los resultados se multiplican. Al fin y al cabo, tanto los departamentos de Desarrollo como de Marketing tienen un interés común; trabajan enfocados al mismo objetivo y para el mismo cliente. Si ambos departamentos cooperan entre sí, las campañas de marketing y publicidad van a ser más exitosas.\nLa publicidad online representa un alto porcentaje de la publicidad mundial a día de hoy, y estos píxeles de los que hablamos son un elemento clave en el proceso, hasta el punto de que incluso existen empresas que se dedican exclusivamente a mejorar la medición y gestión de campañas online. Esto exige nuevos perfiles de profesionales, porque mientras sube la inversión en marketing digital, también debe hacerlo en procesos de análisis y supervisión de esas tareas del rendimiento de cada una de las inversiones que han intervenido. Este perfil profesional es conocido como Tag Manager.\n5 Google Tag Manager Ante esta tendencia, hay muchas empresas que ya han visto la oportunidad y han creado sus softwares de configuración de etiquetas. Hay algunas que son de pago, pero la de Google es gratuita, llamada Google Tag Manager.\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"56c3d159ca51b3324c5fa4cb263dcfe9","permalink":"https://www.marcusrb.com/en/courses/google-marketing-platform/google-ads/gads-display/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/courses/google-marketing-platform/google-ads/gads-display/","section":"courses","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" 1.1. ¿Qué es Google Tag Manager? Google Tag Manager es un sistema de administración de etiquetas que permite recoger datos relacionados con campañas de marketing, tráfico web y el comportamiento del usuario en el sitio web, actualizando de forma fácil y rápida las etiquetas y los fragmentos de código de un sitio web. Es decir, permite editar HTML y JavaScript directamente sin tener que acceder al código de seguimiento. Se pueden añadir y actualizar etiquetas personalizadas, de terceros o de Ads, Google Analytics, Firebase Analytics y Floodlight desde la interfaz de usuario de Tag Manager en lugar de cambiar el código de los sitios web. De este modo, se reduce el número de errores y se evita recurrir a un desarrollador o programador para configurar las etiquetas.\nEn definitiva, una vez conozcamos el funcionamiento de Google Tag Manager, podremos administrar las etiquetas de nuestro sitio web de forma fácil y rápida sin necesidad de depender de un departamento IT o de desarrollo. Además, este software facilita mucho la tarea en una campaña de marketing, porque supone un acceso a la web en tiempo real (recordemos que al insertar manualmente los códigos de seguimiento los datos pueden tardar hasta 24 horas en aparecer en Google Analytics) y nos permite unificar todos los códigos en uno solo, lo que significa trabajar con un solo ID de Google Tag Manager y una sola interfaz de usuario\n1.2. ¿Por qué Google Tag Manager? Cuando se trata de una campaña de publicidad digital o un E-Commerce, si no medimos los resultados y tomamos decisiones estratégicas a través de ellos, no podremos mejorar. Como hemos visto hasta ahora, para poder medir el rendimiento de todas las campañas, visitas y los funcionamientos de los procesos de una página web, necesitamos insertar píxeles o códigos de seguimiento. Éstos sirven para enviar un recuento de conversiones o de visitas y atribuírselo a las campañas adecuadas. Normalmente, quien se encarga de gestionarlo son programadores expertos.\nPongámonos en la situación de un experto y profesional de Marketing Online trabajando en una campaña. Casi todo está listo: la creatividad de los anuncios, los banners están contratados para aparecer en los medios, los enlaces están etiquetados, las redes sociales están actualizadas, etc. ¿Qué pasa cuando llega la hora de trabajar con Google Analytics, donde se tienen que insertar píxeles y códigos de seguimiento referidos a cada página de interés? No todas las empresas disponen de los medios necesarios para tener operativo un departamento de desarrollo de forma constante ni contratar programadores para esta constante tarea de inserción de códigos.\nSi desde el departamento de desarrollo se pueden ahorrar todo este tiempo y este riesgo de cometer errores al insertar códigos, y si esta tarea se convierte en una tarea fácil y rápida de hacer para cualquier profesional del sector de Marketing, los resultados se multiplican. Al fin y al cabo, tanto los departamentos de Desarrollo como de Marketing tienen un interés común; trabajan enfocados al mismo objetivo y para el mismo cliente. Si ambos departamentos cooperan entre sí, las campañas de marketing y publicidad van a ser más exitosas.\nLa publicidad online representa un alto porcentaje de la publicidad mundial a día de hoy, y estos píxeles de los que hablamos son un elemento clave en el proceso, hasta el punto de que incluso existen empresas que se dedican exclusivamente a mejorar la medición y gestión de campañas online. Esto exige nuevos perfiles de profesionales, porque mientras sube la inversión en marketing digital, también debe hacerlo en procesos de análisis y supervisión de esas tareas del rendimiento de cada una de las inversiones que han intervenido. Este perfil profesional es conocido como Tag Manager.\n5 Google Tag Manager Ante esta tendencia, hay muchas empresas que ya han visto la oportunidad y han creado sus softwares de configuración de etiquetas. Hay algunas que son de pago, pero la de Google es gratuita, llamada Google Tag Manager.\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"7ac4ae90fce90a91f32cf77d8cbe6525","permalink":"https://www.marcusrb.com/en/courses/google-marketing-platform/google-ads/gads-remarketing/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/courses/google-marketing-platform/google-ads/gads-remarketing/","section":"courses","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" 1.1. ¿Qué es Google Tag Manager? Google Tag Manager es un sistema de administración de etiquetas que permite recoger datos relacionados con campañas de marketing, tráfico web y el comportamiento del usuario en el sitio web, actualizando de forma fácil y rápida las etiquetas y los fragmentos de código de un sitio web. Es decir, permite editar HTML y JavaScript directamente sin tener que acceder al código de seguimiento. Se pueden añadir y actualizar etiquetas personalizadas, de terceros o de Ads, Google Analytics, Firebase Analytics y Floodlight desde la interfaz de usuario de Tag Manager en lugar de cambiar el código de los sitios web. De este modo, se reduce el número de errores y se evita recurrir a un desarrollador o programador para configurar las etiquetas.\nEn definitiva, una vez conozcamos el funcionamiento de Google Tag Manager, podremos administrar las etiquetas de nuestro sitio web de forma fácil y rápida sin necesidad de depender de un departamento IT o de desarrollo. Además, este software facilita mucho la tarea en una campaña de marketing, porque supone un acceso a la web en tiempo real (recordemos que al insertar manualmente los códigos de seguimiento los datos pueden tardar hasta 24 horas en aparecer en Google Analytics) y nos permite unificar todos los códigos en uno solo, lo que significa trabajar con un solo ID de Google Tag Manager y una sola interfaz de usuario\n1.2. ¿Por qué Google Tag Manager? Cuando se trata de una campaña de publicidad digital o un E-Commerce, si no medimos los resultados y tomamos decisiones estratégicas a través de ellos, no podremos mejorar. Como hemos visto hasta ahora, para poder medir el rendimiento de todas las campañas, visitas y los funcionamientos de los procesos de una página web, necesitamos insertar píxeles o códigos de seguimiento. Éstos sirven para enviar un recuento de conversiones o de visitas y atribuírselo a las campañas adecuadas. Normalmente, quien se encarga de gestionarlo son programadores expertos.\nPongámonos en la situación de un experto y profesional de Marketing Online trabajando en una campaña. Casi todo está listo: la creatividad de los anuncios, los banners están contratados para aparecer en los medios, los enlaces están etiquetados, las redes sociales están actualizadas, etc. ¿Qué pasa cuando llega la hora de trabajar con Google Analytics, donde se tienen que insertar píxeles y códigos de seguimiento referidos a cada página de interés? No todas las empresas disponen de los medios necesarios para tener operativo un departamento de desarrollo de forma constante ni contratar programadores para esta constante tarea de inserción de códigos.\nSi desde el departamento de desarrollo se pueden ahorrar todo este tiempo y este riesgo de cometer errores al insertar códigos, y si esta tarea se convierte en una tarea fácil y rápida de hacer para cualquier profesional del sector de Marketing, los resultados se multiplican. Al fin y al cabo, tanto los departamentos de Desarrollo como de Marketing tienen un interés común; trabajan enfocados al mismo objetivo y para el mismo cliente. Si ambos departamentos cooperan entre sí, las campañas de marketing y publicidad van a ser más exitosas.\nLa publicidad online representa un alto porcentaje de la publicidad mundial a día de hoy, y estos píxeles de los que hablamos son un elemento clave en el proceso, hasta el punto de que incluso existen empresas que se dedican exclusivamente a mejorar la medición y gestión de campañas online. Esto exige nuevos perfiles de profesionales, porque mientras sube la inversión en marketing digital, también debe hacerlo en procesos de análisis y supervisión de esas tareas del rendimiento de cada una de las inversiones que han intervenido. Este perfil profesional es conocido como Tag Manager.\n5 Google Tag Manager Ante esta tendencia, hay muchas empresas que ya han visto la oportunidad y han creado sus softwares de configuración de etiquetas. Hay algunas que son de pago, pero la de Google es gratuita, llamada Google Tag Manager.\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"084c24d06cca7ab85f063d341c96e6cd","permalink":"https://www.marcusrb.com/en/courses/google-marketing-platform/google-ads/gads-search/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/courses/google-marketing-platform/google-ads/gads-search/","section":"courses","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" 1.1. ¿Qué es Google Tag Manager? Google Tag Manager es un sistema de administración de etiquetas que permite recoger datos relacionados con campañas de marketing, tráfico web y el comportamiento del usuario en el sitio web, actualizando de forma fácil y rápida las etiquetas y los fragmentos de código de un sitio web. Es decir, permite editar HTML y JavaScript directamente sin tener que acceder al código de seguimiento. Se pueden añadir y actualizar etiquetas personalizadas, de terceros o de Ads, Google Analytics, Firebase Analytics y Floodlight desde la interfaz de usuario de Tag Manager en lugar de cambiar el código de los sitios web. De este modo, se reduce el número de errores y se evita recurrir a un desarrollador o programador para configurar las etiquetas.\nEn definitiva, una vez conozcamos el funcionamiento de Google Tag Manager, podremos administrar las etiquetas de nuestro sitio web de forma fácil y rápida sin necesidad de depender de un departamento IT o de desarrollo. Además, este software facilita mucho la tarea en una campaña de marketing, porque supone un acceso a la web en tiempo real (recordemos que al insertar manualmente los códigos de seguimiento los datos pueden tardar hasta 24 horas en aparecer en Google Analytics) y nos permite unificar todos los códigos en uno solo, lo que significa trabajar con un solo ID de Google Tag Manager y una sola interfaz de usuario\n1.2. ¿Por qué Google Tag Manager? Cuando se trata de una campaña de publicidad digital o un E-Commerce, si no medimos los resultados y tomamos decisiones estratégicas a través de ellos, no podremos mejorar. Como hemos visto hasta ahora, para poder medir el rendimiento de todas las campañas, visitas y los funcionamientos de los procesos de una página web, necesitamos insertar píxeles o códigos de seguimiento. Éstos sirven para enviar un recuento de conversiones o de visitas y atribuírselo a las campañas adecuadas. Normalmente, quien se encarga de gestionarlo son programadores expertos.\nPongámonos en la situación de un experto y profesional de Marketing Online trabajando en una campaña. Casi todo está listo: la creatividad de los anuncios, los banners están contratados para aparecer en los medios, los enlaces están etiquetados, las redes sociales están actualizadas, etc. ¿Qué pasa cuando llega la hora de trabajar con Google Analytics, donde se tienen que insertar píxeles y códigos de seguimiento referidos a cada página de interés? No todas las empresas disponen de los medios necesarios para tener operativo un departamento de desarrollo de forma constante ni contratar programadores para esta constante tarea de inserción de códigos.\nSi desde el departamento de desarrollo se pueden ahorrar todo este tiempo y este riesgo de cometer errores al insertar códigos, y si esta tarea se convierte en una tarea fácil y rápida de hacer para cualquier profesional del sector de Marketing, los resultados se multiplican. Al fin y al cabo, tanto los departamentos de Desarrollo como de Marketing tienen un interés común; trabajan enfocados al mismo objetivo y para el mismo cliente. Si ambos departamentos cooperan entre sí, las campañas de marketing y publicidad van a ser más exitosas.\nLa publicidad online representa un alto porcentaje de la publicidad mundial a día de hoy, y estos píxeles de los que hablamos son un elemento clave en el proceso, hasta el punto de que incluso existen empresas que se dedican exclusivamente a mejorar la medición y gestión de campañas online. Esto exige nuevos perfiles de profesionales, porque mientras sube la inversión en marketing digital, también debe hacerlo en procesos de análisis y supervisión de esas tareas del rendimiento de cada una de las inversiones que han intervenido. Este perfil profesional es conocido como Tag Manager.\n5 Google Tag Manager Ante esta tendencia, hay muchas empresas que ya han visto la oportunidad y han creado sus softwares de configuración de etiquetas. Hay algunas que son de pago, pero la de Google es gratuita, llamada Google Tag Manager.\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"a3cf75d3edf766d71a0a4500a8f15ecc","permalink":"https://www.marcusrb.com/en/courses/google-marketing-platform/google-tag-manager/gtm01/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/courses/google-marketing-platform/google-tag-manager/gtm01/","section":"courses","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM from Scratch","type":"docs"},{"authors":null,"categories":null,"content":"","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"fd23275ee7685c8e907986883077b21a","permalink":"https://www.marcusrb.com/en/courses/google-marketing-platform/google-analytics/ga101/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/courses/google-marketing-platform/google-analytics/ga101/","section":"courses","summary":"Una pequeña introducción al entorno de Google Analytics, variables, activadores y etiquetas","tags":null,"title":"Google Analytics from Scratch","type":"docs"},{"authors":null,"categories":null,"content":"  Project or software documentation Tutoriales Cheatsheets  Índice Learning Path Estructura programación de Visualización de datos Google Data Studio Power BI fundamentos Power BI avanzado Tableau Visualización en Python Visualización en R Studio  ","date":1586044800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"881c4f723286372e3eda612793e74662","permalink":"https://www.marcusrb.com/en/courses/data-visualization/","publishdate":"2020-04-05T00:00:00Z","relpermalink":"/en/courses/data-visualization/","section":"courses","summary":"Learning Path de Visualización de datos, se indicarán los cursos de Data Studio, Tableau y Power BI.","tags":null,"title":"Cursos y formación en Visualización de datos","type":"docs"},{"authors":null,"categories":null,"content":" Programa En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!\nTidy Vamos a ir a Las Vegas! donde aprenderemos como analizar los resultados de los juegos usando vectores en R! Después de completar este capítulo serás capaz de crear vectores en R, nombrarlos, seleccionar elementos de ellos y comparar diferentes vectores.\n","date":1568505600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"f70af8c4c99dc288c9ed31cb2f2517e6","permalink":"https://www.marcusrb.com/en/courses/r-studio/advanced-r/","publishdate":"2019-09-15T00:00:00Z","relpermalink":"/en/courses/r-studio/advanced-r/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"R Avanzado","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 2 millones de usuarios a nivel mundial, R se convierte rápidamente en el lenguaje de programación líder en estadística y ciencia de datos. Cada año, el número de usuarios de R crece en 40%, y cada vez más organizaciones lo están usando para sus actividades cotidianas. En esta introducción a R, vas a dominar los elementos básicos de este bello lenguaje de programación: vectores, factores, listas y data frames. Con el conocimiento obtenido en este curso serás capaz de llevar a cabo tus propios análisis de datos.\nCómo funciona En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!\n[Basic Data Exploration]() Vamos a ir a Las Vegas! donde aprenderemos como analizar los resultados de los juegos usando vectores en R! Después de completar este capítulo serás capaz de crear vectores en R, nombrarlos, seleccionar elementos de ellos y comparar diferentes vectores.\nEl primer modelo de ML En este capítulo vamos a aprender a trabajar con matrices en R. Al finalizar el capítulo serás capaz de crear matrices y hacer operaciones básicas con éstas. Vamos a analizar cuánto dinero hizo la película Star Wars para ilustrar el uso de matrices en R. Que la fuerza te acompañe!\nFactores Existen un tipo de variables llamadas variables categóricas. Por ejemplo, el género puede ser femenino o masculino. En R las variables categóricas son llamadas factores. Dada la importancia de los factores en el analisis de datos, vamos a aprender a crearlos y a todo lo relacionado con su manejo. Empecemos!\nData frames La mayoría de los datos con los que trabajarás en R van a ser guardados en data frames. Al finalizar este capítulo vas a ser capaz de crear data frames, seleccionar partes del mismo y ordenar los datos que contiene de acuerdo a cierta variable.\n## Listas Las listas, en contraste con los vectores, pueden tener elementos de diferentes tipos. En este capítulo aprenderemos a crear, nombrar y extraer elementos de las listas. ¿Listo? ;-)\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"7ab02ea3e19458ef62aa3cb80b425fb4","permalink":"https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/data-science/intro-machine-learning/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"Introducción Machine Learning","type":"docs"},{"authors":null,"categories":null,"content":" DESCRIPTION - summary TBD\nHello, Python En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!\nFunciones Vamos a ir a Las Vegas! donde aprenderemos como analizar los resultados de los juegos usando vectores en R! Después de completar este capítulo serás capaz de crear vectores en R, nombrarlos, seleccionar elementos de ellos y comparar diferentes vectores.\nMatrices En este capítulo vamos a aprender a trabajar con matrices en R. Al finalizar el capítulo serás capaz de crear matrices y hacer operaciones básicas con éstas. Vamos a analizar cuánto dinero hizo la película Star Wars para ilustrar el uso de matrices en R. Que la fuerza te acompañe!\nFactores Existen un tipo de variables llamadas variables categóricas. Por ejemplo, el género puede ser femenino o masculino. En R las variables categóricas son llamadas factores. Dada la importancia de los factores en el analisis de datos, vamos a aprender a crearlos y a todo lo relacionado con su manejo. Empecemos!\nData frames La mayoría de los datos con los que trabajarás en R van a ser guardados en data frames. Al finalizar este capítulo vas a ser capaz de crear data frames, seleccionar partes del mismo y ordenar los datos que contiene de acuerdo a cierta variable.\n## Listas Las listas, en contraste con los vectores, pueden tener elementos de diferentes tipos. En este capítulo aprenderemos a crear, nombrar y extraer elementos de las listas. ¿Listo? ;-)\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"856ff2b0f7c1b63c942f7d3d022374f8","permalink":"https://www.marcusrb.com/en/courses/python/py101/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/python/py101/","section":"courses","summary":"Una pequeña introducción a la sintaxis de Python, variables y números","tags":null,"title":"Python from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" 2 gtm 2\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"3d7d484edfd962a41528e741d7ea1dbd","permalink":"https://www.marcusrb.com/en/courses/google-marketing-platform/google-tag-manager/gtm02/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/courses/google-marketing-platform/google-tag-manager/gtm02/","section":"courses","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"GTM 2","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Los recursos de este curso están disponibles en:\n Cursos online fundamentos de analítica digital Cursos online Avanzado con analítica digital Video-Tutoriales Google Analytics Test examen Certificación Google Analytics Examénes Google Academy  Índice Learning Path de Google Ads TBD\n","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"a7e733a4e7271342c3f0777de92583f8","permalink":"https://www.marcusrb.com/curso-google-ads/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/curso-google-ads/","section":"courses","summary":"Learning Path de Google Ads. Aprende cómo crear y gestionar campañas de Google Ads de Búsqueda, Display, Remarketing y Ecommerce.","tags":null,"title":"Learning Path de Google Ads","type":"docs"},{"authors":null,"categories":null,"content":" 2  [ ] Google Analytics 4  ","date":1592956800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"56fcf2eddf4f286da8682cd314c2b707","permalink":"https://www.marcusrb.com/en/courses/google-marketing-platform/google-analytics/ga4/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/courses/google-marketing-platform/google-analytics/ga4/","section":"courses","summary":"Una pequeña introducción al entorno de GTM, variables, activadores y etiquetas","tags":null,"title":"Google Analytics 2","type":"docs"},{"authors":null,"categories":null,"content":"","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"b27dad2c93319a83312fba409b27931b","permalink":"https://www.marcusrb.com/en/courses/data-science/data-mining/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/data-science/data-mining/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"Introducción Data Mining","type":"docs"},{"authors":null,"categories":null,"content":" Math for Data Science Useful resources to improve your Math skills -\nCourses 1) Khan Academy is the best online free resource to learn Math for Data Science. (https://lnkd.in/eWZFANt).\n2) Krista King has also done a great job in creating exceptionally good introductory course. She is too good in designing the course. (https://lnkd.in/eyMecjA).\n3) 3Blue1Brown (https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/playlists).\nMUST READ Books 1) The Elements of Statistical Learning(Springer Series).\n2) Introduction to Linear Algebra by Gilbert Strang.\n3) Naked Statistics by Charles Wheelan.\n4) An Introduction to Statistical Learning: with Applications in R.\n5) Pattern Recognition and Machine Learning by Christopher M. Bishop.\n6) Pattern Classification ((A Wiley-Interscience publication).\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"7ae132d76ac2bcfebab4cea08bb6ee87","permalink":"https://www.marcusrb.com/en/courses/data-science/math-data-science/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/data-science/math-data-science/","section":"courses","summary":"En este capítulo vamos a dar nuestros primeros pasos en R. Aprenderemos a usar la consola como calculadora y a asignar variables. También aprenderemos algunos de los tipos básicos de datos en R. Comencemos!","tags":null,"title":"Matemática para Data Science","type":"docs"},{"authors":null,"categories":null,"content":" Learning Path This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  Índice Learning Path Estructura programación de Python  [] Introducción de Python [] Python para NO Desarrolladores [] Advanced Python [] Pandas [] Data Manipulation  ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"298ad7f7e1cf34d15656d8b0546e7486","permalink":"https://www.marcusrb.com/en/courses/python/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/python/","section":"courses","summary":"Learning Path de Python. De fundamentos de programación, material de Python básico hasta avaznado, web app en Flask y Django.","tags":null,"title":"Learning Path de Python","type":"docs"},{"authors":null,"categories":null,"content":" R como recursos para la análisis de datos Gracias a la aportación de una fuerte comunidad, aquí tendré la ocasión de crear una wiki para aprender a utilizar R studio para la análisis de datos.\nQué es R? R es un entorno para la gestión y análisis de datos, primera exploración estadística y visualización gráfica. Es un software de oper source (su distribución tiene licencia GNU GPL) además de multiplataformas, UNIX, Linux, OSx, Windows.\n Online courses Project or software documentation Tutorials Cheatsheets  Estructura programación en R Instalación de R Paquetes Área de trabajo Directorio de trabajo Comandos Objetos y Clases Vectores Factores Listas Índices Funciones script Fórmulas Operadores Datasets de ejemplos R para Data Science  Fuente\nIntroducción · R. avanzado Estás leyendo la primera edición de Advanced R; para la última versión ver la [segunda edición] [1].\nCon más de 10 años de experiencia en programación en R, he tenido el lujo de poder pasar mucho tiempo tratando de descubrir y entender cómo funciona el lenguaje. Este libro es mi intento de transmitir lo que he aprendido para que pueda convertirse rápidamente en un programador de R efectivo. Leerlo te ayudará a evitar los errores que he cometido y los callejones sin salida que he caído, y te enseñará herramientas, técnicas y modismos útiles que pueden ayudarte a atacar muchos tipos de problemas. En el proceso, espero demostrar que, a pesar de sus peculiaridades frustrantes, R es, en esencia, un lenguaje elegante y hermoso, bien adaptado para el análisis de datos y las estadísticas.\nSi eres nuevo en R, te preguntarás qué hace que valga la pena aprender un lenguaje tan peculiar. Para mí, algunas de las mejores características son:\n Es gratis, de código abierto y está disponible en todas las plataformas principales. Como resultado, si realiza su análisis en R, cualquiera puede replicarlo fácilmente. Un conjunto masivo de paquetes para modelado estadístico, aprendizaje automático, visualización e importación y manipulación de datos. Independientemente del modelo o gráfico que intente hacer, es probable que alguien ya haya intentado hacerlo. Como mínimo, puedes aprender de sus esfuerzos. Herramientas de vanguardia. Los investigadores en estadística y aprendizaje automático a menudo publicarán un paquete R para acompañar sus artículos. Esto significa acceso inmediato a las últimas técnicas e implementaciones estadísticas. Soporte de lenguaje profundo para el análisis de datos. Esto incluye características como valores perdidos, marcos de datos y subconjuntos. Una comunidad fantástica. Es fácil obtener ayuda de expertos en la [lista de correo R-help] [2], [stackoverflow] [3] o en listas de correo específicas de temas como [R-SIG-mixed-models] [4] o [ggplot2 ] [5]. También puede conectarse con otros alumnos R a través de [twitter] [6], [linkedin] [7], y a través de muchos [grupos de usuarios] locales [8]. Potentes herramientas para comunicar sus resultados. Los paquetes R facilitan la producción de html o pdf [informes] [9], o la creación de [sitios web interactivos] [10]. Una base sólida en la programación funcional. Las ideas de la programación funcional son adecuadas para resolver muchos de los desafíos del análisis de datos. R proporciona un kit de herramientas potente y flexible que le permite escribir código conciso pero descriptivo. Un [IDE] [11] adaptado a las necesidades de análisis de datos interactivos y programación estadística. Potentes instalaciones de metaprogramación. R no es solo un lenguaje de programación, también es un entorno para el análisis interactivo de datos. Sus capacidades de metaprogramación le permiten escribir funciones mágicamente concisas y concisas y proporcionan un entorno excelente para diseñar lenguajes específicos de dominio. Diseñado para conectarse a lenguajes de programación de alto rendimiento como C, Fortran y C ++.  Por supuesto, R no es perfecto. El mayor desafío de R es que la mayoría de los usuarios de R no son programadores. Esto significa que:\n Gran parte del código R que verá en la naturaleza está escrito a toda prisa para resolver un problema acuciante. Como resultado, el código no es muy elegante, rápido o fácil de entender. La mayoría de los usuarios no revisan su código para abordar estas deficiencias. En comparación con otros lenguajes de programación, la comunidad R tiende a centrarse más en los resultados que en los procesos. El conocimiento de las mejores prácticas de ingeniería de software es irregular: por ejemplo, no hay suficientes programadores de R que usen control de código fuente o pruebas automatizadas. La metaprogramación es una espada de doble filo. Demasiadas funciones de R usan trucos para reducir la cantidad de tipeo a costa de crear código que es difícil de entender y que puede fallar de maneras inesperadas. La inconsistencia abunda en los paquetes contribuidos, incluso dentro de la base R. Te enfrentas a más de 20 años de evolución cada vez que usas R. Aprender R puede ser difícil porque hay muchos casos especiales para recordar. R no es un lenguaje de programación particularmente rápido, y el código R mal escrito puede ser terriblemente lento. R también es un usuario despilfarrador de memoria.  Personalmente, creo que estos desafíos crean una gran oportunidad para que los programadores experimentados tengan un profundo impacto positivo en R y la comunidad R. Los usuarios de R se preocupan por escribir código de alta calidad, particularmente para la investigación reproducible, pero aún no tienen las habilidades para hacerlo. Espero que este libro no solo ayude a más usuarios de R a convertirse en programadores de R, sino que también aliente a los programadores de otros idiomas a contribuir a R.\nQuién debería leer este libro Este libro está dirigido a dos audiencias complementarias:\n Programadores de R intermedios que desean profundizar en R y aprender nuevas estrategias para resolver diversos problemas. Programadores de otros idiomas que están aprendiendo R y quieren entender por qué R funciona de la manera que lo hace.  Para aprovechar al máximo este libro, deberá haber escrito una cantidad decente de código en R u otro lenguaje de programación. Es posible que no conozca todos los detalles, pero debe estar familiarizado con el funcionamiento de las funciones en R y, aunque actualmente puede tener dificultades para usarlas de manera efectiva, debe estar familiarizado con la familia de aplicaciones (como apply () y lapply ()).\nLo que obtendrás de este libro Este libro describe las habilidades que creo que un programador avanzado de R debería tener: la capacidad de producir código de calidad que pueda usarse en una amplia variedad de circunstancias.\nDespués de leer este libro, usted:\n Familiarícese con los fundamentos de R. Comprenderá los tipos de datos complejos y las mejores formas de realizar operaciones en ellos. Tendrá una comprensión profunda de cómo funcionan las funciones y podrá reconocer y utilizar los cuatro sistemas de objetos en R. Comprenda lo que significa la programación funcional y por qué es una herramienta útil para el análisis de datos. Podrá aprender rápidamente cómo usar las herramientas existentes y tener el conocimiento para crear sus propias herramientas funcionales cuando sea necesario. Aprecia la espada de doble filo de la metaprogramación. Podrá crear funciones que utilizan evaluaciones no estándar de una manera basada en principios, ahorrando escritura y creando código elegante para expresar operaciones importantes. También comprenderá los peligros de la metaprogramación y por qué debe tener cuidado con su uso. Tener una buena intuición para las operaciones en R lentas o usar mucha memoria. Sabrás cómo usar la creación de perfiles para identificar los cuellos de botella de rendimiento, y sabrás suficiente C ++ para convertir funciones R lentas en equivalentes C ++ rápidos. Se sienta cómodo leyendo y entendiendo la mayoría del código R. Reconocerá expresiones idiomáticas comunes (incluso si no las usaría usted mismo) y podrá criticar el código de los demás.  Hay dos meta-técnicas que son tremendamente útiles para mejorar sus habilidades como programador de R: leer el código fuente y adoptar una mentalidad científica.\nLeer el código fuente es importante porque te ayudará a escribir un mejor código. Un gran lugar para comenzar a desarrollar esta habilidad es mirar el código fuente de las funciones y paquetes que usa con más frecuencia. Encontrarás cosas que vale la pena emular en tu propio código y desarrollarás un sentido del gusto por lo que hace un buen código R. También verá cosas que no le gustan, ya sea porque sus virtudes no son obvias o ofende su sensibilidad. Sin embargo, dicho código es valioso porque ayuda a concretar sus opiniones sobre el código bueno y el malo.\nUna mentalidad científica es extremadamente útil cuando se aprende R. Si no comprende cómo funciona algo, desarrolle una hipótesis, diseñe algunos experimentos, ejecútelos y registre los resultados. Este ejercicio es extremadamente útil ya que si no puede resolver algo y necesita ayuda, puede mostrar fácilmente a los demás lo que intentó. Además, cuando aprenda la respuesta correcta, estará mentalmente preparado para actualizar su visión del mundo. Cuando describo claramente un problema a otra persona (el arte de crear un [ejemplo reproducible] [12]), a menudo descubro la solución yo mismo.\nLectura recomendada R sigue siendo un idioma relativamente joven, y los recursos para ayudarlo a comprenderlo aún están madurando. En mi viaje personal para comprender R, me ha resultado especialmente útil utilizar recursos de otros lenguajes de programación. R tiene aspectos de lenguajes de programación tanto funcionales como orientados a objetos (OO). Aprender cómo se expresan estos conceptos en R lo ayudará a aprovechar su conocimiento existente de otros lenguajes de programación y lo ayudará a identificar áreas en las que puede mejorar.\nPara entender por qué los sistemas de objetos de R funcionan de la manera que lo hacen, encontré [_La estructura e interpretación de los programas de computadora _] 13 de Harold Abelson y Gerald Jay Sussman, particularmente útil. Es un libro conciso pero profundo. Después de leerlo, sentí por primera vez que realmente podía diseñar mi propio sistema orientado a objetos. El libro fue mi primera introducción al estilo de función genérico de OO común en R. Me ayudó a comprender sus fortalezas y debilidades. SICP también habla mucho sobre programación funcional y cómo crear funciones simples que se vuelven poderosas cuando se combinan.\nPara comprender las compensaciones que R ha realizado en comparación con otros lenguajes de programación, encontré [_Conceptos, técnicas y modelos de programación de computadoras _] [14] de Peter van Roy y Sef Haridi extremadamente útiles. Me ayudó a comprender que la semántica de copiar en modificar de R hace que sea mucho más fácil razonar sobre el código, y que si bien su implementación actual no es particularmente eficiente, es un problema que se puede resolver.\nSi quieres aprender a ser un mejor programador, no hay mejor lugar al que recurrir que [_The Pragmatic Programmer _] [15] de Andrew Hunt y David Thomas. Este libro es independiente del lenguaje y proporciona excelentes consejos sobre cómo ser un mejor programador.\nObteniendo ayuda Actualmente, hay dos lugares principales para obtener ayuda cuando estás atascado y no puedes descubrir qué está causando el problema: [stackoverflow] [16] y la lista de correo de R-help. Puede obtener ayuda fantástica en ambos lugares, pero tienen sus propias culturas y expectativas. Por lo general, es una buena idea pasar un poco de tiempo al acecho, aprendiendo sobre las expectativas de la comunidad, antes de publicar su primera publicación.\nAlgunos buenos consejos generales:\n Asegúrese de tener la última versión de R y del paquete (o paquetes) con los que tiene problemas. Puede ser que su problema sea el resultado de un error recientemente corregido. Pase algún tiempo creando un [ejemplo reproducible] [12]. Esto es a menudo un proceso útil por derecho propio, porque en el curso de hacer que el problema sea reproducible, a menudo descubres lo que está causando el problema. Busque problemas relacionados antes de publicar. Si alguien ya ha hecho su pregunta y ha sido respondida, es mucho más rápido para todos si usa la respuesta existente.  Agradecimientos Me gustaría agradecer a los incansables colaboradores de R-help y, más recientemente, [stackoverflow] [3]. Hay muchos para nombrar individualmente, pero me gustaría agradecer especialmente a Luke Tierney, John Chambers, Dirk Eddelbuettel, JJ Allaire y Brian Ripley por dar generosamente su tiempo y corregir mis innumerables malentendidos.\nEste libro fue [escrito al aire libre] [17], y los capítulos se anunciaron en [twitter] [18] cuando se completó. Es realmente un esfuerzo de la comunidad: muchas personas leen borradores, errores tipográficos, mejoras sugeridas y contenido contribuido. Sin esos colaboradores, el libro no sería tan bueno como es, y estoy profundamente agradecido por su ayuda. Un agradecimiento especial a Peter Li, que leyó el libro de principio a fin y proporcionó muchas soluciones. Otros colaboradores destacados fueron Aaron Schumacher, @crtahlin, Lingbing Feng, @juancentro y @johnbaums.\n","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"d4be78cc212a3ae9916b7da7e965bacb","permalink":"https://www.marcusrb.com/en/courses/r-studio/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/r-studio/","section":"courses","summary":"Learning Path de R. Documentos, tutoriales, cheatsheets para el aprendizaje de R Studio de básico a avanzado.","tags":null,"title":"Learning Path de R","type":"docs"},{"authors":null,"categories":null,"content":"  [Online courses]() Project or software documentation Tutorials Cheatsheets  Índice Learning Path  Data Science A-Z [R Programming]() Python A-Z Machine Learning A-Z Machine Learning Practical Statistics for Business Analytics and Marketing R Programming Advanced Deep Learning A-Z Machine Learning Classification  Estructura programación de Data Science Introducción de Machine Learning Matemática para Data Science [] Minería de datos [] Datatons \u0026amp; competitions  ","date":1567382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"5f0597d20d542f8f029324fd3a67a805","permalink":"https://www.marcusrb.com/en/courses/data-science/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/en/courses/data-science/","section":"courses","summary":"Learning Path de Data Science. Material, tutoriales, documentos técnicos sobre aprendizaje automático, algoritmos, deep learning e inteligencia artificial.","tags":null,"title":"Aprendizaje en Data Science","type":"docs"},{"authors":null,"categories":null,"content":" En la sección de Google Marketing Platform, se presentarán los recursos y las herramientas para empresas pequeñas y las soluciones avanzadas para las grandes empresas.\nQué es Google Marketing platform Google Marketing Platform es una plataforma unificada de publicidad y analíticas que permite a sus equipos de marketing colaborar de forma más sólida gracias a que pueden aprovechar las integraciones existentes entre DoubleClick y Suite Google Analytics 360.\n Vamos a unificar nuestros productos para anunciantes de DoubleClick y Suite Google Analytics 360 bajo una única marca: Google Marketing Platform.\n Como parte del lanzamiento de Google Marketing Platform, la página de inicio de Suite Analytics 360 se convirtió el 24 de julio en la página principal de Marketing Platform.\nEsta nueva marca, Google Marketing Platform, junto con los nuevos nombres y logotipos de los productos, queda reflejada en las interfaces correspondientes, los Centros de Ayuda y la formación, entre otros materiales.\nEstructura Los siguientes productos están divididos en las secciones:\n Google Analytics Google Tag Manager Google Ads  ","date":1567296000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609845561,"objectID":"342104a7aae4f8d896506674c0ed108e","permalink":"https://www.marcusrb.com/en/courses/google-marketing-platform/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/en/courses/google-marketing-platform/","section":"courses","summary":"Recursos y documentación técnica de los cursos del ecosistema de Google: Ads, Analytics, Tag Manager, Data Studio, Optimize, Search Console","tags":null,"title":"Google Marketing Platform","type":"docs"},{"authors":null,"categories":null,"content":" Data Transformation Strategies Within any BI project, it is essential that the data you are working with has been properly scrubbed to ensure accurate results on your reports and dashboards. Applying data cleansing business rules, also known as transforms, is the primary method for correcting inaccurate or malformed data, but the process can often be the most time-consuming part of any corporate BI solution. However, the data transformation capabilities built into Power BI are both very powerful and user-friendly. Using the Power Query Editor, tasks that would typically be difficult or time-consuming in an enterprise BI tool are as simple as right-clicking on a column and selecting the appropriate transform for the field. While interacting with the user interface, the Power Query Editor automatically writes queries using a language called M behind the scenes.\nThrough the course of this chapter, you will explore some of the most common features of the Power Query Editor that make it so highly regarded by its users. Since one sample dataset cannot provide all the problems you will run into, you will be provided with several small, disparate examples to show you what is possible. This chapter will detail the following topics:\n The Power Query Editor Transform basics Advanced data transformation options Leveraging R AI Insights The M formula language  To get started, let\u0026rsquo;s get familiar with the interface known as the Power Query Editor.\nThe Power Query Editor The Power Query Editor is the primary tool that you will utilize for applying transformations and cleansing processes to your data. This editor can be launched as part of establishing a connection to your data, or by simply clicking Transform Data on the Home ribbon of the Power BI Desktop. When the Power Query Editor is opened, you will notice that it has its own separate environment for you to work in. The environment encapsulates a user-friendly method for working with all of the queries that you will define. Before you dive deep into the capabilities of the Power Query Editor, let\u0026rsquo;s first start by reviewing the key areas of the Power Query Editor interface, as shown in Figure 2.1:\nFigure 2.1: First view of the Power Query Editor\nFollowing the numbered figures, let\u0026rsquo;s review some of the most important features of the Power Query Editor:\n New Source: This launches the interface to establish your connection details, which is the same interface as the Get data button that you learned about in Chapter 1, Getting Started with Importing Data Options. The Queries pane: A list of all the queries that you have connected to. From here, you can rename a query, disable the load and modify report refresh capabilities, and organize your queries into groups. Query Settings: Within this pane, you can rename the query, but more importantly, you can see and change the list of steps, or transforms, that have been applied to your query. If you ever accidentally close this pane, you can relaunch it from the View menu. Advanced Editor: By launching the Advanced Editor, you can see the M query that is automatically written for you by the Power Query Editor. Close \u0026amp; Apply: Choosing this option will close the Power Query Editor and load the results into the data model.  With this basic navigation understood, let\u0026rsquo;s start to discuss some of the basics of working with various transforms.\nTransform basics Applying data transformations within the Power Query Editor can be a surprisingly simple thing to do. However, there are a few things to consider as we begin this process. The first is that there are multiple ways to solve a problem. As you work your way through this book, the authors have tried to show you the fastest and easiest methods of solving the problems that are presented, but these solutions will certainly not be the only ways to reach your goals.\nThe next thing you should understand is that every click you do inside the Power Query Editor is automatically converted into a formula language called M. Virtually all the basic transforms you will need can be accomplished by simply interacting with the Power Query Editor user interface, but for more complex business problems there is a good chance you may have to modify the M queries that are written for you by the editor. You will learn more about M later in this chapter.\nFinally, the last important consideration to understand is that all transforms that are created within the editor are stored in the Query Settings pane under a section called Applied Steps. Why is this important to know? The Applied Steps section has many features, but here are some of the most critical to know for now:\n Deleting transforms: If you make a mistake and need to undo a step, you can click the Delete button next to a step. Modifying transforms: This can be done with any step that has a gear icon next to it. Changing the order of transforms: If you realize that it is better for one step to execute before another one, you can change the order of how the steps are executed. Selecting previous steps: Clicking on any step prior to the current one will allow you to see how your query results would change one step earlier in the process.  With this understanding, you will now get hands-on with applying several basic transforms inside the Power Query Editor. The goal of these first sets of examples is to get you comfortable with the Power Query Editor user interface before the more complex use cases are covered.\nUse First Row as Headers Organizing column names or headers is often an important first task when managing your dataset. Providing relevant column names makes many of the downstream processes, such as building reports, much easier. Often, column headers are automatically imported from your data source, but sometimes you may be working with a more unique data source that makes it difficult for Power BI to capture the column header information. This walkthrough will show how to deal with such a scenario:\n Launch Power BI Desktop, and click Get data on the Home ribbon.\n Choose Excel, then navigate to and select Open on the Failed Bank List.xlsx file that is available in the book source files.\n In the Navigator window, select the table called Data, then choose Transform Data. When the Power Query Editor launches, you should notice that the column headers are not automatically imported. In fact, the column headers are in the first row of the data.\n To push the column names that are in the first row of data to the header section, select the transform called\n  Use First Row as Headers\nfrom the  Home\nribbon as shown in  Figure 2.2\n:\nFigure 2.2: Leveraging the Use First Row as Headers transform\nOnce complete, you will see the first row of the dataset has been promoted to the column header area. This is a very common transform that you can expect to use often with flat files. Next, let\u0026rsquo;s look at another commonly used transform, Remove Columns.\nRemove Columns Often, the data sources you will connect to will include many columns that are not necessary for the solution you are designing. It is important to remove these unnecessary columns from your dataset because these unused columns needlessly take up space inside your data model. There are several different methods for removing columns in the Power Query Editor. This example will show one of these methods using the same dataset from the previous demonstration:\n Multi-select (Ctrl + click) the column headers of the columns you wish to keep as part of your solution. In this scenario, select the columns Bank Name, City, ST, and Closing Date.\n With these four columns selected, right-click on any of the selected columns and choose\n  Remove Other Columns\n, as shown in\nFigure 2.3\n:\nFigure 2.3: Selecting the Remove Other Columns transform\nOnce this transform is completed, you should be left with only the columns you need.\nAnother popular method for removing columns is clicking the Choose Columns button on the Home ribbon of the Power Query Editor. This option provides a list of all the columns, and you can choose the columns you wish to keep or exclude.\nYou can also select the columns you wish to remove; right-click on one of the selected columns and click Remove. This seems like the more obvious method. However, this option is not as user-friendly in the long run because it does not provide an option to edit the transform in the Applied Steps section like the first two methods do.\nWith any data cleansing tool, data type manipulation is critical and can help save you from many headaches later in the development of your solution. In the next section, you will learn about how to change data types.\nChange Type Defining column data types properly early on in your data scrubbing process can help to ensure proper business rules can be applied and data is presented properly in reports. The Power Query Editor has various numeric, text, and date-time data types for you to choose from. In our current example, all of the data types were automatically interpreted correctly by the Power Query Editor, but let\u0026rsquo;s look at where you could change this if necessary:\n Locate the data type indicator on the column header to the left of the column name.\n Click the data type icon, and a menu will open that allows you to choose whichever data type you desire, as shown in\n  Figure 2.4\n:\nFigure 2.4: Choosing a different data type\nAnother method you can use for changing column data types is to right-click on the column you wish to change, then select Change Type and choose the new data type. You should always be careful when changing data types to ensure your data supports the change. For instance, if you change a column data type to a Whole Number while it has letters stored in it, Power BI will produce an error.\nIf you want to change multiple column data types at once, you can multi-select the necessary columns, then select the new data type from the Data Type property on the Home ribbon.\nMany of the transforms you will encounter in the future are contextually based on the column data types you are working with. For example, if you have a column that is a date, then you will be provided with special transforms that can only be executed against a date data type, such as extracting the month name from a date column.\nUnderstanding how to properly set data types in Power BI is often the first step to using more exciting transforms. In the next section, you will learn how Power BI can read from an example you provide to automatically create transform rules.\nColumn From Examples One option that can make complex data transformations seem simple is the feature called Add Column From Examples. Using Add Column From Examples, you can provide the Power Query Editor with a sample of what you would like your data to look like, and it can then automatically determine which transforms are required to accomplish your goal. Continuing with the same failed banks example, let\u0026rsquo;s walk through a simple example of how to use this feature:\n Find and select the Add Column tab in the Power Query Editor ribbon.\n Select the\n  Column From Examples\nbutton and, if prompted, choose  From All Columns\n. This will launch a new\nAdd Column From Examples\ninterface:  Figure 2.5: Choosing the Column from Examples transform\n Our goal is to leverage this feature to combine the City and ST columns together. In the first empty cell, type Barboursville, WV and then hit Enter. In Figure 2.5 you will notice that the text you typed has automatically been translated into an M query and applied for every row in the dataset.\n Once you click\n  OK\n, the transform is finalized and automatically added to the overall M query that has been built through the user interface. The newly merged column will be added with the rest of your columns and you can optionally rename the column something more appropriate by double-clicking on the column header:\n![img](https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_07.png)  Figure 2.6: Adding Column from Examples\nAs you can see, the Add Column from Examples feature is great because you don\u0026rsquo;t have to be an expert in which transforms are appropriate because Power BI will automatically choose them for you!\nSometimes, you may encounter scenarios where the Add Column From Examples feature needs more than one example to properly translate your example into an M query function that accomplishes your goal. If this happens, simply provide additional examples of how you would like the data to appear in different rows, and the Power Query Editor should adjust to account for outliers.\nNow that you have learned some basic transforms, let\u0026rsquo;s explore some more complex design patterns that are still used quite frequently.\nAdvanced data transformation options Now that you should be more comfortable working within the Power Query Editor, let\u0026rsquo;s take the next step and discuss more advanced options. Often, you will find the need to go beyond these basic transforms when dealing with data that requires more care. In this section, you will learn about some common advanced transforms that you may have a need for, which include Conditional Columns, Fill Down, Unpivot, Merge Queries, and Append Queries.\nConditional Columns Using the Power Query Editor Conditional Columns functionality is a great way to add new columns to your query that follow logical if/then/else statements. This concept of if/then/else is common across many programming languages, including Excel formulas. Let\u0026rsquo;s review a real-world scenario where you would be required to do some data cleansing on a file before it could be used. In this example, you will be provided with a file of all the counties in the United States, and you must create a new column that extracts the state name from the county column and places it in its own column:\n Start by connecting to the FIPS_CountyName.txt file that is found in the book files using the Text/CSV connector.\n Launch the Power Query Editor by selecting Transform Data, then start by changing the data type of Column1 to Text. When you do this, you will be prompted to replace an existing type conversion. You can accept this by clicking Replace current.\n Now, on Column2, filter out the value UNITED STATES from the column by clicking the arrow next to the column header and unchecking UNITED STATES. Then, click OK.\n Remove the state abbreviation from\nColumn2  by right-clicking on the column header and selecting\n  Split Column\n|  By Delimiter\n. Choose\n\u0026ndash; Custom \u0026ndash;\nfor the delimiter type, and type  ,\nbefore then clicking  OK\n, as shown in\nFigure 2.7\n:\nFigure 2.7: Splitting a column based on a delimiter\n Next, rename the column names Column1, Column 2.1, and Column 2.2, to County Code, County Name, and State Abbreviation, respectively.\n To isolate the full state name into its own column, you will need to implement Conditional Column. Go to the Add Column ribbon and select Conditional Column.\n Change the\n  New column name\nproperty to   State Name  and implement the logic   If State Abbreviation equals null Then return County Name Else return null  as shown in  Figure 2.8\n. To return the value from another column, you must select the icon in the\nOutput\nproperty, then choose  Select a column\n. Once this is complete, click\nOK\n:\nFigure 2.8: Adding a conditional column\nThis results in a new column called State Name, which has the fully spelled-out state name only appearing on rows where the State Abbreviation is null:\nFigure 2.9: End result of following these steps\nThis is only setting the stage to fully scrub this dataset. To complete the data cleansing process for this file, read on to the next section about Fill Down. However, for the purposes of this example, you have now learned how to leverage the capabilities of the Conditional Column transform in the Power Query Editor.\nFill Down Fill Down is a rather unique transform in how it operates. By selecting Fill Down on a particular column, a value will replace all null values below it until another non-null appears. When another non-null value is present, that value will then fill down to all subsequent null values. To examine this transform, you will pick up from where you left off in the Conditional Column example in the previous section:\n Right-click on the State Name column header and select Transform | Capitalize Each Word. This transform should be self-explanatory.\n Next, select the State Name column and, in the Transform ribbon, select Fill | Down. This will take the value in the State Name column and replace all non-null values until there is another State Name value that it can switch to. After performing this transform, scroll through the results to ensure that the value of Alabama switches to Alaska when appropriate.\n To finish this example, filter out any\nnull  values that appear in the\n  State Abbreviation\ncolumn. The final result should look like  Figure 2.10\n, as follows:\n![img](https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_11.png)  Figure 2.10: End result of following these steps\nIn this example, you learned how you can use Fill Down to replace all of the null values below a non-null value. You can also use Fill Up to do the opposite, which would replace all the null values above a non-null value. One important thing to note is that the data must be sorted properly for Fill Down or Fill Up to be successful. In the next section, you will learn about another advanced transform, known as Unpivot.\nUnpivot The Unpivot transform is an incredibly powerful transform that allows you to reorganize your dataset into a more structured format best suited for BI. Let\u0026rsquo;s discuss this by visualizing a practical example to help understand the purpose of Unpivot. Imagine you are provided with a file that contains the populations of US states over the last three years, and looks as in Figure 2.11:\nFigure 2.11: Example data that will cause problems in Power BI\nThe problem with data stored like this is you cannot very easily answer simple questions. For example, how would you answer questions like, What was the total population for all states in the US in 2018? or What was the average state population in 2016? With the data stored in this format, simple reports are made rather difficult to design. This is where the Unpivot transform can be a lifesaver. Using Unpivot, you can change this dataset into something more acceptable for an analytics project, as shown in Figure 2.12:\nFigure 2.12: Results of unpivoted data\nData stored in this format can now easily answer the questions posed earlier by simply dragging a few columns into your visuals. To accomplish this in other programming languages would often require fairly complex logic, while the Power Query Editor does it in just a few clicks.\nThere are three different methods for selecting the Unpivot transform that you should be aware of, and they include the following options:\n Unpivot Columns: Turns any selected columns, headers into row values and the data in those columns into a corresponding row. With this selection, any new columns that may get added to the data source will automatically be included in the Unpivot transform. Unpivot Other Columns: Turns all column headers that are not selected into row values and the data in those columns into a corresponding row. With this selection, any new columns that may get added to the data source will automatically be included in the Unpivot transform. Unpivot Only Selected Columns: Turns any selected columns\u0026rsquo; headers into row values and the data in those columns into a corresponding row. With this selection, any new columns that may get added to the data source will not be included in the Unpivot transform.  Let\u0026rsquo;s walk through two examples of using the Unpivot transform to show you the first two of these methods, and provide an understanding of how this complex problem can be solved with little effort in Power BI. The third method mentioned for doing Unpivot will not be shown since it\u0026rsquo;s so similar to the first option:\n Launch a new instance of the Power BI Desktop, and use the Excel connector to import the workbook called\nIncome Per Person.xlsx  found in the book source files. Once you\n  select this workbook, choose the spreadsheet called\nData\nin the  Navigator\nwindow, and then select  Transform Data\nto launch the Power Query Editor.  Figure 2.13\nshows what our data looks like before the  Unpivot\noperation:  Figure 2.13: Example before Unpivot is performed\n Now, make the first row of data into column headers by selecting the transform called Use First Row as Headers on the Home ribbon.\n Rename the GDP per capita PPP, with projections column to Country.\n If you look closely at the column headers, you can tell that most of the column names are actually years and the values inside those columns are the income for those years. This is not the ideal way to store this data because it would be incredibly difficult to answer a question like, What is the average income per person for Belgium? To make it easier to answer this type of question, right-click on the Country column and select Unpivot Other Columns.\n Rename the columns Attribute and Value to Year and Income, respectively.\n To finish this first example, you should also rename this query\n  Income\n. The results of these first steps can be seen in\nFigure 2.14\n:\nFigure 2.14: Results of unpivoted data\nThis first method walked you through what can often be the fastest method for performing an Unpivot transform, which is by using the Unpivot Other Columns option. In this next example, you will learn how to use the Unpivot Columns method as well:\n Remain in the Power Query Editor, and select  New Source\nfrom the  Home\nribbon. Use the Excel connector to import the   Total Population.xlsx  workbook from the book source files. Once you select this workbook, choose the spreadsheet called  Data\nin the  Navigator\nwindow, and then select  OK\n.\nFigure 2.15\nshows the dataset before  Unpivot\nhas been added:  Figure 2.15: Example before Unpivot is performed\n Like the last example, you will again need to make the first row of data into column headers by selecting the transform called Use First Row as Headers on the Home ribbon.\n Then, rename the column Total population to Country.\n This time, multi-select all the columns except\n  Country\n, then right-click on one of the selected columns and choose\nUnpivot Other Columns\nas shown in  Figure 2.16\n. The easiest way to multi-select these columns is to select the first column then hold\nShift\nbefore clicking the last column:  Figure 2.16: Using the Unpivot Other Columns transform\n Rename the columns from  Attribute\nand  Value\nto  Year\nand  Population\n, respectively, to see the result showing in\nFigure 2.17\n:\nFigure 2.17: Shows the final result of these steps\nIn this section, you learned about two different methods for performing an Unpivot. To complete the data cleansing process on these two datasets, it\u0026rsquo;s recommended that you continue through the next section on merging queries.\nMerge Query Another common requirement when building BI solutions is the need to join two tables together to form a new outcome that includes some columns from both tables in the result. Fortunately, Power BI makes this task very simple with the Merge Queries feature. Using this feature requires that you select two tables and then determine which column or columns will be the basis of how the two queries are merged. After determining the appropriate columns for your join, you will select a join type. The join types are listed here with the description that is provided within the product:\n Left Outer (all rows from the first table, only matching rows from the second) Right Outer (all rows from the second table, only matching rows from the first) Full Outer (all rows from both tables) Inner (only matching rows from both tables) Left Anti (rows only in the first table) Right Anti (rows only in the second table)  Many of you may already be very familiar with these different join terms from SQL programming you have learned in the past. However, if these terms are new to you, I recommend reviewing Visualizing Merge Join Types in Power BI, courtesy of Jason Thomas in the Power BI Data Story Gallery: https://community.powerbi.com/t5/Data-Stories-Gallery/Visualizing-Merge-Join-Types-in-Power-BI/m-p/219906. This visual aid is a favorite of many users that are new to these concepts.\nTo examine the Merge Queries option, you will pick up from where you left off with the Unpivot examples in the previous section:\n With the Population query selected, find and select Merge Queries | Merge Queries as New on the Home ribbon.\n In the Merge dialog box, select the Income query from the drop-down selection in the middle of the screen.\n Then, multi-select the Country and Year columns on the Population query, and do the same under the Income query. This defines which columns will be used to join the two queries together. Ensure that the number indicators next to the column headers match, as demonstrated in Figure 2.18. If they don\u0026rsquo;t, you could accidentally attempt to join on the incorrect columns.\n Next, select\n  Inner\n(  only matching rows\n) for\nJoin Kind\n. This join type will return rows only when the columns you chose to join on have values that exist in both queries. Before you click\nOK\n, confirm that your screen matches\nFigure 2.18\n:\nFigure 2.18: Configuring a merge between two queries\n Once you select OK, this will create a new query called Merge1 that combines the results of the two queries. Go ahead and rename this query Country Stats.\n You will also notice that there is a column called Income that has a value of Table for each row. This column is actually representative of the entire Income query that you joined to. To choose which columns you want from this query, click the Expand button on the column header. After clicking the Expand button, uncheck Country, Year, and Use original column name as prefix, then click OK.\n Rename the column called\nIncome.1  to\nIncome   .\nFigure 2.19\nshows this step completed:  Figure 2.19: Configuring a merge between two queries\n Finally, since you chose the option  Merge Queries as New\nin  Step 1\n, you can disable the load\noption for the original queries that you started with. To do this, right-click on the\nIncome\nquery in the  Queries\npane and click  Enable load\nto disable it. Do the same thing for the  Population\nquery as shown in  Figure 2.20\n. Disabling these queries means that the only query that will be loaded into your Power BI data model is the new one, called\nCountry Stats\n:\nFigure 2.20: Uncheck to disable the loading of this query into the data model\nTo begin using this dataset in a report, you would click Close \u0026amp; Apply. You will learn more about building reports in Chapter 5, Visualizing Data.\nBy default, merging queries together relies on exact matching values between your join column(s). However, you may work with data that does not always provide perfect matching values. For example, a user enters data and misspells their country as \u0026ldquo;Unite States\u0026rdquo; instead of United States. In those cases, you may consider the more advanced feature called Fuzzy Matching. With Fuzzy Matching, Power BI can perform an approximate match and still join on these two values based on the similarity of the values. In this section, you learned how the Merge Queries option is ideal for joining two queries together. In the next section, you will learn how you could solve the problem of performing a union of two or more queries.\nAppend Query Occasionally, you will work with multiple datasets that need to be appended to each other. Here\u0026rsquo;s a scenario: you work for a customer service department for a company that provides credit or loans to customers. You are regularly provided with .csv and .xlsx files that give summaries of customer complaints regarding credit cards and student loans. You would like to analyze both of these data extracts at the same time but, unfortunately, the credit card and student loan complaints are provided in two separate files. In this example, you will learn how to solve this problem by performing an append operation on these two different files:\n Launch a new instance of the Power BI Desktop, and use the Excel connector to import the workbook called Student Loan Complaints.xlsx found in the book source files. Once you select this workbook, choose the spreadsheet called Student Loan Complaints in the Navigator window, and then select Transform Data to launch the Power Query Editor.\n Next, import the credit card data by selecting New Source | Text/CSV, then choose the file called Credit Card Complaints.csv found in the book source files. Click OK to bring this data into the Power Query Editor.\n With the Credit Card Complaints query selected, find and select Append Queries | Append Queries as New on the Home ribbon.\n Select\n  Student Loan Complaints\nas the table to append to, then select  OK\nas shown in  Figure 2.21\n:\nFigure 2.21: Configuring an append between two queries\n Rename the newly created query  All Complaints\nand view the results as seen in  Figure 2.22\n:\nFigure 2.22: Configuring an append between two queries\n Similar to the previous example, you would likely want to disable the load option for the original queries that you started with. To do this, right-click on the Student Load Complaints query in the Queries pane and click Enable load to disable it.\n Do the same to the Credit Card Complaints query, and then select Close \u0026amp; Apply.\n  Now that you have learned about the various methods for combining data, the next section will discuss a more advanced method of working with data using the R programming language.\nLeveraging R R is a very powerful scripting language that is primarily used for advanced analytics tools, but also has several integration points within Power BI. One such integration is the ability to apply business rules to your data with the R language. Why is that important? Well, with this capability you can extend beyond the limits of the Power Query Editor and call functions and libraries from R to do things that would not normally be possible. In the next two sections, you will explore how to set up your machine to leverage R within Power BI and then walk through an example of using an R script transform.\nThere are many additional books and references you can read to learn more about the R scripting language, but for the purposes of this book, our goal is to inform you of what is possible when R and Power BI are combined.\nInstallation and configuration To use R within Power BI, you must first install an R distribution for you to run and execute scripts against. In this book, we will leverage Microsoft\u0026rsquo;s distribution, Microsoft R Open. It is an open source project and free for anyone to use. Once Microsoft R Open has been installed, you can then configure Power BI to recognize the home directory where R libraries may be installed. Let\u0026rsquo;s walk through these setup steps together:\n Navigate to the website https://mran.microsoft.com/download/ to download and install Microsoft R Open.\n For the purposes of our example, you will select Download next to Windows.\n Once the download has completed, run the installation and accept all default settings and user agreements.\n Next, launch a new instance of Power BI Desktop to set up the R integration with Power BI. Click the menu options File | Options and settings | Options.\n Choose the\n  R scripting\nsection and ensure that the  Detected R home directories\nproperty is filled with the R instance you just installed, as shown in  Figure 2.23\n:\nFigure 2.23: Mapping the R home directories in Power BI\n Once this is completed, click OK.  With this setup now complete, let\u0026rsquo;s see how we can take advantage of R within Power BI.\nThe R script transform With the R distribution now installed and configured to integrate with Power BI, you are now ready to see what\u0026rsquo;s possible with these new capabilities. In this example, you will be looking at data from the European stock market. The problem with this dataset, which calls for it to be corrected with R, is that the file provided to you has missing values for certain days. So, to get a more accurate reading of the stock market, you will use an R package called MICE to impute the missing values:\n Before beginning in Power BI, you should ensure that the\nMICE  library is installed and available in the R distribution you set up in the last section. To do this, launch Microsoft R Open from your device. This is the basic RGui that was installed for you to run R scripts with.\n  For many developers, the preferred method for writing R scripts is a free open source tool called RStudio. RStudio includes a code editor, debugging, and visualization tools that many find easier to work with. You can download RStudio from https://www.rstudio.com/.\n Type the following script in the  R Console\nwindow, and then hit  Enter\n:\n install.packages(\u0026quot;mice\u0026quot;)  This input is illustrated in the following screenshot:\nFigure 2.24: Running the library install in RGui\n You can close the  R Console\nwindow and return to Power BI Desktop after it returns an output like the following:   package 'mice' successfully unpacked and MD5 sums checked.   In Power BI Desktop, start by connecting to the required data source called EuStockMarkets_NA.csv from the book source files. Once you connect to the file, click Transform Data to launch the Power Query Editor.\n You will notice that there are a few days missing values in the SMI (Stock Market Index) column. We would like to replace values that show NA with approximate values using an R script. Go to the Transform ribbon, and select the Run R Script button on the far right.\n Use the following R script to call the\nMICE  library that you recently installed to detect what the missing values in this dataset should be:\n # 'dataset' holds the input data for this script library(mice) tempData \u0026lt;- mice(dataset,m=1,maxit=50,meth='pmm',seed=100) completedData \u0026lt;- complete(tempData,1) output \u0026lt;- dataset output$completedValues \u0026lt;- completedData$\u0026quot;SMI missing values\u0026quot;  Click OK. If you are prompted with a warning indicating Information is required about data privacy click Continue.\n Next, click on the hyperlink on the table value next to the completedData row to see the result of the newly implemented transform for detecting missing values.\n  This new output has replaced the missing values with new values that were detected based on the algorithm used within the R script. To now build a set of report visuals on this example, you can click Close \u0026amp; Apply on the Home ribbon.\nThis is just one simple way that R can be used with Power BI. You should note that in addition to using R as a transform, it can also be used as a data source and as a visual within Power BI.\nWhile this book highlights the programming language R to extend the capabilities of Power BI, some might prefer Python. Python is another programming language that allows for extensibility into Power BI to create new data connectors, transforms, and visuals. So, should you choose R or Python? That depends on which you are more comfortable with. If you have already spent time learning Python, then stick with that! In the next section of this chapter, you will learn about Power BI\u0026rsquo;s AI integration features, which give you the ability to call on components of Azure Cognitive Services with the Power Query Editor.\nAI Insights As you learned in the previous section, Power BI integrates and takes advantage of outside tools to enhance the capabilities within itself. That continues to be the case with the AI Insights features. Leveraging the AI Insights capabilities gives you the ability to tap into core features and algorithms within Azure Cognitive Services and expose them within Power BI. So how can this be useful to you?\nImagine you work for a company that runs a vacation rentals website. Customers can book travel and post reviews of their trips on your website. With thousands of customers and hundreds of rental homes, it can be difficult to manage all the reviews that come in to make sure your locations are all meeting the standards your customers expect. With AI Insights you can run algorithms that can perform sentiment analysis, key phrase extraction, language detection, and even image tagging. So, if you have international customers that post reviews, you can use language detection to understand what language the post was written in. Then you can use sentiment analysis to capture whether the review was positive or negative. Finally, using phrase extraction, you can pull out key terms in the reviews to see if the same locations continue to receive feedback regarding similar problems. Furthermore, if your feedback system allows photos to be posted in the reviews, the image tagging capabilities can return a list of characteristics found in the images posted. This would allow for automated categorization of images using AI.\nAs you can see, these are very powerful features that take your analytics processing to the next level. There are limitations, however, that you should be aware of before exploring these features. As of the time that this book was published, Cognitive Services integration is only supported for Power BI Premium capacity nodes EM2, A2, or P1 and above. This means if your company is not currently leveraging Power BI Premium, then these features are not available to you.\nBefore using the AI Insights features in Power BI, you will need to change the capacity settings in the Power BI admin portal to enable the AI workload. After turning on the AI workload setting, you can also set the maximum amount of memory you would like to give the workload. The general recommendation is a memory limit of 20%.\nIn the next section, you will learn how to leverage an AI Insights Text Analytics feature called Sentiment Analysis.\nSentiment Analysis with Text Analytics The Text Analytics features within the AI Insights features can be incredible time-savers. Imagine having to read paragraphs of information and conclude what was important or whether it was written in a positive or negative light. These are exactly the type of things that this feature can do for you. In this next example, you are going to test out one of these features by running a sentiment analysis algorithm on hotel reviews to see how customers feel about staying at your hotel locations:\n Launch a new instance of Power BI Desktop, and use the Excel connector to import the workbook called Hotel Ratings.xlsx found in the book source files. Once you select this workbook, choose the spreadsheet called Reviews in the Navigator window, and then select Transform Data to launch the Power Query Editor.\n Select Text Analytics on the Home ribbon of the Power Query Editor. If this is your first time using this feature, you may be prompted to sign into a Power BI account that has Power BI Premium capacity assigned to it.\n Next, you will be prompted to choose which Text Analytics algorithm you would like to use. Select\n  Score sentiment\n, as shown in\nFigure 2.25\n, and ensure the\nReviewText\nfield is the  Text\nthat will be analyzed. Then click  OK\n:\nFigure 2.25: Using the Text Analytics feature\n If prompted with a data privacy warning, click Continue and then select Ignore Privacy Levels check for this file before clicking Save. This type of warning can occur when you combine two disparate sources or services together and is to ensure it is OK for these data sources to be combined.  This transform will produce a new numeric column with a value between 0 and 1 for every row in the dataset. A sentiment score of .50 is considered neutral, while any score lower is negative and any score higher is generally positive:\nFigure 2.26: Results of Score sentiment\nLooking at Figure 2.26, it looks like the AI integration, with a few exceptions, did a good job determining how to rate each review.\nNext, in the final section of this chapter, you will be introduced to the M formula language.\nThe M formula language The Power Query Editor is the user interface that is used to design and build data imports. However, you should also know that every transform you apply within this editor is actually, quietly and behind the scenes, writing an M query for you. The letter M here is a reference to the language\u0026rsquo;s data mashup capabilities.\nFor simple solutions, it is unlikely that you will ever need to even look at the M query that is being written, but there are some more complex cases where it\u0026rsquo;s helpful to understand how to read and write your own M. For the purposes of this book, covering just the Power BI essentials, you will learn how to find the M query editor within your solution and then understand how to read what it is doing for you.\nFor the purposes of this example, you can open up any previously built example, however, the screenshot used here is from the very first example in this chapter on basic transforms:\n Using any Power BI solution you have designed, launch the Power Query Editor.\n On the\n  Home\nribbon, select  Advanced Editor\nto see the M query that has been written by the user interface.  Figure 2.27\nshows an example of what your  Advanced Editor\nmight show:  Figure 2.27: Understanding the elements of M\nThis query has been formatted to make it easier to read. Let\u0026rsquo;s review the key elements that are present here:\n The let expression: Encapsulates a set of values or named expressions to be computed. Named expressions or variables: The name given to a set of operations in a step. These names can be anything, but you should note that if you wish to have a space in the name of a step then it must be surrounded by #\u0026quot;\u0026quot;. For example, if I wanted something to be called Step 1, then I would have to name an expression #\u0026quot;Step 1\u0026quot;. If a space is not required in the name of your step then the double quotes are not required. M functions: The operations that are used to manipulate the data source. Prior step reference: The M query language generally executes its functions as serial operations, meaning each operation is executed one after the other sequentially. You can see this when you look at a query because each call to an M function always references the prior-named expression, to pick up where it left off. The in expression: Oddly, the in expression is actually a reference to what the query will output. Whichever named expression is referenced in the in expression will be what is returned back in the Power Query Editor preview.  It is important to realize that M is case-sensitive. That means if you ever make a change to a query or write one from scratch, you should be careful because there is a difference between \u0026ldquo;a\u0026rdquo; and \u0026ldquo;A.\u0026rdquo;\n#shared As mentioned previously, this book will not dive deep into writing your own M queries since that would be far beyond the essentials of Power BI. However, there is a great method for exploring the M functions that are available, and how to use them. Within the Power Query Editor, you can use the #shared function to return documentation on every available function in the M library. Let\u0026rsquo;s walk through how you can leverage this tool:\n In a new instance of Power BI Desktop, select Get data and then choose Blank Query. This will launch the Power Query Editor with an empty formula bar waiting for you to provide your own M.\n In this formula bar, type = #shared, then hit Enter. Remember that M is case-sensitive so you must use a lowercase s when typing shared.\n This will return a list of all the available M functions. By selecting the cell that has the hyperlink text of a certain function, you can see documentation on how to use each function.\n  Figure 2.28\nshows an example of this:  Figure 2.28: Example of function documentation\nThis is a great method for learning what M functions are available, and how each may be used. If you are stumped on how to solve a problem using M then make this your first stop to explore what options you have.\nSummary In this chapter, you learned that the Power Query Editor is an extremely powerful tool for applying business rules to incoming data. Implementing data cleansing techniques can be as simple as right-clicking on a column, or more complex, such as when building a conditional column. While the Power Query Editor does have a vast library of transforms available, you also learned that you can tap into the capabilities of R to extend what\u0026rsquo;s possible when designing queries. Finally, this chapter discussed the AI capabilities within the Power Query Editor that allow you to leverage several algorithms available within Azure Cognitive Services. In the next chapter, on building the data model, you will learn about proper techniques for building a well-designed Power BI data model to ensure your solutions can solve all your reporting needs.\n","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609850196,"objectID":"4465c4633eb3ddd3dd2812d61979a959","permalink":"https://www.marcusrb.com/en/power-bi/03-data-transformation-strategies/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/en/power-bi/03-data-transformation-strategies/","section":"resources","summary":"El sistema de archivos controla como se almacenan los archivos en el ordenador. Sus dos tareas principales son guardar y leer archivos previamente guardados.","tags":null,"title":"Data transformation strategies","type":"docs"},{"authors":null,"categories":null,"content":" Importing data Power BI is best known for the impressive data visualizations and dashboard capabilities it has. However, before you can begin building reports, you first need to connect to the necessary data sources. Within Power BI Desktop, a developer has more than 130 unique data connectors to choose from, ranging from traditional file types, database engines, big data solutions, cloud sources, data stored on a web page, and other SaaS providers. This book will not cover all 130 connectors that are available, but it will highlight some of the most popular.\nWhen establishing a connection to a data source, you may be presented with one of three different options regarding how your data will be treated: Import, DirectQuery, or live connection. This section will focus specifically on the Import option.\nChoosing to import data, which is the most common option and default behavior, means that Power BI will physically extract rows of data from the selected source and store it in an in-memory storage engine within Power BI. Power BI Desktop uses a special method for storing data, known as xVelocity, which is an in-memory technology that not only increases the performance of your query results, but can also highly compress the amount of space taken up by your Power BI solution. In some cases, the compression that takes place can even lower the required disk space by up to one-tenth of the original data source size. The xVelocity engine uses a local unseen instance of SQL Server Analysis Services (SSAS) to provide these in-memory capabilities.\nThere are consequences to using the Import option within Power BI that you should also consider. These consequences will be discussed later in this chapter, but as you read on, consider the following:\n How does data that has been imported into Power BI get updated? What if I need a dashboard to show near real-time analytics? How much data can really be imported into an in-memory storage system?  Now that you are familiar with the underlying mechanics of importing data, let\u0026rsquo;s try it out with a few of the most common data source types, starting with Excel.\nExcel as a source Believe it or not, Excel continues to be the most popular application in the world and, as such, you should expect that at some point, you will be using it as a data source:\n To get started, open Power BI Desktop and close the start up screen if it automatically appears.\n Under the Home ribbon, you will find that Get data button, which you already learned is used for selecting and configuring data sources. Selecting the down arrow next to the button will show you the most common data connectors, but selecting the center of the button will launch the full list of all available connectors. Regardless of which way you select the button, you will find Excel at the top of both lists.\n Navigate to and open the file called\nAdventureWorksDW.xlsx  from this book\u0026rsquo;s resources. This will\n  launch the\nNavigator\ndialog, shown in the following screenshot, which is used for selecting the objects in the Excel  workbook you wish to take data from:\nFigure 1.2: The Navigator dialog is used for selecting spreadsheets or tables\n In this example, you can see six separate spreadsheets you can choose from. Clicking once on the spreadsheet name will give you a preview of the data it stores, while clicking the checkbox next to the name will include it as part of the data import. For this example, select the checkboxes next to all of the available objects, then notice the options available at the bottom right.\n Selecting Load will immediately take the data from the selected spreadsheets and import them as separate tables into your Power BI data model. Choosing Transform Data will launch an entirely new window called Power Query Editor, which allows you to apply business rules or transform to your data prior to importing it. You will learn much more about Power Query Editor in Chapter 2, Data Transformation Strategies. Since you will learn more about this later, simply select Load to end this example.\n  Another topic you will learn more about in Chapter 7, Using a Cloud Deployment with the Power BI Service, is the concept of data refreshes. This is important because, when you import data into Power BI, that data remains static until another refresh is initiated. This refresh can either be initiated manually or set on a schedule. This also requires the installation of Data Gateway, the application in charge of securely pushing data into Power BI Service. Feel free to skip to Chapter 7, Using a Cloud Deployment with the Power BI Service, if configuring a data refresh is a subject you need to know about now.\nSQL Server as a source Another common source designed for relational databases is Microsoft SQL Server:\n To connect to SQL Server, select the  Get data\nbutton again, but this time choose  SQL Server\n. The following screenshot shows that you must provide the server, but the database is optional and can be selected later:\nFigure 1.3: Establishing a connection to SQL Server\n On your first use of SQL server, you are asked to choose the type of  Data Connectivity mode\nyou would like. As mentioned previously,  Import\nis the default mode, but you can optionally select  DirectQuery\n. DirectQuery will be discussed in greater detail later in this\nchapter. Expanding the\nAdvanced\noptions provides a way to insert a SQL statement that may be used as your source. For the  following example, the\nServer\nname property is the only property populated before you click  OK\n:\nFigure 1.4: Providing credentials to SQL Server\n Next, you will be prompted, as shown in the preceding screenshot, to provide the credentials you are using to connect to the database server you provided on the previous screen.\n Click Connect after providing the proper credentials. You may be prompted with a warning stating that Power BI is only able to access the data source using an unencrypted connection. Click OK if you encounter this to launch the same Navigator dialog that you may remember from when you connected to Excel. Here, you will select the tables, views, or functions within your SQL Server database that you wish to import into your Power BI solution. Once again, the final step in this dialog allows you to choose to either Load or Transform Data.\n  Now that you have a better understanding of how to connect to some of the most common data sources, let\u0026rsquo;s look at one that is rather unique.\nWeb as a source One pleasant surprise to many Power BI developers is the availability of a web connector. Using this connection type allows you to source data from files that are stored on a website, or even data that has been embedded into an HTML table on a web page. Using this type of connector can often be helpful when you would like to supplement your internal corporate data sources with information that can be publicly found on the internet.\nFor this example, imagine you are working for a major automobile manufacturer in the United States. You have already designed a Power BI solution using data internally available within your organization that shows historical patterns in sales trends. However, you would like to determine whether there are any correlations in periods of historically higher fuel prices and lower automobile sales. Fortunately, you found that the United States Department of Labor publicly posts the historical average consumer prices of many commonly purchased items, including fuel prices:\n Now that you understand the scenario within Power BI Desktop, select the  Get data\nbutton and choose  Web\nas your source. You will then be prompted to provide the  URL\nwhere the data can be found. In this example, the data can be found by  searching on the website\nhttps://www.data.gov/\n. Alternatively, to save you some time, use the direct link:\nhttps://download.bls.gov/pub/time.series/ap/ap.data.2.Gasoline\n. Once you provide the\nURL\n, click\nOK\n, as shown in the following screenshot:\nFigure 1.5: Providing the URL where your data can be found\n Next, you will likely be prompted with an  Access Web Content\ndialog box. This is important  when you are using a data source that requires someone to log in to access it. Since this\ndata source does not require a login to find the data, you can simply select anonymous access, which is the default, and then click\nConnect\n, as shown in the following screenshot:\nFigure 1.6: A preview of the data is shown before you import it into Power BI\n Notice on the next screen that Power BI Desktop recognizes the URL you provided as a tab-delimited file. This can now easily be added to any existing data model you have designed.  Now that you\u0026rsquo;ve learned how to connect to various data sources, it is important to discuss in more depth the different data storage modes.\nDirectQuery Many of you have likely been trying to envision how you may implement these data imports in your environment. You may have asked yourself questions such as the following:\n If data imported into Power BI uses an in-memory technology, did my company provide me with a machine that has enough memory to handle this? Am I really going to import my source table with tens of billions of rows into memory? How do I handle the requirement of displaying results in real time from the source?  These are all excellent questions that would have many negative answers if the only way to connect to your data was by importing your source into Power BI. Fortunately, there is another way. Using DirectQuery, Power BI allows you to connect directly to a data source so that no data is imported or copied into Power BI Desktop.\nWhy is this a good thing? Consider the questions that were asked at the beginning of this section. Since no data is imported to Power BI Desktop, this means it is less important how powerful your personal laptop is. This is because all query results are now processed on the source server instead of your laptop. It also means that there is no need to refresh the results in Power BI, since any reports you design are always pointing to a live version of the data source. That\u0026rsquo;s a huge benefit!\nThe following screenshot shows a connection to a SQL Server database with the DirectQuery option selected:\nFigure 1.7: SQL Server Data Connectivity mode allows you to switch to DirectQuery mode\nEarlier in this chapter, the Data Gateway application was mentioned as a requirement for scheduling data refreshes for sources that used the Import option. This same application is also needed with DirectQuery if your data is an on-premises source. Even though there is no scheduled data refresh, the Data Gateway application is still required to push on-premises data into the cloud. Again, this will be discussed in more depth in Chapter 7, Using a Cloud Deployment with the Power BI Service.\nLimitations So, if DirectQuery is so great, why not choose it every time? Well, with every great feature, you will also find limitations. The first glaring limitation is that not all data sources support DirectQuery. At the time this book was written, the following data sources support DirectQuery in Power BI:\n Amazon Redshift AtScale Cubes Azure HDInsight Spark Azure SQL Database Azure SQL Data Warehouse BI Connector Denodo Dremio Essbase Exasol Google BigQuery HDInsight Interactive Query IBM DB2 IBM Netezza Impala Indexima Intersystems IRIS Jethro ODBC Kyligence Enterprise MarkLogic ODBC Oracle PostgreSQL Power BI datasets QubolePresto SAP Business Warehouse Message Server SAP Business Warehouse Server SAP HANA Snowflake Spark SQL Server Teradata Database Vertica  Depending on the data source you choose, there is a chance of slower query performance when using DirectQuery compared to the default data import option. Keep in mind that when the Import option is selected, it leverages a highly sophisticated in-memory storage engine. When selecting DirectQuery, performance will depend on the source type you have chosen from the preceding list.\nAnother limitation worth noting is that not all Power BI features are supported when you choose DirectQuery. For example, depending on the selected source, some of the Power Query Editor features are disabled and could result in the following message: \u0026ldquo;This step results in a query that is not supported in DirectQuery mode.\u0026rdquo; The following screenshot shows this response:\nFigure 1.8: Certain transforms may force a user out of DirectQuery mode\nThe reason for this limitation is because DirectQuery automatically attempts to convert any Power Query steps into a query in the data source\u0026rsquo;s native language. So, if the source of this solution was SQL Server, then Power BI would attempt to convert this data transformation into a comparable T-SQL script. Once Power BI realizes Power Query Editor used a function that is not compatible with the source, the error is generated.\nComposite models Occasionally, you may find it helpful for your data model to take a hybrid approach regarding how it stores data. For example, you want sales transactions to be displayed in near real time on your dashboard, so you set your SalesTransaction table to use DirectQuery. However, your Product table rarely has values that are added or changed. Having values that do not change often make it a great candidate for the imported data storage method to take advantage of the performance benefits.\nThis describes a perfect scenario for utilizing a feature called composite models. Composite models allow a single Power BI solution to include both DirectQuery and import table connections within one data model. From the Power BI developer\u0026rsquo;s perspective, you can take advantage of the best parts of each data storage mode within your design.\nAnother effective use case for composite models is available due to a feature called aggregations. Leveraging aggregations is one of the best ways to manage extremely large datasets in Power BI. You will learn more about designing aggregations in Chapter 3, Building the Data Model.\nWithin Power BI Desktop, it is clear a solution is leveraging composite models if we view the storage mode in the bottom-right corner of the tool. Clicking this corner, shown in the following screenshot, will allow you to switch all tables to Import mode instead. Optionally, if you need to change the storage mode of individual tables, this can be accomplished in the Model view by selecting individual tables:\nFigure 1.9: The bottom-right corner allows you to switch storage modes for the entire model\nWhile composite models give you the best of DirectQuery and import models, there\u0026rsquo;s a third storage mode that is often used for data sources that are highly groomed by IT.\nLive connection The basic concept of live connection is very similar to that of DirectQuery. Just like DirectQuery, when you use a live connection, no data is actually imported into Power BI. Instead, your solution points directly to the underlying data source and leverages Power BI Desktop simply as a data visualization tool. So, if these two things are so similar, why give them different names? The answer is because even though the basic concept is the same, DirectQuery and live connection vary greatly.\nOne difference that should quickly be noticeable is the query performance experience. It was mentioned in a previous section that DirectQuery can often have poor performance, depending on the data source type. With live connection, you generally will not have any performance problem because it is only supported by the following types of data sources:\n SQL Server Analysis Services database Azure Analysis Services database Power BI datasets  The reason performance does not suffer with these data sources is because they either use the same xVelocity engine that Power BI does, or another high-performance storage engine. To set up your own live connection to one of these sources, you can choose the SQL Server Analysis Services database from the list of connectors after selecting Get data. The following screenshot shows that you can specify that the connection should be set to Connect live:\nFigure 1.10: SQL Server Analysis Services Data Connectivity mode allows you to switch to Connect live mode\nIf a dataset is configured for a Live connection or DirectQuery, then you can expect automatic refreshes to occur approximately each hour by default. You can manually adjust the refresh frequency in the Scheduled cache refresh option in the Power BI service.\nOf course, these benefits don\u0026rsquo;t come without a cost. Let\u0026rsquo;s discuss some of the limitations of Live connection.\nLimitations So far, this sounds great! You have now learned that you can connect directly to your data sources, without importing data into your model, and that you won\u0026rsquo;t have significant performance consequences. Of course, these benefits don\u0026rsquo;t come without giving something up, so what are the limitations of a live connection?\nWhat you will encounter with live connections are limitations that are generally a result of the fact that Analysis Services is an Enterprise BI tool. Thus, if you are going to connect to it, then it has probably already gone through significant data cleansing and modeling by your IT team.\nModeling capabilities such as defining relationships are not available because these would be designed in an Analysis Services Model. Also, Power Query Editor is not available at all against a Live connection source. While, at times, this may be frustrating, it does make sense that it works this way. This is because any of the changes you may desire to make with relationships or in the Power Query Editor should be done in Analysis Services, not Power BI.\nWhich should I choose? Now that you have learned about the three different ways to connect to your data, you are left wondering which option is best for you. It\u0026rsquo;s fair to say that the choice you make will really depend on the requirements of each individual project you have.\nTo summarize, some of the considerations that were mentioned in this chapter are listed in the following table:\n   Consideration Import Data DirectQuery Live connection     Best performance Yes No Yes   Best design experience Yes No No   Best for keeping data up to date No Yes Yes   Data source availability Yes No No   Most scalable No Yes Yes    Some of the items you\u0026rsquo;ll consider may be more important than others. So, to make this more personal, try using the Data Connectivity - Decision Matrix file that is included with this book. In this file, you can rank (from 1 to 10) the importance of each of these considerations to help you choose which option is best for you.\nSince the Import Data option presents the most available features, going forward, this book primarily uses this option. In Chapter 2, Data Transformation Strategies, you will learn how to implement data transformation strategies to ensure all the necessary business rules are applied to your data.\nSummary Power BI provides users with a variety of methods for connecting to data sources with natively built-in data connectors. The connector you choose for your solution will depend on where your data is located. Once you\u0026rsquo;ve connected to a data source, you can decide on what type of query mode best suits your needs. Some connectors allow for little to no latency in your results with options like DirectQuery or live connection. In this chapter, you learned about the benefits and disadvantages of each query mode, and you were given a method for weighting these options using a decision matrix.\nIn the next chapter, you will learn more about how data transformations may be applied to your data import process so that incoming data will be properly cleansed.\n","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609850196,"objectID":"7d411e5f5f20722b84874a6cdf58ac68","permalink":"https://www.marcusrb.com/en/power-bi/02-import-data/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/en/power-bi/02-import-data/","section":"resources","summary":"El sistema de archivos controla como se almacenan los archivos en el ordenador. Sus dos tareas principales son guardar y leer archivos previamente guardados.","tags":null,"title":"Import data in Power BI","type":"docs"},{"authors":null,"categories":null,"content":" Power BI may very well be one of the most aptly named tools ever developed by Microsoft, giving analysts and developers a powerful business intelligence and analytics playground while still packaging it in a surprisingly lightweight application. Using Microsoft Power BI, the processes of data discovery, data modeling, data visualization, and sharing are made elegantly simple using a single product. These processes are so commonplace when developing Power BI solutions that this book has adopted sections that follow this pattern. However, from your perspective, the really exciting thing may be that development problems that would previously take you weeks to solve in a corporate BI solution can now be accomplished in only hours.\nPower BI is a Software as a Service (SaaS) offering in the Azure cloud and, as such, the Microsoft product team follows a strategy of cloud first as they develop and add new features to the product. However, this does not mean that Power BI is only available in the cloud. Microsoft presents two options for sharing your results with others. The first, most often-utilized, method is the cloud-hosted Power BI service, which is available to users for a low monthly subscription fee. The second option is the on-premises Power BI Report Server, which can be obtained through either your SQL Server Enterprise licensing with Software Assurance or a subscription level known as Power BI Premium. Both solutions require a development tool called Power BI Desktop, which is available for free, and is where you must start to design your solutions.\nUsing the Power BI Desktop application enables you to define your data discovery and data preparation steps, organize your data model, and design engaging data visualizations based on your reports. In this first chapter, the development environment will be introduced, and the data discovery process will be explored in depth. The topics detailed in this chapter include the following:\n[ ] Getting started [ ] Importing data Direct query Live connection  Let\u0026rsquo;s first start by learning about what you need on your machine to get started.\nGetting started Power BI Desktop is available for free and can be found via a direct download link at Power BI (https://powerbi.microsoft.com/), or by installing it as an app from Microsoft Store. There are several benefits of using the Microsoft Store Power BI app, including automatic updates, no requirement for admin privileges, and making it easier for planned IT rollout of Power BI.\nOnce you\u0026rsquo;ve downloaded, installed, and launched Power BI Desktop, you will likely be welcomed by the start up screen, which is designed to help new users find their way. Close this start up screen so that we can review some of the most commonly used features of the application:\nFigure 1.1: First view of Power BI Desktop\nFollowing the preceding screenshot, let\u0026rsquo;s learn the names and purposes of some of the most important features in Power BI Desktop:\n Get data: Used for selecting data connectors and configuring data source details. Transform data: Launches the Power Query Editor, which is used to apply data transformations to incoming data. Report view: Report canvas used for designing data visualizations. This is the default view that\u0026rsquo;s open when Power BI Desktop is launched. Data view: Provides a view of the data in your model. This looks similar to a typical Excel spreadsheet, but it is read-only. Model view: Primarily used when your data model has multiple tables and relationships that need to be defined between them.  Now that you have a little familiarity with the basic controls within Power BI Desktop, let\u0026rsquo;s learn about the options you have for connecting to your various data sources.\n","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609850196,"objectID":"f9fd46d1699732ee9adbe78f1c3fb60c","permalink":"https://www.marcusrb.com/en/en/power-bi/01-intro-power-bi/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/en/en/power-bi/01-intro-power-bi/","section":"resources","summary":"Recursos y materiales de la tool de visualización Microsoft Power BI.","tags":null,"title":"Introduction to Power BI","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.\nTus primeros comandos BigQuery Para usar BigQuery, importaremos el paquete de Python a continuación:\nfrom google.cloud import bigquery  El primer paso en el flujo de trabajo es crear un objeto Client. Como pronto verá, este objeto Client desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Trabajaremos con un conjunto de datos de publicaciones en Hacker News, un sitio web que se centra en noticias de informática y seguridad cibernética.\nEn BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos hacker_news está contenido en el proyecto bigquery-public-data. Para acceder al conjunto de datos,\n Comenzamos construyendo una referencia al conjunto de datos con el método dataset(). A continuación, utilizamos el método get_dataset(), junto con la referencia que acabamos de construir, para obtener el conjunto de datos.  # Construct a reference to the \u0026quot;hacker_news\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;hacker_news\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.\nUsamos el método list_tables() para listar las tablas en el conjunto de datos.\n# List all the tables in the \u0026quot;hacker_news\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there are four!) for table in tables: print(table.table_id)  OUTPUT\ncomments full full_201510 stories  De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla full en el conjunto de datos hacker_news.\n# Construct a reference to the \u0026quot;full\u0026quot; table table_ref = dataset_ref.table(\u0026quot;full\u0026quot;) # API request - fetch the table table = client.get_table(table_ref)  En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.\nEsquema de la tabla La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.\nEn este ejemplo, investigaremos la tabla completa full que obtuvimos anteriormente.\n# Print information on all the columns in the \u0026quot;full\u0026quot; table in the \u0026quot;hacker_news\u0026quot; dataset table.schema  OUTPUT\n[SchemaField('by', 'STRING', 'NULLABLE', \u0026quot;The username of the item's author.\u0026quot;, ()), SchemaField('score', 'INTEGER', 'NULLABLE', 'Story score', ()), SchemaField('time', 'INTEGER', 'NULLABLE', 'Unix time', ()), SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', 'Timestamp for the unix time', ()), SchemaField('title', 'STRING', 'NULLABLE', 'Story title', ()), SchemaField('type', 'STRING', 'NULLABLE', 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', ()), SchemaField('url', 'STRING', 'NULLABLE', 'Story url', ()), SchemaField('text', 'STRING', 'NULLABLE', 'Story or comment text', ()), SchemaField('parent', 'INTEGER', 'NULLABLE', 'Parent comment ID', ()), SchemaField('deleted', 'BOOLEAN', 'NULLABLE', 'Is deleted?', ()), SchemaField('dead', 'BOOLEAN', 'NULLABLE', 'Is dead?', ()), SchemaField('descendants', 'INTEGER', 'NULLABLE', 'Number of story or poll descendants', ()), SchemaField('id', 'INTEGER', 'NULLABLE', \u0026quot;The item's unique id.\u0026quot;, ()), SchemaField('ranking', 'INTEGER', 'NULLABLE', 'Comment ranking', ())]  Cada SchemaField nos informa sobre una columna específica (a la que también nos referimos como un campo field). En orden, la información es:\n El nombre de la columna. El tipo de campo (o tipo de datos) en la columna El modo de la columna (\u0026lsquo;NULLABLE\u0026rsquo; significa que una columna permite valores NULL y es el valor predeterminado) Una descripción de los datos en esa columna. El primer campo tiene el SchemaField:  SchemaField (\u0026lsquo;by\u0026rsquo;, \u0026lsquo;string\u0026rsquo;, \u0026lsquo;NULLABLE\u0026rsquo;, \u0026ldquo;El nombre de usuario del autor del elemento\u0026rdquo;, ()\nEsto nos dice:\n el campo (o columna) es llamado por los datos en este campo son cadenas, Se permiten valores NULL y Contiene los nombres de usuario correspondientes al autor de cada elemento.  Podemos usar el método list_rows() para verificar solo las primeras cinco líneas de la tabla completa full para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery RowIterator que se puede convertir rápidamente en un DataFrame de pandas con el método to_dataframe().\n# Preview the first five lines of the \u0026quot;full\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  El método list_rows() también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!\n# Preview the first five entries in the \u0026quot;by\u0026quot; column of the \u0026quot;full\u0026quot; table client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()  EXERCISE (Exercise_ Getting Started With SQL and BigQuery)\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"2cea1a1ef0290d310a3dab0e454b94f2","permalink":"https://www.marcusrb.com/en/courses/cloud-computing/intro-aws/aws101-intro/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/en/courses/cloud-computing/intro-aws/aws101-intro/","section":"courses","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.","tags":null,"title":"Conceptos de Amazon Web Services AWS","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web\nTBD\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"de47c731ad0e0fecf39dbc2e3c903e1d","permalink":"https://www.marcusrb.com/en/courses/cloud-computing/intro-azure/azure101-intro/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/en/courses/cloud-computing/intro-azure/azure101-intro/","section":"courses","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web\nTBD","tags":null,"title":"Conceptos de Microsoft Azure","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web\nTBD\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"a76e344591395887f9595af624042383","permalink":"https://www.marcusrb.com/en/courses/cloud-computing/intro-gcp/gcp101-intro/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/en/courses/cloud-computing/intro-gcp/gcp101-intro/","section":"courses","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web\nTBD","tags":null,"title":"Conceptos de Google Cloud Platform GCP","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.\nTus primeros comandos BigQuery Para usar BigQuery, importaremos el paquete de Python a continuación:\nfrom google.cloud import bigquery  El primer paso en el flujo de trabajo es crear un objeto Client. Como pronto verá, este objeto Client desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Trabajaremos con un conjunto de datos de publicaciones en Hacker News, un sitio web que se centra en noticias de informática y seguridad cibernética.\nEn BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos hacker_news está contenido en el proyecto bigquery-public-data. Para acceder al conjunto de datos,\n Comenzamos construyendo una referencia al conjunto de datos con el método dataset(). A continuación, utilizamos el método get_dataset(), junto con la referencia que acabamos de construir, para obtener el conjunto de datos.  # Construct a reference to the \u0026quot;hacker_news\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;hacker_news\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.\nUsamos el método list_tables() para listar las tablas en el conjunto de datos.\n# List all the tables in the \u0026quot;hacker_news\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there are four!) for table in tables: print(table.table_id)  OUTPUT\ncomments full full_201510 stories  De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla full en el conjunto de datos hacker_news.\n# Construct a reference to the \u0026quot;full\u0026quot; table table_ref = dataset_ref.table(\u0026quot;full\u0026quot;) # API request - fetch the table table = client.get_table(table_ref)  En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.\nEsquema de la tabla La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.\nEn este ejemplo, investigaremos la tabla completa full que obtuvimos anteriormente.\n# Print information on all the columns in the \u0026quot;full\u0026quot; table in the \u0026quot;hacker_news\u0026quot; dataset table.schema  OUTPUT\n[SchemaField('by', 'STRING', 'NULLABLE', \u0026quot;The username of the item's author.\u0026quot;, ()), SchemaField('score', 'INTEGER', 'NULLABLE', 'Story score', ()), SchemaField('time', 'INTEGER', 'NULLABLE', 'Unix time', ()), SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', 'Timestamp for the unix time', ()), SchemaField('title', 'STRING', 'NULLABLE', 'Story title', ()), SchemaField('type', 'STRING', 'NULLABLE', 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', ()), SchemaField('url', 'STRING', 'NULLABLE', 'Story url', ()), SchemaField('text', 'STRING', 'NULLABLE', 'Story or comment text', ()), SchemaField('parent', 'INTEGER', 'NULLABLE', 'Parent comment ID', ()), SchemaField('deleted', 'BOOLEAN', 'NULLABLE', 'Is deleted?', ()), SchemaField('dead', 'BOOLEAN', 'NULLABLE', 'Is dead?', ()), SchemaField('descendants', 'INTEGER', 'NULLABLE', 'Number of story or poll descendants', ()), SchemaField('id', 'INTEGER', 'NULLABLE', \u0026quot;The item's unique id.\u0026quot;, ()), SchemaField('ranking', 'INTEGER', 'NULLABLE', 'Comment ranking', ())]  Cada SchemaField nos informa sobre una columna específica (a la que también nos referimos como un campo field). En orden, la información es:\n El nombre de la columna. El tipo de campo (o tipo de datos) en la columna El modo de la columna (\u0026lsquo;NULLABLE\u0026rsquo; significa que una columna permite valores NULL y es el valor predeterminado) Una descripción de los datos en esa columna. El primer campo tiene el SchemaField:  SchemaField (\u0026lsquo;by\u0026rsquo;, \u0026lsquo;string\u0026rsquo;, \u0026lsquo;NULLABLE\u0026rsquo;, \u0026ldquo;El nombre de usuario del autor del elemento\u0026rdquo;, ()\nEsto nos dice:\n el campo (o columna) es llamado por los datos en este campo son cadenas, Se permiten valores NULL y Contiene los nombres de usuario correspondientes al autor de cada elemento.  Podemos usar el método list_rows() para verificar solo las primeras cinco líneas de la tabla completa full para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery RowIterator que se puede convertir rápidamente en un DataFrame de pandas con el método to_dataframe().\n# Preview the first five lines of the \u0026quot;full\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  El método list_rows() también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!\n# Preview the first five entries in the \u0026quot;by\u0026quot; column of the \u0026quot;full\u0026quot; table client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()  EXERCISE (Exercise_ Getting Started With SQL and BigQuery)\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"888a7142740cb80242d130a5c0341fbc","permalink":"https://www.marcusrb.com/en/courses/cloud-computing/intro-cloud/cloud101-intro/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/en/courses/cloud-computing/intro-cloud/cloud101-intro/","section":"courses","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.","tags":null,"title":"Conceptos de Cloud computing","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.\nTus primeros comandos BigQuery Para usar BigQuery, importaremos el paquete de Python a continuación:\nfrom google.cloud import bigquery  El primer paso en el flujo de trabajo es crear un objeto Client. Como pronto verá, este objeto Client desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Trabajaremos con un conjunto de datos de publicaciones en Hacker News, un sitio web que se centra en noticias de informática y seguridad cibernética.\nEn BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos hacker_news está contenido en el proyecto bigquery-public-data. Para acceder al conjunto de datos,\n Comenzamos construyendo una referencia al conjunto de datos con el método dataset(). A continuación, utilizamos el método get_dataset(), junto con la referencia que acabamos de construir, para obtener el conjunto de datos.  # Construct a reference to the \u0026quot;hacker_news\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;hacker_news\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.\nUsamos el método list_tables() para listar las tablas en el conjunto de datos.\n# List all the tables in the \u0026quot;hacker_news\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there are four!) for table in tables: print(table.table_id)  OUTPUT\ncomments full full_201510 stories  De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla full en el conjunto de datos hacker_news.\n# Construct a reference to the \u0026quot;full\u0026quot; table table_ref = dataset_ref.table(\u0026quot;full\u0026quot;) # API request - fetch the table table = client.get_table(table_ref)  En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.\nEsquema de la tabla La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.\nEn este ejemplo, investigaremos la tabla completa full que obtuvimos anteriormente.\n# Print information on all the columns in the \u0026quot;full\u0026quot; table in the \u0026quot;hacker_news\u0026quot; dataset table.schema  OUTPUT\n[SchemaField('by', 'STRING', 'NULLABLE', \u0026quot;The username of the item's author.\u0026quot;, ()), SchemaField('score', 'INTEGER', 'NULLABLE', 'Story score', ()), SchemaField('time', 'INTEGER', 'NULLABLE', 'Unix time', ()), SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', 'Timestamp for the unix time', ()), SchemaField('title', 'STRING', 'NULLABLE', 'Story title', ()), SchemaField('type', 'STRING', 'NULLABLE', 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', ()), SchemaField('url', 'STRING', 'NULLABLE', 'Story url', ()), SchemaField('text', 'STRING', 'NULLABLE', 'Story or comment text', ()), SchemaField('parent', 'INTEGER', 'NULLABLE', 'Parent comment ID', ()), SchemaField('deleted', 'BOOLEAN', 'NULLABLE', 'Is deleted?', ()), SchemaField('dead', 'BOOLEAN', 'NULLABLE', 'Is dead?', ()), SchemaField('descendants', 'INTEGER', 'NULLABLE', 'Number of story or poll descendants', ()), SchemaField('id', 'INTEGER', 'NULLABLE', \u0026quot;The item's unique id.\u0026quot;, ()), SchemaField('ranking', 'INTEGER', 'NULLABLE', 'Comment ranking', ())]  Cada SchemaField nos informa sobre una columna específica (a la que también nos referimos como un campo field). En orden, la información es:\n El nombre de la columna. El tipo de campo (o tipo de datos) en la columna El modo de la columna (\u0026lsquo;NULLABLE\u0026rsquo; significa que una columna permite valores NULL y es el valor predeterminado) Una descripción de los datos en esa columna. El primer campo tiene el SchemaField:  SchemaField (\u0026lsquo;by\u0026rsquo;, \u0026lsquo;string\u0026rsquo;, \u0026lsquo;NULLABLE\u0026rsquo;, \u0026ldquo;El nombre de usuario del autor del elemento\u0026rdquo;, ()\nEsto nos dice:\n el campo (o columna) es llamado por los datos en este campo son cadenas, Se permiten valores NULL y Contiene los nombres de usuario correspondientes al autor de cada elemento.  Podemos usar el método list_rows() para verificar solo las primeras cinco líneas de la tabla completa full para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery RowIterator que se puede convertir rápidamente en un DataFrame de pandas con el método to_dataframe().\n# Preview the first five lines of the \u0026quot;full\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  El método list_rows() también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!\n# Preview the first five entries in the \u0026quot;by\u0026quot; column of the \u0026quot;full\u0026quot; table client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()  EXERCISE (Exercise_ Getting Started With SQL and BigQuery)\n","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"3ac22310f4eab5699cf64a5ea2952e39","permalink":"https://www.marcusrb.com/en/courses/cloud-computing/intro-aws/aws101-servicios/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/en/courses/cloud-computing/intro-aws/aws101-servicios/","section":"courses","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.","tags":null,"title":"Conceptos de Amazon Web Services AWS","type":"docs"},{"authors":null,"categories":null,"content":" Unix es una familia de sistemas operativos. La primera versión de Linux fue desarrollada a partir de 1969. Unix se caracteriza por ser portable y multitarea.\n\nHoy en día los sistemas operativos Unix son ampliamente utilizados en multitud de dispositivos que abarcan desde los supercomputadores más capaces hasta los teléfonos móviles más populares, pasando por los ordenadores que utilizamos diariamente en nuestros escritorios. La filosofía de los sistemas Unix se caracteriza por:\n un sistema de ficheros jerárquico, una gran colección de pequeños programas que pueden trabajar en serie, el uso de ficheros de texto para almacenar los datos, tratar los dispositivos como ficheros.  Linux y MacOS X son ejemplos de sistemas Unix.\nUnix es un sistema operativo portable, multitarea y multiusuario desarrollado a partir de 1969.\nLinux Linux es una familia de sistemas operativos de tipo Unix que utilizan el kernel Linux. Linux puede instalarse en prácticamente cualquier ordenador personal además en en teléfonos móviles y supercomputadores.\nEl nombre proviene del programador original, un estudiante llamado Linus Torvals, que en 1991 completando las herramientas GNU desarrolladas por el proyecto GNU de la Fundación del Software Libre, creó la primera versión de este sistema operativo. El papel fundamental jugado por estas herramientas libres del proyecto GNU hace que este sistema operativo sea denominado también como GNU/Linux, pero en este texto utilizaremos la denominación más sencilla y corta.\nEl desarrollo de Linux es uno de los ejemplos más claros de desarrollo de software libre por una comunidad dispersa de programadores. Cualquiera puede usar el sistema operativo, estudiarlo y modificarlo. Estos derechos están protegidos por la licencia GPL (GNU General Public License).\nDistribuciones Linux, como cualquier otro sistema operativo, se compone de un gran número de piezas, que, en este caso, son desarrolladas de forma independiente por miles de programadores y proyectos. Normalmente estas piezas son integradas por un distribuidor y Linux es suministrado como una distribución Linux. Las distribuciones Linux incluyen todo el software necesario para instalar un servidor o un escritorio. Algunas de las aplicaciones comúnmente incluidas incluyen: el navegador web Firefox y las aplicaciones de oficina LibreOffice.\nExisten cientos de distribuciones Linux. Estas distribuciones están adaptadas para usuarios o tareas específicas. Algunas de estas distribuciones están desarrolladas o apoyadas por empresas como Fedora (Red Hat) y Ubuntu (Canonical) mientras que otras son mantenidas por la propia comunidad de usuarios como Debian.\nSoftware libre El software libre es software que puede ser utilizado, estudiado, modificado, copiado y redistribuido sin restricciones. Habitualmente el software libre suele ser además gratuito, pero ese no tiene por que ser necesariamente el caso.\nEn la práctica el software libre se distribuye junto al código fuente que lo hace posible y junto a una nota en la que se explican cuales son los derechos y las obligaciones del usuario final. Esta nota se denomina licencia. El movimiento del software libre fue iniciado por Richard Stallman en 1983. Stallman decidió crear un sistema compatible con Unix completamente libre al que llamó GNU (GNU is Not Unix). Con el tiempo este sistema acabaría uniéndose al kernel de Linus para formar un sistema operativo completo.\nDado que las aplicaciones del software libre suelen ser gratuitas, su modelo de negocio suele basarse en el cobro de los servicios de soporte al usuario y de adaptación del software.\nIntroducción a Ubuntu Ubuntu es una distribución Linux mantenida por la empresa Canonical. Está orientada a usuarios de escritorio y sus puntos fuertes son su facilidad de uso y de instalación. Aunque el escritorio es algo distinto al de Windows o Mac OS X familiarizarse con él para un usuario acostumbrado a cualquiera de los otros sistemas operativos no debería presentar muchos problemas.\nSu instalación resulta muy sencilla. Al instalarla, una gran cantidad de software se instala de forma automática para facilitar su uso como escritorio. Ejemplos de estos programas son LibreOffice o Firefox. Además de estos programas instalados por defecto una enorme cantidad de programas se encuentra disponible para ser instalados con unos pocos clicks de ratón.\nUbuntu está basada en una distribución mantenida por la comunidad de usuarios llamada Debian. El principal objetivo de Debian es crear un sistema operativo robusto que incluya la mayor proporción posible de programas libres.\nExisten numerosos manuales de utilización de Ubuntu, pero algunas de las guías más completas son la Ubuntu Desktop Guide y el manual Getting Started with Ubuntu. Vamos a recorrer lo principales conceptos de este sistema operativo basandonos en este manual.\nLos usuarios Los sistemas Unix son multiusuario, es decir soportan que varios usuarios los utilicen simultáneamente. Todos los usuarios, excepto uno, tienen unos privilegios bastante restringidos y no pueden modificar el sistema. De este modo unos usuarios se ven protegidos de las acciones de los otros.\nExiste un usuario especial llamado root con privilegios de administración absolutos sobre el sistema. Para realizar las tareas cotidianas nunca hay que acceder al sistema como root. En Ubuntu este usuario está deshabilitado por defecto y sólo se pueden adquirir los privilegios de administrador temporalmente.\nEl escritorio Todos las distribciones basadas en entronos graficos (GUI) suelen tener varios entornos de escritorio para eleguir. Los entornos de escritorio suelen diferir por:\n El estilo y apariencia del entorno La forma en la que los diferentes elementos se disponen en la pantalla La forma en la que el ususario navega por el escritorio  En el caso de Ubuntu el entorno de escritorio por defecto se denomina Unity. Se caracteriza por tener dos barras, la denominada Menu Bar y el Launcher. El Menu Bar incorpora por una lado los menus de las aplicaciones que estań activas y, por otro, un area de indicadores que nos ofrecen informacion actualizada del sistema en todo momento. El Launcher es la barra vertical que facilita el acceso a las aplicaciones mas usadas y a su estado, además de a los discos montados y a la papelera. Además tenemos el selector de escritorios virtuales. En el launcher encontramos varias aplicaciones especiales:\n El menu El selector de escritorios virtuales La papelera  Los escritorios virtuales sirven para ampliar la zona de trabajo. Por defecto hay 4 escritorios virtuales que amplían nuestro monitor por cuatro.\nNavegando por el sistema de ficheros Al sistema de ficheros se accede a traves del menu. Tenemos la posibilidad de buscar en la barra de busqueda o navegar directamente por el menu. Una vez que seleccionemos la carpeta se abrirá una ventana del navegador de ficheros con la carpeta seleccionada. Por cierto, los términos carpeta y directorio son sinónimos, al igual que fichero y archivo.\nDesde el launcher también podemos acceder a nuestra carpeta personal. Este directorio personal y sus subdirectorios son los únicos lugares en los que podremos almacenar nuestros archivos personales. El resto del sistema de archivos estará restringido para funciones de administración del sistema.\nEjercicios Para comprobar que no tenemos problemas de manejo del sistema vamos a realizar una serie de tareas:\n Explorar el escritorio abriendo programas, moviéndote entre escritorios virtuales, minimizando y maximizando las aplicaciones, etc. Navegar al directorio “Documentos” y comprobar si tenemos algún archivo guardado. Crear un subdirectorio llamado “curso” dentro del directorio “Documentos”. Crear un fichero de texto mediante el editor de textos gedit y guardarlo en el directorio que acabamos de crear. Copiar el fichero anterior al directorio personal. Eliminar el fichero original. Reinicia el sistema operativo. Bloquea la sesión de tu usuario y vuelve a entrar en ella Añade un nuevo usuario al sistema Sal de tu usuario actual y entra como el nuevo usuario Cambiar el password del nuevo usuario Configura el protector de pantalla para que se inicie a los 10 minutos de inactividad Modifica los ajustes de la aplicación terminal para que el tipo de letra tenga un tamaño de 11 puntos Ancla la aplicación terminal a la barra de aplicaciones y desancla el editor de hojas de cálculo LibreOffice  La terminal de UNIX La Shell (o terminal) es un interprete de comandos. Es simplemente un modo alternativo de controlar un ordenador basado en una interfaz de texto. La terminal nos permite ejecutar software escribiendo el nombre del programa que queremos ejecutar en la terminal. Podemos pedirle al ordenador que ejecute un programa mediante el ratón ciclando en distintos lugares del escritorio o podemos escribir una orden para conseguir el mismo objetivo. Por ejemplo, para pedirle al ordenador que nos de una lista de los archivos presentes en un directorio podemos abrir un navegador de archivos o podemos escribir en la terminal:\n$ ls folder_name file_1.txt file_2.txt  Ninguna de las dos formas de comunicarse con el ordenador es mejor que la otra aunque en ciertas ocasiones puede resultar más conveniente utilizar una u otra Las ventajas de la línea de comandos son:\n Necesidad. Existe mucho software que está sólo disponible en la terminal. Esto es especialmente cierto en el área de la bioinformática. Flexibilidad. Los programas gráficos suelen ser muy adecuados para realizar la tarea para la que han sido creados, pero son difíciles de adaptar para otras tareas. Los programas diseñados para ser usados en la línea de comandos suelen ser muy versátiles. Reproducibilidad. Documentar y repetir el proceso seguido para realizar un análisis con un programa gráfico es muy costoso puesto que es difícil describir la secuencia de clicks y doble clicks que hemos realizado. Por el contrario, los procesos realizados mediante la línea de comandos son muy fáciles de documentar puesto que tan sólo debemos guardar el texto que hemos introducido en la pantalla. Fiabilidad. Los programas básicos de Unix fueron creados en los años 70 y han sido probados por innumerables usuarios por lo que se han convertido en piezas de código extraordinariamente confiables. Recursos. Las interfaces gráficas suelen consumir muchos recursos mientras que los programas que funcionan en línea de comandos suelen ser extraordinariamente livianos y rápidos. Este poco uso de recursos facilita, por ejemplo, que se utilice a través de la red.  El problema de la terminal es que para poder utilizarla debemos saber previamente qué queremos hacer y cómo. Es habitual descubrir como funciona un programa con una interfaz gráfica sin tener que leer un manual, esto no sucede en la terminal.\nPara usar la línea de comandos hay que abrir una terminal. Se abrirá una terminal con un mensaje similar a:\nusuario $  Este pequeño mensaje se denomina prompt y el cursor parpadeante que aparece junto al él indica que el ordenador está esperando una orden. El mensaje exacto que aparece en el prompt puede variar ligeramente, pero en Ubuntu suele ser similar a:\nusuario@ordenador:~/documentos$  En el prompt de Ubuntu se nos muestra el nombre del usuario, el nombre del ordenador y el directorio en el que nos encontramos actualmente, es decir, el directorio de trabajo actual.\nCuando el prompt se muestra podemos ejecutar cualquier cosa, por ejemplo le podemos pedir que liste los ficheros mediante el comando ls (LiSt)::\nusuario $ ls lista_libros.txt rectas_cocina/  ls, como cualquier otro comando, es en realidad un programa que el ordenador ejecuta. Cuando escribimos la orden (y pulsamos enter) el programa se ejecuta. Mientras el programa está ejecutándose el prompt desaparece y no podemos ejecutar ningún otro comando. Pasado el tiempo el programa termina su ejecución y el prompt vuelve a aparecer. En el caso del comando ls el tiempo de ejecución es tan pequeño que suele ser imperceptible.\nLos programas suelen tener unas entradas y unas salidas. Dependiendo del caso estas pueden ser ficheros o caracteres introducidos o impresos en la pantalla. Por ejemplo, el resultado de ls es simplemente una lista impresa de ficheros y directorios en la interfaz de comandos.\nNormalmente el comportamiento de los programas puede ser modificado pasándoles parámetros. Por ejemplo, podríamos pedirle al programa ls que nos imprima una lista de ficheros más detallada escribiendo:\n$ ls -l  Ayuda Cada comando tiene unos parámetros y opciones distintos. La forma estándar de pedirles que nos enseñen cuales son estos parámetros suele ser utilizar las opciones ‘–help’, ‘-h’ o ‘-help’, aunque esto puede variar en comandos no estándar.\n$ ls --help Modo de empleo: ls [OPCIÓN]... [FICHERO]... List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort.  Otro modo de acceder a una documentación más detallada es acceder al manual del programa utilizando el comando man (MANual):\n$ man ls (para terminar pulsar \u0026quot;q\u0026quot;)  man es un programa interactivo, cuando ejecutamos el comando el programa se abre y el prompt desaparece. man es en realidad un visor de ficheros de texto por lo que cuando lo ejecutamos la pantalla se rellena con la ayuda del programa que hemos solicitado. Podemos ir hacia abajo o hacia arriba y podemos buscar en el contenido de la ayuda. El prompt y la posibilidad de ejecutar otro programa no volverán a aparecer hasta que no cerremos el programa interactivo. En el caso de man para cerrar el programa hay que pulsar la tecla “q”.\nCompletado automático e historia El intérprete de comandos dispone de algunas utilidades para facilitarnos su uso. Una de las más utilizadas es el completado automático. Podemos evitarnos escribir una gran parte de los comandos haciendo uso de la tecla tabulador. Si empezamos a escribir un comando y pulsamos la tecla tabulador el sistema completará el comando por nosotros. Para probarlo creemos los ficheros datos_1.txt, datos_2.txt y tesis.txt::\n~$ touch datos_1.txt ~$ touch datos_2.txt ~$ touch experimento.txt  Si ahora empezamos a escribir cp e y pulsamos el tabulador dos veces, el intérprete de comandos completará el comando automáticamente::\n~$ cp e ~$ cp experimento.txt  Si el intérprete encuentra varias alternativas completará el comando hasta el punto en el que no haya ambigüedad. Si deseamos que imprima una lista de todas las alternativas disponibles para continuar con el comando deberemos pulsar el tabulador dos veces.\n~$ cp d $ cp datos_ datos_1.txt datos_2.txt ~$ cp datos_  Otra de las funcionalidades que más nos pueden ayudar es la historia. El intérprete recuerda todos los comandos que hemos introducido anteriormente. Si queremos podemos obtener una lista de todo lo que hemos ejecutado utilizando el comando history. Pero lo más socorrido es simplemente utilizar los cursores arriba y abajo para revisar los comandos anteriores. Otra forma de acceder a la historia es utilizar la combinación de teclas control y r. De este modo podemos buscar comandos antiguos sencillamente.\nEjercicio  Lista todos los comandos que empiezan por apt  Bibliografía Existen numerosas fuentes sobre la historia y la filosofía de Unix, de Linux y del software libre. Entre ellas se encuentran:\n Las páginas de la wikipedia sobre: Unix, Linux, Ubuntu y [software libre](https://bioinf.comav.upv.es/courses/unix/. Rebel Code, un libro de Glyn Moody dedicado a la historia del movimiento del software libre. La catedral y el bazar de Eric S. Raymond. Un ensayo sobre los beneficios del modelo de desarrollo asociados al software libre. The Art of Unix Programming (pdf de Eric S. Raymond. Dedicado a la filosofía de los sistemas Unix. El excelente [Ubuntu manual](https://bioinf.comav.upv.es/courses/unix/ [pdf](https://bioinf.comav.upv.es/courses/unix/. La documentación oficial y de la comunidad de Ubuntu.  Hay varios cursos para iniciarse en el uso de la línea de comandos de Unix, como:\n Put Yourself in Command de la Free Software Fundation, copia en pdf. Learning the shell de Linuxcommand.org. Rute User’s Tutorial and Exposition de Paul Sheer, copia en pdf. Learning the Unix Operating System.  ","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"97fb1092a3dd188970aff5edc360385d","permalink":"https://www.marcusrb.com/en/unix/01-unix-intro/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/unix/01-unix-intro/","section":"resources","summary":"Unix es una familia de sistemas operativos. La primera versión de Linux fue desarrollada a partir de 1969. Unix se caracteriza por ser portable y multitarea.","tags":null,"title":"UNIX from Scratch","type":"docs"},{"authors":null,"categories":null,"content":" El sistema de archivos controla como se almacenan los archivos en el ordenador. Sus dos tareas principales son guardar y leer archivos previamente guardados.\nSistemas jerárquicos Los sistemas de archivos suelen tener directorios en los que organizar los archivos y estos directorios suelen estar organizados jerárquicamente. La jerarquía implica que un directorio puede contener subdirectorios. El directorio más alto en la jerarquía del que cuelgan todos los demás se denomina raíz (root). En los sistemas Unix el directorio raíz se representa con una barra “*/*” y sólo existe una jerarquía, es decir, sólo existe un directorio raíz, incluso aunque haya distintos discos duros en el ordenador.\nDentro del directorio raíz podemos encontrar diversos subdirectorios, por ejemplo en Linux existe el directorio home. home es por tanto un subdictorio del directorio raíz. Esta relación se representa como:\n/home  home es el directorio dónde se encuentran los directorios de los usuarios en un sistema Linux. Imaginemos que tiene los subdirectorios alicia y juan. Se representaría como:\n/home/alicia /home/juan  Existe un estándar, denominado Filesystem Hierarchy Standard que define la estructura de directorios de los sistemas Unix. Los sistemas Unix suelen seguir este estándar, aunque a veces lo violan en algunos aspectos. Por ejemplo en MacOS X el directorio donde se encuentran los direcotorios de los usuarios se denomina Users y no home\nEn algunos sistemas operativos no UNIX la barra se escribe al revés “\u0026rdquo;, a pesar de que la convención siempre fue la contraria.\nEn el directorio raíz hay diversos directorios que, en la mayoría de los casos, sólo deberían interesarnos si estamos administrando el ordenador. Los usuarios normalmente sólo escriben dentro de un directorio de su propiedad localizado dentro de /home y denominado como su nombre de usuario.\nLos usuarios también pueden escribir en /tmp aunque normalmente son los procesos lanzados por estos lo que hacen esta escritura. Es importante revisar el espacio libre en la partición en la que se encuentra /tmp para que no se colapse el sistema. Recuerda que /tmp es borrado habitualmente por el sistema. Normalmente con cada nuevo arranque.\nRutas absolutas, relativas y directorio de trabajo Para referirnos a un archivo o a un directorio debemos indicar su ruta (path. Un ejemplo de ruta podría ser:\n/home/alicia/documentos/tesis.md  Este tipo de rutas en las que se especifican todos los subdirectorios empezando desde el directorio raíz se denominan rutas absolutas.\nPara no tener que escribir la ruta absoluta completa cada vez que queremos referirnos a un archivo o a un directorio se crearon los conceptos de directorio de trabajo y de ruta relativa.\nEl directorio de trabajo es una propiedad del terminal (del shell) en la que estamos trabajando. Siempre que estemos trabajando en una terminal tendremos asignado un directorio de trabajo. Por ejemplo, si nuestro usuario es alicia sería normal que al abrir un terminal nuestro directorio de trabajo fuese:\n/home/alicia  El directorio de trabajo se utiliza para escribir rutas a archivos relativas al mismo. De este modo nos ahorramos escribir bastante. Imaginemos que Alicia tiene en su directorio un documento llamado peliculas.txt. La ruta absoluta sería.\n/home/alicia/peliculas.txt  Mientras su directorio de trabajo sea /home/alicia la ruta relativa sería simplemetne:\npeliculas.txt  Es decir, podemos escribir rutas relativas al directorio de trabajo, rutas que en vez de partir del directorio raíz parten desde el directorio de trabajo. Las rutas relativas se diferencian de las absolutas en los sistemas Unix porque las absolutas empiezan por “y” las relativas no.\nEs común referirse al directorio de trabajo de una terminal como a un lugar en el que nos encontramos mientras estamos trabajando en la terminal. Siempre que estemos en una terminal estaremos dentro de un directorio de trabajo.\nPor ejemplo, cuando abrimos un nuevo terminal el directorio de trabajo se sitúa en /home/nombre_de_usuario. Si ejecutamos el comando ls, el programa asumirá que queremos listar los archivos presentes en ese directorio y no en otro cualquiera. Existe un comando que nos informa sobre el directorio de trabajo actual, pwd (Print Working Directory):\n$ pwd /home/alicia  Si deseamos podemos modificar el directorio de trabajo “moviéndonos” a otro directorio. Para lograrlo hay que utilizar el comando cd (Change Directory):\n$ cd documentos $ pwd /home/alicia/documentos  A partir de ese momento los comandos asumirán que si no se les indica lo contrario el directorio desde el que deben trabajar es /home/alicia/documentos.\ncd además tiene algunos parámetros especiales:\ncd Ir al directorio $HOME del usuario. cd - Ir al directorio de trabajo previo  Directorio $HOME El directorio $HOME en los sistemas Unix, que son sistemas multiusuario, es el directorio en el que el usuario debe mantener sus ficheros y directorios. Fuera de este directorio el usuario tendrá unos permisos restringidos puesto que sus acciones podrían afectar a otros usuarios.\nEn Linux los directorios $HOME de los usuarios son subdirectorios del directorio /home.\nEl directorio $HOME de un usuario es además el directorio de trabajo por defecto, es decir, el directorio de trabajo que se establece cuando se abre una terminal.\nMoviendo, renombrando y copiando ficheros En primer lugar vamos a crear un fichero de prueba:\n~$ touch data.txt ~$ ls data.txt  El comando touch, en este caso, ha creado un fichero vacío.\nLos ficheros se copian con el comando cp (CoPy):\n~$ cp data.txt data.bak.txt ~$ ls data.bak.txt data.txt  Se mueven y renombran con el mv (MoVe):\n~$ mv data.txt experimento_1.txt ~$ ls data.bak.txt experimento_1.txt  Para crear un nuevo directorio podemos utilizar la orden mkdir (MaKeDIRectory):\n~$ mkdir exp_1 ~$ ls data.bak.txt exp_1 experimento_1.txt  mv también sirve para mover ficheros entre directorios:\n~$ mv experimento_1.txt exp_1/ ~$ ls data.bak.txt exp_1 ~$ ls exp_1/ experimento_1.txt  Los ficheros se eliminan con la orden rm (ReMove):\n~$ rm data.bak.txt ~$ ls exp_1  En la línea de comandos de los sistemas Unix cuando se borra un fichero se borra definitivamente, no hay papelera. Una vez ejecutado el rm no podremos recuperar el archivo.\nLos comandos cp y rm no funcionarán bien con los directorios a no ser que modifiquemos el comportamiento que muestran por defecto:\n~$ rm exp_1/ rm: cannot remove exp_1/ Is a directory ~$ cp exp_1/ exp_1_bak/ cp: omitting directory exp_1/  Esto sucede porque para copiar o borrar un directorio hay que copiar o borrar todos sus contenidos recursivamente y esto podría alterar muchos datos con un sólo comando. Por esta razón se exige que estos dos comandos incluyan un modificador que les indique que sí deben funcionar recursivamente cuando tratan con directorios:\n~$ cp -r exp_1/ exp_1_bak/ ~$ ls exp_1 exp_1_bak ~$ rm -r exp_1_bak/ ~$ ls exp_1  Nombres de directorios y archivos En Unix los archivos pueden tener prácticamente cualquier nombre. Existe la convención de acabar los nombres con un punto y una pequeña extensión que indica el tipo de archivo. Pero esto es sólo una convención, en realidad podríamos no utilizar este tipo de nomenclatura.\nSi deseamos utilizar nombres de archivos que no vayan a causar extraños comportamientos en el futuro lo mejor sería seguir unas cuantas reglas al nombrar un archivo:\n Añadir una extensión para recordarnos el tipo de archivo, por ejemplo .txt para los archivos de texto. No utilizar en los nombres:  espacios, caracteres no alfanuméricos, ni caracteres no ingleses como letras acentuadas o eñes.   Por supuesto, podríamos crear un archivo denominado “$ñ 1.txt” para referirnos a un archivo de sonido, pero esto conllevaría una sería de problemas que aunque son solventables nos dificultarán el trabajo.\nAdemás es importante recordar que en Unix las mayúsculas y las minúsculas no son lo mismo. Los ficheros “documento.txt”, “Documento.txt” y “DOCUMENTO.TXT” son tres ficheros distintos.\nOtra convención utilizada en los sistema Unix es la de ocultar los archivos cuyos nombres comienzan por punto “.”. Por ejemplo el archivo “.oculto” no aparecerá normalmente cuando pedimos el listado de un directorio. Esto se utiliza normalmente para guardar archivos de configuración que no suelen ser utilizados directamente por los usuarios. Para listar todos los archivos (All), ya sean éstos ocultos o no se puede ejecutar:\n$ ls -a . .fontconfig .HyperTree .pki .. fsm.jpg .ICEauthority .recently-used  Esta convención de ocultar los ficheros cuyo nombre comienza por un punto se mantiene también en el navegador gráfico de ficheros. En este caso podemos pedir que se muestren estos archivos en el menú Ver -\u0026gt; Mostrar los archivos ocultos.\nPara acelerar el acceso a ciertos directorios existen algunos nombres especiales que son bastante útiles:\n* \u0026quot;..\u0026quot; indica el directorio padre del directorio actual * \u0026quot;.\u0026quot; indica el directorio actual * \u0026quot;~\u0026quot; representa la $HOME del usuario  WildCards En muchas ocasiones resulta útil tratar los ficheros de un modo conjunto. Por ejemplo, imaginemos que queremos mover todos los ficheros de texto a un directorio y la imágenes a otro. Creemos una pequeña demostración::\n~$ touch exp_1a.txt ~$ touch exp_1b.txt ~$ touch exp_1b.jpg ~$ touch exp_1a.jpg ~$ ls exp_1 exp_1a.jpg exp_1a.txt exp_1b.jpg exp_1b.txt  Podemos referirnos a todos los archivos que acaban en txt utilizando un asterisco:\n~$ mv *txt exp_1 ~$ ls exp_1 exp_1a.jpg exp_1b.jpg  El asterisco sustituye a cualquier texto, por lo que al escribir *txt incluimos a cualquier fichero que tenga un nombre cualquiera, pero que termine con las letras txt. Podríamos por ejemplo referirnos a los ficheros del experimento 1a:\n~$ ls *1a* exp_1a.jpg  Esta herramienta es muy potente y útil, pero tenemos que tener cuidado con ella, sobre todo cuando la combinamos con rm. Por ejemplo la orden:\n$ rm -r *  Borraría todos los ficheros y directorios que se encuentren bajo el directorio de trabajo actual, si lo hacemos perderemos todos los ficheros y directorios que cuelgan del actual directorio de trabajo, puede que esto sea lo que queramos, pero hemos de andar con cuidado.\nEjercicios  ¿Cuáles son los ficheros y directorios presentes en el directorio raíz? ¿Cuáles son todos los archivos presentes en nuestro directorio de usuario? Crea un directorio llamado experimento. Crea con touch los archivos datos1.txt y datos2.txt dentro del directorio experimento. Vuelve al directorio principal de tu usuario y desde allí lista los archivos presentes en el directorio experimento usando rutas absolutas y relativas Haz del directorio ~/Documentos tu directorio de trabajo y repite el ejercicio anterior Borra todos los archivos que contengan un 2 en el directorio experimento. Copia el directorio experimento a un nuevo directorio llamado exp_seguridad. Borra el directorio experimento. Renombra el directorio exp_seguridad a experimento. Copia el fichero /etc/passwd al directorio ~/Documentos Copia el fichero /etc/passwd al directorio ~/Documentos llamándolo usuarios.txt  Obteniendo información sobre archivos y directorios ls es un comando capaz de mostrarnos información extra sobre los archivos y directorios que lista. Por ejemplo podemos pedirle, usando la opción -l (Long), que nos muestre quién es el dueño del archivo y cuanto ocupa y qué permisos tiene además de otras cosas::\n~$ ls exp_1 ~$ ls -l total 4 drwxr-xr-x 2 usuario usuario 4096 Oct 13 09:48 exp_1  La información sobre la cantidad de disco ocupada la da por defecto en bytes, si la queremos en un formato más inteligible podemos utilizar la opción -h (Human):\n~$ ls -lh total 4.0K drwxr-xr-x 2 usuario usuario 4.0K Oct 13 09:48 exp_1  Podemos consultar el tipo de un archivo mediante el comando file.\n~$ file imagen.png imagen.png: PNG image data, 1920 x 1080, 8-bit/color RGB, non-interlaced  En principio, el tipo de un archivo no está determinado por la extensión, la extensión es sólo parte del nombre, aunque hay software que viola o complementa este principio. El tipo de archivo está determinado por su magic number. El magic number está compuesto por una corta serie de bytes que indican el tipo de archivo.\nPermisos Unix desde su origen ha sido un sistema multiusuario. Para conseguir que cada usuario pueda trabajar en sus archivos, pero que no pueda interferir accidental o deliberadamente con los archivos de otros usuarios se estableció desde el principio un sistema de permisos. Por defecto un usuario tiene permiso para leer y modificar sus propios archivos y directorios, pero no los de los demás. En los sistemas Unix los ficheros pertenecen a un usuario concreto y existen unos permisos diferenciados para este usuario y para el resto. Además el usuario pertenece a un grupo de trabajo. Por ejemplo, imaginemos que la usuaria alicia puede pertenecer al grupo de trabajo “diagnostico”. Si alicia crea un fichero este tendrá unos permisos diferentes para alicia, para el resto de miembros de su grupo y para el resto de usuarios del ordenador. Podemos ver los permisos asociados a los ficheros utilizando el comando ls con la opción -l (Long)::\n~$ ls -l total 7324 -rw-r--r-- 1 alicia diagnostico 1059 Oct 20 12:42 busqueda_leukemia_100.txt -rw-r--r-- 1 alicia diagnostico 0 Oct 13 10:53 datos_1.txt drwxr-xr-x 2 alicia diagnostico 4096 Oct 13 10:29 experimento  En este caso, los ficheros listados pertenecen Alicia y al grupo diagnostico. Los permisos asignados al usuario, a los miembros del grupo y al resto de usuarios están resumidos en la primeras letras de cada línea::\ndrwxr-x---  La primera letra indica el tipo de fichero listado: (d) directorio, (-) fichero u otro tipo especial. Las siguientes nueve letras muestran, en grupos de tres, los permisos para el usuario, para el grupo y para el resto de usuarios del ordenador. Cada grupo de tres letras indica los permisos de lectura (Read), escritura (Write) y ejecución (eXecute). En el caso anterior el usuario tiene permiso de lectura, escritura y ejecución (rwx), el grupo tiene permiso de lectura y ejecución (r-x), es decir no puede modificar el fichero o el directorio, y el resto de usuarios no tienen ningún permiso (—).\nEn los ficheros normales el permiso de lectura indica si el fichero puede ser leído, el de escritura si puede ser modificado y el de ejecución si puede ser ejecutado. En el caso de los directorios el de escritura indica si podemos añadir o borrar ficheros del directorio y el de ejecución si podemos listar los contenidos del directorio.\nEstos permisos pueden ser modificados con la orden chmod. En chmod cada grupo de usuarios se representa por una letra:\n u: usuario dueño del fichero g: grupo de usuarios del dueño del fichero o: todos los otros usuarios a: todos los tipos de usuario (dueño, grupo y otros)  Los tipos de permisos también están abreviados por letras:\n r: lectura w: escritura x: ejecución  Con estas abreviaturas podemos modificar los permisos existentes.\nHacer un fichero ejecutable:\n$ chmod u+x  O:\n$ chmod a+x  También podemos mediante chmod indicar los permisos para un tipo de usuario determinado.\n$ chmod a=rwx  Un modo algo menos intuitivo, pero más útil de utilizar chmod es mediante los números octales que representan los permisos.\n- lectura: 4 - escritura: 2 - ejecución: 1  Para modificar los permisos de este modo debemos indicar el número octal que queremos que represente los permisos del fichero. La primera cifra representará al dueño, la segunda al grupo y la tercera al resto de usuarios. Por ejemplo si queremos que único permiso para el dueño y su grupo sea la lectura y que no haya ningún permiso para el resto de usuarios:\n$ chmod 110 fichero.txt  También podemos combinar permisos sumando los números anteriores. Por ejemplo, permiso para leer y escribir para el dueño y ningún permiso para el resto.\n$ chmod 300 fichero.txt  Permisos de lectura, escritura y ejecución para el dueño y su grupo y ninguno para el resto.\n$ chmod 770 fichero.txt  Las restricciones para los permisos no afectan al usuario root, al administrador del sistema. root también puede modificar quien el dueño y el grupo al que pertenecen los ficheros mediante los comando chown y chgrp.\n$ chown alicia fichero.txt $ chown diagnostico fichero.txt  Obteniendo información sobre el sistema de archivos El sistema de archivos puede abarcar una o más particiones. Una partición es una región de un disco o de cualquier otro medio de almacenamiento. Las instalaciones de Windows tienen normalmente una partición por disco, pero en Linux esto no es tan habitual. Cada partición tiene un sistema de archivos propio, pero en Unix estos sistemas deben estar montados en algún lugar dentro de la jerarquía que cuelga de la raíz. En Windows cada partición tiene por defecto una jerarquía independiente.\nPodemos pedir información sobre el espacio ocupado por las distintas particiones que tenemos actualmente montadas usando el comando df (Disk Free).\n$ df -h S.ficheros Tamaño Usados Disp Uso% Montado en udev 7,8G 0 7,8G 0% /dev tmpfs 1,6G 9,8M 1,6G 1% /run /dev/nvme0n1p2 25G 8,1G 16G 35% / tmpfs 7,8G 5,3M 7,8G 1% /dev/shm tmpfs 5,0M 4,0K 5,0M 1% /run/lock tmpfs 7,8G 0 7,8G 0% /sys/fs/cgroup /dev/nvme0n1p4 206G 18G 178G 9% /home /dev/nvme0n1p1 511M 3,6M 508M 1% /boot/efi /dev/sda1 2,7T 117G 2,5T 5% /home/jose/magnet tmpfs 1,6G 64K 1,6G 1% /run/user/1000  Algunos de los sistemas de archivos montados puede que no se correspondan con particiones en un disco físico sino con espacios de la memoria RAM que son utilizados como sistemas de archivos especiales.\nEl commando du (disk usage) informa sobre el espacio que ocupa un árbol de directorios. Este comando tiene equivalentes gráficos como Baobab o xdiskusage. Podemos pedir a du que nos muestre cuanto espacio ocupan los directorios bajo el directorio analysis:\n$ du -h analyses/ 36K\tanalyses/alicia/cache 204K\tanalyses/alicia/differential_snps/differential 252K\tanalyses/alicia/differential_snps/non_differentia 919M\tanalyses/  Si sólo queremos obtener el resultado para el directorio que le hemos dado y no para sus subdirectorios podemos utilizar el parámetro -s:\n$ du -sh analyses/ 919M\tanalyses/  Si queremos información sobre todos los archivos y no sólo los directorios podemos usar -a:\n$ du -ha analyses/ 32K\tanalyses/alicia/cache/min_called_rate_samples_cache.pickle 36K\tanalyses/alicia/cache 8,0K\tanalyses/alicia/look_for_matching_accessions.py  Ejercicios  ¿Cuáles son los permisos de los directorios presentes en el directorio raíz y en nuestro directorio de usuario? ¿A quién pertenecen los ficheros y qué permisos tienen los distintos usuarios del ordenador? Crea un directorio en tu home y muestra los permisos que tiene. Cambia los permisos para que sólo tu usuario pueda acceder al nuevo directorio Crea un fichero nuevo y dale permisos de ejecución para todos los usuarios Último fichero modificado en el directorio /etc. Lista los ficheros de /etc con su tamaño y ordénalos por tamaño. Copia todos los ficheros y directorios del directorio /etc cuyo nombre comience por s. ¿Has podido copiarlos todos? ¿Cuánto espacio libre queda en las distintas particiones del sistema? ¿Cuánto espacio ocupan todos los ficheros y subdirectorios de tu $HOME?  Compresion y descompresión de ficheros Existen distintos formatos de compresión de ficheros como: gzip, bzip, zip o rar. Los formatos más utilizados en Unix son gzip y bzip.\nComprimir un fichero con gzip o bzip:\n$ gzip informacion_snps.txt $ ls informacion_snps.txt.gz $ bzip2 accs.txt $ ls accs.txt.bz2  bzip2 comprime más que gzip, pero es más lento. gzip también dispone de varios niveles de compresión, cuanto más comprime más lenta suele ser la compresión.\nPodemos descomprimir cualquier fichero utilizando la línea de comandos:\n$ gunzip informacion_snps.txt.gz $ ls informacion_snps.txt $ bunzip2 accs.txt.bz2 $ ls accs.txt  Muchos estamos acostumbrados al formato zip. Un fichero zip no se corresponde en realidad con un sólo fichero comprimido sino con varios. Un fichero zip hace dos cosas: unir varios ficheros en uno y comprimir el resultado. Los comandos que hemos visto (gzip y bzip2) son capaces de comprimir un sólo archivo, pero no pueden unir varios archivos en uno. tar es el comando capaz de unir varios archivos en uno.\n$ ls seq1.fasta seq2.fasta $ tar -cvf secuencias.tar seq* seq1.fasta seq2.fasta $ ls secuencias.tar seq1.fasta seq2.fasta  tar también es capaz de desempaquetar los archivos que habíamos unido.\n$ ls secuencias.tar $ rm seq1.fasta seq2.fasta $ tar -xvf secuencias.tar seq1.fasta seq2.fasta $ ls secuencias.tar seq1.fasta seq2.fasta  El problema es que utilizando el comando tar tal y como lo hemos hecho hemos conseguido unir y separar archivos, pero no hemos comprimido el fichero unido. Para hacerlo podríamos utilizar los comandos gzip o bzip2, pero este no es el modo habitula de hacerlo. Dado que casi siempre que unamos archivos en un archivo tar también querremos comprimir el resultado el comando tar tiene también la capacidad de comprimir y descomprimir utilizando los algoritmos gzip y bzip2. Unir y comprimir con gzip varios archivos:\n$ tar -cvzf secuencias.tar.gz seq* seq1.fasta seq2.fasta $ ls secuencias.tar.gz seq1.fasta seq2.fasta  Descomprimir un archivo tar.gz:\n$ tar -xvzf secuencias.tar.gz seq1.fasta seq2.fasta $ ls secuencias.tar.gz seq1.fasta seq2.fasta  También podemos descomprimir el contenido de un fichero de texto y enviar el resultado a la terminal con el comando zcat.\n$ zcat fichero.txt.gz\nCon bzip2.\n$ tar -cvjf secuencias.tar.bz seq* seq1.fasta seq2.fasta $ ls secuencias.tar.bz seq1.fasta seq2.fasta $ tar -xvjf secuencias.tar.bz seq1.fasta seq2.fasta  Ejercicios  Crea un fichero de texto en el directorio ~/Documentos y comprimelo con gzip Muestra el contenido del fichero anterior en pantalla sin descomprimirlo previamente Crea un archivo tar de todo el contenido del directorio ~/Documentos Comprime el fichero tar anterior Vuelve a hacer los ejercicios 2 y 3, pero en un sólo paso Descomprime el fichero tar.gz anterior en un nuevo directorio llamado Documentos2  Enlaces duros y blandos Podemos pensar en el nombre de un fichero como en una etiqueta que apunta a una posición concreta en el disco duro, en realidad es un puntero a un inodo.\nPodmeos pensar en un enlace duro como en un nombre adicional para un archivo. Si tenemos un archivo en el disco y creamos un enlace duro tendremos dos nombres para ese único archivo.\n$ ls archivo1.txt $ ln archivo1.txt nombre2.txt $ ls archivo1.txt nombre2.txt  Las dos referencias, nombres, al archivo serán indistinguibles. Si borramos un nombre quedará el otro. Si modificamos un archivo se modifica independientemente del nombre por el cual estemos accediendo a él. No es muy común utilizar enlaces duro salvo en aplicaciones muy concretas, por ejemplo en versiones de copias de seguridad.\nUn enlace blando, más comumente conocido como un enlace simbólico, es una referencia al nombre de un archivo, no al archivo en sí.\n$ ls archivo1.txt $ ln -s archivo1.txt nombre3.txt $ ls -l -rw-rw-r-- 1 jose jose 0 sep 27 15:16 archivo1.txt lrwxrwxrwx 1 jose jose 12 sep 27 15:16 nombre3.txt -\u0026gt; archivo1.txt  Si eliminamos el archivo original el enlace quedará roto.\n$ rm archivo1.txt $ cat nombre3.txt cat: nombre3.txt: No existe el archivo o el directorio  El comportamiento de ambos tipos de enlaces cambia si sobreescribimos el fichero.\nx $ echo \u0026quot;hola\u0026quot; \u0026gt; hola.txt $ cat hola.txt hola $ ln hola.txt hola2.txt $ ln -s hola.txt hola3.txt $ ls -l -rw-rw-r-- 2 jose jose 5 sep 27 15:23 hola2.txt lrwxrwxrwx 1 jose jose 8 sep 27 15:25 hola3.txt -\u0026gt; hola.txt -rw-rw-r-- 2 jose jose 5 sep 27 15:23 hola.txt $ echo \u0026quot;adios\u0026quot; \u0026gt; adios.txt $ mv adios.txt hola.txt $ cat hola.txt adios $ cat hola2.txt hola  Los enlaces blandos funcionan incluso entre distintos sistemas de archivos o particiones, los duros no.\nEjercicios  Crea un enlace simbólico a un fichero de texto dentro del direcotorio ~/Documentos Crea un enlace duro al mismo fichero. Edita el fichero de texto y observa como cambian ambos enlaces Crea un nuevo fichero de texto con otro contenido. Sustituye el primer fichero con el segundo y observa el resultado en ambos enlaces Crea dos enlaces, uno simbólico y otro duro, a un fichero. Elimina el fichero y observa el resultado en ambos enlaces  Acceso remoto Una de las grandes ventajas de utilizar la terminal es que podemos acceder a terminales en otros ordenadores muy fácilmente. El protocolo más utilizado para acceder a terminales de forma remota es ssh (Secure Shell). ssh tiene un gran número de posibilidades, pero el uso más habitual es utilizarlo para abrir terminales en ordenadores remotos que tienen un servicio ssh. ssh es seguro porque cifra las comunicaciones entre el cliente y el servidor. ssh se diseñó como una alternativa segura a telnet. No debemos usar el protocolo telnet porque las comunicaciones en telnet, incluidas las claves de acceso, no están cifradas y cualquiera puede tener acceso a ellas.\nPara acceder a una computadora que implemente el protocolo ssh podemos usar el programa ssh, pero previamente tenemos que tener una cuenta en esa computadora. Imaginemos que alicia tiene una cuenta en un ordenador que tiene un servicio ssh. Para conectarse puede hacer:\n$ ssh alicia@ordenador.upv.es  Si el nombre de la cuenta de usuario en el ordenador cliente y en el servidor es el mismo puede obviar el nombre de usuario.\n$ ssh ordenador.upv.es  A continuación el servidor le pedirá la clave correspondiente a ese usuario.\nExisten clientes ssh para windows con los que nos podemos conectar a servidores ssh. Uno muy común es putty.\nUna tarea muy habitual cuando estamos trabajando en un ordenador remoto es enviar o traer ficheros desde el mismo. Esto también lo podemos hacer utilizando el protocolo ssh por lo que podremos hacerlo de un modo seguro en cualquier ordenador que no de acceso ssh. El programa más sencillo para hacerlo desde Unix es scp (Secure CoPy). scp tiene una interfaz muy similar a cp pero acepta que los ficheros de origen y destino estén en distintos ordenadores:\n$ scp alicia@remotehost.edu:/remote/directory/seq.txt /some/local/directory $ scp /some/local/directory/seq.txt alicia@remotehost.edu:/remote/directory/  En windows también hay distintos clientes scp, uno de ellos es winscp.\nUna alternativa a scp que tiene más capacidades, como enviar fragmentos de ficheros, es rsync. rsync está diseñado para mantener varios archivos sincronizados entre dos ordenadores, pero también ser puede utilizar para copiar archivos de un ordenador a otro como scp. rsync puede establecer la conexión utilizando distintos protocolos, pero uno de ellos es ssh por lo que funcionará también con cualquier servidor ssh.\nSi lo que queremos es descargar un fichero desde un servidor en internet, por ejemplo desde una página web, al ordenador remoto en el que estamos trabajando en una sesión ssh podemos utilizar el comando wget o su alternativa curl.\n$ wget https://http://ncbi.nlm.nih.gov/una_secuencia.fasta  Ejercicios  Contectate a un servidor remoto usando ssh Transfiere un fichero desde tu ordenador al servidor Descarga el fichero https://www.gnu.org/licenses/gpl.txt directamente en el ordenador remoto Copia el fichero gpl.txt a tu ordenador  ","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"83b3d56760b885fedb1299386aa3ff8d","permalink":"https://www.marcusrb.com/en/unix/02-sistema-ficheros/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/en/unix/02-sistema-ficheros/","section":"resources","summary":"El sistema de archivos controla como se almacenan los archivos en el ordenador. Sus dos tareas principales son guardar y leer archivos previamente guardados.","tags":null,"title":"Sistema de ficheros","type":"docs"},{"authors":null,"categories":null,"content":" Vivimos en la era de grandes cantidades de datos, computadoras potentes e inteligencia artificial. Este es solo el comienzo. La ciencia de datos y el aprendizaje automático están impulsando el reconocimiento de imágenes, el desarrollo de vehículos autónomos, las decisiones en los sectores financiero y energético, los avances en medicina, el auge de las redes sociales y más. La regresión lineal es una parte importante de esto.\nLa regresión lineal es una de las técnicas estadísticas y de aprendizaje automático fundamentales. Ya sea que desee hacer estadísticas, aprendizaje automático o computación científica, hay buenas posibilidades de que lo necesite. Es recomendable aprenderlo primero y luego proceder hacia métodos más complejos.\nAl final de este artículo, habrás aprendido:\n¿Qué es la regresión lineal? Para qué regresión lineal se utiliza Cómo funciona la regresión lineal Cómo implementar la regresión lineal en Python, paso a paso\nRegresión El análisis de regresión es uno de los campos más importantes en estadística y aprendizaje automático. Hay muchos métodos de regresión disponibles. La regresión lineal es una de ellas.\n¿Qué es la regresión? La regresión busca relaciones entre variables.\nPor ejemplo, puede observar a varios empleados de alguna compañía e intentar comprender cómo sus salarios dependen de las características, como la experiencia, el nivel de educación, el rol, la ciudad en la que trabajan, etc.\nEste es un problema de regresión en el que los datos relacionados con cada empleado representan una observación. La presunción es que la experiencia, la educación, el rol y la ciudad son las características independientes, mientras que el salario depende de ellas.\nDel mismo modo, puede intentar establecer una dependencia matemática de los precios de las casas en sus áreas, número de dormitorios, distancias al centro de la ciudad, etc.\nEn general, en el análisis de regresión, generalmente considera algún fenómeno de interés y tiene una serie de observaciones. Cada observación tiene dos o más características. Siguiendo el supuesto de que (al menos) una de las características depende de las otras, intenta establecer una relación entre ellas.\nEn otras palabras, debe encontrar una función que asigne algunas características o variables a otras lo suficientemente bien.\nLas características dependientes se denominan variables dependientes, salidas o respuestas.\nLas características independientes se denominan variables independientes, entradas o predictores.\nLos problemas de regresión generalmente tienen una variable dependiente continua y sin límites. Sin embargo, las entradas pueden ser datos continuos, discretos o incluso categóricos, como género, nacionalidad, marca, etc.\nEs una práctica común denotar las salidas con 𝑦 y las entradas con 𝑥. Si hay dos o más variables independientes, se pueden representar como el vector 𝐱 = (𝑥₁, \u0026hellip;, 𝑥ᵣ), donde 𝑟 es el número de entradas.\n¿Cuándo necesitas regresión? Por lo general, se necesita una regresión para responder si un fenómeno influye en el otro y cómo se relacionan varias variables. Por ejemplo, puede usarlo para determinar si y en qué medida la experiencia o el género afectan los salarios.\nLa regresión también es útil cuando desea pronosticar una respuesta utilizando un nuevo conjunto de predictores. Por ejemplo, podría intentar predecir el consumo de electricidad de un hogar para la próxima hora dada la temperatura exterior, la hora del día y el número de residentes en ese hogar.\nLa regresión se usa en muchos campos diferentes: economía, ciencias de la computación, ciencias sociales, etc. Su importancia aumenta cada día con la disponibilidad de grandes cantidades de datos y una mayor conciencia del valor práctico de los datos.\nRegresión lineal La regresión lineal es probablemente una de las técnicas de regresión más importantes y ampliamente utilizadas. Es uno de los métodos de regresión más simples. Una de sus principales ventajas es la facilidad de interpretación de los resultados.\nFormulación del problema Al implementar la regresión lineal de alguna variable dependiente 𝑦 en el conjunto de variables independientes 𝐱 = (𝑥₁, \u0026hellip;, 𝑥ᵣ), donde 𝑟 es el número de predictores, se supone una relación lineal entre 𝑦 y 𝐱: 𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽ᵣ𝑥ᵣ + 𝜀. Esta ecuación es la ecuación de regresión. 𝛽₀, 𝛽₁, \u0026hellip;, 𝛽ᵣ son los coeficientes de regresión, y 𝜀 es el error aleatorio.\nLa regresión lineal calcula los estimadores de los coeficientes de regresión o simplemente los pesos predichos, denotados con 𝑏₀, 𝑏₁, \u0026hellip;, 𝑏ᵣ. Definen la función de regresión estimada 𝑓 (𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ. Esta función debe capturar las dependencias entre las entradas y salidas lo suficientemente bien.\nLa respuesta estimada o pronosticada, 𝑓 (𝐱ᵢ), para cada observación 𝑖 = 1, \u0026hellip;, 𝑛, debe estar lo más cerca posible de la respuesta real correspondiente 𝑦ᵢ. Las diferencias 𝑦ᵢ - 𝑓 (𝐱ᵢ) para todas las observaciones 𝑖 = 1, \u0026hellip;, 𝑛, se denominan residuales. La regresión se trata de determinar los mejores pesos pronosticados, es decir, los pesos correspondientes a los residuos más pequeños.\nPara obtener los mejores pesos, generalmente minimiza la suma de los residuos cuadrados (SSR) para todas las observaciones 𝑖 = 1, \u0026hellip;, 𝑛: SSR = Σᵢ (𝑦ᵢ - 𝑓 (𝐱ᵢ)) ². Este enfoque se llama el método de mínimos cuadrados ordinarios.\nRendimiento de regresión La variación de las respuestas reales 𝑦ᵢ, 𝑖 = 1, \u0026hellip;, 𝑛, se debe en parte a la dependencia de los predictores 𝐱ᵢ. Sin embargo, también hay una variación inherente adicional de la salida.\nEl coeficiente de determinación, denotado como 𝑅², le indica qué cantidad de variación en 𝑦 puede explicarse por la dependencia de 𝐱 utilizando el modelo de regresión particular. Mayor 𝑅² indica un mejor ajuste y significa que el modelo puede explicar mejor la variación de la salida con diferentes entradas.\nEl valor 𝑅² = 1 corresponde a SSR = 0, es decir, al ajuste perfecto ya que los valores de las respuestas pronosticadas y reales se ajustan completamente entre sí.\nRegresión lineal simple La regresión lineal simple o de una sola variable es el caso más simple de regresión lineal con una sola variable independiente, 𝐱 = 𝑥.\nLa siguiente figura ilustra la regresión lineal simple:\n","date":1568502000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"ceead108c6e5c838aab37c23575b81ca","permalink":"https://www.marcusrb.com/en/resources/tutorials/python/pytut-linear/","publishdate":"2019-09-15T00:00:00+01:00","relpermalink":"/en/resources/tutorials/python/pytut-linear/","section":"resources","summary":"Vivimos en la era de grandes cantidades de datos, computadoras potentes e inteligencia artificial. Este es solo el comienzo. La ciencia de datos y el aprendizaje automático están impulsando el reconocimiento de imágenes, el desarrollo de vehículos autónomos, las decisiones en los sectores financiero y energético, los avances en medicina, el auge de las redes sociales y más. La regresión lineal es una parte importante de esto.\nLa regresión lineal es una de las técnicas estadísticas y de aprendizaje automático fundamentales.","tags":null,"title":"Python Tutorial - Linear Regression in Python","type":"docs"},{"authors":null,"categories":null,"content":" La ciencia de datos es una disciplina emocionante que le permite convertir datos sin procesar en comprensión, conocimiento y conocimiento. El objetivo de \u0026ldquo;R for Data Science\u0026rdquo; es ayudarlo a aprender las herramientas más importantes en R que le permitirán hacer ciencia de datos. Después de leer este libro, tendrá las herramientas para abordar una amplia variedad de desafíos de la ciencia de datos, utilizando las mejores partes de R.\nLo que vas a aprender La ciencia de datos es un campo enorme, y no hay forma de que puedas dominarlo leyendo un solo libro. El objetivo de este libro es brindarle una base sólida en las herramientas más importantes. Nuestro modelo de las herramientas necesarias en un proyecto típico de ciencia de datos se parece a esto:\n`{r echo = FALSE, out.width =\u0026quot; 75% \u0026quot;} knitr :: include_graphics (\u0026quot;diagrams / data-science.png\u0026quot;)  `\nPrimero debe importar sus datos en R. Esto generalmente significa que toma los datos almacenados en un archivo, base de datos o API web, y los carga en un marco de datos en R. Si no puede ingresar sus datos en R, puede ¡No hagas ciencia de datos!\nUna vez que haya importado sus datos, es una buena idea ponerlos en orden. Poner en orden sus datos significa almacenarlos en una forma consistente que coincida con la semántica del conjunto de datos con la forma en que se almacenan. En resumen, cuando sus datos están ordenados, cada columna es una variable y cada fila es una observación. Los datos ordenados son importantes porque la estructura consistente le permite enfocar su lucha en preguntas sobre los datos, no en luchar para obtener los datos en la forma correcta para diferentes funciones.\nUna vez que tenga datos ordenados, un primer paso común es transformar. La transformación incluye reducir las observaciones de interés (como todas las personas en una ciudad, o todos los datos del año pasado), crear nuevas variables que son funciones de variables existentes (como calcular la velocidad a partir de la distancia y el tiempo) y calcular un conjunto de resumen estadísticas (como recuentos o medios). Juntos, ordenar y transformar se llaman wrangling, ¡porque obtener sus datos en una forma natural para trabajar a menudo se siente como una pelea!\nUna vez que tenga datos ordenados con las variables que necesita, hay dos motores principales de generación de conocimiento: visualización y modelado. Estos tienen fortalezas y debilidades complementarias, por lo que cualquier análisis real se repetirá entre ellos muchas veces.\nLa visualización es una actividad fundamentalmente humana. Una buena visualización le mostrará cosas que no esperaba o planteará nuevas preguntas sobre los datos. Una buena visualización también puede indicar que está haciendo una pregunta incorrecta o que necesita recopilar datos diferentes. Las visualizaciones pueden sorprenderte, pero no escales particularmente bien porque requieren que un humano las interprete.\nLos Modelos son herramientas complementarias para la visualización. Una vez que haya hecho sus preguntas lo suficientemente precisas, puede usar un modelo para responderlas. Los modelos son una herramienta fundamentalmente matemática o computacional, por lo que generalmente escalan bien. Incluso cuando no lo hacen, ¡generalmente es más barato comprar más computadoras que comprar más cerebros! Pero cada modelo hace suposiciones y, por su propia naturaleza, un modelo no puede cuestionar sus propias suposiciones. Eso significa que un modelo no puede sorprenderte fundamentalmente.\nEl último paso de la ciencia de datos es comunicación, una parte absolutamente crítica de cualquier proyecto de análisis de datos. No importa qué tan bien sus modelos y visualización lo hayan llevado a comprender los datos, a menos que también pueda comunicar sus resultados a otros.\nAlrededor de todas estas herramientas está programación. La programación es una herramienta transversal que utiliza en cada parte del proyecto. No es necesario ser un programador experto para ser un científico de datos, pero aprender más sobre programación vale la pena porque convertirse en un mejor programador le permite automatizar tareas comunes y resolver nuevos problemas con mayor facilidad.\nUtilizará estas herramientas en cada proyecto de ciencia de datos, pero para la mayoría de los proyectos no son suficientes. Hay una regla aproximada de 80-20 en juego; puede abordar aproximadamente el 80% de cada proyecto utilizando las herramientas que aprenderá en este libro, pero necesitará otras herramientas para abordar el 20% restante. A lo largo de este libro, le indicaremos los recursos donde puede obtener más información.\nCómo está organizado este libro La descripción anterior de las herramientas de la ciencia de datos está organizada de manera aproximada de acuerdo con el orden en que las usa en un análisis (aunque, por supuesto, las repetirá varias veces). En nuestra experiencia, sin embargo, esta no es la mejor manera de aprenderlos:\n Comenzar con la ingesta de datos y el orden es subóptimo porque el 80% del tiempo es rutinario y aburrido, y el otro 20% del tiempo es extraño y frustrante. ¡Ese es un mal lugar para comenzar a aprender un nuevo tema! En lugar, comenzaremos con la visualización y transformación de datos que ya han sido importado y ordenado. De esa manera, cuando ingiere y ordena sus propios datos, su la motivación se mantendrá alta porque sabes que el dolor lo vale.  Algunos temas se explican mejor con otras herramientas. Por ejemplo, creemos que Es más fácil entender cómo funcionan los modelos si ya conoce visualización, datos ordenados y programación.  Las herramientas de programación no son necesariamente interesantes por derecho propio, pero le permiten abordar problemas considerablemente más desafiantes. Bien darle una selección de herramientas de programación en el medio del libro, y entonces verás cómo se pueden combinar con las herramientas de ciencia de datos para abordar Problemas interesantes de modelado.  Dentro de cada capítulo, tratamos de seguir un patrón similar: comience con algunos ejemplos motivadores para que pueda ver la imagen más grande y luego profundice en los detalles. Cada sección del libro se combina con ejercicios para ayudarlo a practicar lo que ha aprendido. Si bien es tentador saltarse los ejercicios, no hay mejor manera de aprender que practicar en problemas reales.\nLo que no aprenderás Hay algunos temas importantes que este libro no cubre. Creemos que es importante mantenerse centrado sin piedad en lo esencial para que pueda comenzar a trabajar lo más rápido posible. Eso significa que este libro no puede cubrir todos los temas importantes.\nBig data Este libro se centra orgullosamente en pequeños conjuntos de datos en memoria. Este es el lugar correcto para comenzar porque no puede abordar big data a menos que tenga experiencia con datos pequeños. Las herramientas que aprende en este libro manejarán fácilmente cientos de megabytes de datos, y con un poco de cuidado, generalmente puede usarlas para trabajar con 1-2 Gb de datos. Si trabaja habitualmente con datos más grandes (10-100 Gb, por ejemplo), debe obtener más información sobre data.table. Este libro no enseña data.table porque tiene una interfaz muy concisa que hace que sea más difícil de aprender ya que ofrece menos claves lingüísticas. Pero si está trabajando con datos de gran tamaño, la rentabilidad del rendimiento vale la pena el esfuerzo adicional requerido para aprenderlo.\nSi sus datos son más grandes que esto, considere cuidadosamente si su problema de big data podría ser un pequeño problema de datos disfrazado. Si bien los datos completos pueden ser grandes, a menudo los datos necesarios para responder una pregunta específica son pequeños. Es posible que pueda encontrar un subconjunto, una submuestra o un resumen que se ajuste a la memoria y que aún le permita responder la pregunta que le interesa. El desafío aquí es encontrar los datos pequeños correctos, que a menudo requieren mucha iteración.\nOtra posibilidad es que su problema de big data sea en realidad una gran cantidad de problemas de data pequeña. Cada problema individual puede caber en la memoria, pero tiene millones de ellos. Por ejemplo, es posible que desee ajustar un modelo a cada persona en su conjunto de datos. Eso sería trivial si solo tuvieras 10 o 100 personas, pero en cambio tienes un millón. Afortunadamente, cada problema es independiente de los demás (una configuración que a veces se llama vergonzosamente paralela), por lo que solo necesita un sistema (como Hadoop o Spark) que le permita enviar diferentes conjuntos de datos a diferentes computadoras para su procesamiento. Una vez que haya descubierto cómo responder la pregunta para un solo subconjunto utilizando las herramientas descritas en este libro, aprenderá nuevas herramientas como sparklyr, rhipe y ddr para resolverlo para el conjunto de datos completo.\nPython, Julia y amigos En este libro, no aprenderá nada sobre Python, Julia o cualquier otro lenguaje de programación útil para la ciencia de datos. Esto no es porque pensemos que estas herramientas son malas. ¡Ellos no están! Y en la práctica, la mayoría de los equipos de ciencia de datos usan una combinación de lenguajes, a menudo al menos R y Python.\nSin embargo, creemos firmemente que es mejor dominar una herramienta a la vez. Mejorará más rápido si bucea profundamente, en lugar de extenderse poco a poco sobre muchos temas. Esto no significa que solo deba saber una cosa, solo que generalmente aprenderá más rápido si se apega a una cosa a la vez. Debes esforzarte por aprender cosas nuevas a lo largo de tu carrera, pero asegúrate de que tu comprensión sea sólida antes de pasar a la siguiente cosa interesante.\nCreemos que R es un gran lugar para comenzar su viaje de ciencia de datos porque es un entorno diseñado desde cero para apoyar la ciencia de datos. R no es solo un lenguaje de programación, sino que también es un entorno interactivo para hacer ciencia de datos. Para apoyar la interacción, R es un lenguaje mucho más flexible que muchos de sus pares. Esta flexibilidad viene con sus desventajas, pero la gran ventaja es lo fácil que es desarrollar gramáticas adaptadas para partes específicas del proceso de ciencia de datos. Estos mini idiomas lo ayudan a pensar en problemas como científico de datos, al tiempo que respaldan una interacción fluida entre su cerebro y la computadora.\nDatos no rectangulares Este libro se enfoca exclusivamente en datos rectangulares: colecciones de valores que están asociados con una variable y una observación. Hay muchos conjuntos de datos que no encajan naturalmente en este paradigma: incluyendo imágenes, sonidos, árboles y texto. Pero los marcos de datos rectangulares son extremadamente comunes en la ciencia y la industria, y creemos que son un gran lugar para comenzar su viaje de ciencia de datos.\nConfirmación de hipótesis Es posible dividir el análisis de datos en dos campos: generación de hipótesis y confirmación de hipótesis (a veces llamado análisis confirmatorio). El objetivo de este libro es descaradamente la generación de hipótesis o la exploración de datos. Aquí observará profundamente los datos y, en combinación con el conocimiento de su materia, generará muchas hipótesis interesantes para ayudar a explicar por qué los datos se comportan de la manera en que lo hacen. Evalúa las hipótesis de manera informal, utilizando su escepticismo para desafiar los datos de múltiples maneras.\nEl complemento de la generación de hipótesis es la confirmación de hipótesis. La confirmación de la hipótesis es difícil por dos razones:\n Necesita un modelo matemático preciso para generar datos falsificables predicciones Esto a menudo requiere una considerable sofisticación estadística.\n Solo puede usar una observación una vez para confirmar una hipótesis. Tan pronto como lo usa más de una vez que vuelve a hacer análisis exploratorios. Esto significa hacer una confirmación de hipótesis que necesita \u0026ldquo;preregistrarse\u0026rdquo; (escriba de antemano) su plan de análisis, y no se desvíe de él incluso cuando has visto los datos. Hablaremos un poco sobre algunos estrategias que puede usar para facilitar esto en modelado.\n  Es común pensar en el modelado como una herramienta para la confirmación de hipótesis y la visualización como una herramienta para la generación de hipótesis. Pero esa es una falsa dicotomía: los modelos a menudo se usan para exploración, y con un poco de cuidado puede usar la visualización para confirmar. La diferencia clave es con qué frecuencia mira cada observación: si mira solo una vez, es una confirmación; si miras más de una vez, es exploración.\nPrerrequisitos Hemos hecho algunas suposiciones sobre lo que ya sabes para aprovechar al máximo este libro. En general, debe tener conocimientos numéricos y es útil si ya tiene experiencia en programación. Si nunca ha programado antes, es posible que Garrett Hands on Programming with R de Garrett sea un complemento útil de este libro.\nHay cuatro cosas que necesita para ejecutar el código en este libro: R, RStudio, una colección de paquetes R llamada tidyverse, y un puñado de otros paquetes. Los paquetes son las unidades fundamentales del código R reproducible. Incluyen funciones reutilizables, la documentación que describe cómo usarlas y datos de muestra.\nR Para descargar R, vaya a CRAN, el ** c ** omprehensive ** R ** ** a ** rchive ** n ** etwork. CRAN se compone de un conjunto de servidores espejo distribuidos en todo el mundo y se utiliza para distribuir paquetes R y R. No intente elegir un espejo que esté cerca de usted: en su lugar, use el espejo en la nube, https://cloud.r-project.org, que automáticamente lo resuelve por usted.\nUna nueva versión principal de R sale una vez al año, y hay 2-3 lanzamientos menores cada año. Es una buena idea actualizar regularmente. La actualización puede ser una molestia, especialmente para las versiones principales, que requieren que reinstales todos tus paquetes, pero posponerlo solo lo empeora.\nRStudio RStudio es un entorno de desarrollo integrado, o IDE, para la programación R. Descargue e instálelo desde http://www.rstudio.com/download. RStudio se actualiza un par de veces al año. Cuando hay una nueva versión disponible, RStudio se lo informará. Es una buena idea actualizar regularmente para que pueda aprovechar las últimas y mejores funciones. Para este libro, asegúrese de tener RStudio 1.0.0.\nCuando inicie RStudio, verá dos regiones clave en la interfaz:\n`{r echo = FALSE, out.width =\u0026quot; 75% \u0026quot;} knitr :: include_graphics (\u0026quot;diagrams / rstudio-console.png\u0026quot;)  `\nPor ahora, todo lo que necesita saber es que escribe el código R en el panel de la consola y presiona Intro para ejecutarlo. ¡Aprenderás más a medida que avanzamos!\nEl tidyverse También necesitará instalar algunos paquetes de R. Un R paquete es una colección de funciones, datos y documentación que amplía las capacidades de la base R. El uso de paquetes es clave para el uso exitoso de R. La mayoría de los paquetes que aprenderá en este libro son parte de llamado tidyverse. Los paquetes en el tidyverse comparten una filosofía común de datos y programación R, y están diseñados para trabajar juntos de forma natural.\nPuede instalar el tidyverse completo con una sola línea de código:\n`{r, eval = FALSO} install.packages (\u0026quot;tidyverse\u0026quot;)  `\nEn su propia computadora, escriba esa línea de código en la consola y luego presione Intro para ejecutarla. R descargará los paquetes de CRAN y los instalará en su computadora. Si tiene problemas para instalar, asegúrese de estar conectado a Internet y de que https://cloud.r-project.org/ no esté bloqueado por su firewall o proxy.\nNo podrá utilizar las funciones, objetos y archivos de ayuda en un paquete hasta que lo cargue con library (). Una vez que haya instalado un paquete, puede cargarlo con la función library ():\n{r} biblioteca (tidyverse)  `\nEsto le indica que tidyverse está cargando los paquetes ggplot2, tibble, tidyr, readr, purrr y dplyr. Estos se consideran el core del tidyverse porque los usará en casi todos los análisis.\nLos paquetes en el tidyverse cambian con bastante frecuencia. Puede ver si hay actualizaciones disponibles y, opcionalmente, instalarlas ejecutando tidyverse_update ().\nOtros paquetes Hay muchos otros paquetes excelentes que no forman parte del tidyverse, porque resuelven problemas en un dominio diferente o están diseñados con un conjunto diferente de principios subyacentes. Esto no los hace mejores o peores, solo diferentes. En otras palabras, el complemento al tidyverse no es el messyverse, sino muchos otros universos de paquetes interrelacionados. A medida que aborde más proyectos de ciencia de datos con R, aprenderá nuevos paquetes y nuevas formas de pensar sobre los datos.\nEn este libro usaremos tres paquetes de datos externos al tidyverse:\n`{r, eval = FALSO} install.packages (c (\u0026quot;nycflights13\u0026quot;, \u0026quot;gapminder\u0026quot;, \u0026quot;Lahman\u0026quot;))  `\nEstos paquetes proporcionan datos sobre vuelos de aerolíneas, desarrollo mundial y béisbol que usaremos para ilustrar ideas clave de ciencia de datos.\nEjecutando código R La sección anterior le mostró un par de ejemplos de ejecución de código R. El código en el libro se ve así:\n`{r, eval = TRUE} 1 + 2 #\u0026gt; [1] 3  `\nSi ejecuta el mismo código en su consola local, se verá así:\n` \u0026gt; 1 + 2 [1] 3  `\nHay dos diferencias principales. En su consola, escribe después del \u0026gt;, llamado prompt; No mostramos el aviso en el libro. En el libro, la salida se comenta con #\u0026gt;; en tu consola aparece directamente después de tu código. Estas dos diferencias significan que si está trabajando con una versión electrónica del libro, puede copiar fácilmente el código del libro y en la consola.\nA lo largo del libro usamos un conjunto consistente de convenciones para referirnos al código:\n Las funciones están en una fuente de código y seguidas de paréntesis, como sum (), o mean ().\n Otros objetos R (como datos o argumentos de funciones) están en una fuente de código, sin paréntesis, como vuelos ox.  Si queremos dejar en claro de qué paquete proviene un objeto, usaremos el nombre del paquete seguido de dos puntos, como dplyr :: mutate (), o nycflights13 :: vuelos. Este también es un código R válido.\n  Obteniendo ayuda y aprendiendo más Este libro no es una isla; no existe un recurso único que le permita dominar R. Cuando comience a aplicar las técnicas descritas en este libro a sus propios datos, pronto encontrará preguntas que no contesto. Esta sección describe algunos consejos sobre cómo obtener ayuda y para ayudarlo a seguir aprendiendo.\nSi te quedas atascado, comienza con Google. Normalmente, agregar \u0026ldquo;R\u0026rdquo; a una consulta es suficiente para restringirla a resultados relevantes: si la búsqueda no es útil, a menudo significa que no hay resultados específicos de R disponibles. Google es particularmente útil para mensajes de error. Si recibe un mensaje de error y no tiene idea de lo que significa, intente buscarlo en Google. Lo más probable es que alguien más haya estado confundido en el pasado, y habrá ayuda en algún lugar de la web. (Si el mensaje de error no está en inglés, ejecute Sys.setenv (LANGUAGE =\u0026quot; en \u0026quot;) y vuelva a ejecutar el código; es más probable que encuentre ayuda para los mensajes de error en inglés).\nSi Google no ayuda, intente stackoverflow. Comience por pasar un poco de tiempo buscando una respuesta existente, incluyendo [R] para restringir su búsqueda a preguntas y respuestas que usen R. Si no encuentra nada útil, prepare un ejemplo reproducible mínimo o reprex. Un buen reprex hace que sea más fácil para otras personas ayudarte, y a menudo descubrirás el problema tú mismo mientras lo haces.\nHay tres cosas que debe incluir para que su ejemplo sea reproducible: paquetes, datos y código requeridos.\n ** Los paquetes ** deben cargarse en la parte superior del script, por lo que es fácil ver cuáles necesita el ejemplo. Este es un buen momento para comprobar que estás usando la última versión de cada paquete; es posible que hayas descubierto Un error que se ha solucionado desde que instaló el paquete. Para paquetes en tidyverse, la forma más fácil de verificar es ejecutar tidyverse_update ().\n La forma más fácil de incluir ** datos ** en una pregunta es usar dput () para generar el código R para recrearlo. Por ejemplo, para recrear los mtcars conjunto de datos en R, realizaría los siguientes pasos: 1. Ejecute dput (mtcars) en R 2. Copie la salida 3. En mi script reproducible, escriba mtcars \u0026lt;- y luego pegue. Intenta encontrar el subconjunto más pequeño de tus datos que aún revela el problema.\n Dedica un poco de tiempo a asegurarte de que tu ** código ** sea fácil para otros leer:\n   * Asegúrese de haber usado espacios y que sus nombres de variables sean concisos, aún informativo. * Use comentarios para indicar dónde radica su problema. * Haga todo lo posible para eliminar todo lo que no esté relacionado con el problema. Cuanto más corto sea su código, más fácil será comprenderlo y Más fácil es arreglarlo.\nTermine comprobando que realmente ha hecho un ejemplo reproducible comenzando una nueva sesión de R y copiando y pegando su script.\nTambién debe pasar algún tiempo preparándose para resolver los problemas antes de que ocurran. Invertir un poco de tiempo en aprender R cada día dará buenos resultados a largo plazo. Una forma es seguir lo que Hadley, Garrett y todos los demás en RStudio están haciendo en el blog RStudio. Aquí es donde publicamos anuncios sobre nuevos paquetes, nuevas funciones de IDE y cursos presenciales. También puede seguir a Hadley (\\ @hadleywickham) o Garrett (\\ @statgarrett) en Twitter, o seguir \\ @rstudiotips para mantenerse al día con las nuevas funciones en el IDE.\nPara mantenerse al día con la comunidad R en general, recomendamos leer http://www.r-bloggers.com: agrega más de 500 blogs sobre R de todo el mundo. Si eres un usuario activo de Twitter, sigue el hashtag # rstats. Twitter es una de las herramientas clave que Hadley utiliza para mantenerse al día con los nuevos desarrollos en la comunidad.\n","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"866dc41d5598498f9d0583dfccb3a09b","permalink":"https://www.marcusrb.com/en/courses/r-studio/advanced-r/r201-tidy/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/en/courses/r-studio/advanced-r/r201-tidy/","section":"courses","summary":"La ciencia de datos es una disciplina emocionante que le permite convertir datos sin procesar en comprensión, conocimiento y conocimiento. El objetivo de \u0026ldquo;R for Data Science\u0026rdquo; es ayudarlo a aprender las herramientas más importantes en R que le permitirán hacer ciencia de datos. Después de leer este libro, tendrá las herramientas para abordar una amplia variedad de desafíos de la ciencia de datos, utilizando las mejores partes de R.","tags":null,"title":"Introducción","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.\nTus primeros comandos BigQuery Para usar BigQuery, importaremos el paquete de Python a continuación:\nfrom google.cloud import bigquery  El primer paso en el flujo de trabajo es crear un objeto Client. Como pronto verá, este objeto Client desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Trabajaremos con un conjunto de datos de publicaciones en Hacker News, un sitio web que se centra en noticias de informática y seguridad cibernética.\nEn BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos hacker_news está contenido en el proyecto bigquery-public-data. Para acceder al conjunto de datos,\n Comenzamos construyendo una referencia al conjunto de datos con el método dataset(). A continuación, utilizamos el método get_dataset(), junto con la referencia que acabamos de construir, para obtener el conjunto de datos.  # Construct a reference to the \u0026quot;hacker_news\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;hacker_news\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.\nUsamos el método list_tables() para listar las tablas en el conjunto de datos.\n# List all the tables in the \u0026quot;hacker_news\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there are four!) for table in tables: print(table.table_id)  OUTPUT\ncomments full full_201510 stories  De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla full en el conjunto de datos hacker_news.\n# Construct a reference to the \u0026quot;full\u0026quot; table table_ref = dataset_ref.table(\u0026quot;full\u0026quot;) # API request - fetch the table table = client.get_table(table_ref)  En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.\nEsquema de la tabla La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.\nEn este ejemplo, investigaremos la tabla completa full que obtuvimos anteriormente.\n# Print information on all the columns in the \u0026quot;full\u0026quot; table in the \u0026quot;hacker_news\u0026quot; dataset table.schema  OUTPUT\n[SchemaField('by', 'STRING', 'NULLABLE', \u0026quot;The username of the item's author.\u0026quot;, ()), SchemaField('score', 'INTEGER', 'NULLABLE', 'Story score', ()), SchemaField('time', 'INTEGER', 'NULLABLE', 'Unix time', ()), SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', 'Timestamp for the unix time', ()), SchemaField('title', 'STRING', 'NULLABLE', 'Story title', ()), SchemaField('type', 'STRING', 'NULLABLE', 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', ()), SchemaField('url', 'STRING', 'NULLABLE', 'Story url', ()), SchemaField('text', 'STRING', 'NULLABLE', 'Story or comment text', ()), SchemaField('parent', 'INTEGER', 'NULLABLE', 'Parent comment ID', ()), SchemaField('deleted', 'BOOLEAN', 'NULLABLE', 'Is deleted?', ()), SchemaField('dead', 'BOOLEAN', 'NULLABLE', 'Is dead?', ()), SchemaField('descendants', 'INTEGER', 'NULLABLE', 'Number of story or poll descendants', ()), SchemaField('id', 'INTEGER', 'NULLABLE', \u0026quot;The item's unique id.\u0026quot;, ()), SchemaField('ranking', 'INTEGER', 'NULLABLE', 'Comment ranking', ())]  Cada SchemaField nos informa sobre una columna específica (a la que también nos referimos como un campo field). En orden, la información es:\n El nombre de la columna. El tipo de campo (o tipo de datos) en la columna El modo de la columna (\u0026lsquo;NULLABLE\u0026rsquo; significa que una columna permite valores NULL y es el valor predeterminado) Una descripción de los datos en esa columna. El primer campo tiene el SchemaField:  SchemaField (\u0026lsquo;by\u0026rsquo;, \u0026lsquo;string\u0026rsquo;, \u0026lsquo;NULLABLE\u0026rsquo;, \u0026ldquo;El nombre de usuario del autor del elemento\u0026rdquo;, ()\nEsto nos dice:\n el campo (o columna) es llamado por los datos en este campo son cadenas, Se permiten valores NULL y Contiene los nombres de usuario correspondientes al autor de cada elemento.  Podemos usar el método list_rows() para verificar solo las primeras cinco líneas de la tabla completa full para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery RowIterator que se puede convertir rápidamente en un DataFrame de pandas con el método to_dataframe().\n# Preview the first five lines of the \u0026quot;full\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  El método list_rows() también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!\n# Preview the first five entries in the \u0026quot;by\u0026quot; column of the \u0026quot;full\u0026quot; table client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()  EXERCISE (Exercise_ Getting Started With SQL and BigQuery)\n","date":1568070000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"b1e5ee5b52d53a51f7f6759b89ec06d1","permalink":"https://www.marcusrb.com/en/courses/business-analytics/intro-sql/sql101-0-start/","publishdate":"2019-09-10T00:00:00+01:00","relpermalink":"/en/courses/business-analytics/intro-sql/sql101-0-start/","section":"courses","summary":"Introducción El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.\nEn esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.","tags":null,"title":"Getting Started with SQL y Big Query","type":"docs"},{"authors":null,"categories":null,"content":" R es un programa de código abierto del que existen varias distribuciones, que se pueden descargar libremente. Veamos los procedimientos para instalarlo y actualizarlo.\nDownload Primero, por supuesto, debe descargar el paquete básico (70 Mb para la versión 3.3), elegir un \u0026ldquo;espejo\u0026rdquo; en http://cran.r-project.org/mirrors.html, o ir directamente a:\nespejo para sistemas Windows; espejos para sistemas Mac-Os; réplicas para sistemas Linux (Debian, Redhat, Suse, Ubuntu). Recientemente, Microsoft ha puesto a disposición una versión de R, de código abierto, gratuita y que tiene algunas características adicionales que facilitan la reproducibilidad de la búsqueda y el cálculo en paralelo (https://mran.microsoft.com/open).\nR portátil En Sourgeforce.net está disponible una versión portátil de R, que se puede instalar, con todas sus características, en un soporte de memoria externa (disco duro externo, unidad flash USB, etc.). Esta versión de R se puede integrar en la suite PortableApps.\nInstalación En Windows, el software se instala ejecutando el archivo ejecutable (exe) descargado.\nPara usar RCommander y RExcel, es preferible personalizar la instalación:\nEjecute el archivo ejecutable en modo administrador (haga clic en el archivo con el botón derecho del mouse y elija esta opción); Continúe con la instalación hasta la siguiente pantalla: ¿Desea configurar las opciones de arranque? Elige \u0026ldquo;Sí\u0026rdquo;: | Modo de visualización: elija SDI (varias ventanas) Estilo de ayuda: HTML (por defecto) Selección de procesos adicionales: almacena el número de versión en el registro: |\nActualización La forma más fácil de actualizar R y mantener bibliotecas es:\ninstale la nueva versión (se instalará en una nueva carpeta) copie los paquetes instalados desde la carpeta de la biblioteca anterior a la carpeta correspondiente de la nueva instalación iniciar R en modo administrador (en Windows Vista y Windows7) ejecutar - dentro de la nueva R - el comando paquetes de actualización (checkBuilt = TRUE, ask = FALSE) desinstale la versión anterior y elimine el directorio anterior. También puede actualizar R y paquetes con el paquete de instalación.\nR en la web Finalmente, existe la posibilidad de ejecutar R en línea (con diferentes interfaces), a través de sitios web que proporcionan una instalación de servidor R, como: https://www.tutorialspoint.com/execute_r_online.php\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"6d8ceaaa807b7ec7ad4966bf53aa25d7","permalink":"https://www.marcusrb.com/en/courses/r-studio/instalacion-r/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/r-studio/instalacion-r/","section":"courses","summary":"R es un programa de código abierto del que existen varias distribuciones, que se pueden descargar libremente. Veamos los procedimientos para instalarlo y actualizarlo.\nDownload Primero, por supuesto, debe descargar el paquete básico (70 Mb para la versión 3.3), elegir un \u0026ldquo;espejo\u0026rdquo; en http://cran.r-project.org/mirrors.html, o ir directamente a:\nespejo para sistemas Windows; espejos para sistemas Mac-Os; réplicas para sistemas Linux (Debian, Redhat, Suse, Ubuntu). Recientemente, Microsoft ha puesto a disposición una versión de R, de código abierto, gratuita y que tiene algunas características adicionales que facilitan la reproducibilidad de la búsqueda y el cálculo en paralelo (https://mran.","tags":null,"title":"Instalación de R","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"b7d7d7d73d099aa26176aaffc2c2999f","permalink":"https://www.marcusrb.com/en/courses/data-science/math-data-science/math-intro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/data-science/math-data-science/math-intro/","section":"courses","summary":"","tags":null,"title":"Introducción Math for Data Science","type":"docs"},{"authors":null,"categories":null,"content":" Introducción Este curso cubre las habilidades clave de Python que necesitará para que pueda comenzar a usar Python para la ciencia de datos. El curso es ideal para alguien con experiencia previa en codificación que quiera agregar Python a su repertorio o subir de nivel sus habilidades básicas de Python. (Si es un programador por primera vez, puede consultar estos recursos de aprendizaje \u0026ldquo;Python para no programadores\u0026rdquo;).\nComenzaremos con una breve descripción general de la sintaxis de Python, la asignación de variables y los operadores aritméticos. Si tiene experiencia previa en Python, puede pasar directamente al ejercicio práctico.\n## Hello, Python! Python fue nombrado por la compañía de comedia británica Monty Python, por lo que haremos de nuestro primer programa Python un homenaje a su parodia sobre el spam.\nSolo por diversión, intente leer el código a continuación y predecir lo que hará cuando se ejecute. (Si no tienes idea, ¡está bien!)\nLuego haga clic en el botón \u0026ldquo;salida\u0026rdquo; para ver los resultados de nuestro programa.\nspam_amount = 0 print(spam_amount) # Ordering Spam, egg, Spam, Spam, bacon and Spam (4 more servings of Spam) spam_amount = spam_amount + 4 if spam_amount \u0026gt; 0: print(\u0026quot;But I don't want ANY spam!\u0026quot;) viking_song = \u0026quot;Spam \u0026quot; * spam_amount print(viking_song)  0 But I don't want ANY spam! Spam Spam Spam Spam  ¡Hay mucho que desempacar aquí! Este programa tonto demuestra muchos aspectos importantes de cómo se ve el código Python y cómo funciona. Revisemos el código de arriba a abajo.\nspam_amount = 0  Asignación de variables: aquí creamos una variable llamada spam_amount y le asignamos el valor de 0 usando =, que se llama operador de asignación.\nNota: si ha programado en ciertos otros lenguajes (como Java o C ++), puede estar notando algunas cosas que Python no requiere que hagamos aquí:\n no necesitamos \u0026ldquo;declarar\u0026rdquo; spam_amount antes de asignarle no necesitamos decirle a Python a qué tipo de valor se referirá spam_amount. De hecho, incluso podemos reasignar spam_amount para referirnos a un tipo diferente de cosas como una cadena o un booleano.  print(spam_amount)  0  Llamadas de función: print es una función de Python que muestra el valor que se le pasa en la pantalla. Llamamos a las funciones poniendo paréntesis después de su nombre y poniendo las entradas (o argumentos) a la función en esos paréntesis.\n# Ordering Spam, egg, Spam, Spam, bacon and Spam (4 more servings of Spam) spam_amount = spam_amount + 4  La primera línea de arriba es un comentario. En Python, los comentarios comienzan con el símbolo #.\nA continuación vemos un ejemplo de reasignación. La reasignación del valor de una variable existente tiene el mismo aspecto que la creación de una variable: todavía utiliza el operador de asignación =.\nEn este caso, el valor que estamos asignando a _spamamount implica una aritmética simple en su valor anterior. Cuando encuentra esta línea, Python evalúa la expresión en el lado derecho de = (0 + 4 = 4), y luego asigna ese valor a la variable en el lado izquierdo.\nif spam_amount \u0026gt; 0: print(\u0026quot;But I don't want ANY spam!\u0026quot;) viking_song = \u0026quot;Spam Spam Spam\u0026quot; print(viking_song)  But I don't want ANY spam! Spam Spam Spam  No hablaremos mucho sobre \u0026ldquo;condicionales\u0026rdquo; hasta más tarde, pero, incluso si nunca ha codificado antes, probablemente pueda adivinar lo que hace. Python es apreciado por su legibilidad y simplicidad.\nObserve cómo indicamos qué código pertenece al if. \u0026ldquo;¡Pero no quiero NINGÚN spam!\u0026rdquo; solo se supone que se imprime si _spamamount es positivo. Pero el código posterior (como _print (vikingsong)) debe ejecutarse sin importar qué. ¿Cómo lo sabemos (y Python)?\nLos dos puntos (:) al final de la línea if indican que se está iniciando un nuevo \u0026ldquo;bloque de código\u0026rdquo;. Las líneas posteriores que están sangradas son parte de ese bloque de código. Algunos otros idiomas usan {llaves \u0026ldquo;para marcar el comienzo y el final de los bloques de código. El uso de espacios en blanco significativos por Python puede ser sorprendente para los programadores que están acostumbrados a otros lenguajes, pero en la práctica puede conducir a un código más coherente y legible que los lenguajes que no imponen sangría de bloques de código.\nLas líneas posteriores que tratan con _vikingsong no están sangradas con 4 espacios adicionales, por lo que no forman parte del bloque de código if. Veremos más ejemplos de bloques de código sangrados más adelante cuando definamos funciones y usemos bucles.\nEste fragmento de código también es nuestro primer avistamiento de una cadena string en Python:\n\u0026quot;But I don't want ANY spam!\u0026quot;  \u0026quot;But I don't want ANY spam!\u0026quot;  Las cadenas se pueden marcar con comillas dobles o simples. (Pero debido a que esta cadena en particular contiene un carácter de comillas simples, podríamos confundir a Python tratando de rodearla con comillas simples, a menos que tengamos cuidado).\nviking_song = \u0026quot;Spam \u0026quot; * spam_amount print(viking_song)  Spam Spam Spam Spam  El operador * se puede usar para multiplicar dos números (3 * 3 se evalúa como 9), pero de manera bastante divertida, también podemos multiplicar una cadena por un número, para obtener una versión que se ha repetido tantas veces. Python ofrece una serie de trucos descarados y pequeños que ahorran tiempo como este, donde los operadores como * y + tienen un significado diferente según el tipo de cosas a las que se aplican. (El término técnico para esto es operador de sobrecarga)\nNúmeros y aritmética en Python Ya hemos visto un ejemplo de una variable que contiene un número arriba:\nspam_amount = 0  \u0026ldquo;Número\u0026rdquo; es un buen nombre informal para el tipo de cosas, pero si quisiéramos ser más técnicos, podríamos preguntarle a Python cómo describiría el tipo de cosas que es spam_amount:\ntype(spam_amount)  int  Es un int - abreviatura de entero. Hay otro tipo de número que comúnmente encontramos en Python:\nfrom matplotlib import pyplot as plt %matplotlib inline import seaborn as sns df = sns.load_dataset('iris') sns.lmplot(x = 'petal_length', y = 'petal_width', data = df , hue = 'species' , fit_reg = False)  \u0026lt;seaborn.axisgrid.FacetGrid at 0x1a16f2d908\u0026gt;  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"f91d8e8675d5b79e14135426e3e218eb","permalink":"https://www.marcusrb.com/en/courses/python/py101/py101-1-intro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/python/py101/py101-1-intro/","section":"courses","summary":"Introducción Este curso cubre las habilidades clave de Python que necesitará para que pueda comenzar a usar Python para la ciencia de datos. El curso es ideal para alguien con experiencia previa en codificación que quiera agregar Python a su repertorio o subir de nivel sus habilidades básicas de Python. (Si es un programador por primera vez, puede consultar estos recursos de aprendizaje \u0026ldquo;Python para no programadores\u0026rdquo;).\nComenzaremos con una breve descripción general de la sintaxis de Python, la asignación de variables y los operadores aritméticos.","tags":null,"title":"Python 101 - Introducción","type":"docs"},{"authors":null,"categories":null,"content":" Instrucciones\n Da clic a \u0026lsquo;Submit Answer\u0026rsquo; y date cuenta como la consola ejecuta el código de R del editor: la solución 7 aparece como la suma de 3 y 4. El uso más simple de R es como una calculadora y graficadora, pero por supuesto hay mucho, mucho, mucho más! Añade una linea que haga el calculo de 6 y 12.  Script R\n# Este es el editor y la parte de abajo es la consola # El símbolo de numeral # es utilizada para hacer comentarios # Calcula 3 + 4 3 + 4 # Calcula 6 + 12 6 + 12  Aritmética con R Como vimos el uso más simple que se le puede dar a R es el de una calculadora. Consideremos las siguientes operaciones:\nAdición: + Resta: - Multiplicación: * División: / Exponenciación: ^ Modulo: %% Los últimos dos necesitan una breve explicación:\nEl operador ^ eleva el número a la izquierda a la potencia a la derecha 3^2 es 9 El módulo (mod) %% calcula el residuo de la división del número a la izquierda por el número a la derecha, por ejemplo 5 mod 3 o 5%%3 es 2.\nInstrucciones\n Escribe 2^5 en el editor para calcular 2 a la quinta potencia. Escribe 28 %% 6 para calcular el 28 módulo 6. Da clic a \u0026lsquo;Submit Answer\u0026rsquo; para ver el resultado en la consola.  Script R\n# Adición 5 + 5 # Resta 5 - 5 # Multiplicación 3 * 5 # División (5 + 5) / 2 # Exponenciación 2^5 # Modulo 28 %% 6  Variables y asignaciones Un concepto básico en programación es el de variable.\nUna variable nos permite guardar valores (por ejemplo el número 4) o algún objeto (veremos mas adelante de que se trata) en R. Luego puedes acceder al valor guardado en la variable por medio del nombre de la misma.\nPodemos asignar el valor de 4 a la variable mi_variable con el siguiente comando: mi_variable \u0026lt;- 4\nInstrucciones\n Completa el código en el editor de tal manera que el valor de 42 quede asignado a la variable x. Da clic en \u0026lsquo;Submit Answer\u0026rsquo;. Nota al escribir x en R, se imprime el valor de 42 en la consola.  Script R\n# Asigna 42 a x x \u0026lt;- 42 # Imprime el valor de la variable x x  Variables y asignaciones (2) Supongamos que tienes una canasta con cinco manzanas. Para recordarlo, quizás quieras asignar el número de manzanas en una variable llamada numero_manzanas\nInstrucciones\n Escribe el siguiente código: numero_manzanas \u0026lt;- 5 para asignar el numero 5 a la variable numero_manzanas. Escribe: numero_manzanas abajo del segundo comentario. Da clic a \u0026lsquo;Submit Answer\u0026rsquo;, ve la consola: el numero que sea ha impreso es 5.  Script R\n# Asigna el valor de 5 a la variable numero_manzanas numero_manzanas \u0026lt;- 5 # Imprime el valor de la variable numero_manzanas numero_manzanas  Variables y asignaciones (3) Supongamos que ahora tienes 6 naranjas. De nuevo, para no olvidarlo se te ocurre crear una variable llamada numero_naranjas y asignar el valor de 6 a esa variable. Ahora podemos a empezar a utilizar las variables creadas para hacer algo con ellas. Usemos R para saber el número total de frutas con las que contamos, pidamos que haga la cuenta por nosotros: numero_manzanas + numero_naranjas. Al leer este codigo nos damos cuenta de la importancia de dar nombres útiles a nuestras variables.\nInstrucciones\n Asigna a numero_naranjas el valor de 6. R permite combinar estas variables numero_manzanas y numero_naranjas en una nueva variable numero_frutas. Crea la variable numero_frutas y asigna el valor del total de frutas que tenemos.  Script R\n# Asigna el valor de 5 y 6 a las variables numero_manzanas y numero_naranjas respectivamente numero_manzanas \u0026lt;- 5 numero_naranjas \u0026lt;- 6 # Suma estas dos variables e imprime el resultado. numero_manzanas + numero_manzanas #Crea la variable numero_frutas y asigna el resultado de la suma anterior. numero_frutas \u0026lt;- (numero_manzanas + numero_naranjas) numero_frutas  Manzanas y naranjas En la escuela primaria nos decían que no sumáramos manzanas con naranjas, pero es lo que acabamos de hacer :) \\n Sin embargo numero_manzanas y numero_naranjas son dos variables que contienen el mismo tipo de dato: un dato de tipo numérico. El operador + en R funciona con variables de este tipo. Si alguna de nuestras variables no es numérica sino por ejemplo caracter (ver el editor), entonces estaríamos tratando de asignar la suma de un caracter y un número a la variable numero_frutas, lo cual no es posible.\nInstrucciones\n Da clic a \u0026lsquo;Submit Answer\u0026rsquo; lee el mensaje de error, asegúrate de entender que dice (puedes copiar y pegar el texto en ingles en un traductor) Ajusta el código para que R deje de mostrar ese error.  Tipos de datos básicos en R R trabaja con muchos tipos de datos. Para empezar, algunos de los más básicos son:\n Decimales como 4.5 son llamados numeric (numéricos). Números enteros como 4 son llamados (sorpresa!) integer (enteros). Valores Booleanos (TRUE (Verdadero) o FALSE (Falso)) logical (lógicos). Texto (cadenas de caracteres) son characters (caracteres). Nota como utilizamos las comillas para denotar el texto en el editor.  Instrucciones\nCambia los valores de:\n mi_numerica a 42. mi_caracter a \u0026ldquo;cuarenta_y_dos\u0026rdquo;. Nota como utilizamos las comillas. mi_logica a FALSE (Falso). Ten en cuenta que R distingue entre mayúsculas y minúsculas!  Script R\n# Un valor numérico mi_numerica \u0026lt;- 42 # Asignando una cadena de caracteres (o simplemente caracteres) nota el uso de las comillas mi_caracter \u0026lt;- \u0026quot;cuarenta_y_dos\u0026quot; # Asignando un valor lógico verdadero mi_logica \u0026lt;- FALSE  ## ¿Cómo sé el tipo de dato? ¿Recuerdas que cuando añadiste 5 + \u0026ldquo;seis\u0026rdquo; obtuviste un error debido a que los tipos de datos no coincidian? Para evitar estas penosas situaciones :\\ puedes saber de antemano el tipo de dato que tienen tus variables utilizando el código class(nombre_variable)\nInstrucciones\n Completa el código en el editor para imprimir a la consola el tipo de dato de las variables mi_numerica, mi_caracter y mi_logica.  Script R\n# Declarando las variables de diferentes tipos mi_numerica \u0026lt;- 40 mi_caracter \u0026lt;- \u0026quot;cuarenta\u0026quot; mi_logica \u0026lt;- FALSE # Escribe el código para averiguar el tipo de dato de cada variable class(mi_numerica) class(mi_caracter) class(mi_logica)  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"24f7c19c49e5aeb4220f32c7798f588e","permalink":"https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-intro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/r-studio/intro-r/r101-intro/","section":"courses","summary":"Instrucciones\n Da clic a \u0026lsquo;Submit Answer\u0026rsquo; y date cuenta como la consola ejecuta el código de R del editor: la solución 7 aparece como la suma de 3 y 4. El uso más simple de R es como una calculadora y graficadora, pero por supuesto hay mucho, mucho, mucho más! Añade una linea que haga el calculo de 6 y 12.  Script R\n# Este es el editor y la parte de abajo es la consola # El símbolo de numeral # es utilizada para hacer comentarios # Calcula 3 + 4 3 + 4 # Calcula 6 + 12 6 + 12  Aritmética con R Como vimos el uso más simple que se le puede dar a R es el de una calculadora.","tags":null,"title":"Prácticas 1 - Introducción","type":"docs"},{"authors":null,"categories":null,"content":" El objetivo de este documento es ayudar a aquellas personas con un conocimiento básico sobre aprendizaje automático a aprovechar las recomendaciones de Google para aprendizaje automático. Presenta un estilo para aprendizaje automático, similar a la guía de estilo de Google para C++ y otras guías populares para la programación práctica. Si tomaste clases de aprendizaje automático, desarrollaste un modelo de aprendizaje automático o trabajaste en uno, tienes el conocimiento necesario para leer este documento.\nTerminología Los siguientes términos se mencionarán con frecuencia en nuestro debate sobre aprendizaje automático eficaz:\n Instancia: El aspecto sobre el que deseas hacer una predicción. Por ejemplo, la instancia puede ser una página web que deseas clasificar como \u0026ldquo;sobre gatos\u0026rdquo; o \u0026ldquo;no sobre gatos\u0026rdquo;. Etiqueta: Una respuesta a la tarea de predicción, que se puede generar mediante un sistema de aprendizaje automático o a partir de los datos de entrenamiento. Por ejemplo, la etiqueta sobre una página web puede ser \u0026ldquo;sobre gatos\u0026rdquo;. Atributo: Una propiedad de una instancia utilizada en una tarea de predicción. Por ejemplo, una página web puede tener un atributo \u0026ldquo;contiene la palabra gato\u0026rdquo;. Columna de atributos: Un conjunto de atributos relacionados, como el conjunto de todos los países posibles donde es posible que vivan los usuarios. Un ejemplo puede tener uno o más atributos presentes en una columna de atributos. \u0026ldquo;Columna de atributos\u0026rdquo; es terminología específica de Google. Una columna de atributos se conoce como un \u0026ldquo;espacio de nombres\u0026rdquo; en el sistema de VW (en Yahoo/Microsoft) o como un campo. Ejemplo: Una instancia (con sus atributos) y una etiqueta. Modelo: Una representación estadística de una tarea de predicción. Entrenas un modelo basado en ejemplos y, luego, lo usas para hacer predicciones. Métrica: El número que importa. Se puede o no optimizar directamente. Objetivo: Una métrica que tu algoritmo intenta optimizar. Canalización: La infraestructura que rodea al algoritmo de aprendizaje automático. Incluye la recopilación de datos del frontend, su incorporación a los archivos de datos de entrenamiento, el entrenamiento de uno o más modelos y la exportación de los modelos para la producción. Tasa de clics: El porcentaje de visitantes a una página web que hacen clic en un vínculo en un anuncio.  Resumen Para desarrollar buenos productos:\nEn aprendizaje automático, implemente todos los conocimientos de ingeniería que tiene, no los conocimientos de aprendizaje automático que no tiene.\nDe hecho, la mayoría de los problemas que debes resolver son de ingeniería. Incluso con todos los recursos de un experto en aprendizaje automático, la mayoría de los beneficios provienen de los atributos geniales, no de los algoritmos de aprendizaje automático geniales. Por lo tanto, la estrategia básica es la siguiente:\nAsegúrate de que tu canalización sea completamente estable. Comienza con un objetivo razonable. Agrega atributos de sentido común de una forma sencilla. Asegúrate de que la canalización siga siendo estable. Esta estrategia dará resultados positivos por un tiempo prolongado. Modifica esta estrategia solo cuando no queda otra opción para probar. Si sumas complejidad, se retrasan las siguientes versiones.\nUna vez que hayas agotado los trucos simples, puedes pasar a aprendizaje automático de vanguardia. Consulta la sección en los proyectos de aprendizaje automático de la Fase III.\nEste documento se organiza de la siguiente manera:\nLa primera parte te permite determinar si es el momento indicado para desarrollar un sistema de aprendizaje automático. La segunda parte se trata de implementar la primera canalización. La tercera parte cubre el lanzamiento y la iteración mientras se agregan atributos nuevos a la canalización, la evaluación de los modelos y la desviación entre el entrenamiento y la publicación. La parte final trata sobre qué hacer cuando alcanzas una meseta. Luego, hay una lista del trabajo relacionado y un apéndice con información sobre los sistemas que se usan comúnmente como ejemplos en este documento.\nAntes de aprendizaje automático Regla n.º 1: No tengas miedo de lanzar un producto sin aprendizaje automático.\nEl aprendizaje automático es genial, pero necesita datos. En teoría, puedes obtener datos de un problema diferente y, luego, modificar un poco el modelo para un producto nuevo, pero es probable que la heurística básica no funcione como es debido. Si piensas que aprendizaje automático te brindará un aumento del 100%, entonces una heurística te permitirá alcanzar el 50% de ese camino.\nPor ejemplo, si clasificas apps en un mercado de apps, puedes usar la tasa de instalación o la cantidad de instalaciones como la heurística. Si detectas spam, filtra los editores que enviaron spam antes. No tengas miedo de usar la edición humana. Si necesitas clasificar contactos, clasifica el que se usó más recientemente como el primero (o puedes clasificarlos por orden alfabético). Si el aprendizaje automático no es indispensable para tu producto, no lo uses hasta que tengas datos.\nRegla n.º 2: Primero, diseña métricas e impleméntalas.\nAntes de establecer las tareas de tu sistema de aprendizaje automático, realiza un seguimiento del sistema actual. Debes hacerlo por las siguientes razones:\n Es más fácil obtener permiso de los usuarios del sistema al comienzo. Si consideras que algo puede llegar a ser un problema en el futuro, es mejor obtener datos históricos ahora. Si diseñas el sistema con la instrumentación de métricas en mente, las cosas irán mejorando en el futuro. En especial, si no deseas realizar búsquedas globales de strings en registros para instrumentar las métricas. Notarás qué aspectos cambian y cuáles permanecen iguales. Por ejemplo, supongamos que deseas optimizar directamente los usuarios activos por un día. Sin embargo, durante las primeras manipulaciones del sistema, es posible que notes que los cambios drásticos en la experiencia del usuario no afectan de forma notoria esta métrica.  El equipo de Google Plus mide las expansiones por lectura, las veces que se compartió el contenido por lectura, las veces que alguien indico +1 por lectura, los comentarios por lectura, los comentarios por usuario, las veces que se compartió el contenido por usuario, etc. para calcular el desempeño de una publicación durante el tiempo de publicación. También ten en cuenta que es importante el marco de trabajo de un experimento, en el que puedes agrupar usuarios y sumar estadísticas por experimento. Consulta la regla n.º 12.\nSi recopilas métricas con mayor flexibilidad, puedes lograr una imagen más amplia del sistema. ¿Notas un problema? ¡Agrega una métrica para realizar un seguimiento! ¿Te entusiasma algún cambio cuantitativo en la actualización más reciente? ¡Agrega una métrica para realizar un seguimiento!\nRegla n.º 3: Elige aprendizaje automático antes que una heurística compleja. Una heurística sencilla puede ser la clave para lanzar el producto, pero una heurística compleja no es sostenible. Una vez que tengas los datos y una idea básica de lo que intentas lograr, pasa al aprendizaje automático. Como en la mayoría de las tareas de ingeniería de software, buscas actualizar la estrategia de forma constante, ya sea con un modelo heurístico o de aprendizaje automático. Notarás que el modelo de aprendizaje automático es más fácil de actualizar y mantener (consulta la regla n.º 16).\nFase I de aprendizaje automático: Tu primera canalización Concéntrate en la infraestructura de tu sistema para la primera canalización. Si bien es divertido pensar en todo aprendizaje automático teórico que implementarás, es difícil determinar qué sucede si no usas una canalización confiable.\nRegla n.º 4: Procura que el primer modelo se mantenga simple y acierta con la infraestructura. El primer modelo proporciona el mayor impulso para tu producto, por lo que no necesita ser muy sofisticado. Sin embargo, te encontrarás con más problemas de infraestructura de lo que esperas. Antes de que alguien pueda usar tu novedoso sistema de aprendizaje automático, debes determinar lo siguiente:\n Cómo obtener ejemplos para tu algoritmo de aprendizaje. Un primer borrador para definir qué es \u0026ldquo;bueno\u0026rdquo; y qué es \u0026ldquo;malo\u0026rdquo; para tu sistema. Cómo integrar tu modelo a la aplicación. Puedes aplicar el modelo en vivo o calcularlo previamente con ejemplos sin conexión y guardar los resultados en una tabla. Por ejemplo, tal vez deseas preclasificar páginas web y almacenar los resultados en una tabla, pero deseas clasificar los mensajes de chat en vivo.  Si eliges atributos simples, es más sencillo garantizar lo siguiente:\n Que los atributos se comuniquen con tu algoritmo de aprendizaje correctamente. Que el modelo aprenda ponderaciones razonables. Que los atributos se comuniquen con tu modelo en el servidor correctamente. Si logras que el sistema cumpla con estas tres condiciones, habrás completado la mayor parte del trabajo. El modelo simple brinda métricas y un comportamiento de punto de referencia que puedes usar para probar modelos más complejos. Algunos equipos apuntan a un primer lanzamiento \u0026ldquo;neutral\u0026rdquo;: un primer lanzamiento que desprioriza de forma explícita las ganancias de aprendizaje, para no distraerse.  Regla n.º 5: Prueba la infraestructura separada de aprendizaje automático.\nAsegúrate de poder probar la infraestructura y de que las partes de aprendizaje del sistema están aisladas, para que puedas probar las partes a su alrededor. De forma específica:\n Prueba obtener datos para el algoritmo. Comprueba que se completen las columnas de atributos que deben completarse. Cuando la privacidad lo permite, inspecciona de forma manual la entrada al algoritmo de entrenamiento. Si es posible, comprueba las estadísticas en tu canalización para compararlas con las estadísticas de los mismos datos procesados en otro lugar.\n Prueba obtener modelos a partir del algoritmo de entrenamiento. Asegúrate de que el modelo de tu entorno de entrenamiento muestre el mismo resultado que el modelo del entorno de publicación (consulta la regla n.º 37).\n  El aprendizaje automático es un tanto impredecible; por lo tanto, asegúrate de probar el código para crear ejemplos en el entrenamiento y la publicación. Asegúrate también de poder cargar y usar un modelo fijo durante la publicación. Además, es importante comprender los datos: consulta Consejos prácticos para el análisis de conjuntos de datos grandes y complejos.\nRegla n.º 6: Ten cuidado con la pérdida de datos al copiar canalizaciones.\nA menudo, para crear una canalización, copiamos una existente (es decir, la programación a ciegas), y la canalización anterior pierde datos que necesitamos en la canalización nueva. Por ejemplo, la canalización para la vista Lo más interesante de Google+ pierde las publicaciones anteriores (porque intenta calificar las publicaciones nuevas). Esta canalización se copió para usar las Novedades de Google+, donde las publicaciones anteriores todavía tienen valor. Sin embargo, la canalización sigue perdiendo publicaciones antiguas. Otro patrón común es solo registrar datos que vio el usuario. Estos datos no tienen valor si queremos desarrollar un modelo sobre por qué el usuario no vio una publicación específica, ya que se perdieron todos los ejemplos negativos. Ocurrió un problema similar en Play. Durante el trabajo en la página de inicio de Play Apps, se creó una canalización nueva que también contenía ejemplos de la página de destino de Play Juegos sin ningún atributo para que quedara claro de dónde provenía cada ejemplo.\nRegla n.º 7: Convierte la heurística en atributos o gestiónalos de forma externa.\nPor lo general, los problemas que el aprendizaje automático intenta resolver no son completamente nuevos. Ya existe un sistema para la clasificación, el ordenamiento o el problema que intentes resolver. Esto significa que existe una gran cantidad de reglas y heurísticas. Estas mismas heurísticas pueden ser de gran ayuda cuando se modifican con aprendizaje automático. Debes recolectar toda la información que tengan tus heurísticas por dos razones. Primero, facilitará la transición a un sistema de aprendizaje automático. Segundo, por lo general, estas reglas contienen mucha de la intuición del sistema de la que no conviene deshacerse. Existen cuatro formas de usar una heurística existente:\n Realiza un preprocesamiento con la heurística. Si el atributo es increíblemente asombroso, entonces esto es una opción. Por ejemplo, si, en un filtro de spam, el destinatario ya está en la lista negra, no intentes volver a aprender qué significa \u0026ldquo;estar en la lista negra\u0026rdquo;. Bloquea el mensaje. Esta estrategia tiene más sentido en tareas de clasificación binaria. Crea un atributo. Crear directamente un atributo a partir de la heurística es genial. Por ejemplo, si usas una heurística para calcular la calificación de relevancia para el resultado de una consulta, puedes incluir la calificación como el valor de un atributo. Más tarde, es posible que desees usar técnicas de aprendizaje automático para adaptar el valor (por ejemplo, al convertir el valor en uno de un conjunto finito de valores discretos o al combinarlo con otros atributos), pero comienza usando los valores sin procesar que produce la heurística. Recolecta las entradas sin procesar de la heurística. Si existe una heurística para apps que combine la cantidad de instalaciones, la cantidad de caracteres en el texto y el día de la semana, considera separar estos datos y enviar estas entradas al aprendizaje de forma separada. En este caso, puedes usar algunas técnicas que usan los conjuntos (consulta la regla n.º 40). Modifica la etiqueta. Esta es una opción cuando consideras que la heurística captura información que no está contenida actualmente en la etiqueta. Por ejemplo, si intentas maximizar la cantidad de descargas, pero también deseas obtener contenido de calidad, la solución puede ser multiplicar la etiqueta por la cantidad promedio de estrellas que recibió la app. Esta opción permite mucha libertad. Consulta la sección \u0026ldquo;Tu primer objetivo\u0026rdquo;.  Ten en cuenta la mayor complejidad que implica usar heurísticas en un sistema de aprendizaje automático. El uso de heurísticas antiguas en tu algoritmo de aprendizaje automático nuevo puede contribuir a una transición sin sobresaltos, pero piensa si existe una forma más sencilla de lograr el mismo efecto.\nSupervisión En general, mantén el sistema en buen estado, como usar alertas con opciones y contar con una página de panel.\nRegla n.º 8: Conoce los requisitos de antigüedad de tu sistema.\n¿Cuánto se degrada el rendimiento si el modelo tiene un día de antigüedad? ¿Y una semana? ¿Un trimestre? Esta información te puede ayudar a establecer las prioridades de la supervisión. Si pierdes una calidad significativa de los productos cuando el modelo no se actualiza por un día, se justifica contar con un ingeniero que haga un seguimiento constante. La mayoría de los sistemas de publicación de anuncios tiene que gestionar anuncios nuevos cada día, por lo que deben actualizarse. Por ejemplo, si el modelo de AA para Búsqueda de Google Play no está actualizado, puede tener un efecto negativo en menos de un mes. Algunos modelos de la sección Lo más interesante en Google+ no tienen un identificador de publicaciones, por lo que pueden exportar estos modelos de forma poco frecuente. Otros modelos tienen identificadores que se actualizan con una frecuencia mucho mayor. Ten en cuenta que la antigüedad puede cambiar con el tiempo, en especial cuando se agregan o quitan columnas de atributos en el modelo.\nRegla n.º 9: Detecta los problemas antes de exportar los modelos. Existe una etapa en muchos sistemas de aprendizaje automático en la que exportas el modelo para la publicación. Si existe un problema con un modelo exportado, es un problema que notará el usuario.\nComprueba el estado del modelo antes de exportarlo. Específicamente, asegúrate de que el rendimiento del modelo sea consistente con los datos existentes. Si tienes problemas persistentes con los datos, no exportes el modelo. Muchos equipos que implementan continuamente modelos comprueban el área bajo la curva ROC (o AUC) antes de la exportación Los problemas sobre modelos que no se han exportado requieren de una alerta de correo electrónico; los problemas de un modelo para el usuario requieren de una página. Por lo tanto, lo mejor es esperar y estar seguro, antes de hacer algo que afecte a los usuarios.\nRegla n.º 10: Busca los fallos silenciosos. Este es un problema que ocurre con más frecuencia en los sistemas de aprendizaje automático que en otros tipos de sistemas. Supongamos que una tabla particular que se une ya no se actualiza. El sistema de aprendizaje automático se ajustará y el comportamiento continuará siendo razonablemente bueno, con una degradación paulatina. A veces, hay tablas con meses de atraso y basta con una simple actualización en lugar de otro lanzamiento para mejorar el rendimiento en ese trimestre. La cobertura de un atributo puede cambiar debido a las modificaciones en la implementación: por ejemplo, una columna de atributo puede estar completa en un 90% con ejemplos y, de pronto, reducir la cantidad de ejemplos a un 60%. Una vez, había una tabla en Play con un atraso de 6 meses. Una mera actualización aportó un aumento del 2% en la tasa de instalación. Si realizas un seguimiento de las estadísticas de los datos e inspeccionas los datos manualmente cada tanto, puedes reducir este tipo de fallos.\nRegla n.º 11: Documenta y asigna propietarios para las columnas de atributos. Si el sistema es grande y existen muchas columnas de funciones, debes saber quién creó o mantiene cada columna de atributos. Si descubres que la persona que se encarga de una columna de atributos se va, asegúrate de que alguien reciba esa información. Si bien las columnas de atributos tienen nombres descriptivos, es recomendable tener una descripción más detallada de lo que hace un atributo, de dónde proviene y cuál es la contribución esperada.\nTu primer objetivo Si bien te importan varias métricas o mediciones sobre el sistema, el algoritmo de aprendizaje automático a menudo requiere un objetivo único, un número que el algoritmo \u0026ldquo;intenta\u0026rdquo; optimizar. Hago una diferencia entre objetivos y métricas: una métrica es cualquier número que genera el sistema, que puede o no ser importante. Consulta también la regla n.º 2.\nRegla n.º 12: No pienses demasiado qué objetivo debes optimizar directamente. Quieres ganar dinero, que tus usuarios estén contentos y que el mundo sea un lugar mejor. Hay cientos de métricas para tener en cuenta y debes medirlas a todas (consulta la regla n.º 2). Sin embargo, en el comienzo del proceso de aprendizaje automático, notarás que todas aumentan, incluso aquellas que no optimizaste directamente. Por ejemplo, supongamos que te importan la cantidad de clics y el tiempo de visita en el sitio. Si optimizas la cantidad de clics, es probable que también aumente el tiempo de visita.\nPor lo tanto, hay que simplificar y no pensar demasiado en el equilibrio entre las diferentes métricas cuando puedes aumentar fácilmente todas las métricas. Tampoco fuerces esta regla, es decir, no confundas tu objetivo con el estado general del sistema (consulta la regla n.º 39). Además, si aumentas la métrica optimizada directamente, pero decides no ejecutar el sistema, es posible que debas revisar los objetivos.\nRegla n.º 13: Elige una métrica simple, observable y con atributos para tu primer objetivo. Por lo general, no sabes cuál es el verdadero objetivo. Crees que sí, pero luego, al observar los datos y comparar el sistema anterior con el nuevo sistema de aprendizaje automático, te das cuenta de que debes modificar el objetivo. Además, muchas veces, los miembros del equipo no se ponen de acuerdo con el objetivo verdadero. El objetivo del aprendizaje automático debe ser algo que sea fácil de medir y una representación del \u0026ldquo;verdadero\u0026rdquo; objetivo. De hecho, a menudo no existe un \u0026ldquo;verdadero\u0026rdquo; objetivo (consulta la regla n.º 39). Por lo tanto, implementa el entrenamiento para un objetivo sencillo de aprendizaje automático y considera contar con una \u0026ldquo;capa de políticas\u0026rdquo; que te permita agregar más lógica (con suerte, una lógica muy sencilla) para la calificación final.\nLa forma más sencilla de lograr un modelo es el comportamiento del usuario que se observa directamente y se atribuye a una acción en el sistema:\n ¿El usuario hizo clic en este vínculo clasificado? ¿El usuario descargó este objeto clasificado? ¿El usuario reenvió, respondió o envió por correo electrónico este objeto clasificado? ¿El usuario calificó este objeto clasificado? ¿El usuario denunció este objeto mostrado como spam, pornografía u ofensivo?  Al comienzo, evita los efectos indirectos del modelado:\n ¿El usuario realizó otra visita al día siguiente? ¿Cuánto tiempo duró la visita al sitio del usuario? ¿Cuáles fueron los usuarios activos por día? Los efectos indirectos logran excelentes métricas y se pueden usar en pruebas A/B y decisiones de lanzamiento.  Por último, no intentes hacer que el aprendizaje automático responda a estas preguntas:\n ¿El usuario está feliz con el producto? ¿El usuario está satisfecho con la experiencia? ¿El producto mejora el bienestar general del usuario? ¿Cómo afectará el estado general de la empresa?  Estas preguntas son importantes, pero muy difíciles de medir. En su lugar, usa representantes: si el usuario está feliz, permanecerá en el sitio por más tiempo. Si el usuario está satisfecho, volverá a visitar el sitio mañana. En cuanto al bienestar y el estado de la empresa, se requiere el criterio humano para conectar el objetivo de aprendizaje automático con la naturaleza del producto que vendes y tu plan comercial.\nRegla n.º 14: Comenzar con un modelo interpretativo facilita la depuración. La regresión lineal, la regresión logística y la regresión de Poisson están directamente relacionadas con un modelo probabilístico. Cada predicción se interpreta como una probabilidad o un valor esperado. Esto facilita la depuración, en comparación con los modelos que usan objetivos (pérdida de cero uno, diferentes pérdidas de bisagra y más) que intentan optimizar directamente el rendimiento o la precisión de la clasificación. Por ejemplo, si las probabilidades en la capacitación son diferentes de las probabilidades predichas en la comparación o la inspección del sistema de producción, es posible que esta diferencia indique un problema.\nPor ejemplo, en la regresión lineal, logística o de Poisson, existen subconjuntos de datos donde la expectativa promedio de las predicciones equivale a la etiqueta promedio (calibrado con un momento o simplemente calibrado). Esto es verdadero en líneas generales siempre y cuando no tengas una regularización y el algoritmo se haya convergido. Si tienes un atributo que es 1 o 0 para cada ejemplo, significa que el conjunto de 3 ejemplos donde el atributo es 1 está calibrado. Además, si tienes un atributo que es 1 para cada ejemplo, entonces el conjunto de todos los ejemplos está calibrado.\nCon modelos simples, es más fácil lidiar con ciclos de reacción (consulta la regla n.º 36). A menudo, usamos estas predicciones probabilísticas para tomar una decisión: por ejemplo, calificar publicaciones según el valor esperado decreciente (es decir, la probabilidad de hacer clic, descargar, etc.). Sin embargo, recuerda que cuando debes elegir qué modelo usar, la decisión importa más que la probabilidad de los datos según el modelo (consulta la regla n.° 27).\nRegla n.º 15: Separa el filtro de spam y la clasificación de calidad en una capa de política. La clasificación de calidad es un arte delicado, pero el filtro de spam es una guerra. Las señales que usas para determinar las publicaciones de calidad alta serán obvias para los usuarios de tu sistema; ellos podrán modificar sus publicaciones para que tengan estas propiedades. Además, tu clasificación de calidad debe centrarse en calificar el contenido que se publica de buena fe. No subestimes al modelo por darle una clasificación demasiado alta al spam. De forma similar, el contenido \u0026ldquo;subido de tono\u0026rdquo; debe separarse de la clasificación de calidad. El filtro de spam es una historia diferente. Es esperable que los atributos que debes generar cambiarán constantemente. A menudo, incluyes reglas obvias en el sistema (por ejemplo, si una publicación tiene más de tres votos de spam, no hay que recuperarla). Los modelos aprendidos deben actualizarse a diario o de forma más frecuente. La reputación del creador del contenido tiene un peso importante.\nEn algún nivel, el resultado de estos dos sistemas debe integrarse. Ten en cuenta que el filtro de spam en resultados de la búsqueda debe ser más agresivo que en mensajes de correo electrónico. Esto es así si no tienes ninguna regularización y el algoritmo está convergido. En general, es de este modo. Además, es una práctica estándar para quitar el spam de los datos de entrenamiento para el clasificador de calidad.\nFase II de aprendizaje automático: Ingeniería de atributos En la primera fase del ciclo de vida de un sistema de aprendizaje automático, la prioridad es mandar los datos de entrenamiento al sistema de aprendizaje, lograr instrumentar las métricas de interés y crear una infraestructura de publicación. Una vez que cuentas con un sistema integral en funcionamiento con pruebas de unidades y del sistema instrumentadas, comienza la fase II.\nEn la segunda fase, hay muchas recompensas a corto plazo. Existe una variedad de atributos obvios que se pueden agregar al sistema. Además, la segunda fase de aprendizaje automático implica agregar tantos atributos como sea posible y combinarlos de formas intuitivas. Durante esta fase, todas las métricas deben continuar subiendo. Habrá muchos lanzamientos, y es una excelente oportunidad para incorporar muchos ingenieros que puedan recopilar todos los datos que necesitas para crear un sistema de aprendizaje verdaderamente sorprendente.\nRegla n.º 16: Planifica el lanzamiento y la iteración. No esperes que el modelo en el que trabajas ahora sea el último que lanzarás o, incluso, que dejarás de lanzar modelos. Por lo tanto, ten en cuenta que la complejidad de este lanzamiento retrasará los lanzamientos futuros. Muchos equipos han lanzado uno o más modelos por trimestre durante años. Existen tres razones básicas para lanzar modelos nuevos:\nTienes atributos nuevos. Deseas ajustar la regularización y combinar atributos antiguos de formas nuevas. Deseas ajustar el objetivo. Independientemente de la razón, es recomendable poner atención en el modelo: analizar los datos que se ingresan en el ejemplo te permite encontrar señales nuevas y detectar señales antiguas con problemas. Entonces, a medida que desarrollas el modelo, piensa en lo fácil que es agregar, quitar o recombinar atributos. Piensa en lo fácil que es crear una copia nueva de la canalización y verifica que sea correcta. Piensa en si es posible ejecutar dos o tres copias en paralelo. Por último, no te preocupes si no logras incluir todos los atributos en esta versión de la canalización. Las incluirás el próximo trimestre.\nRegla n.º 17: Comienza con los atributos directamente observados e informados, en lugar de los atributos aprendidos. Este puede ser un punto controversial, pero evita muchos problemas. Primero, describamos qué es un atributo aprendido. Un atributo aprendido es un atributo generado por un sistema externo (como un sistema de agrupación en clústeres sin supervisar) o el mismo modelo (por ejemplo, mediante un modelo factorizado o aprendizaje profundo). Ambas opciones pueden ser útiles, pero tienen muchos problemas, por lo que no es conveniente incluirlas en el primer modelo.\nSi usas un sistema externo para crear un atributo, recuerda que ese sistema tiene su propio objetivo. Es posible que el objetivo del sistema externo no tenga mucha relación con tu objetivo actual. Si realizas una instantánea del sistema externo, es posible que esté desactualizada. Si actualizas los atributos desde el sistema externo, es posible que los significados cambien. Si usas un sistema externo para proporcionar un atributo, debes tener mucho cuidado con ese enfoque.\nEl principal problema de los modelos multiplicados y los modelos profundos es que no son convexos. Por lo tanto, no hay garantía de encontrar una solución óptima (o de aproximarse a esta). Además, el mínimo local de cada iteración puede ser diferente. Esta variación no permite juzgar con exactitud si el impacto de un cambio en tu sistema es relevante o contingente. Si creas un modelo sin atributos profundos, obtienes un sistema de referencia con buen rendimiento. Una vez que alcanzas este punto de referencia, puedes probar enfoques menos ortodoxos.\nRegla n.º 18: Prueba atributos de contenido que generalicen en contextos. A menudo, un sistema de aprendizaje automático es una parte pequeña de una entidad mucho más grande. Por ejemplo, si imaginas una publicación que se puede usar en Lo Más Interesante, mucha gente hará +1, la compartirá o escribirá un comentario en ella antes de que llegue a mostrarse en Lo más interesante. Si proporcionas esas estadísticas al modelo, este puede promocionar publicaciones nuevas para las que no tiene datos en el contexto que está optimizando. YouTube Watch Next puede usar la cantidad de videos que miraste, o que miraste de forma secuencial (la cantidad de veces que se miró un video después de otro) en la búsqueda de YouTube. También puedes usar clasificaciones de usuarios explícitas. Por último, si usas una acción de usuario como etiqueta, ver esa acción en el documento en un contexto diferente puede ser un excelente atributo. Todos estos atributos te permiten aportar contenido nuevo al contexto. Ten en cuenta que esto no se trata de personalización: primero, descubre si a alguien le gusta el contenido en este contexto; luego, descubre a quién le gusta más o menos.\nRegla n.º 19: Usa atributos muy específicos cuando sea posible. Gracias a la gran cantidad de datos, es más fácil aprender millones de atributos sencillos que unos pocos atributos complejos. Los identificadores de documentos que se obtienen y las consultas canónicas no brindan mucha generalización, pero alinean las clasificaciones con las etiquetas en las consultas principales. Por lo tanto, no temas agrupar atributos si cada uno se aplica a una fracción muy pequeña de datos, pero la cobertura total es de más del 90%. Puedes usar la regularización para eliminar los atributos que se aplican a pocos ejemplos.\nRegla n.º 20: Combina y modifica los atributos existentes para crear atributos nuevos de una forma legible. Hay diferentes formas de combinar y modificar atributos. Los sistemas de aprendizaje automático como TensorFlow te permiten preprocesar los datos mediante transformaciones. Los dos enfoques estándar son las \u0026ldquo;discretizaciones\u0026rdquo; y las \u0026ldquo;combinaciones\u0026rdquo;.\nLa discretización consiste en tomar un atributo continuo y crear varios atributos discretos. Considera un atributo continuo como la edad. Puedes crear un atributo que es 1 cuando la edad es menor de 18, otro atributo que es 1 cuando la edad es entre 18 y 35, etc. No pienses demasiado en los límites de estos histogramas; los cuantiles básicos serán los más eficaces.\nLas combinaciones unen dos o más columnas de atributos. En la terminología de TensorFlow, una columna de atributos es un conjunto de atributos homogéneos (p. ej., {masculino, femenino}, {EE.UU., Canadá, México}, etc.). Una combinación es una nueva columna de atributos que incluye, p. ej., {masculino, femenino} × {EE.UU., Canadá, México}. Esta nueva columna de atributos contendrá el atributo (masculino, Canadá). Si usas TensorFlow y le indicas que cree esta combinación, este atributo (masculino, Canadá) aparecerá en los ejemplos que representan canadienses masculinos. Ten en cuenta que se necesita una enorme cantidad de datos para aprender modelos con combinaciones de tres, cuatro o más columnas de atributos básicos.\nLas combinaciones que producen columnas de atributos muy grandes pueden producir un sobreajuste. Por ejemplo, imagina que haces algún tipo de búsqueda y tienes una columna de atributos con palabras en la consulta y otra con palabras en el documento. Si unes estas columnas con una combinación, terminarás con muchos atributos (consulta la regla n.º 21).\nCuando trabajas con texto, existen dos alternativas. La más rigurosa es un producto escalar. En su forma más simple, un producto escalar simplemente cuenta la cantidad de palabras en común entre la consulta y el documento. Por lo tanto, este atributo se puede discretizar. Otro enfoque es una intersección: tenemos un atributo que está presente solo si la palabra \u0026ldquo;poni\u0026rdquo; aparece en el documento y en la consulta, y otro atributo que está presente solo si la palabra \u0026ldquo;el\u0026rdquo; aparece en el documento y en la consulta.\nRegla n.º 21: La cantidad de ponderaciones de atributos que puedes aprender en un modelo lineal es casi proporcional a la cantidad de datos que tienes. Existen resultados teóricos fascinantes sobre aprendizaje estadístico relacionados con el nivel de complejidad correspondiente de un modelo, pero esta regla es todo lo que necesitas saber. Tuve discusiones con personas que ponían en duda que se pudiera aprender algo con mil ejemplos o que nunca se necesitan más de un millón de ejemplos, porque están empecinados con un determinado método de aprendizaje. La clave es escalar el aprendizaje a la medida del tamaño de los datos:\nSi trabajas con un sistema de ranking de búsquedas, existen millones de palabras diferentes en los documentos y la consulta, y tienes 1000 ejemplos etiquetados, debes usar un producto escalar entre los atributos de consultas y de documentos, TF-IDF y una media docena de otros atributos desarrollados por humanos. 1000 ejemplos, una docena de atributos. Si tienes un millón de ejemplos, intersecta la columna de atributos de consultas con la de documentos, y aplica regularización y, posiblemente, selección de atributos. Esto te brindará un millón de atributos, pero, con la regularización, tendrás menos. 10 millones de ejemplos, tal vez 100,000 atributos. Si tienes miles o cientos de miles de millones, puedes combinar las columnas de atributos con tokens de consultas y de documentos mediante la regularización y la selección de atributos. Tendrás miles de millones de ejemplos y 10 millones de atributos. La teoría de aprendizaje estadístico raramente establece límites rígidos, pero ofrece un buen punto de partida. En última instancia, usa la regla n.º 28 para decir qué atributos usarás.\nRegla n.º 22: Quita los atributos que ya no uses. Los atributos sin usar crean deuda técnica. Si descubres que no estás usando un atributo y que no sirve combinarlo con otros atributos, quítalo de la infraestructura. Debes mantener limpia tu infraestructura para que puedas probar los atributos más prometedores tan rápido como sea posible. Si es necesario, tu atributo se puede volver a agregar en cualquier momento.\nTen en cuenta la cobertura cuando analices qué atributos agregarás o conservarás. ¿Cuántos ejemplos cubre el atributo? Por ejemplo, si tienes algunos atributos de personalización, pero solo el 8% de los usuarios tiene atributos de personalización, eso no será muy eficaz.\nAl mismo tiempo, algunos atributos te sorprenden gratamente. Por ejemplo, si tienes un atributo que cubre solo el 1% de los datos, pero el 90% de los ejemplos de este atributo son positivos, es un excelente atributo que agregar.\nAnálisis humano del sistema Antes de avanzar a la tercera fase de aprendizaje automático, es importante hablar de algo que no se enseña en ninguna clase de aprendizaje automático: cómo analizar un modelo existente y mejorarlo. Esto es más un arte que una ciencia. Aun así, existen muchos antipatrones que es conveniente evitar.\nRegla n.º 23: No eres el típico usuario final. Probablemente, esta es la razón más común por la que un equipo no progresa. Si bien usar un prototipo con tu equipo o usar un prototipo en tu empresa tiene muchos beneficios, los empleados deben analizar si el rendimiento es correcto. Si bien un cambio que sea evidentemente malo no debe usarse, cualquier función que parezca lista para la producción debe probarse aún más, ya sea contratando a gente común para que responda preguntas en una plataforma de participación colectiva o mediante un experimento en vivo con usuarios reales.\nHay dos razones para ello. Estás demasiado familiarizado con el código. Es posible que busques un aspecto específico de las publicaciones, o que estés muy involucrado (p. ej., sesgo de confirmación). La segunda razón es que tu tiempo es demasiado valioso. Considera el costo de nueve ingenieros en una reunión de una hora de duración y cuántas etiquetas logradas con trabajo humano puedes obtener en una plataforma de participación colectiva.\nSi realmente quieres comentarios de usuarios, implementa metodologías de experiencia de usuario. Crea usuarios persona (puedes encontrar una descripción en el libro Sketching User Experiences [Cómo diseñar experiencias de usuario] de Bill Buxton) al comienzo del proceso y, luego, implementa una prueba de usabilidad (puedes encontrar una descripción en el libro Don’t Make Me Think [No me hagas pensar] de Steve Krug). Los usuarios persona implican crear un usuario hipotético. Por ejemplo, si tu equipo se compone solo por hombres, será beneficioso diseñar un usuario persona femenina de 35 años (completa con atributos de usuario) y observar los resultados que genera, en lugar de los 10 resultados para hombres de 25 a 40 años. También puedes lograr una nueva perspectiva al evaluar cómo reaccionan las personas reales a tu sitio (de forma local o remota) en una prueba de usabilidad.\nRegla n.º 24: Mide el delta entre los modelos. Una de las formas más sencillas y, a veces, más útiles de realizar mediciones que puedes usar antes de que los usuarios vean tu nuevo modelo es calcular la diferencia entre los nuevos resultados y los obtenidos con el sistema en producción. Por ejemplo, si tienes un problema de ranking, ejecuta ambos modelos con una muestra de consultas en todo el sistema y observa el tamaño de la diferencia simétrica de los resultados (ponderados según la posición en el ranking). Si la diferencia es muy pequeña, entonces puedes deducir que habrá poco cambio, sin necesidad de ejecutar un experimento. Si la diferencia es muy grande, entonces debes asegurarte de que el cambio sea positivo. Analizar las consultas donde la diferencia simétrica es alta te puede ayudar a comprender de forma cualitativa cómo fue el cambio. Sin embargo, asegúrate de que el sistema sea estable. Cuando compares un modelo consigo mismo, asegúrate de que tenga una diferencia simétrica baja (idealmente cero).\nRegla n.º 25: Cuando elijas un modelo, el rendimiento utilitario predomina por sobre el poder de predicción. Tu modelo puede intentar predecir la tasa de clics. Sin embargo, en última instancia, la pregunta clave es lo que haces con esa predicción. Si la usas para clasificar documentos, entonces la calidad del ranking final importa más que la predicción en sí misma. Si predices la probabilidad de que un documento sea spam y, luego, tienes un punto límite sobre lo que se bloquea, entonces la precisión de lo que se permite tiene más importancia. La mayoría de las veces, estos dos aspectos coinciden; cuando no es así, es probable que haya una pequeña ganancia. Además, si hay algún cambio que mejore la pérdida logística, pero que reduzca el rendimiento del sistema, busca otro atributo. Si esto comienza a suceder con más frecuencia, es hora de volver a evaluar el objetivo del modelo.\nRegla n.º 26: Busca patrones en los errores observados y crea atributos nuevos. Supongamos que ves un ejemplo de entrenamiento que el modelo \u0026ldquo;no entendió\u0026rdquo;. En una tarea de clasificación, este error puede ser un falso positivo o un falso negativo. En una tarea de clasificación, el error puede ser un par donde un positivo tiene un ranking menor que un negativo. El punto más importante es que sea un ejemplo que el sistema de aprendizaje automático sepa que no lo entendió y que lo corrija si tiene la oportunidad. Si le agregas un atributo al modelo para que pueda corregir el error, el modelo intentará usarlo.\nPor otro lado, si intentas crear un atributo basado en ejemplos que el sistema no considera errores, el atributo se ignorará. Por ejemplo, supongamos que, en la búsqueda de apps de Play, alguien busca \u0026ldquo;juegos gratuitos\u0026rdquo;. Supongamos que uno de los primeros resultados es una app de bromas menos relevante. Entonces, creas un atributo para \u0026ldquo;apps de bromas\u0026rdquo;. Sin embargo, si maximizas la cantidad de instalaciones y las personas instalan una app de bromas cuando buscan juegos gratuitos, el atributo para \u0026ldquo;apps de bromas\u0026rdquo; no tendrá el efecto deseado.\nUna vez que tengas ejemplos que el modelo no haya entendido, busca las tendencias que estén fuera del conjunto de atributos actual. Por ejemplo, si parece que el sistema penaliza las publicaciones más largas, agrega la longitud de la publicación. No seas demasiado específico sobre los atributos que agregues. Si agregas la longitud de la publicación, no intentes adivinar qué significa \u0026ldquo;largo\u0026rdquo;, solo agrega una decena de atributos y permite que el modelo descubra qué hacer con ellos (consulta la regla n.º 21). Esta es la forma más fácil de obtener el resultado deseado.\nRegla n.º 27: Intenta cuantificar el comportamiento no deseado que observes. Algunos miembros de tu equipo comenzarán a frustrarse con las propiedades del sistema que no les gusten, ya que la función de pérdida existente no las captura. En este punto, deben hacer lo que sea necesario para convertir sus quejas en números sólidos. Por ejemplo, si consideran que se muestran demasiadas \u0026ldquo;apps de bromas\u0026rdquo; en la búsqueda de Play, se pueden contratar a evaluadores humanos para que identifiquen las apps de bromas. (Puedes usar datos etiquetados por humanos en este caso, ya que una fracción relativamente pequeña de consultas representan una gran fracción del tráfico). Si los problemas se pueden medir, puedes comenzar a usarlos como atributos, objetivos o métricas. La regla general es \u0026ldquo;medir primero, optimizar después\u0026rdquo;.\nRegla n.º 28: Ten en cuenta que el comportamiento idéntico a corto plazo no implica un comportamiento idéntico a largo plazo. Imagina que tienes un sistema nuevo que analiza cada doc_id y exact_query, y luego calcula la probabilidad de clic para cada documento y cada consulta. Descubres que este comportamiento es casi idéntico a tu sistema actual en la comparación y la prueba A/B; por lo tanto, dado su simplicidad, lo ejecutas. Sin embargo, notas que no se muestran apps nuevas. ¿Por qué? Bueno, dado que tu sistema solo muestra un documento basado en su propio historial con esa consulta, no hay forma de aprender qué documento nuevo debe mostrar.\nLa única forma de entender cómo debe funcionar un sistema a largo plazo es entrenarlo solo con datos adquiridos cuando el modelo está publicado. Esto es muy difícil.\nDesviación entre el entrenamiento y la publicación La desviación entre el entrenamiento y la publicación es la diferencia entre el rendimiento del entrenamiento y el de la publicación. Existen diferentes razones para esta desviación:\nuna discrepancia entre cómo manipulas los datos en las canalizaciones de entrenamiento y del servidor un cambio en los datos entre el momento del entrenamiento y el del servidor un ciclo de retroalimentación entre el modelo y el algoritmo Hemos observado sistemas de aprendizaje automático de producción en Google con una desviación entre el entrenamiento y la publicación que afecta negativamente el rendimiento. La mejor solución es supervisarlo de forma explícita para que los cambios en el sistema y en los datos no generen una desviación inadvertida.\nRegla n.º 29: La mejor manera de asegurarte de que el entrenamiento se asemeja a la publicación es guardar el conjunto de atributos que usas en la publicación y canalizar esos atributos en un registro para luego usarlos en el entrenamiento. Incluso si no lo puedes hacer para cada ejemplo, hazlo para una fracción pequeña, de forma tal que puedas verificar la coherencia entre la publicación y el entrenamiento (consulta la regla n.º 37). En Google, los equipos que hicieron esta medición se sorprendieron a menudo por los resultados. La página de inicio de YouTube cambió a atributos del registro en el servidor, lo que generó mejoras de calidad importantes y redujo la complejidad del código. Ahora, muchos equipos están cambiando sus infraestructuras.\nRegla n.º 30: ¡Realiza muestras con datos con ponderación por importancia, no los quites! Cuando tienes muchos datos, es tentador usar los archivos del 1 al 12 e ignorar los archivos del 13 al 99. Esto es un error. Si bien se pueden quitar los datos que nunca se mostraron al usuario, la ponderación por importancia es la mejor opción para el resto. La ponderación por importancia implica que, si decides hacer una muestra con el ejemplo X con una probabilidad del 30%, la ponderación será de 10\u0026frasl;3. Con la ponderación por importancia, se mantienen todas las propiedades de calibración que analizamos en la regla n.º 14.\nRegla n.º 31: Ten en cuenta que, si cruzas datos de una tabla durante el entrenamiento y la publicación, los datos en la tabla pueden cambiar. Digamos que cruzas ID de documentos a una tabla que contiene atributos para esos documentos (como la cantidad de comentarios o clics). Entre el entrenamiento y el servidor, es posible que cambien los atributos en la tabla. Por lo tanto, puede cambiar predicción del modelo para el mismo documento entre el entrenamiento y la publicación. La forma más sencilla de evitar este tipo de problemas es registrar los atributos durante la publicación (consulta la regla n.º 32). Si la tabla cambia lentamente, puedes tomar una instantánea de la tabla a cada hora o cada día para obtener datos razonablemente cercanos. Ten en cuenta que esto no resuelve completamente el problema.\nRegla n.º 32: Reutiliza el código entre la canalización de entrenamiento y la canalización del servidor, siempre que sea posible. El procesamiento por lotes es diferente al procesamiento en línea. En el procesamiento en línea, debes responder cada solicitud a medida que llega (p. ej., debes realizar una búsqueda separada para cada consulta). En el procesamiento por lotes, puedes combinar tareas (p. ej., unir funciones). En la publicación, implementas el procesamiento en línea, mientras que el entrenamiento es una tarea de procesamiento por lotes. Sin embargo, hay varias formas de reutilizar el código. Por ejemplo, puedes crear un objeto que sea específico para tu sistema, donde el resultado de cualquier consulta o unión se puede almacenar de una forma legible y donde los errores se pueden probar fácilmente. Luego, una vez que hayas reunido toda la información, durante el entrenamiento o la publicación, ejecutas un método común para conectar el objeto legible específico del sistema y el formato que espera el sistema de aprendizaje automático. Esto elimina una fuente de desviación entre el entrenamiento y la publicación. Como corolario, intenta no usar dos lenguajes de programación diferentes entre el entrenamiento y la publicación. Si haces esto, será casi imposible compartir el código.\nRegla n.º 33: Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. En general, mide el rendimiento de un modelo con datos reunidos en forma posterior a aquellos con los que se ha entrenado el modelo, ya que refleja de forma más precisa qué hará el sistema en la producción. Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. El rendimiento no debería ser tan bueno en los datos nuevos, pero no debería ser mucho peor. Como puede haber efectos diarios, es posible que no predigas la tasa de clics promedio o la tasa de conversión, pero el área bajo la curva, que representa la posibilidad de darle una puntuación más alta al ejemplo positivo que al ejemplo negativo, debería ser razonablemente parecida.\nRegla n.º 34: En la clasificación binaria para filtrado (como la detección de spam o la identificación de correos electrónicos de interés), realiza pequeños sacrificios a corto plazo en el rendimiento para lograr datos más claros. En la tarea de filtrado, los ejemplos que se marcan como negativos no se muestran al usuario. Supongamos que tienes un filtro que bloquea el 75% de los ejemplos negativos durante la publicación. Puede surgir la tentación de obtener más datos de entrenamiento a partir de las instancias que se muestran a los usuarios. Por ejemplo, si un usuario marca como spam un correo electrónico que permitió tu filtro, se puede aprender de esta acción.\nPero este enfoque introduce un sesgo en la muestra. Puedes obtener datos más claros si etiquetas el 1% de todo el tráfico como \u0026ldquo;retenido\u0026rdquo; durante la publicación y envías todos los ejemplos retenidos al usuario. Ahora, el filtro bloqueará al menos el 74% de los ejemplos negativos. Los ejemplos retenidos se convertirán en los datos de entrenamiento.\nSi el filtro bloquea el 95% o más de los ejemplos negativos, este enfoque se hace menos viable. Aun así, si deseas medir el rendimiento en la publicación, puedes hacer una muestra todavía más pequeña (p. ej., 0.1% o 0.001%). Diez mil ejemplos son suficientes para estimar el rendimiento de forma precisa.\nRegla n.º 35: Ten en cuenta la desviación inherente a los problemas de ranking. Si cambias el algoritmo de clasificación lo suficiente como para ver resultados diferentes, habrás logrado modificar los datos que el algoritmo verá en el futuro. Este tipo de desviación aparecerá, y debes tenerla en cuenta al diseñar el modelo. Existen varias estrategias diferentes que sirven para favorecer los datos que tu modelo ya vio.\nPermite tener una regularización más alta en los atributos que cubren más consultas, a diferencia de esos atributos que solo abarcan una consulta. De esta forma, el modelo favorecerá los atributos que son específicos a una o pocas consultas por sobre los atributos que se generalizan a todas las consultas. Esta estrategia permite evitar que los resultados muy populares acaben en consultas irrelevantes. Este enfoque es contrario a la sugerencia más tradicional de contar con más regularización en columnas de funciones con más valores únicos. Permite que los atributos solo tengan ponderaciones positivas. Además, cualquier atributo bueno será mejor que uno \u0026ldquo;desconocido\u0026rdquo;. No uses atributos asociados solo al documentos. Esto es una versión extrema de la regla n.º 1. Por ejemplo, incluso si una app determinada es una descarga popular, más allá de la consulta, no es necesario mostrarla en todos lados. Esto se simplifica al no tener atributos solo de documentos. La razón por la que no deseas mostrar una app popular específica en todos lados está relacionada con la importancia de lograr que las apps que deseas estén disponibles. Por ejemplo, si alguien busca \u0026ldquo;apps para observar pájaros\u0026rdquo;, es posible que descarguen \u0026ldquo;Angry birds\u0026rdquo;, pero de forma claramente accidental. Si se muestra esta app, es posible que mejore la tasa de descarga, pero no se cumplen con las necesidades del usuario. Regla n.º 36: Evita los ciclos de retroalimentación con atributos posicionales. La posición del contenido afecta enormemente la probabilidad de que el usuario interactúe con este. Si ubicas una app en la primera posición, se seleccionará con más frecuencia y te dará la impresión de que es más probable que se seleccione. Una forma de lidiar con eso es agregar atributos de posición, es decir, atributos sobre la posición del contenido en la página. Entrena el modelo con atributos de posición para que aprenda a darle una mayor ponderación al atributo \u0026ldquo;primera posición\u0026rdquo;, por ejemplo. Así, el modelo les asigna una ponderación menor a otros factores con ejemplos de \u0026ldquo;primera posición=verdadero\u0026rdquo;. Entonces, en la publicación, no asignas ninguna instancia al atributo de posición (o le asignas el mismo atributo predeterminado), porque calificas candidatos antes de que hayas decidido el orden en el que se mostrarán.\nTen en cuenta que es importante mantener cualquier atributo de posición separado del resto del modelo, debido a la asimetría entre el entrenamiento y la prueba. Lo ideal es que el modelo sea la suma de una función de los atributos posicionales y una función del resto de atributos. Por ejemplo, no cruces los atributos de posición con cualquier atributo de documentos.\nRegla n.º 37: Mide la desviación entre el entrenamiento y la publicación. En el sentido más general, existen diversas razones para la desviación. Además, se puede dividir en varias partes:\nLa diferencia entre el rendimiento en los datos de entrenamiento y los datos retenidos. En general, esta diferencia siempre existe y no siempre es negativa. La diferencia entre el rendimiento en los datos retenidos y los datos \u0026ldquo;del día siguiente\u0026rdquo;. De nuevo, esta diferencia siempre existe. Debes ajustar la regularización para maximizar el rendimiento del día siguiente. Sin embargo, caídas notables en el rendimiento entre los datos retenidos y los datos del día siguiente pueden ser un indicador de que algunos atributos dependen del tiempo y posiblemente afecten al rendimiento del modelo de forma negativa. La diferencia entre el rendimiento en los datos del día siguiente y los datos en vivo. Si aplicas un modelo a un ejemplo en los datos de entrenamiento y el mismo ejemplo en la publicación, deberías obtener el mismo resultado (consulta la regla n.º 5). Por lo tanto, si aparece una discrepancia, probablemente indique un error de ingeniería. Fase III de aprendizaje automático: Crecimiento reducido, refinamiento de la optimización y modelos complejos Existen ciertos indicios de que la segunda fase llega a su fin. Primero, las ganancias mensuales comienzan a disminuir. Las métricas comenzarán a emparejarse: en algunos experimentos, verás que algunas aumentan y otras disminuyen. Este es un momento interesante. Dado que las ganancias son más difíciles de obtener, el aprendizaje automático debe sofisticarse aún más. Una advertencia: esta sección contiene más reglas especulativas que las secciones anteriores. Hemos visto a muchos equipos realizar grandes progresos en la Fase I y la Fase II de aprendizaje automático. Una vez que alcanzan la Fase III, los equipos deben buscar su propio camino.\nRegla n.º 38: No pierdas tiempo en nuevos atributos si los objetivos no alineados son un problema. Cuando las mediciones comienzan a estabilizarse, el equipo comenzará a buscar problemas fuera del alcance de los objetivos de tu sistema de aprendizaje automático actual. Como indicamos anteriormente, si el objetivo algorítmico existente no abarca los propósitos del producto, debes cambiar el objetivo o los propósitos. Por ejemplo, puedes optimizar clics, +1 o descargas, pero toma las decisiones de lanzamiento según los evaluadores humanos.\nRegla n.º 39: Las decisiones de lanzamiento representan los objetivos a largo plazo del producto. Alice tiene una idea para reducir la pérdida logística de las instalaciones predichas. Agrega un atributo. Se reduce la pérdida logística. Cuando hace un experimento en vivo, observa un aumento en la tasa de instalación. Sin embargo, cuando realiza una reunión para evaluar el lanzamiento, alguien menciona que la cantidad de usuarios activos diarios cayó un 5%. El equipo decide no lanzar el modelo. Alice está decepcionada, pero ahora se da cuenta de que las decisiones de lanzamiento dependen de varios factores y solo algunos de ellos se pueden optimizar directamente con AA.\nLa verdad es que el mundo real no es un juego de rol; no hay \u0026ldquo;puntos de ataque\u0026rdquo; que indican el estado de tu producto. El equipo debe usar las estadísticas que reúne para intentar predecir de forma eficaz qué tan bueno será el sistema en el futuro. Deben preocuparse por la participación, el número de usuarios activos en un día (DAU), el número de usuarios activos en 30 días (30 DAU), la ganancia y el retorno de la inversión del anunciante. Estas métricas que se miden en las pruebas A/B representan los objetivos a largo plazo: satisfacer a los usuarios, aumentar la cantidad de usuarios, satisfacer a los socios y obtener ganancias. A su vez, puedes considerar estos propósitos como representantes de otros propósitos: lograr un producto útil y de calidad, y que la empresa prospere de aquí a cinco años.\nLas únicas decisiones de lanzamiento fáciles son cuando todas las métricas mejoran (o al menos no empeoran). Si el equipo puede elegir entre un algoritmo de aprendizaje automático sofisticado o una simple heurística (que funciona mejor en todas las métricas), debe elegir la heurística. Además, no existe una clasificación explícita para todos los valores de métricas posibles. En especial, considera estos dos escenarios siguientes:\nExperimento Usuarios activos por día Ganancia/día A 1 millón $4 millones B 2 millones $2 millones Si el sistema actual es A, entonces es poco probable que el equipo cambie a B. Si el sistema actual es B, entonces es poco probable que el equipo cambie a A. Esto parece contradecir el comportamiento racional; sin embargo, las predicciones de las métricas cambiantes pueden o no cumplirse. Por lo tanto, cada cambio conlleva un riesgo grande. Cada métrica abarca una cierta cantidad de riesgo que preocupa al equipo.\nAdemás, ninguna métrica representa la preocupación máxima del equipo, \u0026ldquo;¿dónde estará mi producto de aquí a cinco años?\u0026rdquo;.\nPor otro lado, las personas tienden a favorecer un objetivo que pueden optimizar directamente. La mayoría de las herramientas de aprendizaje automático favorecen dicho entorno. Un ingeniero agregando atributos nuevos puede lograr un flujo constante de lanzamiento en dicho entorno. Existe un tipo de aprendizaje automático, el aprendizaje de multiobjetivo, que comienza a resolver este problema. Por ejemplo, se puede formular un problema de satisfacción de restricciones con límites inferiores en cada métrica y optimiza alguna combinación lineal de las métricas. Pero, aun así, no todas las métricas se enmarcan fácilmente como objetivos de aprendizaje automático: si el usuario hace clic en un documento o instala una app, se debe a que se mostró el contenido. Pero es mucho más difícil determinar la razón por la que un usuario visita tu sitio. Cómo predecir el éxito futuro de un sitio de forma integral depende IA-completo: es tan difícil como la visión por computadora o el procesamiento de lenguajes naturales.\nRegla n.º 40: Mantén las combinaciones simples. Los modelos unificados que aceptan atributos sin procesar y clasifican contenido directamente son los modelos más sencillos de depurar y comprender. Sin embargo, una combinación de modelos (un modelo que combina los resultados de otros modelos) puede funcionar mejor. Para mantener las cosas simples, cada modelo debe ser una combinación de modelos que solo acepta como entrada el resultado de otros modelos o un modelo básico que acepta muchos atributos, pero no ambos. Si tienes modelos sobre otros modelos que se entrenan de forma separada y los combinas, se puede generar un comportamiento erróneo.\nUsa un modelo simple como combinación, que solo acepte los resultados de los modelos \u0026ldquo;básicos\u0026rdquo; como entradas. También debes implementar propiedades en esos modelos de conjuntos. Por ejemplo, un aumento en el resultado generado por un modelo básico no debe reducir el resultado del combinado. Además, es mejor que los modelos entrantes se puedan interpretar de forma semántica (p. ej., calibrados), para que los cambios en los modelos subyacentes no confundan al modelo combinado. Asegúrate también que un aumento en la probabilidad predicha de un clasificador subyacente no reduzca la probabilidad predicha del conjunto.\nRegla n.º 41: Cuando el rendimiento se estanque, busca nuevas fuentes de información de forma cualitativa para agregar, en lugar de refinar las señales existentes. Agregaste cierta información demográfica sobre el usuario. Agregaste cierta información sobre las palabras en el documento. Finalizaste la exploración de combinación de atributos y ajustaste la regularización. No observaste un lanzamiento con más de un 1% de mejora en las métricas clave, en varios trimestres. ¿Ahora qué?\nEs hora de desarrollar la infraestructura para atributos radicalmente diferentes, como el historial de los documentos a los que este usuario accedió en el último día, semana o año, o los datos de una propiedad diferente. Usa entidades de wikidatos o algún recurso interno de tu empresa (como el Gráfico de conocimiento de Google). Usa el aprendizaje profundo. Comienza a ajustar tus expectativas sobre el retorno de la inversión esperado y aumenta tus esfuerzos adecuadamente. Como en cualquier proyecto de ingeniería, debes comparar el beneficio de agregar nuevos atributos con el costo de una mayor complejidad.\nRegla n.º 42: No esperes que la diversidad, la personalización o la relevancia se correlacionen con la popularidad. La diversidad en un conjunto de contenidos puede significar muchas cosas; la más común es la diversidad de la fuente del contenido. La personalización implica que cada usuario obtiene sus propios resultados. La relevancia implica que los resultados para una consulta específica son más apropiados para esa consulta que para otra. Además, por definición, estas tres propiedades se diferencian de lo común.\nEl problema es que lo común tiende a ser difícil de superar.\nTen en cuenta que tu sistema mide clics, el tiempo dedicado, reproducciones, +1, veces que se comparte el contenido, etc., es decir, la popularidad del contenido. A veces, los equipos intentan aprender un modelo personal con diversidad. Para implementar la personalización, agregan atributos que le permiten al sistema la personalización (algunos atributos que representan el interés del usuario) o la diversificación (atributos que indican si este documento tiene algún atributo en común con otros documentos de los resultados, como el autor o el contenido), y descubren que esos atributos obtienen una ponderación menor (o a veces, un signo diferente) al que esperaban.\nEsto no significa que la diversidad, la personalización o la relevancia no sean valiosas. Como se indicó en la regla anterior, puedes hacer un posprocesamiento para aumentar la diversidad o la relevancia. Si observas que los objetivos a largo plazo aumentan, puedes declarar que la diversidad o la relevancia son valiosas, más allá de la popularidad. Puedes continuar usando el posprocesamiento o directamente modificar el objetivo según la diversidad o la relevancia.\nRegla n.º 43: Tus amigos tienden a ser los mismos en diferentes productos. No así tus intereses. Varios equipos en Google ganaron mucho terreno al tomar que un modelo que predice la cercanía de una conexión con un producto y lograr que funcione en otro producto. Tus amigos no cambian. Por otro lado, observé a muchos equipos lidiar con atributos de personalización entre productos. Sí, parece que debería funcionar. Por ahora, parece que no. Un método que a veces funciona es usar los datos sin procesar de una propiedad para predecir el comportamiento en otra. Ten en cuenta que también puede ayudar saber que un usuario tiene un historial en otra propiedad. Por ejemplo, la presencia de actividad de usuario en dos propiedades puede ser un indicativo en sí misma.\nTrabajo relacionado Existen muchos documentos sobre aprendizaje automático en Google y en otras fuentes.\nCurso intensivo de aprendizaje automático: Una introducción al aprendizaje automático aplicado. Aprendizaje automático: Un enfoque probabilístico de Kevin Murphy, para comprender el campo de aprendizaje automático. Consejos prácticos para el análisis de conjuntos de datos grandes y complejos: Un enfoque de ciencia de datos sobre los conjuntos de datos. Aprendizaje profundo de Ian Goodfellow et al, para aprendizaje de modelos no lineales. Documento de Google sobre la deuda técnica, con muchos consejos generales. Documentación de Tensorflow. Agradecimientos Agradezco a David Westbrook, Peter Brandt, Samuel Ieong, Chenyu Zhao, Li Wei, Michalis Potamias, Evan Rosen, Barry Rosenberg, Christine Robson, James Pine, Tal Shaked, Tushar Chandra, Mustafa Ispir, Jeremiah Harmsen, Konstantinos Katsiapis, Glen Anderson, Dan Duckworth, Shishir Birmiwal, Gal Elidan, Su Lin Wu, Jaihui Liu, Fernando Pereira y Hrishikesh Aradhye por las numerosas correcciones, sugerencias y ejemplos útiles para este documento. Agradezco también a Kristen Lefevre, Suddha Basu y Chris Berg quienes colaboraron en una versión anterior. Cualquier error, omisión u opiniones controversiales es mi responsabilidad.\nAnexo Existen diferentes referencias a productos de Google en este documento. Para proporcionar más contexto, agregué una descripción breve de los ejemplos más comunes a continuación.\nDescripción de YouTube YouTube es un servicio de transmisión de videos. Tanto los equipos de YouTube Watch Next y de la página principal de YouTube usan modelos de AA para clasificar recomendaciones de videos. Watch Next recomienda videos para ver después del que se está reproduciendo, la página principal recomienda videos para los usuarios que exploran la página principal.\nDescripción general de Google Play Google Play tiene muchos modelos para resolver diferentes problemas. Las búsqueda de apps en Play, las recomendaciones personalizadas en la página principal de Play y \u0026ldquo;Otros usuarios también instalaron\u0026rdquo; usan aprendizaje automático.\nDescripción de Google Plus Google Plus usa aprendizaje automático en diferentes situaciones: clasificar las publicaciones en la sección \u0026ldquo;Novedades\u0026rdquo; que ve el usuario, las publicaciones en la sección \u0026ldquo;Lo más interesante\u0026rdquo; (las publicaciones que son más populares), las personas que conoces, etc.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"a1cf54ca301882fe30ad5cf0e94a0024","permalink":"https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-0-reglas1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/data-science/intro-machine-learning/ml101-0-reglas1/","section":"courses","summary":"El objetivo de este documento es ayudar a aquellas personas con un conocimiento básico sobre aprendizaje automático a aprovechar las recomendaciones de Google para aprendizaje automático. Presenta un estilo para aprendizaje automático, similar a la guía de estilo de Google para C++ y otras guías populares para la programación práctica. Si tomaste clases de aprendizaje automático, desarrollaste un modelo de aprendizaje automático o trabajaste en uno, tienes el conocimiento necesario para leer este documento.","tags":null,"title":"Reglas del aprendizaje automático - Fase I","type":"docs"},{"authors":null,"categories":null,"content":" Introducción El objetivo de la primera parte de este libro es ponerlo al día con las herramientas básicas de __ exploración de datos__ lo más rápido posible. La exploración de datos es el arte de mirar sus datos, generar hipótesis rápidamente, probarlas rápidamente y luego repetir una y otra vez. El objetivo de la exploración de datos es generar muchos clientes potenciales prometedores que luego puede explorar con mayor profundidad.\nEn esta parte del libro aprenderá algunas herramientas útiles que tienen una recompensa inmediata:\n La visualización es un excelente lugar para comenzar con la programación R, porque el la recompensa es muy clara: puedes hacer tramas elegantes e informativas que ayudan Entiendes los datos. En [visualización de datos] te sumergirás en la visualización,aprender la estructura básica de un diagrama de ggplot2 y técnicas poderosas para convirtiendo datos en tramas.\n La visualización por sí sola no suele ser suficiente, por lo que en [transformación de datos] aprenderá los verbos clave que le permiten seleccionar variables importantes, filtrar observaciones clave, crear nuevas variables y calcular resúmenes.  Finalmente, en [análisis exploratorio de datos], combinará la visualización y transformación con tu curiosidad y escepticismo para preguntar y responder Preguntas interesantes sobre los datos.\n  El modelado es una parte importante del proceso exploratorio, pero aún no tiene las habilidades para aprenderlo o aplicarlo de manera efectiva. Volveremos a ello en modelado, una vez que esté mejor equipado con más herramientas de programación y disputas de datos.\nEntre estos tres capítulos que le enseñan las herramientas de exploración hay tres capítulos que se centran en su flujo de trabajo de R. En [workflow: basics], [workflow: scripts] y [workflow: projects] aprenderá buenas prácticas para escribir y organizar su código R. Estos lo prepararán para el éxito a largo plazo, ya que le brindarán las herramientas para mantenerse organizado cuando aborde proyectos reales.\n","date":1568502000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"833ae4ca9dc2d32a9fe6d327cc3e5417","permalink":"https://www.marcusrb.com/en/courses/r-studio/advanced-r/r201-program/","publishdate":"2019-09-15T00:00:00+01:00","relpermalink":"/en/courses/r-studio/advanced-r/r201-program/","section":"courses","summary":"Introducción El objetivo de la primera parte de este libro es ponerlo al día con las herramientas básicas de __ exploración de datos__ lo más rápido posible. La exploración de datos es el arte de mirar sus datos, generar hipótesis rápidamente, probarlas rápidamente y luego repetir una y otra vez. El objetivo de la exploración de datos es generar muchos clientes potenciales prometedores que luego puede explorar con mayor profundidad.","tags":null,"title":"Programa R para Data Science","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568070000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"65e20c5d65d3860ed0bfe80e0e2b380f","permalink":"https://www.marcusrb.com/en/courses/python/py101/py101-2-funciones/","publishdate":"2019-09-10T00:00:00+01:00","relpermalink":"/en/courses/python/py101/py101-2-funciones/","section":"courses","summary":"","tags":null,"title":"Python 101 - Funciones","type":"docs"},{"authors":null,"categories":null,"content":" Introducción Ahora que sabe cómo acceder y examinar un conjunto de datos, ¡está listo para escribir su primera consulta SQL! Como pronto verá, las consultas SQL lo ayudarán a clasificar un conjunto de datos masivo, para recuperar solo la información que necesita.\nComenzaremos usando las palabras clave SELECT, FROM y WHERE para obtener datos de columnas específicas según las condiciones que especifique.\nPara mayor claridad, trabajaremos con un pequeño conjunto de datos imaginario pet_records que contiene solo una tabla, llamada mascotas pets.\n   ID Name Animal     1 Dr. Harris Rabbit   2 Moon Dog   3 Ripley Cat   4 Tom Cat    SELECT\u0026hellip;.FROM La consulta SQL más básica selecciona una sola columna de una sola tabla. Para hacer esto,\n especifique la columna que desea después de la palabra SELECT, y luego especifique la tabla después de la palabra FROM. Por ejemplo, para seleccionar la columna Name (de la tabla de mascotas pets en la base de datos pet_records en el proyecto bigquery-public-data), nuestra consulta aparecerá de la siguiente manera:  Tenga en cuenta que al escribir una consulta SQL, el argumento que pasamos a FROM no está entre comillas simples o dobles (\u0026lsquo;o \u0026ldquo;). Está en comillas invertidas (`).\nWHERE \u0026hellip; Los conjuntos de datos de BigQuery son grandes, por lo que generalmente querrá devolver solo las filas que cumplan condiciones específicas. Puede hacerlo utilizando la cláusula WHERE.\nLa consulta a continuación devuelve las entradas de la columna Nombre Name que están en filas donde la columna Animal tiene el texto \u0026lsquo;Cat\u0026rsquo;.\nEjemplo: ¿Cuáles son todas las ciudades de EE. UU. En el conjunto de datos OpenAQ? Ahora que ya tiene lo básico, veamos un ejemplo con un conjunto de datos real. Utilizaremos un conjunto de datos OpenAQ sobre la calidad del aire.\nPrimero, configuraremos todo lo que necesitamos para ejecutar consultas y echar un vistazo rápido a las tablas que hay en nuestra base de datos.\nfrom google.cloud import bigquery # Create a \u0026quot;Client\u0026quot; object client = bigquery.Client() # Construct a reference to the \u0026quot;openaq\u0026quot; dataset dataset_ref = client.dataset(\u0026quot;openaq\u0026quot;, project=\u0026quot;bigquery-public-data\u0026quot;) # API request - fetch the dataset dataset = client.get_dataset(dataset_ref) # List all the tables in the \u0026quot;openaq\u0026quot; dataset tables = list(client.list_tables(dataset)) # Print names of all tables in the dataset (there's only one!) for table in tables: print(table.table_id)  El conjunto de datos contiene solo una tabla, llamada global_air_quality. Buscaremos la tabla y echaremos un vistazo a las primeras filas para ver qué tipo de datos contiene.\n# Construct a reference to the \u0026quot;global_air_quality\u0026quot; table table_ref = dataset_ref.table(\u0026quot;global_air_quality\u0026quot;) # API request - fetch the table table = client.get_table(table_ref) # Preview the first five lines of the \u0026quot;global_air_quality\u0026quot; table client.list_rows(table, max_results=5).to_dataframe()  ¡Todo se ve bien! Entonces, hagamos una consulta. Supongamos que queremos seleccionar todos los valores de la columna de la ciudad city que están en filas donde la columna del país country es \u0026lsquo;US\u0026rsquo; (Para \u0026ldquo;Estados Unidos\u0026rdquo;).\n# Query to select all the items from the \u0026quot;city\u0026quot; column where the \u0026quot;country\u0026quot; column is 'US' query = \u0026quot;\u0026quot;\u0026quot; SELECT city FROM `bigquery-public-data.openaq.global_air_quality` WHERE country = 'US' \u0026quot;\u0026quot;\u0026quot;  Tómese el tiempo ahora para asegurarse de que esta consulta se alinee con lo que aprendió anteriormente.\nEnviando la consulta al conjunto de datos¶ Estamos listos para usar esta consulta para obtener información del conjunto de datos OpenAQ. Como en el tutorial anterior, el primer paso es crear un objeto Client.\n# Create a \u0026quot;Client\u0026quot; object client = bigquery.Client()  Comenzamos configurando la consulta con el método query(). Ejecutamos el método con los parámetros predeterminados, pero este método también nos permite especificar configuraciones más complicadas sobre las que puede leer en la documentación. Volveremos sobre esto más tarde.\n# Set up the query query_job = client.query(query)  A continuación, ejecutamos la consulta y convertimos los resultados en un DataFrame de pandas.\n# API request - run the query, and return a pandas DataFrame us_cities = query_job.to_dataframe()  Ahora tenemos un DataFrame de pandas llamado us_cities, que podemos usar como cualquier otro DataFrame.\n# What five cities have the most measurements? us_cities.city.value_counts().head()  ","date":1568070000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"a30a1d5da59cf10016667bbeb69d3bfc","permalink":"https://www.marcusrb.com/en/courses/business-analytics/intro-sql/sql101-1-select/","publishdate":"2019-09-10T00:00:00+01:00","relpermalink":"/en/courses/business-analytics/intro-sql/sql101-1-select/","section":"courses","summary":"Introducción Ahora que sabe cómo acceder y examinar un conjunto de datos, ¡está listo para escribir su primera consulta SQL! Como pronto verá, las consultas SQL lo ayudarán a clasificar un conjunto de datos masivo, para recuperar solo la información que necesita.\nComenzaremos usando las palabras clave SELECT, FROM y WHERE para obtener datos de columnas específicas según las condiciones que especifique.\nPara mayor claridad, trabajaremos con un pequeño conjunto de datos imaginario pet_records que contiene solo una tabla, llamada mascotas pets.","tags":null,"title":"Select, From \u0026 Where","type":"docs"},{"authors":null,"categories":null,"content":" R es un programa modular. Hay muchas funciones disponibles en la distribución estándar (consulte Instalación de R), pero se pueden agregar muchas más gracias a paquetes y complementos adicionales.\nInstalación La forma más fácil de instalar paquetes, con una conexión a Internet activa, es a través del menú R: Paquetes → Instalar paquetes.\nA veces, el funcionamiento de un paquete depende de la presencia de otros paquetes (dependencias). Para asegurarse de instalar tanto el paquete como las dependencias, es preferible usar el comando:\ninstall.packages(\u0026quot;Rcmdr\u0026quot;, dependencies = TRUE)  Uso Los paquetes adicionales no se activan automáticamente cuando se abre R, pero deben \u0026ldquo;llamarse\u0026rdquo; con la biblioteca de comandos (). Por ejemplo, para usar RCommander:\nlibrary(Rcmdr)  Al llamar a un paquete, si faltan las dependencias (cualquier paquete del que dependa), se abrirá un cuadro de diálogo que le permitirá descargarlo e instalarlo automáticamente.\nTambién se pueden llamar paquetes desde el menú de la consola: Paquetes → Cargar paquete.\nActualización Periódicamente, es necesario verificar la existencia de versiones más actualizadas de los paquetes R. Es preferible ejecutar este procedimiento desde la ventana R, y haber cerrado RCommander, que a su vez debe actualizarse.\nPara actualizar paquetes en Windows, debe ejecutar R desde los administradores.\nEl comando para actualizar los paquetes está en el menú Paquetes (Actualizar paquetes \u0026hellip;), y el procedimiento es completamente automático: solo siga las instrucciones en pantalla.\nTambién es posible actualizar los paquetes escribiendo el comando\nupdate.packages()  Comandos principales Para saber qué paquetes están instalados en su sistema:\nlibrary()  Para cargar la ayuda del paquete:\nhelp(package)  Para eliminar el paquete del sistema\nremove.packages( \u0026quot;\\_nombre_paquete\\_\u0026quot;)  Para averiguar dónde está instalado un archivo de paquete:\nsystem.file(package = \u0026quot;nombre_paquete\u0026quot;)  Para aprender más\nTodos los paquetes disponibles\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"fdfdcd0973cbdbeddc302670a47c3034","permalink":"https://www.marcusrb.com/en/courses/r-studio/paquetes/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/r-studio/paquetes/","section":"courses","summary":"R es un programa modular. Hay muchas funciones disponibles en la distribución estándar (consulte Instalación de R), pero se pueden agregar muchas más gracias a paquetes y complementos adicionales.\nInstalación La forma más fácil de instalar paquetes, con una conexión a Internet activa, es a través del menú R: Paquetes → Instalar paquetes.\nA veces, el funcionamiento de un paquete depende de la presencia de otros paquetes (dependencias). Para asegurarse de instalar tanto el paquete como las dependencias, es preferible usar el comando:","tags":null,"title":"Paquetes","type":"docs"},{"authors":null,"categories":null,"content":" Fase II de aprendizaje automático: Ingeniería de atributos En la primera fase del ciclo de vida de un sistema de aprendizaje automático, la prioridad es mandar los datos de entrenamiento al sistema de aprendizaje, lograr instrumentar las métricas de interés y crear una infraestructura de publicación. Una vez que cuentas con un sistema integral en funcionamiento con pruebas de unidades y del sistema instrumentadas, comienza la fase II.\nEn la segunda fase, hay muchas recompensas a corto plazo. Existe una variedad de atributos obvios que se pueden agregar al sistema. Además, la segunda fase de aprendizaje automático implica agregar tantos atributos como sea posible y combinarlos de formas intuitivas. Durante esta fase, todas las métricas deben continuar subiendo. Habrá muchos lanzamientos, y es una excelente oportunidad para incorporar muchos ingenieros que puedan recopilar todos los datos que necesitas para crear un sistema de aprendizaje verdaderamente sorprendente.\nRegla n.º 16: Planifica el lanzamiento y la iteración. No esperes que el modelo en el que trabajas ahora sea el último que lanzarás o, incluso, que dejarás de lanzar modelos. Por lo tanto, ten en cuenta que la complejidad de este lanzamiento retrasará los lanzamientos futuros. Muchos equipos han lanzado uno o más modelos por trimestre durante años. Existen tres razones básicas para lanzar modelos nuevos:\nTienes atributos nuevos. Deseas ajustar la regularización y combinar atributos antiguos de formas nuevas. Deseas ajustar el objetivo. Independientemente de la razón, es recomendable poner atención en el modelo: analizar los datos que se ingresan en el ejemplo te permite encontrar señales nuevas y detectar señales antiguas con problemas. Entonces, a medida que desarrollas el modelo, piensa en lo fácil que es agregar, quitar o recombinar atributos. Piensa en lo fácil que es crear una copia nueva de la canalización y verifica que sea correcta. Piensa en si es posible ejecutar dos o tres copias en paralelo. Por último, no te preocupes si no logras incluir todos los atributos en esta versión de la canalización. Las incluirás el próximo trimestre.\nRegla n.º 17: Comienza con los atributos directamente observados e informados, en lugar de los atributos aprendidos. Este puede ser un punto controversial, pero evita muchos problemas. Primero, describamos qué es un atributo aprendido. Un atributo aprendido es un atributo generado por un sistema externo (como un sistema de agrupación en clústeres sin supervisar) o el mismo modelo (por ejemplo, mediante un modelo factorizado o aprendizaje profundo). Ambas opciones pueden ser útiles, pero tienen muchos problemas, por lo que no es conveniente incluirlas en el primer modelo.\nSi usas un sistema externo para crear un atributo, recuerda que ese sistema tiene su propio objetivo. Es posible que el objetivo del sistema externo no tenga mucha relación con tu objetivo actual. Si realizas una instantánea del sistema externo, es posible que esté desactualizada. Si actualizas los atributos desde el sistema externo, es posible que los significados cambien. Si usas un sistema externo para proporcionar un atributo, debes tener mucho cuidado con ese enfoque.\nEl principal problema de los modelos multiplicados y los modelos profundos es que no son convexos. Por lo tanto, no hay garantía de encontrar una solución óptima (o de aproximarse a esta). Además, el mínimo local de cada iteración puede ser diferente. Esta variación no permite juzgar con exactitud si el impacto de un cambio en tu sistema es relevante o contingente. Si creas un modelo sin atributos profundos, obtienes un sistema de referencia con buen rendimiento. Una vez que alcanzas este punto de referencia, puedes probar enfoques menos ortodoxos.\nRegla n.º 18: Prueba atributos de contenido que generalicen en contextos. A menudo, un sistema de aprendizaje automático es una parte pequeña de una entidad mucho más grande. Por ejemplo, si imaginas una publicación que se puede usar en Lo Más Interesante, mucha gente hará +1, la compartirá o escribirá un comentario en ella antes de que llegue a mostrarse en Lo más interesante. Si proporcionas esas estadísticas al modelo, este puede promocionar publicaciones nuevas para las que no tiene datos en el contexto que está optimizando. YouTube Watch Next puede usar la cantidad de videos que miraste, o que miraste de forma secuencial (la cantidad de veces que se miró un video después de otro) en la búsqueda de YouTube. También puedes usar clasificaciones de usuarios explícitas. Por último, si usas una acción de usuario como etiqueta, ver esa acción en el documento en un contexto diferente puede ser un excelente atributo. Todos estos atributos te permiten aportar contenido nuevo al contexto. Ten en cuenta que esto no se trata de personalización: primero, descubre si a alguien le gusta el contenido en este contexto; luego, descubre a quién le gusta más o menos.\nRegla n.º 19: Usa atributos muy específicos cuando sea posible. Gracias a la gran cantidad de datos, es más fácil aprender millones de atributos sencillos que unos pocos atributos complejos. Los identificadores de documentos que se obtienen y las consultas canónicas no brindan mucha generalización, pero alinean las clasificaciones con las etiquetas en las consultas principales. Por lo tanto, no temas agrupar atributos si cada uno se aplica a una fracción muy pequeña de datos, pero la cobertura total es de más del 90%. Puedes usar la regularización para eliminar los atributos que se aplican a pocos ejemplos.\nRegla n.º 20: Combina y modifica los atributos existentes para crear atributos nuevos de una forma legible. Hay diferentes formas de combinar y modificar atributos. Los sistemas de aprendizaje automático como TensorFlow te permiten preprocesar los datos mediante transformaciones. Los dos enfoques estándar son las \u0026ldquo;discretizaciones\u0026rdquo; y las \u0026ldquo;combinaciones\u0026rdquo;.\nLa discretización consiste en tomar un atributo continuo y crear varios atributos discretos. Considera un atributo continuo como la edad. Puedes crear un atributo que es 1 cuando la edad es menor de 18, otro atributo que es 1 cuando la edad es entre 18 y 35, etc. No pienses demasiado en los límites de estos histogramas; los cuantiles básicos serán los más eficaces.\nLas combinaciones unen dos o más columnas de atributos. En la terminología de TensorFlow, una columna de atributos es un conjunto de atributos homogéneos (p. ej., {masculino, femenino}, {EE.UU., Canadá, México}, etc.). Una combinación es una nueva columna de atributos que incluye, p. ej., {masculino, femenino} × {EE.UU., Canadá, México}. Esta nueva columna de atributos contendrá el atributo (masculino, Canadá). Si usas TensorFlow y le indicas que cree esta combinación, este atributo (masculino, Canadá) aparecerá en los ejemplos que representan canadienses masculinos. Ten en cuenta que se necesita una enorme cantidad de datos para aprender modelos con combinaciones de tres, cuatro o más columnas de atributos básicos.\nLas combinaciones que producen columnas de atributos muy grandes pueden producir un sobreajuste. Por ejemplo, imagina que haces algún tipo de búsqueda y tienes una columna de atributos con palabras en la consulta y otra con palabras en el documento. Si unes estas columnas con una combinación, terminarás con muchos atributos (consulta la regla n.º 21).\nCuando trabajas con texto, existen dos alternativas. La más rigurosa es un producto escalar. En su forma más simple, un producto escalar simplemente cuenta la cantidad de palabras en común entre la consulta y el documento. Por lo tanto, este atributo se puede discretizar. Otro enfoque es una intersección: tenemos un atributo que está presente solo si la palabra \u0026ldquo;poni\u0026rdquo; aparece en el documento y en la consulta, y otro atributo que está presente solo si la palabra \u0026ldquo;el\u0026rdquo; aparece en el documento y en la consulta.\nRegla n.º 21: La cantidad de ponderaciones de atributos que puedes aprender en un modelo lineal es casi proporcional a la cantidad de datos que tienes. Existen resultados teóricos fascinantes sobre aprendizaje estadístico relacionados con el nivel de complejidad correspondiente de un modelo, pero esta regla es todo lo que necesitas saber. Tuve discusiones con personas que ponían en duda que se pudiera aprender algo con mil ejemplos o que nunca se necesitan más de un millón de ejemplos, porque están empecinados con un determinado método de aprendizaje. La clave es escalar el aprendizaje a la medida del tamaño de los datos:\nSi trabajas con un sistema de ranking de búsquedas, existen millones de palabras diferentes en los documentos y la consulta, y tienes 1000 ejemplos etiquetados, debes usar un producto escalar entre los atributos de consultas y de documentos, TF-IDF y una media docena de otros atributos desarrollados por humanos. 1000 ejemplos, una docena de atributos. Si tienes un millón de ejemplos, intersecta la columna de atributos de consultas con la de documentos, y aplica regularización y, posiblemente, selección de atributos. Esto te brindará un millón de atributos, pero, con la regularización, tendrás menos. 10 millones de ejemplos, tal vez 100,000 atributos. Si tienes miles o cientos de miles de millones, puedes combinar las columnas de atributos con tokens de consultas y de documentos mediante la regularización y la selección de atributos. Tendrás miles de millones de ejemplos y 10 millones de atributos. La teoría de aprendizaje estadístico raramente establece límites rígidos, pero ofrece un buen punto de partida. En última instancia, usa la regla n.º 28 para decir qué atributos usarás.\nRegla n.º 22: Quita los atributos que ya no uses. Los atributos sin usar crean deuda técnica. Si descubres que no estás usando un atributo y que no sirve combinarlo con otros atributos, quítalo de la infraestructura. Debes mantener limpia tu infraestructura para que puedas probar los atributos más prometedores tan rápido como sea posible. Si es necesario, tu atributo se puede volver a agregar en cualquier momento.\nTen en cuenta la cobertura cuando analices qué atributos agregarás o conservarás. ¿Cuántos ejemplos cubre el atributo? Por ejemplo, si tienes algunos atributos de personalización, pero solo el 8% de los usuarios tiene atributos de personalización, eso no será muy eficaz.\nAl mismo tiempo, algunos atributos te sorprenden gratamente. Por ejemplo, si tienes un atributo que cubre solo el 1% de los datos, pero el 90% de los ejemplos de este atributo son positivos, es un excelente atributo que agregar.\nAnálisis humano del sistema Antes de avanzar a la tercera fase de aprendizaje automático, es importante hablar de algo que no se enseña en ninguna clase de aprendizaje automático: cómo analizar un modelo existente y mejorarlo. Esto es más un arte que una ciencia. Aun así, existen muchos antipatrones que es conveniente evitar.\nRegla n.º 23: No eres el típico usuario final. Probablemente, esta es la razón más común por la que un equipo no progresa. Si bien usar un prototipo con tu equipo o usar un prototipo en tu empresa tiene muchos beneficios, los empleados deben analizar si el rendimiento es correcto. Si bien un cambio que sea evidentemente malo no debe usarse, cualquier función que parezca lista para la producción debe probarse aún más, ya sea contratando a gente común para que responda preguntas en una plataforma de participación colectiva o mediante un experimento en vivo con usuarios reales.\nHay dos razones para ello. Estás demasiado familiarizado con el código. Es posible que busques un aspecto específico de las publicaciones, o que estés muy involucrado (p. ej., sesgo de confirmación). La segunda razón es que tu tiempo es demasiado valioso. Considera el costo de nueve ingenieros en una reunión de una hora de duración y cuántas etiquetas logradas con trabajo humano puedes obtener en una plataforma de participación colectiva.\nSi realmente quieres comentarios de usuarios, implementa metodologías de experiencia de usuario. Crea usuarios persona (puedes encontrar una descripción en el libro Sketching User Experiences [Cómo diseñar experiencias de usuario] de Bill Buxton) al comienzo del proceso y, luego, implementa una prueba de usabilidad (puedes encontrar una descripción en el libro Don’t Make Me Think [No me hagas pensar] de Steve Krug). Los usuarios persona implican crear un usuario hipotético. Por ejemplo, si tu equipo se compone solo por hombres, será beneficioso diseñar un usuario persona femenina de 35 años (completa con atributos de usuario) y observar los resultados que genera, en lugar de los 10 resultados para hombres de 25 a 40 años. También puedes lograr una nueva perspectiva al evaluar cómo reaccionan las personas reales a tu sitio (de forma local o remota) en una prueba de usabilidad.\nRegla n.º 24: Mide el delta entre los modelos. Una de las formas más sencillas y, a veces, más útiles de realizar mediciones que puedes usar antes de que los usuarios vean tu nuevo modelo es calcular la diferencia entre los nuevos resultados y los obtenidos con el sistema en producción. Por ejemplo, si tienes un problema de ranking, ejecuta ambos modelos con una muestra de consultas en todo el sistema y observa el tamaño de la diferencia simétrica de los resultados (ponderados según la posición en el ranking). Si la diferencia es muy pequeña, entonces puedes deducir que habrá poco cambio, sin necesidad de ejecutar un experimento. Si la diferencia es muy grande, entonces debes asegurarte de que el cambio sea positivo. Analizar las consultas donde la diferencia simétrica es alta te puede ayudar a comprender de forma cualitativa cómo fue el cambio. Sin embargo, asegúrate de que el sistema sea estable. Cuando compares un modelo consigo mismo, asegúrate de que tenga una diferencia simétrica baja (idealmente cero).\nRegla n.º 25: Cuando elijas un modelo, el rendimiento utilitario predomina por sobre el poder de predicción. Tu modelo puede intentar predecir la tasa de clics. Sin embargo, en última instancia, la pregunta clave es lo que haces con esa predicción. Si la usas para clasificar documentos, entonces la calidad del ranking final importa más que la predicción en sí misma. Si predices la probabilidad de que un documento sea spam y, luego, tienes un punto límite sobre lo que se bloquea, entonces la precisión de lo que se permite tiene más importancia. La mayoría de las veces, estos dos aspectos coinciden; cuando no es así, es probable que haya una pequeña ganancia. Además, si hay algún cambio que mejore la pérdida logística, pero que reduzca el rendimiento del sistema, busca otro atributo. Si esto comienza a suceder con más frecuencia, es hora de volver a evaluar el objetivo del modelo.\nRegla n.º 26: Busca patrones en los errores observados y crea atributos nuevos. Supongamos que ves un ejemplo de entrenamiento que el modelo \u0026ldquo;no entendió\u0026rdquo;. En una tarea de clasificación, este error puede ser un falso positivo o un falso negativo. En una tarea de clasificación, el error puede ser un par donde un positivo tiene un ranking menor que un negativo. El punto más importante es que sea un ejemplo que el sistema de aprendizaje automático sepa que no lo entendió y que lo corrija si tiene la oportunidad. Si le agregas un atributo al modelo para que pueda corregir el error, el modelo intentará usarlo.\nPor otro lado, si intentas crear un atributo basado en ejemplos que el sistema no considera errores, el atributo se ignorará. Por ejemplo, supongamos que, en la búsqueda de apps de Play, alguien busca \u0026ldquo;juegos gratuitos\u0026rdquo;. Supongamos que uno de los primeros resultados es una app de bromas menos relevante. Entonces, creas un atributo para \u0026ldquo;apps de bromas\u0026rdquo;. Sin embargo, si maximizas la cantidad de instalaciones y las personas instalan una app de bromas cuando buscan juegos gratuitos, el atributo para \u0026ldquo;apps de bromas\u0026rdquo; no tendrá el efecto deseado.\nUna vez que tengas ejemplos que el modelo no haya entendido, busca las tendencias que estén fuera del conjunto de atributos actual. Por ejemplo, si parece que el sistema penaliza las publicaciones más largas, agrega la longitud de la publicación. No seas demasiado específico sobre los atributos que agregues. Si agregas la longitud de la publicación, no intentes adivinar qué significa \u0026ldquo;largo\u0026rdquo;, solo agrega una decena de atributos y permite que el modelo descubra qué hacer con ellos (consulta la regla n.º 21). Esta es la forma más fácil de obtener el resultado deseado.\nRegla n.º 27: Intenta cuantificar el comportamiento no deseado que observes. Algunos miembros de tu equipo comenzarán a frustrarse con las propiedades del sistema que no les gusten, ya que la función de pérdida existente no las captura. En este punto, deben hacer lo que sea necesario para convertir sus quejas en números sólidos. Por ejemplo, si consideran que se muestran demasiadas \u0026ldquo;apps de bromas\u0026rdquo; en la búsqueda de Play, se pueden contratar a evaluadores humanos para que identifiquen las apps de bromas. (Puedes usar datos etiquetados por humanos en este caso, ya que una fracción relativamente pequeña de consultas representan una gran fracción del tráfico). Si los problemas se pueden medir, puedes comenzar a usarlos como atributos, objetivos o métricas. La regla general es \u0026ldquo;medir primero, optimizar después\u0026rdquo;.\nRegla n.º 28: Ten en cuenta que el comportamiento idéntico a corto plazo no implica un comportamiento idéntico a largo plazo. Imagina que tienes un sistema nuevo que analiza cada doc_id y exact_query, y luego calcula la probabilidad de clic para cada documento y cada consulta. Descubres que este comportamiento es casi idéntico a tu sistema actual en la comparación y la prueba A/B; por lo tanto, dado su simplicidad, lo ejecutas. Sin embargo, notas que no se muestran apps nuevas. ¿Por qué? Bueno, dado que tu sistema solo muestra un documento basado en su propio historial con esa consulta, no hay forma de aprender qué documento nuevo debe mostrar.\nLa única forma de entender cómo debe funcionar un sistema a largo plazo es entrenarlo solo con datos adquiridos cuando el modelo está publicado. Esto es muy difícil.\nDesviación entre el entrenamiento y la publicación La desviación entre el entrenamiento y la publicación es la diferencia entre el rendimiento del entrenamiento y el de la publicación. Existen diferentes razones para esta desviación:\nuna discrepancia entre cómo manipulas los datos en las canalizaciones de entrenamiento y del servidor un cambio en los datos entre el momento del entrenamiento y el del servidor un ciclo de retroalimentación entre el modelo y el algoritmo Hemos observado sistemas de aprendizaje automático de producción en Google con una desviación entre el entrenamiento y la publicación que afecta negativamente el rendimiento. La mejor solución es supervisarlo de forma explícita para que los cambios en el sistema y en los datos no generen una desviación inadvertida.\nRegla n.º 29: La mejor manera de asegurarte de que el entrenamiento se asemeja a la publicación es guardar el conjunto de atributos que usas en la publicación y canalizar esos atributos en un registro para luego usarlos en el entrenamiento. Incluso si no lo puedes hacer para cada ejemplo, hazlo para una fracción pequeña, de forma tal que puedas verificar la coherencia entre la publicación y el entrenamiento (consulta la regla n.º 37). En Google, los equipos que hicieron esta medición se sorprendieron a menudo por los resultados. La página de inicio de YouTube cambió a atributos del registro en el servidor, lo que generó mejoras de calidad importantes y redujo la complejidad del código. Ahora, muchos equipos están cambiando sus infraestructuras.\nRegla n.º 30: ¡Realiza muestras con datos con ponderación por importancia, no los quites! Cuando tienes muchos datos, es tentador usar los archivos del 1 al 12 e ignorar los archivos del 13 al 99. Esto es un error. Si bien se pueden quitar los datos que nunca se mostraron al usuario, la ponderación por importancia es la mejor opción para el resto. La ponderación por importancia implica que, si decides hacer una muestra con el ejemplo X con una probabilidad del 30%, la ponderación será de 10\u0026frasl;3. Con la ponderación por importancia, se mantienen todas las propiedades de calibración que analizamos en la regla n.º 14.\nRegla n.º 31: Ten en cuenta que, si cruzas datos de una tabla durante el entrenamiento y la publicación, los datos en la tabla pueden cambiar. Digamos que cruzas ID de documentos a una tabla que contiene atributos para esos documentos (como la cantidad de comentarios o clics). Entre el entrenamiento y el servidor, es posible que cambien los atributos en la tabla. Por lo tanto, puede cambiar predicción del modelo para el mismo documento entre el entrenamiento y la publicación. La forma más sencilla de evitar este tipo de problemas es registrar los atributos durante la publicación (consulta la regla n.º 32). Si la tabla cambia lentamente, puedes tomar una instantánea de la tabla a cada hora o cada día para obtener datos razonablemente cercanos. Ten en cuenta que esto no resuelve completamente el problema.\nRegla n.º 32: Reutiliza el código entre la canalización de entrenamiento y la canalización del servidor, siempre que sea posible. El procesamiento por lotes es diferente al procesamiento en línea. En el procesamiento en línea, debes responder cada solicitud a medida que llega (p. ej., debes realizar una búsqueda separada para cada consulta). En el procesamiento por lotes, puedes combinar tareas (p. ej., unir funciones). En la publicación, implementas el procesamiento en línea, mientras que el entrenamiento es una tarea de procesamiento por lotes. Sin embargo, hay varias formas de reutilizar el código. Por ejemplo, puedes crear un objeto que sea específico para tu sistema, donde el resultado de cualquier consulta o unión se puede almacenar de una forma legible y donde los errores se pueden probar fácilmente. Luego, una vez que hayas reunido toda la información, durante el entrenamiento o la publicación, ejecutas un método común para conectar el objeto legible específico del sistema y el formato que espera el sistema de aprendizaje automático. Esto elimina una fuente de desviación entre el entrenamiento y la publicación. Como corolario, intenta no usar dos lenguajes de programación diferentes entre el entrenamiento y la publicación. Si haces esto, será casi imposible compartir el código.\nRegla n.º 33: Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. En general, mide el rendimiento de un modelo con datos reunidos en forma posterior a aquellos con los que se ha entrenado el modelo, ya que refleja de forma más precisa qué hará el sistema en la producción. Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. El rendimiento no debería ser tan bueno en los datos nuevos, pero no debería ser mucho peor. Como puede haber efectos diarios, es posible que no predigas la tasa de clics promedio o la tasa de conversión, pero el área bajo la curva, que representa la posibilidad de darle una puntuación más alta al ejemplo positivo que al ejemplo negativo, debería ser razonablemente parecida.\nRegla n.º 34: En la clasificación binaria para filtrado (como la detección de spam o la identificación de correos electrónicos de interés), realiza pequeños sacrificios a corto plazo en el rendimiento para lograr datos más claros. En la tarea de filtrado, los ejemplos que se marcan como negativos no se muestran al usuario. Supongamos que tienes un filtro que bloquea el 75% de los ejemplos negativos durante la publicación. Puede surgir la tentación de obtener más datos de entrenamiento a partir de las instancias que se muestran a los usuarios. Por ejemplo, si un usuario marca como spam un correo electrónico que permitió tu filtro, se puede aprender de esta acción.\nPero este enfoque introduce un sesgo en la muestra. Puedes obtener datos más claros si etiquetas el 1% de todo el tráfico como \u0026ldquo;retenido\u0026rdquo; durante la publicación y envías todos los ejemplos retenidos al usuario. Ahora, el filtro bloqueará al menos el 74% de los ejemplos negativos. Los ejemplos retenidos se convertirán en los datos de entrenamiento.\nSi el filtro bloquea el 95% o más de los ejemplos negativos, este enfoque se hace menos viable. Aun así, si deseas medir el rendimiento en la publicación, puedes hacer una muestra todavía más pequeña (p. ej., 0.1% o 0.001%). Diez mil ejemplos son suficientes para estimar el rendimiento de forma precisa.\nRegla n.º 35: Ten en cuenta la desviación inherente a los problemas de ranking. Si cambias el algoritmo de clasificación lo suficiente como para ver resultados diferentes, habrás logrado modificar los datos que el algoritmo verá en el futuro. Este tipo de desviación aparecerá, y debes tenerla en cuenta al diseñar el modelo. Existen varias estrategias diferentes que sirven para favorecer los datos que tu modelo ya vio.\nPermite tener una regularización más alta en los atributos que cubren más consultas, a diferencia de esos atributos que solo abarcan una consulta. De esta forma, el modelo favorecerá los atributos que son específicos a una o pocas consultas por sobre los atributos que se generalizan a todas las consultas. Esta estrategia permite evitar que los resultados muy populares acaben en consultas irrelevantes. Este enfoque es contrario a la sugerencia más tradicional de contar con más regularización en columnas de funciones con más valores únicos. Permite que los atributos solo tengan ponderaciones positivas. Además, cualquier atributo bueno será mejor que uno \u0026ldquo;desconocido\u0026rdquo;. No uses atributos asociados solo al documentos. Esto es una versión extrema de la regla n.º 1. Por ejemplo, incluso si una app determinada es una descarga popular, más allá de la consulta, no es necesario mostrarla en todos lados. Esto se simplifica al no tener atributos solo de documentos. La razón por la que no deseas mostrar una app popular específica en todos lados está relacionada con la importancia de lograr que las apps que deseas estén disponibles. Por ejemplo, si alguien busca \u0026ldquo;apps para observar pájaros\u0026rdquo;, es posible que descarguen \u0026ldquo;Angry birds\u0026rdquo;, pero de forma claramente accidental. Si se muestra esta app, es posible que mejore la tasa de descarga, pero no se cumplen con las necesidades del usuario. Regla n.º 36: Evita los ciclos de retroalimentación con atributos posicionales. La posición del contenido afecta enormemente la probabilidad de que el usuario interactúe con este. Si ubicas una app en la primera posición, se seleccionará con más frecuencia y te dará la impresión de que es más probable que se seleccione. Una forma de lidiar con eso es agregar atributos de posición, es decir, atributos sobre la posición del contenido en la página. Entrena el modelo con atributos de posición para que aprenda a darle una mayor ponderación al atributo \u0026ldquo;primera posición\u0026rdquo;, por ejemplo. Así, el modelo les asigna una ponderación menor a otros factores con ejemplos de \u0026ldquo;primera posición=verdadero\u0026rdquo;. Entonces, en la publicación, no asignas ninguna instancia al atributo de posición (o le asignas el mismo atributo predeterminado), porque calificas candidatos antes de que hayas decidido el orden en el que se mostrarán.\nTen en cuenta que es importante mantener cualquier atributo de posición separado del resto del modelo, debido a la asimetría entre el entrenamiento y la prueba. Lo ideal es que el modelo sea la suma de una función de los atributos posicionales y una función del resto de atributos. Por ejemplo, no cruces los atributos de posición con cualquier atributo de documentos.\nRegla n.º 37: Mide la desviación entre el entrenamiento y la publicación. En el sentido más general, existen diversas razones para la desviación. Además, se puede dividir en varias partes:\nLa diferencia entre el rendimiento en los datos de entrenamiento y los datos retenidos. En general, esta diferencia siempre existe y no siempre es negativa. La diferencia entre el rendimiento en los datos retenidos y los datos \u0026ldquo;del día siguiente\u0026rdquo;. De nuevo, esta diferencia siempre existe. Debes ajustar la regularización para maximizar el rendimiento del día siguiente. Sin embargo, caídas notables en el rendimiento entre los datos retenidos y los datos del día siguiente pueden ser un indicador de que algunos atributos dependen del tiempo y posiblemente afecten al rendimiento del modelo de forma negativa. La diferencia entre el rendimiento en los datos del día siguiente y los datos en vivo. Si aplicas un modelo a un ejemplo en los datos de entrenamiento y el mismo ejemplo en la publicación, deberías obtener el mismo resultado (consulta la regla n.º 5). Por lo tanto, si aparece una discrepancia, probablemente indique un error de ingeniería. Fase III de aprendizaje automático: Crecimiento reducido, refinamiento de la optimización y modelos complejos Existen ciertos indicios de que la segunda fase llega a su fin. Primero, las ganancias mensuales comienzan a disminuir. Las métricas comenzarán a emparejarse: en algunos experimentos, verás que algunas aumentan y otras disminuyen. Este es un momento interesante. Dado que las ganancias son más difíciles de obtener, el aprendizaje automático debe sofisticarse aún más. Una advertencia: esta sección contiene más reglas especulativas que las secciones anteriores. Hemos visto a muchos equipos realizar grandes progresos en la Fase I y la Fase II de aprendizaje automático. Una vez que alcanzan la Fase III, los equipos deben buscar su propio camino.\nRegla n.º 38: No pierdas tiempo en nuevos atributos si los objetivos no alineados son un problema. Cuando las mediciones comienzan a estabilizarse, el equipo comenzará a buscar problemas fuera del alcance de los objetivos de tu sistema de aprendizaje automático actual. Como indicamos anteriormente, si el objetivo algorítmico existente no abarca los propósitos del producto, debes cambiar el objetivo o los propósitos. Por ejemplo, puedes optimizar clics, +1 o descargas, pero toma las decisiones de lanzamiento según los evaluadores humanos.\nRegla n.º 39: Las decisiones de lanzamiento representan los objetivos a largo plazo del producto. Alice tiene una idea para reducir la pérdida logística de las instalaciones predichas. Agrega un atributo. Se reduce la pérdida logística. Cuando hace un experimento en vivo, observa un aumento en la tasa de instalación. Sin embargo, cuando realiza una reunión para evaluar el lanzamiento, alguien menciona que la cantidad de usuarios activos diarios cayó un 5%. El equipo decide no lanzar el modelo. Alice está decepcionada, pero ahora se da cuenta de que las decisiones de lanzamiento dependen de varios factores y solo algunos de ellos se pueden optimizar directamente con AA.\nLa verdad es que el mundo real no es un juego de rol; no hay \u0026ldquo;puntos de ataque\u0026rdquo; que indican el estado de tu producto. El equipo debe usar las estadísticas que reúne para intentar predecir de forma eficaz qué tan bueno será el sistema en el futuro. Deben preocuparse por la participación, el número de usuarios activos en un día (DAU), el número de usuarios activos en 30 días (30 DAU), la ganancia y el retorno de la inversión del anunciante. Estas métricas que se miden en las pruebas A/B representan los objetivos a largo plazo: satisfacer a los usuarios, aumentar la cantidad de usuarios, satisfacer a los socios y obtener ganancias. A su vez, puedes considerar estos propósitos como representantes de otros propósitos: lograr un producto útil y de calidad, y que la empresa prospere de aquí a cinco años.\nLas únicas decisiones de lanzamiento fáciles son cuando todas las métricas mejoran (o al menos no empeoran). Si el equipo puede elegir entre un algoritmo de aprendizaje automático sofisticado o una simple heurística (que funciona mejor en todas las métricas), debe elegir la heurística. Además, no existe una clasificación explícita para todos los valores de métricas posibles. En especial, considera estos dos escenarios siguientes:\nExperimento Usuarios activos por día Ganancia/día A 1 millón $4 millones B 2 millones $2 millones Si el sistema actual es A, entonces es poco probable que el equipo cambie a B. Si el sistema actual es B, entonces es poco probable que el equipo cambie a A. Esto parece contradecir el comportamiento racional; sin embargo, las predicciones de las métricas cambiantes pueden o no cumplirse. Por lo tanto, cada cambio conlleva un riesgo grande. Cada métrica abarca una cierta cantidad de riesgo que preocupa al equipo.\nAdemás, ninguna métrica representa la preocupación máxima del equipo, \u0026ldquo;¿dónde estará mi producto de aquí a cinco años?\u0026rdquo;.\nPor otro lado, las personas tienden a favorecer un objetivo que pueden optimizar directamente. La mayoría de las herramientas de aprendizaje automático favorecen dicho entorno. Un ingeniero agregando atributos nuevos puede lograr un flujo constante de lanzamiento en dicho entorno. Existe un tipo de aprendizaje automático, el aprendizaje de multiobjetivo, que comienza a resolver este problema. Por ejemplo, se puede formular un problema de satisfacción de restricciones con límites inferiores en cada métrica y optimiza alguna combinación lineal de las métricas. Pero, aun así, no todas las métricas se enmarcan fácilmente como objetivos de aprendizaje automático: si el usuario hace clic en un documento o instala una app, se debe a que se mostró el contenido. Pero es mucho más difícil determinar la razón por la que un usuario visita tu sitio. Cómo predecir el éxito futuro de un sitio de forma integral depende IA-completo: es tan difícil como la visión por computadora o el procesamiento de lenguajes naturales.\nRegla n.º 40: Mantén las combinaciones simples. Los modelos unificados que aceptan atributos sin procesar y clasifican contenido directamente son los modelos más sencillos de depurar y comprender. Sin embargo, una combinación de modelos (un modelo que combina los resultados de otros modelos) puede funcionar mejor. Para mantener las cosas simples, cada modelo debe ser una combinación de modelos que solo acepta como entrada el resultado de otros modelos o un modelo básico que acepta muchos atributos, pero no ambos. Si tienes modelos sobre otros modelos que se entrenan de forma separada y los combinas, se puede generar un comportamiento erróneo.\nUsa un modelo simple como combinación, que solo acepte los resultados de los modelos \u0026ldquo;básicos\u0026rdquo; como entradas. También debes implementar propiedades en esos modelos de conjuntos. Por ejemplo, un aumento en el resultado generado por un modelo básico no debe reducir el resultado del combinado. Además, es mejor que los modelos entrantes se puedan interpretar de forma semántica (p. ej., calibrados), para que los cambios en los modelos subyacentes no confundan al modelo combinado. Asegúrate también que un aumento en la probabilidad predicha de un clasificador subyacente no reduzca la probabilidad predicha del conjunto.\nRegla n.º 41: Cuando el rendimiento se estanque, busca nuevas fuentes de información de forma cualitativa para agregar, en lugar de refinar las señales existentes. Agregaste cierta información demográfica sobre el usuario. Agregaste cierta información sobre las palabras en el documento. Finalizaste la exploración de combinación de atributos y ajustaste la regularización. No observaste un lanzamiento con más de un 1% de mejora en las métricas clave, en varios trimestres. ¿Ahora qué?\nEs hora de desarrollar la infraestructura para atributos radicalmente diferentes, como el historial de los documentos a los que este usuario accedió en el último día, semana o año, o los datos de una propiedad diferente. Usa entidades de wikidatos o algún recurso interno de tu empresa (como el Gráfico de conocimiento de Google). Usa el aprendizaje profundo. Comienza a ajustar tus expectativas sobre el retorno de la inversión esperado y aumenta tus esfuerzos adecuadamente. Como en cualquier proyecto de ingeniería, debes comparar el beneficio de agregar nuevos atributos con el costo de una mayor complejidad.\nRegla n.º 42: No esperes que la diversidad, la personalización o la relevancia se correlacionen con la popularidad. La diversidad en un conjunto de contenidos puede significar muchas cosas; la más común es la diversidad de la fuente del contenido. La personalización implica que cada usuario obtiene sus propios resultados. La relevancia implica que los resultados para una consulta específica son más apropiados para esa consulta que para otra. Además, por definición, estas tres propiedades se diferencian de lo común.\nEl problema es que lo común tiende a ser difícil de superar.\nTen en cuenta que tu sistema mide clics, el tiempo dedicado, reproducciones, +1, veces que se comparte el contenido, etc., es decir, la popularidad del contenido. A veces, los equipos intentan aprender un modelo personal con diversidad. Para implementar la personalización, agregan atributos que le permiten al sistema la personalización (algunos atributos que representan el interés del usuario) o la diversificación (atributos que indican si este documento tiene algún atributo en común con otros documentos de los resultados, como el autor o el contenido), y descubren que esos atributos obtienen una ponderación menor (o a veces, un signo diferente) al que esperaban.\nEsto no significa que la diversidad, la personalización o la relevancia no sean valiosas. Como se indicó en la regla anterior, puedes hacer un posprocesamiento para aumentar la diversidad o la relevancia. Si observas que los objetivos a largo plazo aumentan, puedes declarar que la diversidad o la relevancia son valiosas, más allá de la popularidad. Puedes continuar usando el posprocesamiento o directamente modificar el objetivo según la diversidad o la relevancia.\nRegla n.º 43: Tus amigos tienden a ser los mismos en diferentes productos. No así tus intereses. Varios equipos en Google ganaron mucho terreno al tomar que un modelo que predice la cercanía de una conexión con un producto y lograr que funcione en otro producto. Tus amigos no cambian. Por otro lado, observé a muchos equipos lidiar con atributos de personalización entre productos. Sí, parece que debería funcionar. Por ahora, parece que no. Un método que a veces funciona es usar los datos sin procesar de una propiedad para predecir el comportamiento en otra. Ten en cuenta que también puede ayudar saber que un usuario tiene un historial en otra propiedad. Por ejemplo, la presencia de actividad de usuario en dos propiedades puede ser un indicativo en sí misma.\nTrabajo relacionado Existen muchos documentos sobre aprendizaje automático en Google y en otras fuentes.\nCurso intensivo de aprendizaje automático: Una introducción al aprendizaje automático aplicado. Aprendizaje automático: Un enfoque probabilístico de Kevin Murphy, para comprender el campo de aprendizaje automático. Consejos prácticos para el análisis de conjuntos de datos grandes y complejos: Un enfoque de ciencia de datos sobre los conjuntos de datos. Aprendizaje profundo de Ian Goodfellow et al, para aprendizaje de modelos no lineales. Documento de Google sobre la deuda técnica, con muchos consejos generales. Documentación de Tensorflow. Agradecimientos Agradezco a David Westbrook, Peter Brandt, Samuel Ieong, Chenyu Zhao, Li Wei, Michalis Potamias, Evan Rosen, Barry Rosenberg, Christine Robson, James Pine, Tal Shaked, Tushar Chandra, Mustafa Ispir, Jeremiah Harmsen, Konstantinos Katsiapis, Glen Anderson, Dan Duckworth, Shishir Birmiwal, Gal Elidan, Su Lin Wu, Jaihui Liu, Fernando Pereira y Hrishikesh Aradhye por las numerosas correcciones, sugerencias y ejemplos útiles para este documento. Agradezco también a Kristen Lefevre, Suddha Basu y Chris Berg quienes colaboraron en una versión anterior. Cualquier error, omisión u opiniones controversiales es mi responsabilidad.\nAnexo Existen diferentes referencias a productos de Google en este documento. Para proporcionar más contexto, agregué una descripción breve de los ejemplos más comunes a continuación.\nDescripción de YouTube YouTube es un servicio de transmisión de videos. Tanto los equipos de YouTube Watch Next y de la página principal de YouTube usan modelos de AA para clasificar recomendaciones de videos. Watch Next recomienda videos para ver después del que se está reproduciendo, la página principal recomienda videos para los usuarios que exploran la página principal.\nDescripción general de Google Play Google Play tiene muchos modelos para resolver diferentes problemas. Las búsqueda de apps en Play, las recomendaciones personalizadas en la página principal de Play y \u0026ldquo;Otros usuarios también instalaron\u0026rdquo; usan aprendizaje automático.\nDescripción de Google Plus Google Plus usa aprendizaje automático en diferentes situaciones: clasificar las publicaciones en la sección \u0026ldquo;Novedades\u0026rdquo; que ve el usuario, las publicaciones en la sección \u0026ldquo;Lo más interesante\u0026rdquo; (las publicaciones que son más populares), las personas que conoces, etc.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"f5753e037c0db22c30dc33f36f37ca6e","permalink":"https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-0-reglas2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/data-science/intro-machine-learning/ml101-0-reglas2/","section":"courses","summary":"Fase II de aprendizaje automático: Ingeniería de atributos En la primera fase del ciclo de vida de un sistema de aprendizaje automático, la prioridad es mandar los datos de entrenamiento al sistema de aprendizaje, lograr instrumentar las métricas de interés y crear una infraestructura de publicación. Una vez que cuentas con un sistema integral en funcionamiento con pruebas de unidades y del sistema instrumentadas, comienza la fase II.\nEn la segunda fase, hay muchas recompensas a corto plazo.","tags":null,"title":"Reglas del aprendizaje automático - Fase II","type":"docs"},{"authors":null,"categories":null,"content":" Creando vectores Te sientes de suerte?\nEso espero, porque en este capítulo vamos de viaje a la Ciudad del Pecado, también conocida como el \u0026ldquo;Paraíso del estadístico\u0026rdquo; ;-).\nGracias a R y a tus nuevas habilidades analíticas, vas a aprender cómo mejorar tus ganancias en el casino y empezar una lucrativa carrera como jugador profesional. En este capítulo veremos cómo puedes fácilmente llevar la cuenta de tus apuestas y como hacer análisis simples de tus jugadas. Próxima parada\u0026hellip; Viva las Vegas!!!\nInstrucciones\n Esperamos que recuerdes lo que aprendiste en el capítulo anterior. Asigna el valor \u0026ldquo;Alla vamos!\u0026rdquo; a la variable Vegas  Script R\n# Define la variable Vegas Vegas \u0026lt;- \u0026quot;Alla vamos!\u0026quot;  Creando vectores (2) Primero concentrémonos.\nEn nuestro camino a para quebrar al casino, haremos uso extensivo de los vectores. Un vector es un arreglo unidimensional que puede contener datos numéricos, caracteres, o valores lógicos. En otras palabras un vector es una herramienta simple para guardar un conjunto de datos del mismo tipo. Por ejemplo podemos llevar la cuenta de las ganancias y pérdidas en los juegos de casino.\nEn R, se crea un vector con la función c(). La función se llama c por \u0026ldquo;combinar\u0026rdquo;. Se ponen los valores o elementos del vector dentro de paréntesis, separados por coma. Por ejemplo:\nvector_numerico \u0026lt;- c(1, 2, 3) vector_caracter \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) vector_logico \u0026lt;- c(TRUE, FALSE)  Una vez has creado estos vectores en R puedes usarlos para hacer cálculos.\nInstrucciones\n Completa el código de tal manera que el vector_logico contenga tres elementos: TRUE, FALSE y TRUE (en ese orden).  Script R\nvector_numerico \u0026lt;- c(1, 10, 49) vector_caracter \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) # Completa el código para vector_logico vector_logico \u0026lt;- c(TRUE, FALSE, TRUE)  Creando vectores (3) Después de una semana en Las Vegas y cero Ferraris en tu garage, decides que es hora de empezar a utilizar tus super-poderes analíticos.\nAntes de hacer un primer análisis, decides llevar un registro de todas tus ganancias y pérdidas de la semana pasada:\nEn el poker:\n Lunes: ganaste 140 Martes: perdiste 50 Miercoles: ganaste 20 Jueves: perdiste 120 Viernes: ganaste 240  En la ruleta:\n Lunes: perdiste 24 Martes: perdiste 50 Miercoles: ganaste 100 Jueves: perdiste 350 Viernes: ganaste 10  Solamente has jugado poker y ruleta, porque un grupo de adivinos se ha apoderado de la mesa de dados. Para poder utilizar esos datos en R, decides crear las variables vector_poker y vector_ruleta.\nInstrucciones\n Ahora asigna los días de la semana como nombres a los vectores vector_poker y vector_ruleta. Usa los nombres con mayúscula inicial y sin acentos (esto no es clase de ortografía\u0026hellip;): Lunes, Martes, Miercoles, Jueves y Viernes.\n Imprime los vectores en la consola\n  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Asigna los nombres a los vectores names(vector_poker) \u0026lt;- c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;,\u0026quot;Miercoles\u0026quot;,\u0026quot;Jueves\u0026quot;,\u0026quot;Viernes\u0026quot;) names(vector_ruleta) \u0026lt;- c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;,\u0026quot;Miercoles\u0026quot;,\u0026quot;Jueves\u0026quot;,\u0026quot;Viernes\u0026quot;) # Imprime los vectores en la consola vector_poker vector_ruleta  Nombrando elementos (2) Si quieres ser un buen estadístico, tienes que ser un poco perezoso. (Si ya eres perezoso, quizás hayas nacido con esos excepcionales talentos estadísticos.)\nEn los ejercicios anteriores probablemente sentiste que es aburrido escribir una y otra vez información como los días de la semana. Sin embargo, existe una manera mas eficiente de lograr esto, que tal si asignamos el vector que contiene los días de la semana a una variable.\nAsí como lo hiciste con las ganancias/pérdidas de la ruleta y el poker, puedes también crear un vector que contenga los días de la semana. Al asignarlo a una variable, éste puede ser reusado.\nInstrucciones\n Crea la variable vector_dias que contenga un vector con los días de la semana, de Lunes a Viernes. Usa la variable vector_dias para asignar nombres a los elementos de vector_poker y vector_ruleta.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Crea la variable vector_dias vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;,\u0026quot;Miercoles\u0026quot;,\u0026quot;Jueves\u0026quot;,\u0026quot;Viernes\u0026quot;) # Asigna los nombres a los elementos de vector_poker y vector_ruleta names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias  Calculando las ganancias totales Ahora que tienes los resultados del poker y la ruleta en un vector con elementos apropiadamente nombrados, puedes empezar a hacer análisis.\nVeremos como encontrar la siguiente información:\n La ganancia/pérdida total por cada día de la semana. La ganancia/pérdida total en la semana. Sabremos si estas ganando o perdiendo dinero en el poker y en la ruleta.  Para saber lo que nos proponemos tenemos que hacer cálculos aritméticos con los vectores. Cuando sumamos dos vectores en R, éste hace la suma elemento por elemento, por ejemplo las siguientes líneas de código son completamente equivalentes:\nc(1, 2, 3) + c(4, 5, 6) c(1 + 4, 2 + 5, 3 + 6) c(5, 7, 9)  Primero experimentemos sumando!\nInstrucciones\n Toma la suma de las variables vector_A y vector_B y asígnala a vector_total. Mira el resultado imprimiendo el valor de vector_total en la consola.  Script R\nvector_A \u0026lt;- c(1, 2, 3) vector_B \u0026lt;- c(4, 5, 6) # Asigna la suma de vector_A y vector_B vector_total \u0026lt;- vector_A + vector_B # Imprime el vector_total a la consola vector_total  Calculando las ganancias totales (2) ¿Entendiste como R realiza aritmética con los vectores?\nYa es hora de tener uno de esos Ferraris en el garage! Primero, hay que saber cuál fue la ganancia/pérdida por cada día. La ganancia neta de cada día es la suma de las ganancias/pérdidas que hiciste en el poker y en la ruleta.\nEn R, este cálculo es simplemente la suma de vector_ruleta y vector_poker.\nInstrucciones\n Asigna a la variable total_diario lo que ganaste/perdiste cada día en total (poker y ruleta combinado).\n Nombra los elementos de total_diario utilizando vector_dias.\n Imprime total_diario en la consola para que puedas ver tus resultados totales en cada día de la semana. Cuáles fueron los días buenos y los días malos?\n  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Calculando las ganancias/pérdidas diarias: total_diario \u0026lt;- vector_ruleta + vector_poker # Dando nombres a total_diario names(total_diario) \u0026lt;- vector_dias # Imprime total_diario a la consola para ver tus resultados por día total_diario  Calculando ganancias totales (3) Basado en los análisis previos, parece que tuviste una mezcla de días buenos y malos. Esto no era lo que tu ego esperaba, y te preguntas si quizás hay una (muy pequeña) posibilidad de que hayas perdido dinero en la semana.\nLa función que nos ayudará a contestar esta pregunta es sum(). Ésta función calcula la suma de todos los elementos de un vector. Por ejemplo para calcular (y asignar a una variable) el monto total de dinero que has ganado/perdido en el poker en la semana, escribe:\ntotal_poker \u0026lt;- sum(vector_poker)  Instrucciones\n Calcula el monto total de dinero que has ganado/perdido en la ruleta, asígnalo a la variable _totalruleta. Ahora que tienes los totales para la ruleta y el poker, puedes fácilmente calcular el valor de _totalsemana, que es la suma de todas las ganancias y pérdidas en la semana. Imprime el valor de _totalsemana a la consola. ¿En total ganaste o perdiste en la semana?  Script R\n# Resultados en la mesa de Poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Ganancias totales en el poker total_poker \u0026lt;- sum(vector_poker) # Ahora completa el código: total_ruleta \u0026lt;- sum(vector_ruleta) total_semana \u0026lt;- sum(total_poker + total_ruleta) #Imprime el total que ganaste/perdiste en la semana total_semana  Comparando ganancias totales Mmmmm\u0026hellip;. parece que estás perdiendo dinero. Que sorpresa! Hora de repensar tu estrategia! Esto va a requerir un análisis más profundo.\nDespués de una breve lluvia de ideas en el jacuzzi del hotel, piensas que una posible explicación puede ser que tus habilidades en la ruleta no están tan bien desarrolladas como las de poker. Así que talvez tus ganancias totales en el poker son más grandes (\u0026gt;) que en la ruleta.\nInstrucciones\n Calcula total_poker y total_ruleta como en el ejercicio anterior. Fíjate si tus ganancias totales en el poker son más altas que en la ruleta haciendo una comparación. Asigna el resultado de esta comparación a la variable respuesta. ¿Cuál es tu conclusión? ¿Deberías dedicarte a la ruleta o al poker? Imprime respuesta a la consola.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Ganancias totales en el poker y en la ruleta total_poker \u0026lt;- sum(vector_poker) total_ruleta \u0026lt;- sum(vector_ruleta) # Probando si las ganancias en el poker son más altas que en la ruleta: respuesta \u0026lt;- total_poker \u0026gt; total_ruleta # Imprime respuesta a la consola respuesta  Seleccionando elementos: los buenos tiempos La corazonada parece que fue cierta. Parece que te va mejor en el poker que en la ruleta.\nOtro posible aspecto a investigar son tus resultados al comienzo de la semana comparados con los últimos días. Talvez te excediste en los tequilas al final de la semana\u0026hellip;\nPara responder esta pregunta, concentrémonos en solo unos elementos del vector_total. En otras palabras, nuestro objetivo es seleccionar elementos específicos de los vectores. Para seleccionar elementos de un vector (y luego matrices, data frames, etc.), puedes utilizar los corchetes. Entre corchetes indicamos los elementos que queremos seleccionar. Por ejemplo, para seleccionar el primer elemento de un vector, escribes vector_poker[1]. Para seleccionar el segundo elemento escribimos vector_poker[2], etc.\nInstrucciones\n Asigna tu ganancia del Miércoles a la variable _pokermiercoles  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Asigna el valor de lo ganado el Miercoles en el poker poker_miercoles \u0026lt;- vector_poker[3]  Seleccionando elementos: los buenos tiempos (2) ¿Qué tal si analizamos los resultados en los días medios de la semana?\nPara seleccionar múltiples elementos de un vector, puedes hacerlo usando corchetes. En el ejercicio anterior pusimos un número, llamado índice, entre los corchetes y obtuvimos el elemento del vector correspondiente a ese índice. Para seleccionar varios elementos podemos utilizar otro vector cuyos elementos son los índices que deseamos seleccionar. Por ejemplo, para seleccionar el primero y el quinto elemento usamos el vector c(1,5) dentro de los corchetes. El siguiente código selecciona el primero y quinto elementos de vector_poker:\nvector_poker[c(1,5)]  Instrucciones\n Asigna los resultados de obtenidos del poker de los días Martes, Miercoles y Jueves a la variable dias_medios_poker. Imprime el vector dias_medios_poker a la consola para ver sus elementos.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Define la nueva variable: dias_medios_poker \u0026lt;- vector_poker[c(2,3,4)] # Imprime dias_medios_poker dias_medios_poker  Seleccionando elementos: los buenos tiempos (3) Seleccionar varios elementos del vector_poker con c(2,3,4) quizás no sea muy conveniente. Recordemos como estadísticos nuestras raíces perezosas, así que usemos una notación que R utiliza para crear vectores de números consecutivos c(2,3,4) se puede crear con el código 2:4, que genera el vector que contiene los números naturales del 2 al 4 (incluyendo ambos extremos).\nAsí que ahora tenemos otra manera de seleccionar los elementos que corresponden a los días medios de la semana: vector_poker[2:4].\nQuizás en este ejemplo no haya mucha diferencia entre c(2,3,4) y 2:4 pero imagina que tuviéramos que extraer los primeros 80 elementos consecutivos de un vector, con la primera notación tendríamos que escribir todos los números del 1 al 80, sin embargo ahora sabemos que podemos hacerlo así de simple: 1:80.\nInstrucciones\n Asigna los resultados del Martes al Viernes del vector_ruleta a la variable martes_a_viernes_ruleta. Utiliza la notación de dos puntos:. Imprime martes_a_viernes_ruleta en la consola.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Define la nueva variable siguiendo las instrucciones martes_a_viernes_ruleta \u0026lt;- vector_ruleta[c(2:5)] #Imprime martes_a_viernes_ruleta en la consola martes_a_viernes_ruleta  Seleccionando elementos: los buenos tiempos (4) Otra forma de abordar este problema es usar los nombres de los elementos (Lunes, Martes, etc.) en lugar de sus índices. Por ejemplo:\nvector_poker[\u0026quot;Lunes\u0026quot;]  Seleccionará el primer elemento de vector_poker porque el primer elemento es el que tiene el nombre \u0026ldquo;Lunes\u0026rdquo;. De la misma manera en que lo hicimos en el ejercicio anterior, puedes utilizar un vector que tenga los nombres de los elementos que deseas extraer:\nvector_poker[c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;)]  Instrucciones\n Calcula tus ganancias promedio durante los primeros tres días de la semana seleccionando los elementos por nombre. Asigna este valor a promedio_primeros3_dias. Puedes usar la función mean() para obtener el promedio de los elementos de un vector.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Only poker results promedio_primeros3_dias \u0026lt;- mean((vector_poker)[c(\u0026quot;Lunes\u0026quot;,\u0026quot;Martes\u0026quot;,\u0026quot;Miercoles\u0026quot;)])  Selección por comparación - Paso 1 Al utilizar los operadores de comparación, podemos abordar las preguntas anteriores de una manera más interesante.\nLos operadores de comparación en R son los siguientes:\n \u0026lt; menor que \u0026gt; mayor que \u0026gt;= mayor o igual que == igualdad != no igual  Como ya hemos visto, 6 \u0026gt; 5 da un valor verdadero: TRUE. Una característica muy buena de R es que puedes utilizar operaciones de comparación también en vectores. por ejemplo c(4,5,6) \u0026gt; 5 resulta en: FALSE, FALSE, TRUE. En otras palabras R hace la comparación por cada elemento y responde TRUE o FALSE dependiendo del resultado de la comparación. Esto es muy interesante y puede ser difícil de entender, practícalo en la consola!\nInternamente, R recicla el valor 5 cuando ejecuta c(4,5,6) \u0026gt; 5. R quiere hacer una comparación elemento por elemento de c(4,5,6) con 5, pero 5 no es un vector de tamaño 3. Para resolver esto R crea el vector c(5,5,5) y luego hace la comparación elemento por elemento.\nInstrucciones\n Obtengamos los días en que obtuvimos valores positivos (\u0026gt; 0) en el poker (vector_poker) y asignémoslo a la variable positivos_poker.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Que días obtiviste ganancias en el poker? positivos_poker \u0026lt;- vector_poker \u0026gt; 0 positivos_poker  Selección por comparación - Paso 2 Trabajar con comparaciones hará tu vida analítica más fácil. En lugar de seleccionar \u0026ldquo;a mano\u0026rdquo; un conjunto de días para analizar (como en los primeros ejercicios), puedes decirle a R que te seleccione solo aquellos días en los que tuviste ganancias en el poker.\nEn el ejercicio anterior usaste positivos_poker \u0026lt;- vector_poker \u0026gt;0 para encontrar aquellos días en los cuales tuviste ganancias. Ahora, nos gustaría saber no solo los días, sino las cantidades que ganaste en esos días.\nPuedes seleccionar los elementos deseados usando positivos_poker entre corchetes para seleccionar los elementos de vector_poker. Esto funciona, porque R seleccionará solo aquellos elementos en los cuales el vector positivos_poker tiene un valor verdadero (TRUE).\nInstrucciones - Asigna las ganancias en el poker a la variable ganancias_poker.\nScript R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Que días obtuviste ganancias en el poker? selection_vector \u0026lt;- vector_poker \u0026gt; 0 # Selecciona de vector_poker los días con ganancias ganancias_poker \u0026lt;- vector_poker[selection_vector] #Imprime ganancias_poker  Selección Avanzada Así como lo hiciste con el poker, también quisieras saber aquellos días en los que tuviste ganancia en la ruleta.\nInstrucciones\n Asigna los valores de las ganancias que tuviste en la ruleta a la variable ganancias_ruleta. Es decir selecciona solo aquellos valores de vector_ruleta que son positivos y asignalos a la variable ganancias_ruleta.  Script R\n# Resultados en la mesa de poker en la semana vector_poker \u0026lt;- c(140, -50, 20, -120, 240) # Resultados en la ruleta en la semana vector_ruleta \u0026lt;- c(-24, -50, 100, -350, 10) # Dando nombres a vector_poker y vector_ruleta vector_dias \u0026lt;- c(\u0026quot;Lunes\u0026quot;, \u0026quot;Martes\u0026quot;, \u0026quot;Miercoles\u0026quot;, \u0026quot;Jueves\u0026quot;, \u0026quot;Viernes\u0026quot;) names(vector_poker) \u0026lt;- vector_dias names(vector_ruleta) \u0026lt;- vector_dias # Que días obtiviste ganancias en la ruleta? positivos_ruleta \u0026lt;- vector_ruleta \u0026gt; 0 # Selecciona los valores de vector_ruleta que fueron ganancias ganancias_ruleta \u0026lt;- vector_ruleta[positivos_ruleta] #Imprime ganancias_ruleta  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"519df0a13c2039e541539818c4132a9e","permalink":"https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-vectores/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/r-studio/intro-r/r101-vectores/","section":"courses","summary":"Creando vectores Te sientes de suerte?\nEso espero, porque en este capítulo vamos de viaje a la Ciudad del Pecado, también conocida como el \u0026ldquo;Paraíso del estadístico\u0026rdquo; ;-).\nGracias a R y a tus nuevas habilidades analíticas, vas a aprender cómo mejorar tus ganancias en el casino y empezar una lucrativa carrera como jugador profesional. En este capítulo veremos cómo puedes fácilmente llevar la cuenta de tus apuestas y como hacer análisis simples de tus jugadas.","tags":null,"title":"Prácticas 2 - Vectores","type":"docs"},{"authors":null,"categories":null,"content":" El workspace es el área de trabajo de la sesión R e incluye todos los objetos en uso. En RStudio, los objetos en el espacio de trabajo se pueden explorar en el panel Entorno:\nPanel de entorno RStudio [IMG]\nR no guarda objetos relacionados con una sesión de trabajo individualmente: los conjuntos de datos y las salidas se almacenan en un solo archivo, cuyo nombre predeterminado es .RData. Este archivo es la imagen del área de trabajo activa durante una sesión.\nGuardar y cargar el espacio de trabajo Al final de una sesión R, puede guardar una imagen del espacio de trabajo, que se volverá a cargar automáticamente en el próximo inicio. Ej.:\nsave.image( \u0026quot;C: //.../myfile.RData\u0026quot;)  Para cargar la imagen del espacio de trabajo, use el comando de carga:\nload( \u0026quot;C: //.../myfile.RData\u0026quot;)  El menú Archivo de R La imagen del espacio de trabajo también se puede guardar o cargar desde el menú R, posiblemente con un nombre elegido por el usuario (por defecto no tiene nombre: .RData).\nEnumerar los objetos presentes. Para obtener una lista de los objetos y clases contenidos en el espacio de trabajo, puede usar el comando de menú Varios / Lista de objetos, o escribir el comando\nls()  Enumere los objetos que contienen (por ejemplo: la palabra tab):\nls(pattern = \u0026quot;tab\u0026quot;)  ## Eliminar los objetos\nPara eliminar los objetos:\nrm(objeto) # Podemos utilizar también remove(objeto)  Vaciar el área de trabajo. Para eliminar todos los objetos y limpiar el espacio de trabajo (corresponde a borrar el espacio de trabajo o borrar el entorno en otros programas):\nrm(list = ls())  Elimine los objetos que contienen (por ejemplo: la palabra tab):\nrm(list = ls(pattern = \u0026quot;tab\u0026quot;))  objeto = nombre del objeto, sin comillas El argumento de lista especifica la lista de objetos que se eliminarán.\nÁrea de trabajo de R y RCommander RCommander comparte el espacio de trabajo R, y los marcos de datos que forman parte del espacio de trabajo (área de trabajo) se pueden mostrar en la esquina superior izquierda (conjunto de datos o conjunto de datos).\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"a22c35132a044c94161ad1be5e4959cc","permalink":"https://www.marcusrb.com/en/courses/r-studio/workspace/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/r-studio/workspace/","section":"courses","summary":"El workspace es el área de trabajo de la sesión R e incluye todos los objetos en uso. En RStudio, los objetos en el espacio de trabajo se pueden explorar en el panel Entorno:\nPanel de entorno RStudio [IMG]\nR no guarda objetos relacionados con una sesión de trabajo individualmente: los conjuntos de datos y las salidas se almacenan en un solo archivo, cuyo nombre predeterminado es .RData. Este archivo es la imagen del área de trabajo activa durante una sesión.","tags":null,"title":"Área de trabajo","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"58231713414be23dcbb5f9f733a714c3","permalink":"https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-matrices/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/en/courses/r-studio/intro-r/r101-matrices/","section":"courses","summary":"","tags":null,"title":"Prácticas 3 - Matrices","type":"docs"},{"authors":null,"categories":null,"content":" Introducción Comenzaremos con una descripción general de cómo funcionan los modelos de aprendizaje automático y cómo se usan. Esto puede parecer básico si ya ha realizado modelos estadísticos o aprendizaje automático. No se preocupe, progresaremos para construir modelos potentes pronto.\nEste micro curso le permitirá construir modelos a medida que avance en el siguiente escenario:\nSu primo ha ganado millones de dólares especulando con bienes raíces. Se ofreció a convertirse en socio comercial con usted debido a su interés en la ciencia de datos. Él proporcionará el dinero, y usted proporcionará modelos que predicen cuánto valen varias casas.\nLe preguntas a tu primo cómo ha predicho los valores inmobiliarios en el pasado. y dice que es solo intuición. Pero más preguntas revelan que ha identificado patrones de precios de casas que ha visto en el pasado, y usa esos patrones para hacer predicciones para las casas nuevas que está considerando.\nEl aprendizaje automático funciona de la misma manera. Comenzaremos con un modelo llamado Árbol de decisión. Hay modelos más elegantes que dan predicciones más precisas. Pero los árboles de decisión son fáciles de entender y son el bloque de construcción básico para algunos de los mejores modelos en ciencia de datos.\nPara simplificar, comenzaremos con el árbol de decisión más simple posible.\nDivide las casas en solo dos categorías. El precio previsto para cualquier casa en consideración es el precio promedio histórico de las casas en la misma categoría.\nUsamos datos para decidir cómo dividir las casas en dos grupos, y luego nuevamente para determinar el precio previsto en cada grupo. Este paso de capturar patrones de datos se llama ajuste (fitting) o capacitación del modelo (training). Los datos utilizados para ajustarse fit al modelo se denominan datos de entrenamiento.\nLos detalles de cómo se ajusta el modelo (por ejemplo, cómo dividir los datos) son lo suficientemente complejos como para guardarlos para más adelante. Después de que el modelo se haya ajustado, puede aplicarlo a nuevos datos para predecir los precios de viviendas adicionales.\nMejorando el árbol de decisiones ¿Cuál de los siguientes dos árboles de decisiones es más probable que resulte de ajustar los datos de capacitación de bienes raíces?\nEl árbol de decisión de la izquierda (Árbol de decisión 1) probablemente tenga más sentido, porque captura la realidad de que las casas con más habitaciones tienden a venderse a precios más altos que las casas con menos habitaciones. El mayor inconveniente de este modelo es que no captura la mayoría de los factores que afectan el precio de la vivienda, como la cantidad de baños, el tamaño del lote, la ubicación, etc.\nPuede capturar más factores utilizando un árbol que tiene más \u0026ldquo;divisiones\u0026rdquo;. Estos se llaman árboles \u0026ldquo;más profundos\u0026rdquo;. Un árbol de decisión que también considera el tamaño total del lote de cada casa podría verse así:\nUsted predice el precio de cualquier casa rastreando a través del árbol de decisión, siempre eligiendo la ruta correspondiente a las características de esa casa. El precio previsto para la casa está en la parte inferior del árbol. El punto en la parte inferior donde hacemos una predicción se llama hoja (leaf).\nLas divisiones y los valores en las hojas estarán determinados por los datos, por lo que es hora de que revise los datos con los que trabajará.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"8bfb262388722c838ad697029c733ff7","permalink":"https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-1-como-funciona/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/data-science/intro-machine-learning/ml101-1-como-funciona/","section":"courses","summary":"Introducción Comenzaremos con una descripción general de cómo funcionan los modelos de aprendizaje automático y cómo se usan. Esto puede parecer básico si ya ha realizado modelos estadísticos o aprendizaje automático. No se preocupe, progresaremos para construir modelos potentes pronto.\nEste micro curso le permitirá construir modelos a medida que avance en el siguiente escenario:\nSu primo ha ganado millones de dólares especulando con bienes raíces. Se ofreció a convertirse en socio comercial con usted debido a su interés en la ciencia de datos.","tags":null,"title":"Cómo funcionan los modelos","type":"docs"},{"authors":null,"categories":null,"content":" LOREM IPSUM\nEstructura del programa y cursos La estructura del programa formativo se compone en dos grandes bloques:\n Power BI fundamentos 101 Power BI avanzado 201  ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.\n","date":1609714800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"6b51a0470006b1bf441695c6065dfa23","permalink":"https://www.marcusrb.com/recursos-big-data/","publishdate":"2021-01-04T00:00:00+01:00","relpermalink":"/recursos-big-data/","section":"resources","summary":"El programa formativo de Power BI eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Recursos de Big Data","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Google Data Studio es una herramienta gratuita de visualización e informes de datos basada en la nube que se conecta a muchas fuentes de datos diferentes, y convierte esos datos en paneles informativos e informes que son fáciles de entender y compartir, y son totalmente personalizables.\n- Guía de Microsoft Power BI - Guía de Tableau - Guía de Google Data Studio - Guía de Grafana  ","date":1609714800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609850196,"objectID":"3848bc6be694d773725789d024797a0f","permalink":"https://www.marcusrb.com/en/data-visualization-resources/","publishdate":"2021-01-04T00:00:00+01:00","relpermalink":"/en/data-visualization-resources/","section":"resources","summary":"El programa formativo de Data Studio cubre todos los aspectos de la herramienta. Curso a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Data visualizacion resourcs","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Power BI te guiaré paso a paso para obtener habilidades de Power Bi. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Power BI con éxito.\nLa aplicación de dashboard con Power BI es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.\nEstructura del programa y cursos La estructura del programa formativo se compone en dos grandes bloques:\n Power BI fundamentos 101 Power BI avanzado 201  ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.\nCurso de fundamentos en Power BI (~20 horas)  Introducción a Analítica de datos Business Intelligence vs Big Data Fundamentales de Visualización de datos Proyectos de Data Discovery y Agilismo Introducción a Power BI y conexión a fuentes de datos Transformación de datos con Power Query Modelado de datos y SQL Analysis Services DAX, funciones y mejores prácticas Reportes y cuadro de mandos en Power BI Power BI App y conexiones on-premise Power BI Flow y Data Pipeline Labs y Prácticas  Realización de PoC (Proof of Concept) con Power BI También realizo pruebas de conceptos en su empresa, con una muestra de datos (también del tipo dummies), me ocupo de transformar su necesidad en un panel de control dinámico y eficiente.\nSe realizará una sesión de 4 horas máximo de data discovery y otras sesiones de detección de KPI e indicadores importantes.\n¿Está interesado en capacitarse en Power BI ? Puedes adquirir el curso de Power BI en la plataforma UDEMY a un precio reducido y totalmente online.\nSi necesitas un curso en remoto, en tu empresa o a nivel profesional puedes llamar hoy o utilizar el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Formación de fundamentos en Microsoft PowerBI\nDEMO Data Analysis con Power BI   ","date":1591743600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"aabbc0978f04c57e8c816ac3ce0c2eb1","permalink":"https://www.marcusrb.com/curso-power-bi-fundamentos/","publishdate":"2020-06-10T00:00:00+01:00","relpermalink":"/curso-power-bi-fundamentos/","section":"courses","summary":"El programa formativo de Power BI eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Formación fundamentos en Microsoft Power BI","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Power BI te guiaré paso a paso para obtener habilidades de Power Bi. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Power BI con éxito.\nLa aplicación de dashboard con Power BI es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.\nEstructura del programa y cursos La estructura del programa formativo se compone en dos grandes bloques:\n Power BI fundamentos 101 Power BI avanzado 201  ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.\nCurso avanzado en DAX y Power Query de Power BI (~70 horas) Incluye el curso de fundamentos 101 más:\nCentralización de datos Creación de flujos de datos, configuración de puerta de enlace, actualización de flujos de datos, creación de informes a partir de flujos de datos, centralización de conjuntos de datos, certificación de conjuntos de datos, creación de informes a partir de conjuntos de datos\nEl lenguaje de fórmulas de Power Query Usando La Barra De Fórmula; Usando El Editor Avanzado; Descripción general del lenguaje M; Explorando M usando #shared\nComprensión del código generado automáticamente Excel.Workbook; File.Contents; Table.TransformColumns; Table.TransformColumnTypes; Table.UnpivotColumns; Table.UnpivotOtherColumns\nCrear funciones personalizadas en M Definiendo una función; Definición de parámetros de entrada; El operador de acceso; Definir el cuerpo de la función; Usando parámetros opcionales; Funciones de llamada\nTécnicas de iteración Beneficio de generar listas; Generando listas de números; Generando listas de fechas; Generando listas alfanuméricas; Usando cada función; Aplicar una función a una lista de archivos\nDAX avanzado Usando DAX Studio; Escribir fórmulas complejas; Usando variables; Calcular promedios móviles; Cálculo de totales acumulados; Cálculos de percentiles; Creación de fórmulas avanzadas de inteligencia de tiempo; Usando múltiples tablas de fechas; Trabajando con calendarios no estándar\nTrabajando con tablas calculadas Crear tablas calculadas; Funciones DAX que devuelven tablas; La función CALCULATETABLE; La función ADDCOLUMNS; La función RESUMEN; RESUMEN con ROLLUP; VALORES y funciones DISTINCT; La función CROSSJOIN; La función TOPN; La función ROW; Usar tablas calculadas dentro del modelo de datos\nUsar tablas de parámetros ¿Qué es una tabla de parámetros? Cuándo usar tablas de parámetros; Usando la función HASONEVALUE; Usando la función VALUES; Crear rebanadoras personalizadas; Crear múltiples soluciones de tabla de parámetros\nModelado y visualización de datos avanzados Trabajar con múltiples tablas de hechos, Usar relaciones activas e inactivas, Usar la función USERELATIONSHIP, Crear propiedades visuales dinámicas, Usar BLANK () para hacer que la visibilidad sea dinámica\nDashboards avanzados Agregar enlaces personalizados a un tablero de instrumentos; Usando el widget de contenido web; Usando el widget de video; Mosaicos de panel de transmisión en tiempo real, integración de Power Automation y PowerApps\nProgramas personalizados en empresas y pymes Si requieres una formación personalizada tanto sea de fundamentos que conceptos avanzados de inteligencia de negocio, SQL Server Analysis, DAX, modelado de datos y ETL con Power Query, puedo adaptar el contenido de Power BI para un mínimo de 4 personas hasta un máximo de 20.\nRealización de PoC (Proof of Concept) con Power BI También realizo pruebas de conceptos en su empresa, con una muestra de datos (también del tipo dummies), me ocupo de transformar su necesidad en un panel de control dinámico y eficiente.\nSe realizará una sesión de 4 horas máximo de data discovery y otras sesiones de detección de KPI e indicadores importantes.\n¿Está interesado en capacitarse en Power BI ? Si necesitas un curso en remoto, en tu empresa o a nivel profesional puedes llamar hoy o utilizar el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Formación avanzada en Microsoft PowerBI\nDEMO Data Analysis con Power BI   ","date":1591743600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"2db49df0d50a23b7a281f66e98990b39","permalink":"https://www.marcusrb.com/curso-power-bi-avanzado/","publishdate":"2020-06-10T00:00:00+01:00","relpermalink":"/curso-power-bi-avanzado/","section":"courses","summary":"El programa formativo de Power BI eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Formación avanzada en Microsoft Power BI","type":"docs"},{"authors":null,"categories":null,"content":" Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Tableau te guiaré paso a paso para obtener habilidades de Tableau. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Tableau con éxito.\nLa aplicación de dashboard con Tableau es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.\nEstructura del programa La estructura del programa formativo se compone en dos grandes bloques: - fundamentos - avanzado\nambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.\nCurso en Tableau Public, Desktop y Prep (~30 horas)  Introducción a Analítica de datos Business Intelligence vs Big Data Fundamentales de Visualización de datos Proyectos de Data Discovery y Agilismo Introducción a Tableau y conexión a fuentes de datos Transformación de datos con Tableau Prep creación campos calculados, medidas, parámetros y filtros dinámicos Reportes y cuadro de mandos en Tableau Tableau Public, Tableau App, Tableau Reader Data Pipeline Labs y Prácticas  Programas personalizados en empresas y pymes Si requieres una formación personalizada tanto sea de fundamentos que conceptos avanzados de inteligencia de negocio, base de datos, modelado de datos y ETL con Tableau Prep, puedo adaptar el contenido de Tableau Desktop para un mínimo de 4 personas hasta un máximo de 20.\nRealización de PoC (Proof of Concept) con Tableau También realizo pruebas de conceptos en su empresa, con una muestra de datos (también del tipo dummies), me ocupo de transformar su necesidad en un panel de control dinámico y eficiente.\nSe realizará una sesión de 4 horas máximo de data discovery y otras sesiones de detección de KPI e indicadores importantes.\n¿Está interesado en capacitarse en Tableau ? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Formación en Tableau\n","date":1591743600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"e62390f311d96ede1801686838f742fe","permalink":"https://www.marcusrb.com/curso-tableau/","publishdate":"2020-06-10T00:00:00+01:00","relpermalink":"/curso-tableau/","section":"courses","summary":"El programa formativo de Tableau eliges entre el módulo fundamentos y avanzado a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Formación Tableau Desktop y Tableau Prep","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"d9327813c473fa4f2fdac8148d6b0efc","permalink":"https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-factores/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/en/courses/r-studio/intro-r/r101-factores/","section":"courses","summary":"","tags":null,"title":"Prácticas 4 - Factores","type":"docs"},{"authors":null,"categories":null,"content":" Explorando los datos Usando pandas para familiarizarse con sus datos El primer paso en cualquier proyecto de aprendizaje automático es familiarizarse con los datos. Usarás la biblioteca Pandas para esto. Pandas es la herramienta principal de datos que los científicos usan para explorar y manipular datos. La mayoría de las personas abrevian pandas en su código como pd. Hacemos esto con el comando\nimport pandas as pd  La parte más importante de la biblioteca Pandas es el DataFrame. Un DataFrame contiene el tipo de datos que podría considerar como una tabla. Esto es similar a una hoja en Excel, o una tabla en una base de datos SQL.\nPandas tiene métodos poderosos para la mayoría de las cosas que querrás hacer con este tipo de datos.\nComo ejemplo, veremos datos sobre los precios de la vivienda en Melbourne, Australia. En los ejercicios prácticos, aplicará los mismos procesos a un nuevo conjunto de datos, que tiene los precios de las viviendas en Iowa.\nLos datos de ejemplo (Melbourne) están en la ruta del archivo ../input/melbourne-housing-snapshot/melb_data.csv.\nCargamos y exploramos los datos con los siguientes comandos:\n# save filepath to variable for easier access melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv' # read the data and store data in DataFrame titled melbourne_data melbourne_data = pd.read_csv(melbourne_file_path) # print a summary of the data in Melbourne data melbourne_data.describe()  Interpretación de la descripción de datos Los resultados muestran 8 números para cada columna en su conjunto de datos original. El primer número, el recuento, muestra cuántas filas tienen valores no faltantes.\nLos valores perdidos surgen por muchas razones. Por ejemplo, el tamaño de la segunda habitación no se recogería al inspeccionar una casa de 1 habitación. Volveremos al tema de los datos faltantes.\nEl segundo valor es la media, que es el promedio. Debajo de eso, std es la desviación estándar, que mide la extensión numérica de los valores.\nPara interpretar los valores mínimo, 25%, 50%, 75% y máximo, imagine ordenar cada columna del valor más bajo al más alto. El primer valor (el más pequeño) es el mínimo. Si recorre un cuarto de camino en la lista, encontrará un número que es mayor que el 25% de los valores y menor que el 75% de los valores. Ese es el valor del 25% (pronunciado \u0026ldquo;percentil 25\u0026rdquo;). Los percentiles 50 y 75 se definen de forma análoga, y el máximo es el número más grande.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"f8716ea9578c0da0db4cbed18221aff3","permalink":"https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-2-basic-data-exploration/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/data-science/intro-machine-learning/ml101-2-basic-data-exploration/","section":"courses","summary":"Explorando los datos Usando pandas para familiarizarse con sus datos El primer paso en cualquier proyecto de aprendizaje automático es familiarizarse con los datos. Usarás la biblioteca Pandas para esto. Pandas es la herramienta principal de datos que los científicos usan para explorar y manipular datos. La mayoría de las personas abrevian pandas en su código como pd. Hacemos esto con el comando\nimport pandas as pd  La parte más importante de la biblioteca Pandas es el DataFrame.","tags":null,"title":"Explorando los datos","type":"docs"},{"authors":null,"categories":null,"content":" Estructura recursos Google Data Studio es una herramienta gratuita de visualización e informes de datos basada en la nube que se conecta a muchas fuentes de datos diferentes, y convierte esos datos en paneles informativos e informes que son fáciles de entender y compartir, y son totalmente personalizables.\nCaracterísticas Data Studio Google Data Studio es intuitivo, rápido, flexible y permite una gran cantidad de opciones de diseño y presentación.\nAmplia gama de conectores de datos. Data Studio tiene 17 conectores de datos internos y alrededor de 108 de terceros para elegir Funciones fáciles de usar Data Studio proporciona docenas de funciones matemáticas, de cadena, de fecha y otras para transformar sus datos en valores más útiles. Variedad de formas, imágenes y texto. Data Studio le permite agregar formas, imágenes y texto a sus informes y paneles para que sean más fáciles de leer. Niveles de permisos Aprovechando la tecnología Google Drive, puede administrar fácilmente a todos sus usuarios y su nivel de acceso Mezcla de datos, ahora una realidad Data Studio le permite agregar datos de múltiples fuentes para tener una vista comparativa de ellos a la vez\nLos recursos de este curso están disponibles en:\n Cursos online Google Data Studio Video-Tutoriales Google Data Studio Recursos para Google Academy for Ads  Novedades de Google Data studio\ngoogle-data-studio-1\nDEMO Data Analysis con Google Data Studio   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"6801fa8c729740895a529e2b92011894","permalink":"https://www.marcusrb.com/curso-google-data-studio/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/curso-google-data-studio/","section":"courses","summary":"El programa formativo de Data Studio cubre todos los aspectos de la herramienta. Curso a empresas, autónomos y clases privadas en modalidad remoto o presencial.","tags":null,"title":"Formación en Google Data Studio","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"7559e17db0dad0f0939779821f1bc636","permalink":"https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-dataframes/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/en/courses/r-studio/intro-r/r101-dataframes/","section":"courses","summary":"","tags":null,"title":"Prácticas 5 - Data Frames","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"3bd15a0b7c1ff918a4fc8652808355b1","permalink":"https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-3-exercise-1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/data-science/intro-machine-learning/ml101-3-exercise-1/","section":"courses","summary":"","tags":null,"title":"Caso Práctico 1 - Explorando los datos","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568415600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"faec3eff33233a879432610ef782eab4","permalink":"https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-listas/","publishdate":"2019-09-14T00:00:00+01:00","relpermalink":"/en/courses/r-studio/intro-r/r101-listas/","section":"courses","summary":"","tags":null,"title":"Prácticas 6 - Listas","type":"docs"},{"authors":null,"categories":null,"content":" Seleccionar datos para modelar Su conjunto de datos tenía demasiadas variables para entenderlo, o incluso para imprimirlo bien. ¿Cómo puede reducir esta cantidad abrumadora de datos a algo que pueda entender?\nComenzaremos eligiendo algunas variables usando nuestra intuición. Los cursos posteriores le mostrarán técnicas estadísticas para priorizar automáticamente las variables.\nPara elegir variables / columnas, necesitaremos ver una lista de todas las columnas en el conjunto de datos. Eso se hace con la propiedad de columnas del DataFrame (la línea inferior del código a continuación).\nimport pandas as pd melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv' melbourne_data = pd.read_csv(melbourne_file_path) melbourne_data.columns  # The Melbourne data has some missing values (some houses for which some variables weren't recorded.) # We'll learn to handle missing values in a later tutorial. # Your Iowa data doesn't have missing values in the columns you use. # So we will take the simplest option for now, and drop houses from our data. # Don't worry about this much for now, though the code is: # dropna drops missing values (think of na as \u0026quot;not available\u0026quot;) melbourne_data = melbourne_data.dropna(axis=0)  Hay muchas formas de seleccionar un subconjunto de sus datos. El Micro Curso de Pandas los cubre con más profundidad, pero por ahora nos centraremos en dos enfoques.\nNotación de puntos, que usamos para seleccionar el \u0026ldquo;objetivo de predicción\u0026rdquo; Seleccionando con una lista de columnas, que usamos para seleccionar las \u0026ldquo;características\u0026rdquo;\nSelecting The Prediction Target You can pull out a variable with dot-notation. This single column is stored in a Series, which is broadly like a DataFrame with only a single column of data.\nWe\u0026rsquo;ll use the dot notation to select the column we want to predict, which is called the prediction target. By convention, the prediction target is called y. So the code we need to save the house prices in the Melbourne data is\ny = melbourne_data.Price  Elegir \u0026ldquo;Características\u0026rdquo; Las columnas que se ingresan en nuestro modelo (y luego se usan para hacer predicciones) se denominan \u0026ldquo;características\u0026rdquo;. En nuestro caso, esas serían las columnas utilizadas para determinar el precio de la vivienda. A veces, usará todas las columnas excepto el objetivo como características. Otras veces estará mejor con menos funciones.\nPor ahora, crearemos un modelo con solo algunas características. Más adelante verá cómo iterar y comparar modelos creados con diferentes características.\nSeleccionamos múltiples características al proporcionar una lista de nombres de columnas entre paréntesis. Cada elemento de esa lista debe ser una cadena (con comillas).\nAquí hay un ejemplo:\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']  Por conveniencia atribuimos los datos a la variable X\nX = melbourne_data[melbourne_features]  Revisemos rápidamente los datos que usaremos para predecir los precios de la vivienda utilizando el método de descripción y el método de encabezado, que muestra las pocas filas superiores.\nX.describe()  X.head()  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"6f29ca725a1ba72c2ed942a6ad1fd5fe","permalink":"https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-4-datos-modelo/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/data-science/intro-machine-learning/ml101-4-datos-modelo/","section":"courses","summary":"Seleccionar datos para modelar Su conjunto de datos tenía demasiadas variables para entenderlo, o incluso para imprimirlo bien. ¿Cómo puede reducir esta cantidad abrumadora de datos a algo que pueda entender?\nComenzaremos eligiendo algunas variables usando nuestra intuición. Los cursos posteriores le mostrarán técnicas estadísticas para priorizar automáticamente las variables.\nPara elegir variables / columnas, necesitaremos ver una lista de todas las columnas en el conjunto de datos. Eso se hace con la propiedad de columnas del DataFrame (la línea inferior del código a continuación).","tags":null,"title":"Seleccionar los datos para el modelo","type":"docs"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Aprende a enviar los valores de e commerce al pixel de Google Ads con Tag Manager. El paso más tedioso de todos marketeros es registrar correctamente las transacciones desde la web a Google Analytics o cualquier herramienta de analítica. Pero un solo check no es suficiente para poder llevar todas tipos de personalizaciones de comercio electrónico y aprovechar las bondades del dataLayer presente en nuestro site. Un ejemplo es la personalización de los eventos de comercio electrónico mejorado a eventos, o mejor, hacía las plataforams de Advertising, como Google Ads, Facebook, Linkedin, hasta Twitter, Tik Tok y Bing Ads. Es importante tener el dataLayer implementado (bien por plugin o bien via desarrollo), y el resto lo haremos en Google Tag Manager a través de pequeñas funciones en JavaScript. ¿Estás listo para ser el ninja de Google Tag Manager? En este video tutorial te explicaremos como realizar unos pasos más habituales y el resto podrás realizarlos sin dificultades.\nVideo   ","date":1600279200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"14ac9a62cf80c580ef682793a4cbc54d","permalink":"https://www.marcusrb.com/en/talk/google-ads-gtm/","publishdate":"2020-09-01T20:00:00Z","relpermalink":"/en/talk/google-ads-gtm/","section":"talk","summary":"Tutorial - Enviamos los valores de e-commerce al pixel de Google Ads con Tag Manager.","tags":["googleads","googletagmanager"],"title":"Tutorial - Enviamos los valores de e-commerce al pixel de Google Ads con Tag Manager.","type":"talk"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Aprende a rear una etiqueta en Google Tag Manager para registrar las llamadas. ¿Necesitas registrar las llamadas efectuadas directamente desde la web? Con la personalización de la etiqueta de seguimiento de eventos de llamadas es posible tanto para Google Ads, como este video tutorial, como el resto de plataformas de publicidad. Aunque la etiqueta está bien escondida dentro del repositorio en Google Tag Manager, veamos como poder realizar con simples pasos su personalización y tener constancia del tráfico entrante de nuestros clientes potenciales y grabar sus interacciones correctamente para la mejor atribución de las conversiones en Google Ads u lo que sea. No hace apoyo técnico, solo una cuenta de Google Ads, Google Tag Manager y seguir los pasos del vídeo. En caso de tener otro tipo de evento, como slider o eventos tipo Ajax, entonces se requiere una personalización avanzada con la llamada a la librería de Ajax o jQuery.\nVideo   ","date":1599674400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"0c1f7356902c60a296debdd373a4d529","permalink":"https://www.marcusrb.com/en/talk/llamadas-gtm/","publishdate":"2020-09-01T20:00:00Z","relpermalink":"/en/talk/llamadas-gtm/","section":"talk","summary":"Tutorial - crear una etiqueta en Google Tag Manager para registrar las llamadas.","tags":["leads","googletagmanager","llamadas"],"title":"Tutorial - crear una etiqueta en Google Tag Manager para registrar las llamadas.","type":"talk"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Aprende cómo personalizar el pixel de Bing Ads con Google Tag Manager. En este tutorial vemos como personalizar el pixel de seguimiento del segundo buscador más importante, Bing.\nAunque no sea muy extendido su uso, Bing Ads cada vez más está ganando posiciones de mercado, como en muchos paises europeos es tan importante más que Google (por la cuestión privacidad), siendo el canal habitual en Francia, Alemania y UK. Gracias al gesto de etiquetas Google Tag Manager, podemos sin intervención de un desarrollador implementar el pixel o los diferentes de eventos, donde necesitamos recoger las interacciones de los usuarios, eventos de usabilidad y todas aquellos relacionados con el comercio electrónico. Gracias a la etiqueta nativa presente en Google Tag Manager, podemos en pocos clics crear diferentes pixel, el principal como base, y el resto para las interacciones. Es importante contar también con el dataLayer de comercio electrónico (clásico o mejorado), para recolectar el resto de eventos específicos delos pasos del funnel, al igual que hacemos con Google Ads y Google Analytics..\n¿Quién es el ponente? Marco Russo\nConsultor y Especialista en Data \u0026amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.\nVideo   ","date":1598983200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"b626f185ec2961823763029e120b0c0b","permalink":"https://www.marcusrb.com/en/talk/bing_ads_gtm/","publishdate":"2020-09-01T20:00:00Z","relpermalink":"/en/talk/bing_ads_gtm/","section":"talk","summary":"Tutorial - Aprende cómo personalizar el pixel de Bing Ads con Google Tag Manager.","tags":["bingads","googletagmanager"],"title":"Tutorial - Aprende cómo personalizar el pixel de Bing Ads con Google Tag Manager.","type":"talk"},{"authors":null,"categories":null,"content":" Como realizar una consultoría o auditoría de Google Analytics ¿Necesita ayuda con Google Analytics? Permita que le muestre cómo utilizar los datos analíticos para cuantificar sus esfuerzos de SEO, aumentar el tráfico web, convertir a más visitantes en clientes potenciales y mejorar de forma mensurable su desempeño de marketing.\n¿Qué es Google Analytics? Google Analytics es una herramienta de análisis web que es extremadamente poderosa, además de ser gratuita. Los propietarios de negocios, ejecutivos de marketing y webmasters pueden usarlo para: medir y rastrear la actividad del sitio web, optimizar el rendimiento del sitio web, mejorar las tasas de conversión (por ejemplo, visitante a líder o visitante a venta) e incluso mejorar el rendimiento del marketing offline. Google Analytics está \u0026ldquo;basado en cookies\u0026rdquo; (a diferencia de una herramienta de análisis web que analiza los archivos de registro o logs).\nPara usar Google Analytics, simplemente hay que implementar una pequeña porción del código de JavaScript en cada página de su sitio web que desea rastrear. Relativamente hablando, esto hace que Google Analytics sea extremadamente fácil de usar y algunos sitios web se pueden configurar en menos de 30 minutos. Google Analytics se puede personalizar de varias maneras, por lo que es casi tan poderoso como muchas soluciones analíticas de sitios web pagados.\n¿Necesitas ayuda con Google Analytics y la nueva versión GA4? Si eres como la mayoría de los dueños de negocios, pones Google Analytics en tu sitio web, lo miras por unos meses y luego te ocupas de otras cosas. ¿Quién tiene tiempo para ver toda esta información? ¿Suena familiar? Únete a la multitud. Una de las quejas más comunes que tienen los propietarios de negocios locales sobre los datos de análisis de marketing y Google Analytics en particular es que hay una tonelada de datos: demasiados datos y no suficientes conocimientos procesables. ¡No necesita otro informe para analizar ni una hoja de cálculo para interpretar!\nTe ofrezco una solución de \u0026ldquo;hágalo por mí\u0026rdquo; que ayuda a los propietarios de negocios pequeños (y no tan pequeños) a personalizar Google Analytics para satisfacer mejor sus necesidades, examinar los informes interminables para identificar los más relevantes para el cliente y convertir los datos en resultados accionables! Actualizado también a la nueva versión Google Analytics 4 o GA4, incluido Firebase Analytics.\n¿Está interesado en consultar o capacitarse en Google Analytics o análisis digital? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Google Analytics - GA4\nRealizar una consultoría y/o auditoría de Google Tag Manager Google Tag Manager, la herramienta más utilizada para etiquetar eventos e interacciones, será el rol más importante para detectar nuevos insight en tu negocio, tener claro desde principio que medir y como medirlo,\nEl Administrador de etiquetas de Google es una gran herramienta para que los dueños de negocios o los equipos de marketing vean cómo funciona su sitio web: qué áreas están funcionando, qué rendimiento tiene bajo rendimiento, qué funciones no se están usando como creía, etc. Lo vemos como una excelente forma de recopilar datos de experiencia de usuario UX \u0026ldquo;User Experience\u0026rdquo;, y mejorar el ratio de conversión, CRO \u0026ldquo;Conversion Rate Optimization\u0026rdquo; en vivo en su sitio de clientes reales.\nGoogle proporciona una guía de desarrollador para usar la herramienta de análisis, tengo experiencia de casi 5 años en la herramienta de analísis para grandes clientes y cuentas, además de tener un background de programad web, especializado en la configuración de Google Tag Manager, así como en el análisis de los datos que proporciona (porque, sin información accionable, ¿qué valor tiene? datos proporcionan en sí mismo para obtener más de su sitio web?).\nQue servicio incluye el servicio de consultoría de Tag Manager? Como servicio de consultoría de Tag Manager, primero tendré que ver su objetivo empresarial y qué quieres que se analice, puedo analizar todo un sitio web, pero la pregunta sería: ¿Todos los datos valen lo mismo para su empresa? Así que ofrezco:\n Auditoría de sitios web y mapeo de etiquetas\n Configuración y configuración\n Aplicación y monitoreo\n Formación  Plan de implementación de Tag Manager Google Tag Manager permite el etiquetado inmediato y la inserción de fragmentos de código a su sitio web, pero sin un experto en la mejor forma de configurarlo o vincularlo con su seguimiento actual de Google Analytics, puede parecer perdido. Realizaré la configuración y la migración de datos del Administrador de etiquetas de Google, lo que permite un seguimiento y generación de informes analíticos eficientes. En caso de ayuda, podré necesitaré apoyo de vustro departamento IT.\nOs asesoraré para que el Administrador de etiquetas de Google será realizado de la mejor configuración junto con el mapeo de las etiquetas en función del objetivo de su sitio (lo que le permite realizar un seguimiento de todo, desde diferentes formularios de captura de clientes hasta clics de enlaces específicos).\nAuditoría de Google Tag Manager Teniendo en cuenta que además de un servicio de consultoría e implementación, también podré asesorarle sobre los puntos claves para su correcta instalación, en el caso ya esté realizado el mismo por vosotros o terceros. Así que me dedicaré con este servicio a la búsqueda optima de su correcta implementación, auditando todo el proceso de recogida de los datos, hasta la correcta visualización en las etiquetas, sean de Google Analytics, Google Ads, Facebook Ads u otras etiquetas o pixel de conversiones, además de todos los puntos de contacto llamados micro-conversiones, pasos previo a su conversión.\nFormación in-company de Google Tag Manager Si tiene un equipo de marketing familiarizado con Google Analytics y sus productos relacionados y solo está buscando ayuda para comenzar a implementar y usar el Administrador de etiquetas de Google, también ofrezco sesiones de capacitación personalizadas. Para estas sesiones, trabajaré con usted personalmente o en equipo para asegurarnos de que confía en su configuración y en el uso del Administrador de etiquetas de Google en su sitio. Incluso ofrezco controles de seguimiento y monitoreo después de mis capacitaciones para garantizar que se sienta cómodo con la herramienta y cómo puede desarrollarse para que coincida con el crecimiento y los cambios de su sitio web.\nMis servicios de formación in-company serán presenciales en vuestras oficinas en un máximo de 8 - 12 horas, siendo posible más horas a distancia via streaming, u online a través de la plataforma Moodle de propiedad de una escuela digital.\n¿Está interesado en consultar o capacitarse en Google Tag Manager? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Google Tag Manager\n","date":1591807974,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"af25dbaf730e7b3aca1f92b260e3585d","permalink":"https://www.marcusrb.com/servicios-freelance-google-analytics/","publishdate":"2020-06-10T17:52:54+01:00","relpermalink":"/servicios-freelance-google-analytics/","section":"services","summary":"Ofrezco servicios de auditoria, implementación y formación de la herramienta de análisis web, Google Analytics y creación de plan de medición en Google Tag Manager.","tags":null,"title":"Consultor freelance de analítica digital, Google Analytics, GA4 y GTM","type":"page"},{"authors":null,"categories":null,"content":" En los últimos 10 años, he participado en los programas formativos de muchas escuelas de negocio, universidades y centro de formación de la administración pública. He realizado servicio de tutoría, docencia de asignaturas, formación a grandes empresas, pymes - autónomos, particulares.\nDocencia para programas de posgrados, máster, cursos y webinar. Con más de 10 años de expieriencia en sector formativo, colaboro con diferentes escuelas de negocio y universidades, así como administración públicas para realizar cursos, seminarios o participar en congresos en materia de analítica digital, analítica de datos y visualización.\nHe preparado varios programas, módulos y cursos personalizados de fundamentos hasta niveles más avanzados con duración máxima de 120 horas de formación a un público de casi 100 alumnos en algunos casos. Las formaciones pueden ser presenciales, a distancia, grabadas o en formato blended.\nGestiono y preparo: - el cuadro formativo - tutoría y asesoramiento - los módulos en formato presentación - los casos prácticos - los laboratorios durante las horas lectivas - las prácticas y soluciones - grabaciones\nFormación in-company en Analítica de negocio Tanto si tiene un equipo de marketing familiarizado con herramientas de análisis digitales como Google Analytics y sus productos relacionados y solo está buscando ayuda para comenzar a implementar y usar el Administrador de etiquetas de Tag Manager, también ofrezco sesiones de capacitación personalizadas. Para estas sesiones, trabajaré con usted personalmente o en equipo para asegurarnos de que confía en su configuración y en el uso del Administrador de etiquetas de Google en su sitio. Incluso ofrezco controles de seguimiento y monitoreo después de mis capacitaciones para garantizar que se sienta cómodo con la herramienta y cómo puede desarrollarse para que coincida con el crecimiento y los cambios de su sitio web. Tengo amplia experiencia también en otras herramientas de publicidad Google Ads y FacebookAds , me limito a formación y sesiones de auditoría.\nPara el resto de formaciones a medidas con un mínimo de 10 horas, relacionadas con el resto de disciplinas de Data, tengo años de experiencia en profesorado y formaciones en:\n Sesiones de Data Discovery Preparación de indicadores y fuentes de datos SQL y bases de datos Modelado de datos ETL, preparación y transformación de datos Visualización con Power BI, Tableau, Data Studio, AWS QuickSight Minería de datos y Machine Learning Deep Learning Python para data science y analísis de datos, Pandas - Numpy - Scipy - Scikit-learn, visualización Matplotlib / Seaborn R Studio Cloud Engineer en AWS, Google Cloud Platform, Microsoft Azure y DataBricks  Mis servicios de formación in-company serán presenciales en vuestras oficinas, siendo posible a distancia via streaming u online a través de las plataformas habituales (Zoom, Google Meet, Teams, etc.)\n¿Está interesado en capacitarse en una de estas disciplinas o quieres contarme algo más? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para Formación en Datos\n","date":1591036301,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"1454801cb74f52e1c5c7161d7b46b313","permalink":"https://www.marcusrb.com/formacion-in-company-docencia-escuelas-negocio/","publishdate":"2020-06-01T18:31:41Z","relpermalink":"/formacion-in-company-docencia-escuelas-negocio/","section":"services","summary":"Formación a medida para tu empresa y/o clases particulares de analítica digital, analítica de datos y visualización con Power BI, Tableau y Data Studio.","tags":null,"title":"Formación in-company \u0026 docencia en escuelas de negocios","type":"page"},{"authors":null,"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Cómo participar en retos Kaggle deData Science. Nivel 1.\n16 de Abril, 12-1h\nCómo participar en retos Kaggle de Data Science. Nivel 1. Tanto si ya tienes algo de experiencia en Data Analytics como si no, no puedes perderte este evento de Data Science en el que trabajaremos directamente en la plataforma Kaggle con Python. Haremos el reto más \u0026ldquo;famoso\u0026rdquo;, pero también descubriremos métodos y técnicas útiles para otros retos. ¡Por cierto! habrá un premio para el primero.\nSe requiere la instalación previa en vuestro ordenador del programa Anaconda, Docker con Jupyter y Visual Studio. No dudes en contactar con nosotros antes del Webinar por si necesitas instalar alguna herramienta adicional.\nQué veremos:\n Organización del entorno de trabajo Aplicación de Metodologías para la exploración de datos Entrenamiento de tu primer modelo de aprendizaje automático Enfrentarse a las competiciones de \u0026ldquo;Primeros pasos\u0026hellip;\u0026rdquo; Competir para maximizar los aprendizajes  Para apuntaros al webinar, tenéis que acceder al siguiente formulario (https://eventos.paradigmadigital.com/kaggle-data-science) e introducir vuestros datos. El día del evento, recibiréis por email la url del webinar para que podáis conectaros y participar en el mismo.\n¿Quién es el ponente? Marco Russo\nConsultor y Especialista en Data \u0026amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.\nVideo   ","date":1587038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"94724ef16381452eedae5ec3e16c691e","permalink":"https://www.marcusrb.com/en/talk/kaggle_1/","publishdate":"2020-04-16T00:00:00Z","relpermalink":"/en/talk/kaggle_1/","section":"talk","summary":"Como participar en retos Kaggle de Data Science - Nivel 1","tags":["datascience","kaggle"],"title":"Kaggle nivel 1","type":"talk"},{"authors":null,"categories":null,"content":" Se creará una sección especial con varios ejemplos del lenguaje R, para aprender la análisis de datos. R es un lenguaje de programación para la gestión y la análisis de datos, además de visualización de gráficos. Es un software libre y disponible en diferentes entornos (Unix, Linux, MacOS, Windows).\nEsta primera sección se especificará como instalar y como utilizarlas. Además de contribuir a añadir varios ejemplos de script para la exploración de datos, limpieza, uso de funciones matemáticas y estadísticas, aprendizaje automático y casos de uso como solución de negocio.\nUnos de los primeros proyectos realizados será la exploración de los datos, o EDA (Explorationa Data Analysis), pero con datos de Google Analytics, es decir exploraremos los datos de un sitio web y que conclusiones podemos sacar con esto.\nCheatsheets\n","date":1567375200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"4e3516537972809789c94b1eed37faa1","permalink":"https://www.marcusrb.com/en/projects/r-studio/","publishdate":"2019-09-02T00:00:00+02:00","relpermalink":"/en/projects/r-studio/","section":"projects","summary":"Directorio de proyectos realizado en R studio","tags":["machine learning","lenguaje r","data mining","exploración datos"],"title":"Proyectos realizados en R Studio","type":"projects"},{"authors":null,"categories":null,"content":" Machine Learning and Deep Learning Consulting ¿Cuál es la diferencia entre ML y consultoría de IA?\nAunque el aprendizaje automático (ML) es el subcampo de la IA con la mayoría de las aplicaciones comerciales, es mejor distinguirlas.\nIA: incluye todas las aplicaciones en las que la computadora imita la inteligencia humana ML: aplicaciones que utilizan datos conocidos para crear modelos que se pueden utilizar para clasificar / procesar nuevos datos  ¿ML consulting = consultoría de aprendizaje profundo?\nNo exactamente. El aprendizaje profundo es un subconjunto del aprendizaje automático. Sin embargo, el aprendizaje profundo es la técnica de aprendizaje automático más exitosa en términos de precisión a partir de 2019 en la mayoría de las áreas.\nNo es raro ver que en la industria se implementen técnicas alternativas como los bosques de decisiones en lugar del aprendizaje profundo. Esto se debe a que la falta de explicación de los resultados es un desafío para los modelos de aprendizaje profundo. Hay casos en los que los modelos de aprendizaje profundo no se implementan en producción.\nya que los gerentes no se sienten cómodos con modelos que no comprenden y que no brindan una explicación de los resultados. en los casos en que se requiera auditabilidad. Por ejemplo, la legislación laboral prohíbe la discriminación. Cualquier algoritmo que utilice criterios que se hayan utilizado anteriormente para la discriminación (es decir, género o raza) no puede tomar decisiones de recursos humanos legalmente sin proporcionar una justificación que involucre razones distintas a esos criterios. Lamentablemente, excluir del modelo criterios potencialmente discriminatorios no resuelve el problema. Por ejemplo, el nombre, los patrones en PTO, la brecha salarial y muchos otros puntos de datos podrían usarse para incluir indirectamente el género en la toma de decisiones. Los modelos de caja negra, sin importar cuán precisos o útiles sean sus resultados, no se pueden implementar en tales situaciones.  Explicar el aprendizaje profundo es un área activa de investigación llamada XAI (IA explicable). ¿Cuáles son las barreras para la adopción del AA?\nComo destaca Deloitte, estas son las barreras mencionadas con mayor frecuencia según los profesionales:\nEscasez de talento: a agosto de 2018, había 150.000 puestos de trabajo de ciencia de datos sin completar en los EE. UU. Que Linkedin describió como una escasez aguda en las grandes ciudades de EE. UU. Inmadurez de la infraestructura y los procesos de ML: ML es un nuevo paradigma de programación, que deriva reglas de los datos en lugar de la entrada del programador. Nos tomó decenas de años crear Scrum, el enfoque de programación ágil, que la mayoría de los equipos utilizan actualmente. Del mismo modo, se necesitará tiempo para que los procesos y marcos de ML alcancen la madurez. TensorFlow, uno de los marcos de aprendizaje automático más utilizados, se publicó a finales de 2015. La mayoría de las técnicas de aprendizaje automático consumen mucha información: los datos de entrenamiento etiquetados con precisión requieren mucho tiempo y son costosos de generar. Los profesionales del aprendizaje automático deben ser creativos al aprovechar los datos públicos o etiquetar los datos necesarios. Es por eso que se fundaron numerosas empresas de etiquetado de datos desde la década de 2010. Otra solución a esto es el aprendizaje de una sola vez y otros enfoques que requieren menos datos; sin embargo, esta es un área de investigación en curso. El aprendizaje profundo no se puede explicar. Como se discutió, esto está obstaculizando el progreso y XAI intenta abordarlo.  ¿Cuál es el futuro de la consultoría de aprendizaje automático?\nLa consultoría de ML crecerá al abordar los problemas identificados:\nExpansión del grupo de talentos: la mayoría de las consultorías están analizando su fuerza laboral en detalle para identificar a aquellos que son capaces de la ciencia de datos. Una formación en programación, estadística o matemáticas tiende a ser suficiente para que las personas trabajen como científicos de datos después de una formación relativamente rápida.\nMejorar la infraestructura y los procesos de ML: a medida que ML madura como paradigma de programación, mejores procesos, mejores recursos informáticos (es decir, GPU y chips de IA) y más automatización harán que ML sea más rápido y más fácil.\nSer creativo con los datos: los avances en el procesamiento del lenguaje natural (NLP) se debieron a la amplia disponibilidad de documentos gubernamentales traducidos en Canadá y Europa. Si bien la búsqueda de datos es una solución relativamente sencilla, las áreas de investigación de la inteligencia artificial, como el aprendizaje por transferencia o la síntesis de datos, podrían ser soluciones más técnicas.\nTambién se esperan avances en la IA explicable que aumentarían la confianza en los sistemas ML y permitirían su adopción más generalizada.\nPor último, es probable que las aplicaciones locales de aprendizaje automático hagan que las aplicaciones de IoT sean más inteligentes y rápidas al llevar la toma de decisiones a los dispositivos periféricos. ¿Cuáles son las actividades típicas de consultoría de ML? Comprender las necesidades comerciales\nComo en toda consultoría, todo comienza con la necesidad empresarial. Ya sea que se trate de predecir dónde instalar estaciones base de telecomunicaciones o a quién mostrar anuncios, malinterpretar los requisitos comerciales sigue siendo una de las principales razones de la falta de éxito de los proyectos de consultoría y software. La consultoría ML, en la intersección de la consultoría y el software, es especialmente propensa a este problema. Configurar el equipo y el proceso\nNo todos los problemas necesitan aprendizaje automático. El aprendizaje automático y otros enfoques heurísticos tienen sentido en problemas que no pueden reducirse a un conjunto de reglas. Si las reglas son bien conocidas y simples, los sistemas basados ​​en reglas superan al aprendizaje automático y son más simples de mantener.\nSi ML es un buen ajuste fo es necesario delinear un problema, el equipo del proyecto, las partes interesadas y los objetivos de alto nivel. Recolección y exploración de datos\nSi la empresa tiene los datos, este es un paso relativamente sencillo. Los consultores deben trabajar con las empresas para validar que los datos estén correctamente etiquetados y no sean contradictorios.\nSi los datos no están disponibles, se deben considerar las técnicas descritas anteriormente, como aprovechar los datos en línea, pagar por el etiquetado de datos y enfoques novedosos de ML, como el aprendizaje de una sola vez. Modelo de desarrollo\nSe necesitan miles de experimentos para desarrollar un modelo de aprendizaje automático de alto rendimiento. Este es un proceso iterativo que tiene en cuenta las últimas investigaciones, comprende la dinámica empresarial y la exploración de datos.\nEn última instancia, todos los modelos se evalúan con el mismo conjunto de datos de prueba para evaluar su precisión. Desarrollo de aplicaciones de pila completa\nLlevar un modelo a producción requiere trabajo adicional de desarrollo e integración de software.\nLa mayoría de las veces, los modelos ML están encapsulados en API que son fáciles de integrar con cualquier aplicación. El desarrollo de la aplicación que operacionalizará el modelo ML y lo hará parte del proceso de toma de decisiones puede ser más difícil que construir el modelo. El desarrollo de aplicaciones puede requerir la integración a los sistemas empresariales existentes, lo que requiere trabajar con desarrolladores externos.\nLos problemas de escalabilidad y seguridad de los datos también deben abordarse como parte de la puesta en funcionamiento del modelo.\n¿Está interesado en consultar o capacitarse en Aprendizaje automático? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información en Machine Learning\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"4cd88c1a2f9787139ff327e6350e1ed1","permalink":"https://www.marcusrb.com/consultoria-freelance-machine-learning/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/consultoria-freelance-machine-learning/","section":"services","summary":"Servicio que ofrece creación de algoritmos de aprendizaje automático.","tags":null,"title":"Consultor freelance Machine Learning \u0026 AI","type":"page"},{"authors":null,"categories":null,"content":" Servicios freelance de Google Data Studio Google Data Studio es una herramienta gratuita de visualización e informes de datos basada en la nube que se conecta a muchas fuentes de datos diferentes y convierte esos datos en paneles informativos e informes que son fáciles de entender y compartir, y son totalmente personalizables.\nVea algunos de nuestros paneles en vivo, por ejemplo, informes de paneles de datos de Google Data Studio accionables\nCaracterísticas clave Google Data Studio es intuitivo, rápido, flexible y permite una gran cantidad de opciones de diseño y presentación.\nAmplia gama de conectores de datos. Data Studio tiene 17 conectores de datos internos y alrededor de 108 de terceros para elegir\nFunciones fáciles de usar Data Studio proporciona docenas de funciones matemáticas, de cadena, de fecha y otras para transformar sus datos en valores más útiles.\nVariedad de formas, imágenes y texto. Data Studio le permite agregar formas, imágenes y texto a sus informes y paneles para que sean más fáciles de leer.\nNiveles de permisos Aprovechando la tecnología Google Drive, puede administrar fácilmente a todos sus usuarios y su nivel de acceso\nMezcla de datos, ahora una realidad Data Studio le permite agregar datos de múltiples fuentes para tener una vista comparativa de ellos a la vez\n¿Está interesado en servicios o capacitarse en visualización en Google Data Studio ? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para la consultoría freelance en Data Studio\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"683ca00fb2f4f7ae7af6ae22f5fd52e5","permalink":"https://www.marcusrb.com/servicios-freelance-google-data-studio/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/servicios-freelance-google-data-studio/","section":"services","summary":"Servicio de consultoría, implementación y mantenimiento cuadros de mando, dashboard y analítica avanzada con Google Data Studio | BigQuery","tags":null,"title":"Consultoría freelance reportes Google Data Studio","type":"page"},{"authors":null,"categories":null,"content":" ¿Qué es Microsoft Power BI? Ha pasado algún tiempo desde que Microsoft lanzó Power BI y la forma en que las cosas habían progresado para esta herramienta excepcional de Business Intelligence and Analytics, parecía que solo sería cuestión de tiempo antes de que se convirtiera en la plataforma preferida para BI y análisis con el mayoría de las empresas con visión de futuro. Power BI es una herramienta poderosa en manos de las empresas que desean extraer y convertir datos de múltiples fuentes dispares para obtener información significativa. Ofrece una experiencia de usuario sin precedentes con oportunidades de visualización interactiva junto con verdaderas capacidades de análisis de autoservicio. Todo esto ayuda a ver los mismos datos desde una variedad de ángulos, sin mencionar que los informes y paneles pueden ser creados por cualquier persona en la organización sin la ayuda de los administradores de TI. Algunos de los beneficios únicos de Power BI son:\n* Potentes gráficos y visualizaciones del tablero que se actualizan continuamente. * Función de análisis en memoria y base de datos en columna que admite datos tabulares. * Lo mejor de ambos mundos cuando se trata de facilidad de uso y rendimiento en una sola herramienta de BI. * Geo-mapping interactivo con Bing Maps. * Secuencias de comandos de expresiones de análisis de datos (DAX) para crear medidas y columnas.  Vea algunos de nuestros paneles en vivo, por ejemplo, informes de panel de control de Power BI procesables\n¿Por qué necesita Power BI? Permita que sus empleados y tomadores de decisiones analicen los datos más rápido con una mejor eficiencia y comprensión utilizando el servicio de análisis basado en la nube Power BI proporciona a los usuarios una amplia gama de información a través de paneles simplificados pero efectivos, informes precisos y visualizaciones atractivas, convirtiéndose así en una herramienta perfecta para dar vida a los datos.\nPower BI se conecta a cualquier fuente de datos y ofrece información empresarial convincente a un costo muy bajo, brindando análisis en tiempo real utilizando un tablero efectivo en varios dispositivos como computadoras de escritorio, dispositivos móviles, tabletas, etc. Microsoft Power BI está diseñado de una manera que no requiere finalización que los usuarios tengan habilidades de programación para explorar, analizar y procesar los datos para tomar decisiones comerciales mejor informadas.\n¿Está interesado en consultoría o capacitarse en visualización en Power BI ? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para la consultoría en Power BI\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"fe1bb69fe3e2922d202d5ca20ecf21b5","permalink":"https://www.marcusrb.com/servicios-freelance-analisis-dax-power-bi/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/servicios-freelance-analisis-dax-power-bi/","section":"services","summary":"Consultor de inteligencia de negocio, implementación y optimización de DAX, Power Query y modelo de datos en Power BI | SQL Server | Azure","tags":null,"title":"Consultoría optimización y análisis en Microsoft Power BI","type":"page"},{"authors":null,"categories":null,"content":" Data Analytics \u0026amp; Business Intelligence freelance It is a sum total concept that includes data analysis, for companies and to tell you that these are the absolute points that are missing in the growth of your company and with the help of these we can record enormous growth, I will provide you with a general summary of your data and we will generate an anonymous number of reports that will be a key aspect in the growth of our clients.\nAre you interested in freelance services or training in Data Analytics and Business Intelligence? Call me today or use the online form below. Thanks a lot!\nRequest information in Data analytics, Business Intelligence\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"0d3a96c4104ac95ba665e99a5730373b","permalink":"https://www.marcusrb.com/en/freelance-data-analytics-business-intelligence-service/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/en/freelance-data-analytics-business-intelligence-service/","section":"services","summary":"Data analytics services help capture this essence of Big Data Analytics and management to help you understand new opportunities, hidden threats, your competitors, etc.","tags":null,"title":"Freelance Data Analytics and Business Intelligence","type":"page"},{"authors":null,"categories":null,"content":" Servicio de consultoría freelance en Tableau Creemos en el futuro del análisis visual del mundo empresarial. Si aterrizaste aquí, probablemente estés en algún lugar del mismo camino.\nPara ayudarlo en su viaje, ofrecemos ayuda y consultoría de Tableau.\nSomos un socio de Tableau que trabajamos para ayudar a las personas a aprovechar al máximo el software y responder las preguntas correctas con datos.\n¿Por qué necesita consultar Tableau? ¡Aceptar asistencia no es una debilidad! Realmente creemos que un buen líder sabe cuándo pedir ayuda. Probablemente todos hemos aprendido que en algún momento podríamos usar algo de soporte.\n¡La velocidad es vital en el análisis de autoservicio! Permítanme repetirlo: ¡la velocidad es el juego!\nSi tiene la oportunidad de encontrar un consultor de Tableau que obtenga sus puntos débiles, es posible que pueda acelerar su progreso de una manera que no creía posible.\nEntonces, ¿cómo podemos ayudar? Un enfoque creativo para la resolución de problemas. Ayuda diligente de Tableau, comunicación clara, hacer las cosas de la manera correcta.\n¡Simple como eso!\nUna pasión por la visualización de datos Estamos trabajando con Tableau todos los días. ¡Alimentamos incansablemente a la bestia que es nuestro apetito por responder preguntas con datos! Nuestra experiencia profesional en análisis web nos brinda una vanguardia. Sobre todo entendemos la importancia de la corrección y la calidad de los datos.\nCreemos que cuando te apasiona algo, el trabajo se vuelve mucho más fácil. Para nosotros, ¡ni siquiera parece trabajo! El amor por trabajar con datos es lo que nos impulsa. Tenacidad, dedicación y creatividad son lo que obtendrá de nosotros.\nBusiness Analytics vs. Business Intelligence Creemos que puede beneficiarse de nuestro trabajo en ambas áreas. Al combinar la eficiencia (BI) con la creatividad (BA), estamos tratando de ayudar a las empresas a responder preguntas urgentes y formular otras nuevas.\nHemos estado utilizando un amplio espectro de fuentes de datos: desde Big Data como Exasol o Vertica hasta bases de datos clásicas como MySQL o Postgres, y otras menos tradicionales como Google Analytics y conectores de datos web.\nDiseño e implementación del tablero de instrumentos Para nosotros, un Tableau Dashboard es solo la punta del iceberg. La regla de pasar el 80% del tiempo trabajando con los datos y el 20% en la visualización en sí se aplica a nuestro caso.\nNo solo eso, sino que el principio de Pareto también es relevante para la cantidad de hojas que creamos versus cuántas guardamos en la versión final del tablero.\n¡La velocidad de probar diferentes enfoques en Tableau es fantástica! Sería una pena si no lo usáramos.\nPor lo general, comenzamos construyendo muchos gráficos diferentes (a veces cientos), y conservamos solo los que consideramos relevantes para la historia.\nPuede encontrar algunos ejemplos de visualizaciones de datos que hemos creado al final de esta página. Y algunos paneles más creativos en el perfil de Tableau Public de Dorian (nuestro jefe de Tableau).\nOptimización del rendimiento de Tableau Tenemos un estudio de caso sobre el ajuste del rendimiento de Tableau, que debe leer si tiene ~ 15 minutos disponibles.\nLa idea principal es que no nos centremos únicamente en la rapidez con la que podemos hacer un tablero, sino también en lo rápido que puede responder a sus preguntas.\nAdemás, trabajar con grandes cantidades de datos es complicado. Sin embargo, estamos aquí para romper esos muros.\nLicencias y soporte ¿Desea tomar la decisión correcta con respecto a la configuración de Tableau dentro de su organización?\nPodemos ayudarlo a descubrir cuál es la combinación correcta de:\n Productos de Tableau: escritorio, preparación, servidor, en línea Roles de usuario: (Creador, Explorador, Visor) Licencias: licencias básicas basadas en el usuario, análisis integrados, gestión de datos  También estamos disponibles para ofrecer soporte técnico para su servidor si lo necesita.\nUno de nuestros objetivos es ayudar a los clientes a dormir mejor. Esta parte cae directamente en esa área.\nExperiencia complementaria con R y Python Tecnologías como R y Python nos permiten trabajar con los datos en el nivel más cercano.\nPor nombrar algunas cosas para las que hemos utilizado modelos estadísticos creados con R junto con Tableau para duplicar el análisis:\n Modelos de atribución de Markov: atribución de canal de marketing más inteligente que el promedio para tiendas de comercio electrónico Pronosticar usando algoritmos que son un poco más avanzados (por ejemplo, el Profeta de Facebook) que los básicos (como el que usa Tableau) Cálculos bayesianos: los desarrollamos para cortar y cortar los resultados de las pruebas A / B dentro de Tableau Comprobaciones de impacto causal: estamos trabajando con este algoritmo para evaluar si los crecimientos o las caídas son el resultado de algo que hicimos o simplemente ruido aleatorio en los datos Flujos de trabajo de Business Intelligence: programación de informes usando Python junto con tabcmd (la interfaz de línea de comando para Tableau Server)  ¿Está interesado en consultoría o capacitarse en visualización en Tableau ? Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!\nSolicita información para la consultoría en Tableau\n","date":1567363400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"7e7aaf086bd775117c4e2d8c447949f4","permalink":"https://www.marcusrb.com/servicios-freelance-tableau/","publishdate":"2019-09-01T18:43:20Z","relpermalink":"/servicios-freelance-tableau/","section":"services","summary":"Servicio de consultoría, implementación y mantenimiento cuadros de mando, dashboard y analítica avanzada con Tableau Desktop, Tableau Prep.","tags":null,"title":"Freelance en gestión y mantenimiento dashboard en Tableau","type":"page"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file and adding markup: mmark to your page front matter.\nTo render inline or block math, wrap your LaTeX math with $$...$$.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n\\[\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}\\]\nExample inline math $$\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2$$ renders as \\(\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2\\) .\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n\\[f(k;p_0^*) = \\begin{cases} p_0^* \u0026 \\text{if }k=1, \\\\ 1-p_0^* \u0026 \\text {if }k=0.\\end{cases}\\]\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D; ```  renders as\ngraph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D;  An example sequence diagram:\n```mermaid sequenceDiagram participant Alice participant Bob Alice-\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram participant Alice participant Bob Alice-\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d ```  renders as\ngantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a Markdown extension for asides, also referred to as notices or hints. By prefixing a paragraph with A\u0026gt;, it will render as an aside. You can enable this feature by adding markup: mmark to your page front matter, or alternatively using the Alert shortcode.\nA\u0026gt; A Markdown aside is useful for displaying notices, hints, or definitions to your readers.  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.\n Did you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://www.marcusrb.com/en/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/en/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Destripando Google Tag Manager Nivel 1.\n30 octubre 2018\nDestripando Google Tag Manager Después del primer Meetup sobre los fundamentos de Google Tag Manager, que está disponible en nuestro canal de YouTube, en este webinar vamos a realizar todo tipo de prácticas que nos permitirán conectar la interfaz más utilizada con el ecosistema de Google: Analytics, Marketing Platform como también Facebook Ads y herramientas de CRO. Pasaremos sucesivamente a las configuraciones avanzadas que nos permitirán subir de nivel y poder disponer de las métricas esenciales sin miedo de perder datos a la hora de analizar tu negocio.\n¿Quién es el ponente? Marco Russo\nConsultor y Especialista en Data \u0026amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.\nVideo   ","date":1561485600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"2d6a7fe39685a5303c3dc500f026fbb8","permalink":"https://www.marcusrb.com/en/talk/google_tag_manager_2/","publishdate":"2019-06-25T20:00:00Z","relpermalink":"/en/talk/google_tag_manager_2/","section":"talk","summary":"[Meetup] Escalando de nivel con Google Tag Manager","tags":["analytics","googletagmanager"],"title":"Meetup - Escalando de nivel con Google Tag Manager","type":"talk"},{"authors":["Marco Russo"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://www.marcusrb.com/en/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":null,"categories":null,"content":" Se creará una sección especial con varios ejemplos del lenguaje R, para aprender la análisis de datos. R es un lenguaje de programación para la gestión y la análisis de datos, además de visualización de gráficos. Es un software libre y disponible en diferentes entornos (Unix, Linux, MacOS, Windows).\nEsta primera sección se especificará como instalar y como utilizarlas. Además de contribuir a añadir varios ejemplos de script para la exploración de datos, limpieza, uso de funciones matemáticas y estadísticas, aprendizaje automático y casos de uso como solución de negocio.\nUnos de los primeros proyectos realizados será la exploración de los datos, o EDA (Explorationa Data Analysis), pero con datos de Google Analytics, es decir exploraremos los datos de un sitio web y que conclusiones podemos sacar con esto.\n","date":1550530800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"78e70a446500553ae54824df2902463a","permalink":"https://www.marcusrb.com/en/projects/analisis-datos-con-r/","publishdate":"2019-02-19T00:00:00+01:00","relpermalink":"/en/projects/analisis-datos-con-r/","section":"projects","summary":"Se pondrán ejemplos de lenguaje R para Data Mining","tags":["machine learning","lenguaje r","data mining","exploración datos"],"title":"Análisis de datos con R","type":"projects"},{"authors":["marcusRB"],"categories":null,"content":"  Click on the Slides button above to view the built-in slides feature.   Destripando Google Tag Manager Nivel 1.\n30 octubre 2018\nDestripando Google Tag Manager Hemos puesto este título un poco en honor a la fiesta de Halloween, pero realmente hay cierto \u0026ldquo;miedo\u0026rdquo; al utilizar la herramienta de medición más utilizada del mundo por parte de muchos usuarios.\nEn esta ocasión le toca el turno a \u0026ldquo;Google Tag Manager\u0026rdquo;, vamos a conocer los casos típicos de uso, y no solo de la forma sencilla, gracias a nuestro experto Marco Russo.\nAunque generalmente es utilizada por profesionales de digital marketing y analítica digital, es una herramienta todo-terreno, válida también para realizar pruebas de performance, testing front y back-end, interactuando con el DOM, jugar con script de Google AppScript y Google Spreadsheet, etc.\nPretendemos dar una visión general sobre las variadas opciones de GTM, y de cómo explotarlas al máximo mostrando ideas y casos prácticos.\nMarco Russo es consultor y especialista en Digital Data Analytics a nivel internacional trabajando para diferentes sectores industriales. Además, desde hace 6 años compagina su trabajo con la formación in-company y en diferentes escuelas de negocios y Cámara de Comercio, realizando módulos y cursos de Analytics, DataViz, CRO y Google Tag Manager. Cuando no piensa en los datos, además de compaginar su tiempo con la familia, puedes encontrarlo escalando montañas en la sierra de Madrid con su bici de carretera o en una cancha de basket.\n¿Quién es el ponente? Marco Russo\nConsultor y Especialista en Data \u0026amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.\nVideo   ","date":1540922400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"e72d14c48c8178dc191814eb5b2e24ca","permalink":"https://www.marcusrb.com/en/talk/google_tag_manager_1/","publishdate":"2018-10-30T00:00:00Z","relpermalink":"/en/talk/google_tag_manager_1/","section":"talk","summary":"[Meetup] Destripando Google Tag Manager","tags":["analytics","googletagmanager"],"title":"Meetup - Destripando Google Tag Manager","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"6f8c6b33263a59bd4e4de690decef4a9","permalink":"https://www.marcusrb.com/en/projects/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/en/projects/external-project/","section":"projects","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"projects"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"79cfc0c3ddcfb9255d2520d5f850143e","permalink":"https://www.marcusrb.com/en/projects/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/en/projects/internal-project/","section":"projects","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"projects"},{"authors":["Marco Russo","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://www.marcusrb.com/en/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Marco Russo","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609845561,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://www.marcusrb.com/en/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]