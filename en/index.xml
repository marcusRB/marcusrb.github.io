<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MarcusRB | data specialist &amp; professor • AI • Big Data</title>
    <link>https://www.marcusrb.com/en/</link>
      <atom:link href="https://www.marcusrb.com/en/index.xml" rel="self" type="application/rss+xml" />
    <description>MarcusRB | data specialist &amp; professor • AI • Big Data</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-UK</language><copyright>© 2021</copyright><lastBuildDate>Mon, 04 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.marcusrb.com/img/icon-192.png</url>
      <title>MarcusRB | data specialist &amp; professor • AI • Big Data</title>
      <link>https://www.marcusrb.com/en/</link>
    </image>
    
    <item>
      <title>Data transformation strategies</title>
      <link>https://www.marcusrb.com/en/power-bi/03-data-transformation-strategies/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/power-bi/03-data-transformation-strategies/</guid>
      <description>

&lt;h1 id=&#34;data-transformation-strategies&#34;&gt;Data Transformation Strategies&lt;/h1&gt;

&lt;p&gt;Within any BI project, it is essential that the  data you are working with has been properly scrubbed to ensure accurate  results on your reports and dashboards. Applying data cleansing business rules, also known as transforms, is the primary method for correcting  inaccurate or malformed data, but the process can often be the most  time-consuming part of any corporate BI solution. However, the data  transformation capabilities built into Power BI are both very powerful  and user-friendly. Using the Power Query Editor, tasks that would  typically be difficult or time-consuming in an enterprise BI tool are as simple as right-clicking on a column and selecting the appropriate  transform for the field. While interacting with the user interface, the  Power Query Editor automatically writes queries using a language called M behind the scenes.&lt;/p&gt;

&lt;p&gt;Through the course of this chapter, you will  explore some of the most common features of the Power Query Editor that  make it so highly regarded by its users. Since one sample dataset cannot provide all the problems you will run into, you will be provided with  several small, disparate examples to show you what is possible. This  chapter will detail the following topics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Power Query Editor&lt;/li&gt;
&lt;li&gt;Transform basics&lt;/li&gt;
&lt;li&gt;Advanced data transformation options&lt;/li&gt;
&lt;li&gt;Leveraging R&lt;/li&gt;
&lt;li&gt;AI Insights&lt;/li&gt;
&lt;li&gt;The M formula language&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get started, let&amp;rsquo;s get familiar with the interface known as the Power Query Editor.&lt;/p&gt;

&lt;h1 id=&#34;the-power-query-editor&#34;&gt;The Power Query Editor&lt;/h1&gt;

&lt;p&gt;The &lt;strong&gt;Power Query Editor&lt;/strong&gt; is the primary tool that you will utilize for applying transformations and cleansing processes to your data. This editor can be launched as part of  establishing a connection to your data, or by simply clicking &lt;strong&gt;Transform Data&lt;/strong&gt; on the &lt;strong&gt;Home&lt;/strong&gt; ribbon of the Power BI Desktop. When the Power Query Editor is opened,  you will notice that it has its own separate environment for you to work in. The environment encapsulates a user-friendly method for working  with all of the queries that you will define. Before you dive deep into  the capabilities of the Power Query Editor, let&amp;rsquo;s first start by  reviewing the key areas of the Power Query Editor interface, as shown in &lt;em&gt;Figure 2.1&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/1.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.1: First view of the Power Query Editor&lt;/p&gt;

&lt;p&gt;Following the numbered figures, let&amp;rsquo;s review some of the most important features of the Power Query Editor:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;New Source&lt;/strong&gt;: This launches the interface to establish your connection details, which is the same interface as the &lt;strong&gt;Get data&lt;/strong&gt; button that you learned about in &lt;em&gt;Chapter 1&lt;/em&gt;, &lt;em&gt;Getting Started with Importing Data Options&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Queries&lt;/strong&gt; pane: A list of all the queries that you have connected to. From here, you  can rename a query, disable the load and modify report refresh  capabilities, and organize your queries into groups.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Query Settings&lt;/strong&gt;: Within this pane, you can rename the query, but more importantly, you  can see and change the list of steps, or transforms, that have been  applied to your query. If you ever accidentally close this pane, you can relaunch it from the &lt;strong&gt;View&lt;/strong&gt; menu.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advanced Editor&lt;/strong&gt;: By launching the &lt;strong&gt;Advanced Editor&lt;/strong&gt;, you can see the M query that is automatically written for you by the Power Query Editor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Close &amp;amp; Apply&lt;/strong&gt;: Choosing this option will close the Power Query Editor and load the results into the data model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With this basic navigation understood, let&amp;rsquo;s start to discuss some of the basics of working with various transforms.&lt;/p&gt;

&lt;h1 id=&#34;transform-basics&#34;&gt;Transform basics&lt;/h1&gt;

&lt;p&gt;Applying data transformations within the Power Query Editor can be a surprisingly simple thing to do. However, there are a few things to consider as we begin this process.  The first is that there are multiple ways to solve a problem. As you  work your way through this book, the authors have tried to show you the  fastest and easiest methods of solving the problems that are presented,  but these solutions will certainly not be the only ways to reach your  goals.&lt;/p&gt;

&lt;p&gt;The next thing you should understand is that every click you do inside the Power Query Editor is automatically converted  into a formula language called M. Virtually all the basic transforms you will need can be accomplished by simply interacting with the Power  Query Editor user interface, but for more complex business problems  there is a good chance you may have to modify the M queries that are  written for you by the editor. You will learn more about M later in this chapter.&lt;/p&gt;

&lt;p&gt;Finally, the last important consideration to  understand is that all transforms that are created within the editor are stored in the &lt;strong&gt;Query Settings&lt;/strong&gt; pane under a section called &lt;strong&gt;Applied Steps&lt;/strong&gt;. Why is this important to know? The &lt;strong&gt;Applied Steps&lt;/strong&gt; section has many features, but here are some of the most critical to know for now:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Deleting transforms&lt;/strong&gt;: If you make a mistake and need to undo a step, you can click the &lt;strong&gt;Delete&lt;/strong&gt; button next to a step.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modifying transforms&lt;/strong&gt;: This can be done with any step that has a gear icon next to it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changing the order of transforms&lt;/strong&gt;: If you realize that it is better for one step to execute before another one, you can change the order of how the steps are executed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Selecting previous steps&lt;/strong&gt;: Clicking on any step prior to the current one will allow you to see how your query results would change one step earlier in the process.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this understanding, you will now get hands-on with applying several basic transforms inside the Power Query Editor.  The goal of these first sets of examples is to get you comfortable with the Power Query Editor user interface before the more complex use cases are covered.&lt;/p&gt;

&lt;h2 id=&#34;use-first-row-as-headers&#34;&gt;Use First Row as Headers&lt;/h2&gt;

&lt;p&gt;Organizing column names or headers is often an important first task when managing your dataset. Providing relevant column names makes many of the downstream processes, such as building  reports, much easier. Often, column headers are automatically imported  from your data source, but sometimes you may be working with a more  unique data source that makes it difficult for Power BI to capture the  column header information. This walkthrough will show how to deal with  such a scenario:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Launch Power BI Desktop, and click &lt;strong&gt;Get data&lt;/strong&gt; on the &lt;strong&gt;Home&lt;/strong&gt; ribbon.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Choose &lt;strong&gt;Excel&lt;/strong&gt;, then navigate to and select &lt;strong&gt;Open&lt;/strong&gt; on the &lt;code&gt;Failed Bank List.xlsx&lt;/code&gt; file that is available in the book source files.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the &lt;strong&gt;Navigator&lt;/strong&gt; window, select the table called &lt;strong&gt;Data&lt;/strong&gt;, then choose &lt;strong&gt;Transform Data&lt;/strong&gt;. When the Power Query Editor launches, you should notice that the column headers are not automatically imported. In fact, the column headers are in the first row of the data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To push the column names that are in the first row of data to the header section, select the transform called&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Use First Row as Headers&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Home&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ribbon as shown in 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.2&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_03.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.2: Leveraging the Use First Row as Headers transform&lt;/p&gt;

&lt;p&gt;Once complete, you will see the first row of the  dataset has been promoted to the column header area. This is a very  common transform that you can expect to use often with flat files. Next, let&amp;rsquo;s look at another commonly used transform, &lt;strong&gt;Remove Columns&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;remove-columns&#34;&gt;Remove Columns&lt;/h2&gt;

&lt;p&gt;Often, the data sources you will connect to will include many columns that are not necessary for the solution you are designing. It is important to remove these  unnecessary columns from your dataset because these unused columns  needlessly take up space inside your data model. There are several  different methods for removing columns in the Power Query Editor. This  example will show one of these methods using the same dataset from the  previous demonstration:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Multi-select (&lt;em&gt;Ctrl&lt;/em&gt; + click) the column headers of the columns you wish to keep as part of your solution. In this scenario, select the columns &lt;strong&gt;Bank Name&lt;/strong&gt;, &lt;strong&gt;City&lt;/strong&gt;, &lt;strong&gt;ST&lt;/strong&gt;, and &lt;strong&gt;Closing Date&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;With these four columns selected, right-click on any of the selected columns and choose&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Remove Other Columns&lt;/p&gt;

&lt;p&gt;, as shown in&lt;/p&gt;

&lt;p&gt;Figure 2.3&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_04.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.3: Selecting the Remove Other Columns transform&lt;/p&gt;

&lt;p&gt;Once this transform is completed, you should be left with only the columns you need.&lt;/p&gt;

&lt;p&gt;Another popular method for removing columns is clicking the &lt;strong&gt;Choose Columns&lt;/strong&gt; button on the &lt;strong&gt;Home&lt;/strong&gt; ribbon of the Power Query Editor. This option provides a list of all  the columns, and you can choose the columns you wish to keep or exclude.&lt;/p&gt;

&lt;p&gt;You can also select the columns you wish to remove; right-click on one of the selected columns and click &lt;strong&gt;Remove&lt;/strong&gt;. This seems like the more obvious method. However, this option is not as user-friendly in the long run because it does not provide an option to  edit the transform in the &lt;strong&gt;Applied Steps&lt;/strong&gt; section like the first two methods do.&lt;/p&gt;

&lt;p&gt;With any data cleansing tool, data type manipulation is critical and can help save you from many headaches later in the development of your solution. In the next section, you will learn about how to change data types.&lt;/p&gt;

&lt;h2 id=&#34;change-type&#34;&gt;Change Type&lt;/h2&gt;

&lt;p&gt;Defining column data types properly early on in your data scrubbing process can help to ensure proper business rules can be applied and data is presented properly in reports. The Power  Query Editor has various numeric, text, and date-time data types for you to choose from. In our current example, all of the data types were  automatically interpreted correctly by the Power Query Editor, but let&amp;rsquo;s look at where you could change this if necessary:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Locate the data type indicator on the column header to the left of the column name.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click the data type icon, and a menu will open that allows you to choose whichever data type you desire, as shown in&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figure 2.4&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_05.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.4: Choosing a different data type&lt;/p&gt;

&lt;p&gt;Another method you can use for changing column data types is to right-click on the column you wish to change, then select &lt;strong&gt;Change Type&lt;/strong&gt; and choose the new data type. You should always be careful when  changing data types to ensure your data supports the change. For instance, if you change a column data type to a Whole Number while it has letters stored in it, Power BI will produce an error.&lt;/p&gt;

&lt;p&gt;If you want to change multiple  column data types at once, you can multi-select the necessary columns,  then select the new data type from the &lt;strong&gt;Data Type&lt;/strong&gt; property on the &lt;strong&gt;Home&lt;/strong&gt; ribbon.&lt;/p&gt;

&lt;p&gt;Many of the transforms you will encounter in the  future are contextually based on the column data types you are working  with. For example, if you have a column that is a date, then you will be provided with special transforms that can only be executed against a  date data type, such as extracting the month name from a date column.&lt;/p&gt;

&lt;p&gt;Understanding how to properly set data types in  Power BI is often the first step to using more exciting transforms. In  the next section, you will learn how Power BI can read from an example  you provide to automatically create transform rules.&lt;/p&gt;

&lt;h2 id=&#34;column-from-examples&#34;&gt;Column From Examples&lt;/h2&gt;

&lt;p&gt;One option that can make complex data transformations seem simple is the feature called &lt;strong&gt;Add Column From Examples&lt;/strong&gt;. Using &lt;strong&gt;Add Column From Examples&lt;/strong&gt;, you can provide the Power Query Editor with a sample of what you would  like your data to look like, and it can then automatically determine  which transforms are required to accomplish your goal. Continuing with  the same failed banks example, let&amp;rsquo;s walk through a simple example of  how to use this feature:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Find and select the &lt;strong&gt;Add Column&lt;/strong&gt; tab in the Power Query Editor ribbon.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select the&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Column From Examples&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;button and, if prompted, choose 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From All Columns&lt;/p&gt;

&lt;p&gt;. This will launch a new&lt;/p&gt;

&lt;p&gt;Add Column From Examples&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;interface:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_06.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.5: Choosing the Column from Examples transform&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Our goal is to leverage this feature to combine the &lt;code&gt;City&lt;/code&gt; and &lt;code&gt;ST&lt;/code&gt; columns together. In the first empty cell, type &lt;code&gt;Barboursville, WV&lt;/code&gt; and then hit &lt;em&gt;Enter&lt;/em&gt;. In &lt;em&gt;Figure 2.5&lt;/em&gt; you will notice that the text you typed has automatically been  translated into an M query and applied for every row in the dataset.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Once you click&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;p&gt;, the transform is finalized and automatically added to the overall M  query that has been built through the user interface. The newly merged  column will be added with the rest of your columns and you can  optionally rename the column something more appropriate by  double-clicking on the column header:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;![img](https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_07.png)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.6: Adding Column from Examples&lt;/p&gt;

&lt;p&gt;As you can see, the &lt;strong&gt;Add Column from Examples&lt;/strong&gt; feature is great because you don&amp;rsquo;t have to be an expert in which  transforms are appropriate because Power BI will automatically choose  them for you!&lt;/p&gt;

&lt;p&gt;Sometimes, you may encounter scenarios where the &lt;strong&gt;Add Column From Examples&lt;/strong&gt; feature needs more than one example to properly translate your example  into an M query function that accomplishes your goal. If this happens,  simply provide additional examples of how you would like the data to  appear in different rows, and the Power Query Editor should adjust to  account for outliers.&lt;/p&gt;

&lt;p&gt;Now that you have learned some basic transforms, let&amp;rsquo;s explore some more complex design patterns that are still used quite frequently.&lt;/p&gt;

&lt;h1 id=&#34;advanced-data-transformation-options&#34;&gt;Advanced data transformation options&lt;/h1&gt;

&lt;p&gt;Now that you should be more comfortable working within the Power Query Editor, let&amp;rsquo;s take the next step and discuss more advanced options. Often, you will find the need to go beyond these basic transforms when dealing with data that requires more care. In  this section, you will learn about some common advanced transforms that  you may have a need for, which include &lt;strong&gt;Conditional Columns&lt;/strong&gt;, &lt;strong&gt;Fill Down&lt;/strong&gt;, &lt;strong&gt;Unpivot&lt;/strong&gt;, &lt;strong&gt;Merge Queries&lt;/strong&gt;, and &lt;strong&gt;Append Queries&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;conditional-columns&#34;&gt;Conditional Columns&lt;/h2&gt;

&lt;p&gt;Using the Power Query Editor &lt;strong&gt;Conditional Columns&lt;/strong&gt; functionality is a great way to add new columns to your query that follow logical &lt;code&gt;if&lt;/code&gt;/&lt;code&gt;then&lt;/code&gt;/&lt;code&gt;else&lt;/code&gt; statements. This concept of &lt;code&gt;if&lt;/code&gt;/&lt;code&gt;then&lt;/code&gt;/&lt;code&gt;else&lt;/code&gt; is common across many programming languages, including Excel formulas.  Let&amp;rsquo;s review a real-world scenario where you would be required to do  some data cleansing on a file before it could be used. In this example,  you will be provided with a file of all the counties in the United  States, and you must create a new column that extracts the state name  from the county column and places it in its own column:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Start by connecting to the &lt;code&gt;FIPS_CountyName.txt&lt;/code&gt; file that is found in the book files using the &lt;strong&gt;Text&lt;/strong&gt;/&lt;strong&gt;CSV&lt;/strong&gt; connector.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Launch the Power Query Editor by selecting &lt;strong&gt;Transform Data&lt;/strong&gt;, then start by changing the data type of &lt;code&gt;Column1&lt;/code&gt; to &lt;strong&gt;Text&lt;/strong&gt;. When you do this, you will be prompted to replace an existing type conversion. You can accept this by clicking &lt;strong&gt;Replace current&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Now, on &lt;code&gt;Column2&lt;/code&gt;, filter out the value &lt;strong&gt;UNITED STATES&lt;/strong&gt; from the column by clicking the arrow next to the column header and unchecking &lt;strong&gt;UNITED STATES&lt;/strong&gt;. Then, click &lt;strong&gt;OK&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Remove the state abbreviation from&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Column2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;by right-clicking on the column header and selecting&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Split Column&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;| 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By Delimiter&lt;/p&gt;

&lt;p&gt;. Choose&lt;/p&gt;

&lt;p&gt;&amp;ndash; Custom &amp;ndash;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for the delimiter type, and type 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;before then clicking 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;p&gt;, as shown in&lt;/p&gt;

&lt;p&gt;Figure 2.7&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_08.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.7: Splitting a column based on a delimiter&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Next, rename the column names &lt;code&gt;Column1&lt;/code&gt;, &lt;code&gt;Column 2.1&lt;/code&gt;, and &lt;code&gt;Column 2.2&lt;/code&gt;, to &lt;code&gt;County Code&lt;/code&gt;, &lt;code&gt;County Name&lt;/code&gt;, and &lt;code&gt;State Abbreviation&lt;/code&gt;, respectively.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To isolate the full state name into its own column, you will need to implement &lt;strong&gt;Conditional Column&lt;/strong&gt;. Go to the &lt;strong&gt;Add Column&lt;/strong&gt; ribbon and select &lt;strong&gt;Conditional Column&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Change the&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;New column name&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;property to 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;   State Name
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;and implement the logic 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;   If State Abbreviation equals null Then return County Name Else return null
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;as shown in 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.8&lt;/p&gt;

&lt;p&gt;. To return the value from another column, you must select the icon in the&lt;/p&gt;

&lt;p&gt;Output&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;property, then choose 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Select a column&lt;/p&gt;

&lt;p&gt;. Once this is complete, click&lt;/p&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_09.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.8: Adding a conditional column&lt;/p&gt;

&lt;p&gt;This results in a new column called &lt;strong&gt;State Name&lt;/strong&gt;, which has the fully spelled-out state name only appearing on rows where the &lt;code&gt;State Abbreviation&lt;/code&gt; is &lt;code&gt;null&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_10.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.9: End result of following these steps&lt;/p&gt;

&lt;p&gt;This is only setting the stage to fully scrub this dataset. To complete the data cleansing process for this file, read on  to the next section about &lt;strong&gt;Fill Down&lt;/strong&gt;. However, for the purposes of this example, you have now learned how to leverage the capabilities of the &lt;strong&gt;Conditional Column&lt;/strong&gt; transform in the Power Query Editor.&lt;/p&gt;

&lt;h2 id=&#34;fill-down&#34;&gt;Fill Down&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Fill Down&lt;/strong&gt; is a rather unique transform in how it operates. By selecting &lt;strong&gt;Fill Down&lt;/strong&gt; on a particular column, a value will replace all &lt;code&gt;null&lt;/code&gt; values below it until another non-null appears. When another non-null value is present, that value will then fill down to all subsequent &lt;code&gt;null&lt;/code&gt; values. To examine this transform, you will pick up from where you left off in the &lt;strong&gt;Conditional Column&lt;/strong&gt; example in the previous section:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Right-click on the &lt;strong&gt;State Name&lt;/strong&gt; column header and select &lt;strong&gt;Transform&lt;/strong&gt; | &lt;strong&gt;Capitalize Each Word&lt;/strong&gt;. This transform should be self-explanatory.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next, select the &lt;strong&gt;State Name&lt;/strong&gt; column and, in the &lt;strong&gt;Transform&lt;/strong&gt; ribbon, select &lt;strong&gt;Fill&lt;/strong&gt; | &lt;strong&gt;Down&lt;/strong&gt;. This will take the value in the &lt;strong&gt;State Name&lt;/strong&gt; column and replace all non-null values until there is another &lt;strong&gt;State Name&lt;/strong&gt; value that it can switch to. After performing this transform, scroll through the results to ensure that the value of &lt;code&gt;Alabama&lt;/code&gt; switches to &lt;code&gt;Alaska&lt;/code&gt; when appropriate.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To finish this example, filter out any&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;values that appear in the&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;State Abbreviation&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;column. The final result should look like 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.10&lt;/p&gt;

&lt;p&gt;, as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;![img](https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_11.png)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.10: End result of following these steps&lt;/p&gt;

&lt;p&gt;In this example, you learned how you can use &lt;strong&gt;Fill Down&lt;/strong&gt; to replace all of the &lt;code&gt;null&lt;/code&gt; values below a non-null value. You can also use &lt;strong&gt;Fill Up&lt;/strong&gt; to do the opposite, which would replace all the &lt;code&gt;null&lt;/code&gt; values above a non-null value. One important thing to note is that the data must be sorted properly for &lt;strong&gt;Fill Down&lt;/strong&gt; or &lt;strong&gt;Fill Up&lt;/strong&gt; to be successful. In the next section, you will learn about another advanced transform, known as &lt;strong&gt;Unpivot&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;unpivot&#34;&gt;Unpivot&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Unpivot&lt;/strong&gt; transform is an incredibly powerful transform that allows you to reorganize your dataset into a more structured format best suited for BI. Let&amp;rsquo;s discuss this by visualizing a practical example to help understand the purpose of &lt;strong&gt;Unpivot&lt;/strong&gt;. Imagine you are provided with a file that contains the populations of US states over the last three years, and looks as in &lt;em&gt;Figure 2.11&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_12.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.11: Example data that will cause problems in Power BI&lt;/p&gt;

&lt;p&gt;The problem with data stored like this is you  cannot very easily answer simple questions. For example, how would you  answer questions like, &lt;em&gt;What was the total population for all states in the US in 2018?&lt;/em&gt; or &lt;em&gt;What was the average state population in 2016?&lt;/em&gt; With the data stored in this format, simple reports are made rather difficult to design. This is where the &lt;strong&gt;Unpivot&lt;/strong&gt; transform can be a lifesaver. Using &lt;strong&gt;Unpivot&lt;/strong&gt;, you can change this dataset into something more acceptable for an analytics project, as shown in &lt;em&gt;Figure 2.12&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_13.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.12: Results of unpivoted data&lt;/p&gt;

&lt;p&gt;Data stored in this format can now easily answer  the questions posed earlier by simply dragging a few columns into your  visuals. To accomplish this in other programming languages would often require fairly complex logic, while the Power Query Editor does it in just a few clicks.&lt;/p&gt;

&lt;p&gt;There are three different methods for selecting the &lt;strong&gt;Unpivot&lt;/strong&gt; transform that you should be aware of, and they include the following options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Unpivot Columns&lt;/strong&gt;: Turns any selected columns, headers into row values and the data in  those columns into a corresponding row. With this selection, any new  columns that may get added to the data source &lt;em&gt;will&lt;/em&gt; automatically be included in the &lt;strong&gt;Unpivot&lt;/strong&gt; transform.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unpivot Other Columns&lt;/strong&gt;: Turns all column headers that &lt;em&gt;are not&lt;/em&gt; selected into row values and the data in those columns into a  corresponding row. With this selection, any new columns that may get  added to the data source &lt;em&gt;will&lt;/em&gt; automatically be included in the &lt;strong&gt;Unpivot&lt;/strong&gt; transform.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unpivot Only Selected Columns&lt;/strong&gt;: Turns any selected columns&amp;rsquo; headers into row values and the data in  those columns into a corresponding row. With this selection, any new  columns that may get added to the data source &lt;em&gt;will not&lt;/em&gt; be included in the &lt;strong&gt;Unpivot&lt;/strong&gt; transform.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s walk through two examples of using the &lt;strong&gt;Unpivot&lt;/strong&gt; transform to show you the first two of these methods, and provide an  understanding of how this complex problem can be solved with little  effort in Power BI. The third method mentioned for doing &lt;strong&gt;Unpivot&lt;/strong&gt; will not be shown since it&amp;rsquo;s so similar to the first option:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Launch a new instance of the Power BI Desktop, and use the Excel connector to import the workbook called&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Income Per Person.xlsx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;found in the book source files. Once you&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;select this workbook, choose the spreadsheet called&lt;/p&gt;

&lt;p&gt;Data&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;in the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Navigator&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;window, and then select 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Transform Data&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;to launch the Power Query Editor. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.13&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shows what our data looks like before the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unpivot&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;operation:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_14.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.13: Example before Unpivot is performed&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Now, make the first row of data into column headers by selecting the transform called &lt;strong&gt;Use First Row as Headers&lt;/strong&gt; on the &lt;strong&gt;Home&lt;/strong&gt; ribbon.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rename the &lt;strong&gt;GDP per capita PPP, with projections&lt;/strong&gt; column to &lt;strong&gt;Country&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you look closely at the column headers,  you can tell that most of the column names are actually years and the  values inside those columns are the income for those years. This is not  the ideal way to store this data because it would be incredibly  difficult to answer a question like, &lt;em&gt;What is the average income per person for Belgium?&lt;/em&gt; To make it easier to answer this type of question, right-click on the &lt;strong&gt;Country&lt;/strong&gt; column and select &lt;strong&gt;Unpivot Other Columns&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rename the columns &lt;code&gt;Attribute&lt;/code&gt; and &lt;code&gt;Value&lt;/code&gt; to &lt;code&gt;Year&lt;/code&gt; and &lt;code&gt;Income&lt;/code&gt;, respectively.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To finish this first example, you should also rename this query&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Income&lt;/p&gt;

&lt;p&gt;. The results of these first steps can be seen in&lt;/p&gt;

&lt;p&gt;Figure 2.14&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_15.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.14: Results of unpivoted data&lt;/p&gt;

&lt;p&gt;This first method walked you through what can often be the fastest method for performing an &lt;strong&gt;Unpivot&lt;/strong&gt; transform, which is by using the &lt;strong&gt;Unpivot Other Columns&lt;/strong&gt; option. In this next example, you will learn how to use the &lt;strong&gt;Unpivot Columns&lt;/strong&gt; method as well:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Remain in the Power Query Editor, and select&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;New Source&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Home&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ribbon. Use the Excel connector to import the 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;   Total Population.xlsx
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;workbook from the book source files. Once you select this workbook, choose the spreadsheet called 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Data&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;in the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Navigator&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;window, and then select 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;Figure 2.15&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shows the dataset before 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unpivot&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;has been added:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_16.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.15: Example before Unpivot is performed&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Like the last example, you will again need to make the first row of data into column headers by selecting the transform called &lt;strong&gt;Use First Row as Headers&lt;/strong&gt; on the &lt;strong&gt;Home&lt;/strong&gt; ribbon.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then, rename the column &lt;strong&gt;Total population&lt;/strong&gt; to &lt;strong&gt;Country&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This time, multi-select all the columns except&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Country&lt;/p&gt;

&lt;p&gt;, then right-click on one of the selected columns and choose&lt;/p&gt;

&lt;p&gt;Unpivot Other Columns&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;as shown in 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.16&lt;/p&gt;

&lt;p&gt;. The easiest way to multi-select these columns is to select the first column then hold&lt;/p&gt;

&lt;p&gt;Shift&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;before clicking the last column:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_17.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.16: Using the Unpivot Other Columns transform&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Rename the columns from&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Attribute&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;and 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Value&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;to 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Year&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;and 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Population&lt;/p&gt;

&lt;p&gt;, respectively, to see the result showing in&lt;/p&gt;

&lt;p&gt;Figure 2.17&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_18.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.17: Shows the final result of these steps&lt;/p&gt;

&lt;p&gt;In this section, you learned about two different methods for performing an &lt;strong&gt;Unpivot&lt;/strong&gt;. To complete the data cleansing process on these two datasets, it&amp;rsquo;s  recommended that you continue through the next section on merging  queries.&lt;/p&gt;

&lt;h2 id=&#34;merge-query&#34;&gt;Merge Query&lt;/h2&gt;

&lt;p&gt;Another common requirement when building BI solutions is the need to join two tables together to form a new outcome that includes some columns from both tables in the result.  Fortunately, Power BI makes this task very simple with the &lt;strong&gt;Merge Queries&lt;/strong&gt; feature. Using this feature requires that you select two tables and  then determine which column or columns will be the basis of how the two  queries are merged. After determining the appropriate columns for your  join, you will select a join type. The join types are listed here with  the description that is provided within the product:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Left Outer&lt;/strong&gt; (all rows from the first table, only matching rows from the second)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Right Outer&lt;/strong&gt; (all rows from the second table, only matching rows from the first)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Full Outer&lt;/strong&gt; (all rows from both tables)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inner&lt;/strong&gt; (only matching rows from both tables)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Left Anti&lt;/strong&gt; (rows only in the first table)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Right Anti&lt;/strong&gt; (rows only in the second table)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many of you may already be very familiar with these different join terms from SQL programming you have  learned in the past. However, if these terms are new to you, I recommend reviewing &lt;em&gt;Visualizing Merge Join Types in Power BI&lt;/em&gt;, courtesy of Jason Thomas in the &lt;em&gt;Power BI Data Story Gallery&lt;/em&gt;: &lt;a href=&#34;https://community.powerbi.com/t5/Data-Stories-Gallery/Visualizing-Merge-Join-Types-in-Power-BI/m-p/219906&#34; target=&#34;_blank&#34;&gt;https://community.powerbi.com/t5/Data-Stories-Gallery/Visualizing-Merge-Join-Types-in-Power-BI/m-p/219906&lt;/a&gt;. This visual aid is a favorite of many users that are new to these concepts.&lt;/p&gt;

&lt;p&gt;To examine the &lt;strong&gt;Merge Queries&lt;/strong&gt; option, you will pick up from where you left off with the &lt;strong&gt;Unpivot&lt;/strong&gt; examples in the previous section:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;With the &lt;strong&gt;Population&lt;/strong&gt; query selected, find and select &lt;strong&gt;Merge Queries&lt;/strong&gt; | &lt;strong&gt;Merge Queries as New&lt;/strong&gt; on the &lt;strong&gt;Home&lt;/strong&gt; ribbon.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the &lt;strong&gt;Merge&lt;/strong&gt; dialog box, select the &lt;strong&gt;Income&lt;/strong&gt; query from the drop-down selection in the middle of the screen.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then, multi-select the &lt;strong&gt;Country&lt;/strong&gt; and &lt;strong&gt;Year&lt;/strong&gt; columns on the &lt;strong&gt;Population&lt;/strong&gt; query, and do the same under the &lt;strong&gt;Income&lt;/strong&gt; query. This defines which columns will be used to join the two queries  together. Ensure that the number indicators next to the column headers  match, as demonstrated in &lt;em&gt;Figure 2.18&lt;/em&gt;. If they don&amp;rsquo;t, you could accidentally attempt to join on the incorrect columns.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next, select&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Inner&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;only matching rows&lt;/p&gt;

&lt;p&gt;) for&lt;/p&gt;

&lt;p&gt;Join Kind&lt;/p&gt;

&lt;p&gt;. This join type will return rows only when the columns you chose to join on have values that exist in both queries. Before you click&lt;/p&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;p&gt;, confirm that your screen matches&lt;/p&gt;

&lt;p&gt;Figure 2.18&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_19.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.18: Configuring a merge between two queries&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Once you select &lt;strong&gt;OK&lt;/strong&gt;, this will create a new query called &lt;code&gt;Merge1&lt;/code&gt; that combines the results of the two queries. Go ahead and rename this query &lt;code&gt;Country Stats&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You will also notice that there is a column called &lt;code&gt;Income&lt;/code&gt; that has a value of &lt;code&gt;Table&lt;/code&gt; for each row. This column is actually representative of the entire &lt;code&gt;Income&lt;/code&gt; query that you joined to. To choose which columns you want from this query, click the &lt;strong&gt;Expand&lt;/strong&gt; button on the column header. After clicking the &lt;strong&gt;Expand&lt;/strong&gt; button, uncheck &lt;strong&gt;Country&lt;/strong&gt;, &lt;strong&gt;Year&lt;/strong&gt;, and &lt;strong&gt;Use original column name as prefix&lt;/strong&gt;, then click &lt;strong&gt;OK&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rename the column called&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Income.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Income
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;Figure 2.19&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shows this step completed:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_20.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.19: Configuring a merge between two queries&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Finally, since you chose the option&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Merge Queries as New&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;in 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Step 1&lt;/p&gt;

&lt;p&gt;, you can disable the load&lt;/p&gt;

&lt;p&gt;option for the original queries that you started with. To do this, right-click on the&lt;/p&gt;

&lt;p&gt;Income&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query in the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Queries&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pane and click 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Enable load&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;to disable it. Do the same thing for the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Population&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query as shown in 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.20&lt;/p&gt;

&lt;p&gt;. Disabling these queries means that the only query that will be loaded into your Power BI data model is the new one, called&lt;/p&gt;

&lt;p&gt;Country Stats&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_21.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.20: Uncheck to disable the loading of this query into the data model&lt;/p&gt;

&lt;p&gt;To begin using this dataset in a report, you would click &lt;strong&gt;Close &amp;amp; Apply&lt;/strong&gt;. You will learn more about building reports in &lt;em&gt;Chapter 5&lt;/em&gt;, &lt;em&gt;Visualizing Data&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;By default, merging queries together relies on  exact matching values between your join column(s). However, you may work with data that does not always provide perfect matching values. For  example, a user enters data and misspells their country as &amp;ldquo;Unite  States&amp;rdquo; instead of &lt;code&gt;United States&lt;/code&gt;. In those cases, you may consider the more advanced feature called Fuzzy Matching. With Fuzzy Matching, Power BI can perform an approximate match and  still join on these two values based on the similarity of the values. In this section, you learned how the &lt;strong&gt;Merge Queries&lt;/strong&gt; option is ideal for joining two queries together. In the next section,  you will learn how you could solve the problem of performing a union of  two or more queries.&lt;/p&gt;

&lt;h2 id=&#34;append-query&#34;&gt;Append Query&lt;/h2&gt;

&lt;p&gt;Occasionally, you will work with multiple datasets that need to be appended to each other. Here&amp;rsquo;s a scenario: you work for a customer service department for a company that provides  credit or loans to customers. You are regularly provided with &lt;code&gt;.csv&lt;/code&gt; and &lt;code&gt;.xlsx&lt;/code&gt; files that give summaries of customer complaints regarding credit cards and student loans. You would like to analyze both of these data  extracts at the same time but, unfortunately, the credit card and  student loan complaints are provided in two separate files. In this  example, you will learn how to solve this problem by performing an  append operation on these two different files:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Launch a new instance of the Power BI Desktop, and use the Excel connector to import the workbook called &lt;code&gt;Student Loan Complaints.xlsx&lt;/code&gt; found in the book source files. Once you select this workbook, choose the spreadsheet called &lt;strong&gt;Student Loan Complaints&lt;/strong&gt; in the &lt;strong&gt;Navigator&lt;/strong&gt; window, and then select &lt;strong&gt;Transform Data&lt;/strong&gt; to launch the Power Query Editor.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next, import the credit card data by selecting &lt;strong&gt;New Source&lt;/strong&gt; | &lt;strong&gt;Text/CSV&lt;/strong&gt;, then choose the file called &lt;code&gt;Credit Card Complaints.csv&lt;/code&gt; found in the book source files. Click &lt;strong&gt;OK&lt;/strong&gt; to bring this data into the Power Query Editor.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;With the &lt;code&gt;Credit Card Complaints&lt;/code&gt; query selected, find and select &lt;strong&gt;Append Queries&lt;/strong&gt; | &lt;strong&gt;Append Queries as New&lt;/strong&gt; on the &lt;strong&gt;Home&lt;/strong&gt; ribbon.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Student Loan Complaints&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;as the table to append to, then select 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;as shown in 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.21&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_22.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.21: Configuring an append between two queries&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Rename the newly created query&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All Complaints&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;and view the results as seen in 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.22&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_23.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.22: Configuring an append between two queries&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Similar to the  previous example, you would likely want to disable the load option for  the original queries that you started with. To do this, right-click on  the &lt;strong&gt;Student Load Complaints&lt;/strong&gt; query in the &lt;strong&gt;Queries&lt;/strong&gt; pane and click &lt;strong&gt;Enable load&lt;/strong&gt; to disable it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Do the same to the &lt;strong&gt;Credit Card Complaints&lt;/strong&gt; query, and then select &lt;strong&gt;Close &amp;amp; Apply&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that you have learned about the various  methods for combining data, the next section will discuss a more  advanced method of working with data using the R programming language.&lt;/p&gt;

&lt;h1 id=&#34;leveraging-r&#34;&gt;Leveraging R&lt;/h1&gt;

&lt;p&gt;R is a very powerful scripting language that is primarily used for advanced analytics tools, but also has several integration points within Power BI. One such  integration is the ability to apply business rules to your data with the R language. Why is that important? Well, with this capability you can  extend beyond the limits of the Power Query Editor and call functions  and libraries from R to do things that would not normally be possible.  In the next two sections, you will explore how to set up your machine to leverage R within Power BI and then walk through an example of using an R script transform.&lt;/p&gt;

&lt;p&gt;There are many additional books and references you can read to learn more about the R scripting  language, but for the purposes of this book, our goal is to inform you  of what is possible when R and Power BI are combined.&lt;/p&gt;

&lt;h2 id=&#34;installation-and-configuration&#34;&gt;Installation and configuration&lt;/h2&gt;

&lt;p&gt;To use R within Power BI, you must first install an R distribution for you to run and execute scripts against. In this book, we will leverage Microsoft&amp;rsquo;s distribution, Microsoft R Open. It is an open source project and free for anyone to use. Once Microsoft R Open has  been installed, you can then configure Power BI to recognize the home  directory where R libraries may be installed. Let&amp;rsquo;s walk through these  setup steps together:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Navigate to the website &lt;a href=&#34;https://mran.microsoft.com/download/&#34; target=&#34;_blank&#34;&gt;https://mran.microsoft.com/download/&lt;/a&gt; to download and install Microsoft R Open.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For the purposes of our example, you will select &lt;strong&gt;Download&lt;/strong&gt; next to &lt;strong&gt;Windows&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Once the download has completed, run the installation and accept all default settings and user agreements.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next, launch a new instance of Power BI Desktop to set up the R integration with Power BI. Click the menu options &lt;strong&gt;File&lt;/strong&gt; | &lt;strong&gt;Options and settings&lt;/strong&gt; | &lt;strong&gt;Options&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Choose the&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;R scripting&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;section and ensure that the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Detected R home directories&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;property is filled with the R instance you just installed, as shown in 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.23&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_24.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.23: Mapping the R home directories in Power BI&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Once this is completed, click &lt;strong&gt;OK&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With this setup now complete, let&amp;rsquo;s see how we can take advantage of R within Power BI.&lt;/p&gt;

&lt;h2 id=&#34;the-r-script-transform&#34;&gt;The R script transform&lt;/h2&gt;

&lt;p&gt;With the R distribution now installed and configured to integrate with Power BI, you are now ready to see what&amp;rsquo;s possible with these new capabilities. In this example, you will be  looking at data from the European stock market. The problem with this  dataset, which calls for it to be corrected with R, is that the file  provided to you has missing values for certain days. So, to get a more  accurate reading of the stock market, you will use an R package called &lt;code&gt;MICE&lt;/code&gt; to impute the missing values:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Before beginning in Power BI, you should ensure that the&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MICE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;library is installed and available in the R distribution you set up in  the last section. To do this, launch Microsoft R Open from your device.  This is the basic RGui that was installed for you to run R scripts with.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For many developers, the preferred method for writing &lt;code&gt;R&lt;/code&gt; scripts is a free open source tool called RStudio. RStudio includes a code editor, debugging, and visualization tools that many find easier to work with. You can download RStudio from &lt;a href=&#34;https://www.rstudio.com/&#34; target=&#34;_blank&#34;&gt;https://www.rstudio.com/&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Type the following script in the&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;R Console&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;window, and then hit 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Enter&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       install.packages(&amp;quot;mice&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This input is illustrated in the following screenshot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_25.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.24: Running the library install in RGui&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You can close the&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;R Console&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;window and return to Power BI Desktop after it returns an output like the following:        
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;   package &#39;mice&#39; successfully unpacked and MD5 sums checked.
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;In Power BI Desktop, start by connecting to the required data source called &lt;code&gt;EuStockMarkets_NA.csv&lt;/code&gt; from the book source files. Once you connect to the file, click &lt;strong&gt;Transform Data&lt;/strong&gt; to launch the Power Query Editor.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You will notice that there are a few days missing values in the &lt;strong&gt;SMI&lt;/strong&gt; (&lt;strong&gt;Stock Market Index&lt;/strong&gt;) column. We would like to replace values that show &lt;strong&gt;NA&lt;/strong&gt; with approximate values using an R script. Go to the &lt;strong&gt;Transform&lt;/strong&gt; ribbon, and select the &lt;strong&gt;Run R Script&lt;/strong&gt; button on the far right.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the following R script to call the&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MICE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;library that you recently installed to detect what the missing values in this dataset should be:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   # &#39;dataset&#39; holds the input data for this script
   library(mice)
   tempData &amp;lt;- mice(dataset,m=1,maxit=50,meth=&#39;pmm&#39;,seed=100)
   completedData &amp;lt;- complete(tempData,1)
   output &amp;lt;- dataset
   output$completedValues &amp;lt;- completedData$&amp;quot;SMI missing values&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;OK&lt;/strong&gt;. If you are prompted with a warning indicating &lt;strong&gt;Information is required about data privacy&lt;/strong&gt; click &lt;strong&gt;Continue&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next, click on the hyperlink on the table value next to the &lt;code&gt;completedData&lt;/code&gt; row to see the result of the newly implemented transform for detecting missing values.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This new output has replaced the missing values  with new values that were detected based on the algorithm used within  the R script. To now build a set of report visuals on this example, you  can click &lt;strong&gt;Close &amp;amp; Apply&lt;/strong&gt; on the &lt;strong&gt;Home&lt;/strong&gt; ribbon.&lt;/p&gt;

&lt;p&gt;This is just one simple way that R can be used  with Power BI. You should note that in addition to using R as a  transform, it can also be used as a data source and as a visual within  Power BI.&lt;/p&gt;

&lt;p&gt;While this book highlights the programming  language R to extend the capabilities of Power BI, some might prefer  Python. Python is another programming language that allows for  extensibility into Power BI to create new  data connectors, transforms, and visuals. So, should you choose R or  Python? That depends on which you are more comfortable with. If you have already spent time learning &lt;code&gt;Python&lt;/code&gt;, then stick with that! In the next section of this chapter, you will  learn about Power BI&amp;rsquo;s AI integration features, which give you the  ability to call on components of Azure Cognitive Services with the Power Query Editor.&lt;/p&gt;

&lt;h1 id=&#34;ai-insights&#34;&gt;AI Insights&lt;/h1&gt;

&lt;p&gt;As you learned in the previous section, Power BI integrates and takes advantage of outside tools to enhance the capabilities within itself. That continues to be the case with the  AI Insights features. Leveraging the AI Insights capabilities gives you  the ability to tap into core features and algorithms within Azure  Cognitive Services and expose them within Power BI. So how can this be  useful to you?&lt;/p&gt;

&lt;p&gt;Imagine you work for a company that runs a  vacation rentals website. Customers can book travel and post reviews of  their trips on your website. With thousands of customers and hundreds of rental homes, it can be difficult to manage all the reviews that come  in to make sure your locations are all meeting the standards your  customers expect. With AI Insights you can run algorithms that can  perform sentiment analysis, key phrase extraction, language detection,  and even image tagging. So, if you have international customers that  post reviews, you can use language detection to understand what language the post was written in. Then you can use sentiment analysis to capture whether the review was positive or negative. Finally, using phrase  extraction, you can pull out key terms in the reviews to see if the same locations continue to receive feedback regarding similar problems.  Furthermore, if your feedback system allows photos to be posted in the  reviews, the image tagging capabilities can return a list of  characteristics found in the images posted. This would allow for  automated categorization of images using AI.&lt;/p&gt;

&lt;p&gt;As you can see, these are very powerful features  that take your analytics processing to the next level. There are  limitations, however, that you should be aware of before exploring these features. As of the time that this book was published, Cognitive  Services integration is only supported for Power BI Premium capacity  nodes &lt;code&gt;EM2&lt;/code&gt;, &lt;code&gt;A2&lt;/code&gt;, or &lt;code&gt;P1&lt;/code&gt; and above. This means if your company is not currently leveraging Power BI Premium, then these features are not available to you.&lt;/p&gt;

&lt;p&gt;Before using the AI Insights  features in Power BI, you will need to change the capacity settings in  the Power BI admin portal to enable the AI workload. After turning on  the AI workload setting, you can also set the maximum amount of memory  you would like to give the workload. The general recommendation is a  memory limit of 20%.&lt;/p&gt;

&lt;p&gt;In the next section, you will learn how to leverage an AI Insights Text Analytics feature called Sentiment Analysis.&lt;/p&gt;

&lt;h2 id=&#34;sentiment-analysis-with-text-analytics&#34;&gt;Sentiment Analysis with Text Analytics&lt;/h2&gt;

&lt;p&gt;The Text Analytics features within the AI Insights features can be incredible time-savers. Imagine  having to read paragraphs of information and conclude what was important or whether it was written in a positive or negative light. These are  exactly the type of things that this feature can do for you. In this  next example, you are going to test out one of these features by running a sentiment analysis algorithm on hotel reviews to see how customers  feel about staying at your hotel locations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Launch a new instance of Power BI Desktop, and use the Excel connector to import the workbook called &lt;code&gt;Hotel Ratings.xlsx&lt;/code&gt; found in the book source files. Once you select this workbook, choose the spreadsheet called &lt;strong&gt;Reviews&lt;/strong&gt; in the &lt;strong&gt;Navigator&lt;/strong&gt; window, and then select &lt;strong&gt;Transform Data&lt;/strong&gt; to launch the Power Query Editor.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select &lt;strong&gt;Text Analytics&lt;/strong&gt; on the &lt;strong&gt;Home&lt;/strong&gt; ribbon of the Power Query Editor. If this is your first time using this feature, you may be prompted to sign into a Power BI account that has  Power BI Premium capacity assigned to it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next, you will be prompted to choose which Text Analytics algorithm you would like to use. Select&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Score sentiment&lt;/p&gt;

&lt;p&gt;, as shown in&lt;/p&gt;

&lt;p&gt;Figure 2.25&lt;/p&gt;

&lt;p&gt;, and ensure the&lt;/p&gt;

&lt;p&gt;ReviewText&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;field is the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Text&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;that will be analyzed. Then click 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_26.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.25: Using the Text Analytics feature&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If prompted with a data privacy warning, click &lt;strong&gt;Continue&lt;/strong&gt; and then select &lt;strong&gt;Ignore Privacy Levels check&lt;/strong&gt; for this file before clicking &lt;strong&gt;Save&lt;/strong&gt;. This type of warning can occur when you combine two disparate sources  or services together and is to ensure it is OK for these data sources to be combined.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This transform will produce a new numeric column with a value between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; for every row in the dataset. A sentiment score of &lt;code&gt;.50&lt;/code&gt; is considered neutral, while any score lower is negative and any score higher is generally positive:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_27.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.26: Results of Score sentiment&lt;/p&gt;

&lt;p&gt;Looking at &lt;em&gt;Figure 2.26&lt;/em&gt;, it looks like the AI integration, with a few exceptions, did a good job determining how to rate each review.&lt;/p&gt;

&lt;p&gt;Next, in the final section of this chapter, you will be introduced to the M formula language.&lt;/p&gt;

&lt;h1 id=&#34;the-m-formula-language&#34;&gt;The M formula language&lt;/h1&gt;

&lt;p&gt;The Power Query Editor is the user interface that is used to design and build data imports. However,  you should also know that every transform you apply within this editor  is actually, quietly and behind the scenes, writing an M query for you.  The letter M here is a reference to the language&amp;rsquo;s data mashup  capabilities.&lt;/p&gt;

&lt;p&gt;For simple solutions, it is unlikely that you will ever need to even look at the M query that is being written, but there  are some more complex cases where it&amp;rsquo;s helpful to understand how to read and write your own M. For the purposes of this book, covering just the  Power BI essentials, you will learn how to find the M query editor  within your solution and then understand how to read what it is doing  for you.&lt;/p&gt;

&lt;p&gt;For the purposes of this example, you can open up  any previously built example, however, the screenshot used here is from  the very first example in this chapter on basic transforms:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Using any Power BI solution you have designed, launch the Power Query Editor.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On the&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Home&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ribbon, select 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Advanced Editor&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;to see the M query that has been written by the user interface. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2.27&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shows an example of what your 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Advanced Editor&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;might show:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_28.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.27: Understanding the elements of M&lt;/p&gt;

&lt;p&gt;This query has been formatted to make it easier to read. Let&amp;rsquo;s review the key elements that are present here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;The let expression&lt;/strong&gt;: Encapsulates a set of values or named expressions to be computed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Named expressions or variables&lt;/strong&gt;: The name given to a set of operations in a step. These names can be  anything, but you should note that if you wish to have a space in the  name of a step then it must be surrounded by &lt;code&gt;#&amp;quot;&amp;quot;&lt;/code&gt;. For example, if I wanted something to be called &lt;em&gt;Step 1&lt;/em&gt;, then I would have to name an expression &lt;code&gt;#&amp;quot;Step 1&amp;quot;&lt;/code&gt;. If a space is not required in the name of your step then the double quotes are not required.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M functions&lt;/strong&gt;: The operations that are used to manipulate the data source.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prior step reference&lt;/strong&gt;: The M query language generally executes its functions as serial  operations, meaning each operation is executed one after the other  sequentially. You can see this when you look at a query because each  call to an M function always references the prior-named expression, to  pick up where it left off.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The in expression&lt;/strong&gt;: Oddly, the &lt;code&gt;in&lt;/code&gt; expression is actually a reference to what the query will output. Whichever named expression is referenced in the &lt;code&gt;in&lt;/code&gt; expression will be what is returned back in the &lt;code&gt;Power Query Editor&lt;/code&gt; preview.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is important to realize that M is case-sensitive. That means if you ever make a change to a query or write one from scratch, you should be careful because there is a  difference between &amp;ldquo;a&amp;rdquo; and &amp;ldquo;A.&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;shared&#34;&gt;#shared&lt;/h2&gt;

&lt;p&gt;As mentioned  previously, this book will not dive deep into writing your own M queries since that would be far beyond the essentials of Power BI. However, there is a great method for exploring the M functions that are available, and how to use them. Within the Power Query Editor, you can  use the &lt;code&gt;#shared&lt;/code&gt; function to  return documentation on every available function in the M library. Let&amp;rsquo;s walk through how you can leverage this tool:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;In a new instance of Power BI Desktop, select &lt;strong&gt;Get data&lt;/strong&gt; and then choose &lt;strong&gt;Blank Query&lt;/strong&gt;. This will launch the Power Query Editor with an empty formula bar waiting for you to provide your own M.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In this formula bar, type &lt;code&gt;= #shared&lt;/code&gt;, then hit &lt;em&gt;Enter&lt;/em&gt;. Remember that M is case-sensitive so you must use a lowercase &lt;code&gt;s&lt;/code&gt; when typing &lt;code&gt;shared&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This will return a list of all the available M functions. By selecting the cell that has the hyperlink text of a  certain function, you can see documentation on how to use each function.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figure 2.28&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shows an example of this:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_02_29.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.28: Example of function documentation&lt;/p&gt;

&lt;p&gt;This is a great method for learning what M functions are available, and how each may be used.  If you are stumped on how to solve a problem using M then make this your first stop to explore what options you have.&lt;/p&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this chapter, you learned that the Power Query  Editor is an extremely powerful tool for applying business rules to  incoming data. Implementing data cleansing techniques can be as simple  as right-clicking on a column, or more complex, such as when building a  conditional column. While the Power Query Editor does have a vast  library of transforms available, you also learned that you can tap into  the capabilities of R to extend what&amp;rsquo;s possible when designing queries.  Finally, this chapter discussed the AI capabilities within the Power  Query Editor that allow you to leverage several algorithms available  within Azure Cognitive Services. In the next chapter, on building the  data model, you will learn about proper techniques for building a  well-designed Power BI data model to ensure your solutions can solve all your reporting needs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Import data in Power BI</title>
      <link>https://www.marcusrb.com/en/power-bi/02-import-data/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/power-bi/02-import-data/</guid>
      <description>

&lt;h1 id=&#34;importing-data&#34;&gt;Importing data&lt;/h1&gt;

&lt;p&gt;Power BI is best known for the impressive data visualizations and dashboard capabilities it has. However, before you can begin building reports, you first need to connect to the necessary  data sources. Within Power BI Desktop, a developer has more than 130  unique data connectors to choose from, ranging from traditional file  types, database engines, big data solutions, cloud sources, data stored  on a web page, and other SaaS providers. This book will not cover all  130 connectors that are available, but it will highlight some of the  most popular.&lt;/p&gt;

&lt;p&gt;When establishing a connection to a data source,  you may be presented with one of three different options regarding how  your data will be treated: &lt;strong&gt;Import&lt;/strong&gt;, &lt;strong&gt;DirectQuery&lt;/strong&gt;, or live connection. This section will focus specifically on the Import option.&lt;/p&gt;

&lt;p&gt;Choosing to import data, which is the most common  option and default behavior, means that Power BI will physically extract rows of data from the selected source and store it in an in-memory  storage engine within Power BI. Power BI Desktop uses a special method  for storing data, known as xVelocity,  which is an in-memory technology that not only increases the performance of your query results, but can also highly compress the amount of space taken up by your Power BI solution. In &lt;em&gt;some&lt;/em&gt; cases, the compression that takes place can even lower the required disk space by up to one-tenth of the original data source size. The xVelocity engine uses a local unseen instance of &lt;strong&gt;SQL Server Analysis Services&lt;/strong&gt; (&lt;strong&gt;SSAS&lt;/strong&gt;) to provide these in-memory capabilities.&lt;/p&gt;

&lt;p&gt;There are consequences to using the &lt;strong&gt;Import&lt;/strong&gt; option within Power BI that you should also consider. These  consequences will be discussed later in this chapter, but as you read  on, consider the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How does data that has been imported into Power BI get updated?&lt;/li&gt;
&lt;li&gt;What if I need a dashboard to show near real-time analytics?&lt;/li&gt;
&lt;li&gt;How much data can really be imported into an in-memory storage system?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that you are familiar with the underlying  mechanics of importing data, let&amp;rsquo;s try it out with a few of the most  common data source types, starting with Excel.&lt;/p&gt;

&lt;h2 id=&#34;excel-as-a-source&#34;&gt;Excel as a source&lt;/h2&gt;

&lt;p&gt;Believe it or not, Excel continues to be the most popular application in the world and, as such, you should expect that at some point, you will be using it as a data source:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;To get started, open Power BI Desktop and close the start up screen if it automatically appears.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Under the &lt;strong&gt;Home&lt;/strong&gt; ribbon, you will find that &lt;strong&gt;Get data&lt;/strong&gt; button, which you already learned is used for selecting and configuring data sources. Selecting the down arrow next to the button will show you the most common data connectors, but selecting the center of the button will launch the full list of all available connectors. Regardless of  which way you select the button, you will find Excel at the top of both  lists.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Navigate to and open the file called&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AdventureWorksDW.xlsx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;from this book&amp;rsquo;s resources. This will&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;launch the&lt;/p&gt;

&lt;p&gt;Navigator&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dialog, shown in the following screenshot, which is used for selecting the objects in the Excel 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;workbook you wish to take data from:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_02.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.2: The Navigator dialog is used for selecting spreadsheets or tables&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;In this example, you can see six separate  spreadsheets you can choose from. Clicking once on the spreadsheet name  will give you a preview of the data it stores, while clicking the  checkbox next to the name will include it as part of the data import.  For this example, select the checkboxes next to all of the available  objects, then notice the options available at the bottom right.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Selecting &lt;strong&gt;Load&lt;/strong&gt; will immediately take the data from the selected spreadsheets and import them as  separate tables into your Power BI data model. Choosing &lt;strong&gt;Transform Data&lt;/strong&gt; will launch an entirely new window called Power Query Editor, which allows you to apply business rules or transform to your data prior to  importing it. You will learn much more about Power Query Editor in &lt;em&gt;Chapter 2&lt;/em&gt;, &lt;em&gt;Data Transformation Strategies&lt;/em&gt;. Since you will learn more about this later, simply select &lt;strong&gt;Load&lt;/strong&gt; to end this example.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Another topic you will learn more about in &lt;em&gt;Chapter 7&lt;/em&gt;, &lt;em&gt;Using a Cloud Deployment with the Power BI Service&lt;/em&gt;, is the concept of data refreshes. This is important because, when  you import data into Power BI, that data remains static until another  refresh is initiated. This refresh can either be initiated manually or set on a schedule. This also requires the installation of  Data Gateway, the application in charge of securely pushing data into  Power BI Service. Feel free to skip to &lt;em&gt;Chapter 7&lt;/em&gt;, &lt;em&gt;Using a Cloud Deployment with the Power BI Service&lt;/em&gt;, if configuring a data refresh is a subject you need to know about now.&lt;/p&gt;

&lt;h2 id=&#34;sql-server-as-a-source&#34;&gt;SQL Server as a source&lt;/h2&gt;

&lt;p&gt;Another common source designed for relational databases is Microsoft SQL Server:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;To connect to SQL Server, select the&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Get data&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;button again, but this time choose 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SQL Server&lt;/p&gt;

&lt;p&gt;. The following screenshot shows that you must provide the server, but the database is optional and can be selected later:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_03.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.3: Establishing a connection to SQL Server&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;On your first use of SQL server, you are asked to choose the type of&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Data Connectivity mode&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;you would like. As mentioned previously, 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;is the default mode, but you can optionally select 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DirectQuery&lt;/p&gt;

&lt;p&gt;. DirectQuery will be discussed in greater detail later in this&lt;/p&gt;

&lt;p&gt;chapter. Expanding the&lt;/p&gt;

&lt;p&gt;Advanced&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;options provides a way to insert a SQL statement that may be used as your source. For the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;following example, the&lt;/p&gt;

&lt;p&gt;Server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name property is the only property populated before you click 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_04.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.4: Providing credentials to SQL Server&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Next, you will be prompted, as shown in the  preceding screenshot, to provide the credentials you are using to  connect to the database server you provided on the previous screen.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;Connect&lt;/strong&gt; after providing the proper credentials. You may be prompted with a  warning stating that Power BI is only able to access the data source  using an unencrypted connection. Click &lt;strong&gt;OK&lt;/strong&gt; if you encounter this to launch the same &lt;strong&gt;Navigator&lt;/strong&gt; dialog that you may remember from when you connected to Excel. Here, you will select the  tables, views, or functions within your SQL Server database that you  wish to import into your Power BI solution. Once again, the final step in this dialog allows you to choose to either &lt;strong&gt;Load&lt;/strong&gt; or &lt;strong&gt;Transform Data&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that you have a better understanding of how to connect to some of the most common data sources, let&amp;rsquo;s look at one that is rather unique.&lt;/p&gt;

&lt;h2 id=&#34;web-as-a-source&#34;&gt;Web as a source&lt;/h2&gt;

&lt;p&gt;One pleasant surprise to many Power BI developers is the availability of a web connector. Using this connection type allows you to source data from files that are stored on a website, or even data that has been embedded into an HTML table on a web page. Using this type of connector can  often be helpful when you would like to supplement your internal  corporate data sources with information that can be publicly found on  the internet.&lt;/p&gt;

&lt;p&gt;For this example, imagine you are working for a  major automobile manufacturer in the United States. You have already  designed a Power BI solution using data internally available within your organization that shows historical patterns in sales trends. However,  you would like to determine whether there are any correlations in  periods of historically higher fuel prices and lower automobile sales.  Fortunately, you found that the United States Department of Labor  publicly posts the historical average consumer prices of many commonly  purchased items, including fuel prices:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Now that you understand the scenario within Power BI Desktop, select the&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Get data&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;button and choose 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Web&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;as your source. You will then be prompted to provide the 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;URL&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;where the data can be found. In this example, the data can be found by 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;searching on the website&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.data.gov/&#34; target=&#34;_blank&#34;&gt;https://www.data.gov/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;. Alternatively, to save you some time, use the direct link:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://download.bls.gov/pub/time.series/ap/ap.data.2.Gasoline&#34; target=&#34;_blank&#34;&gt;https://download.bls.gov/pub/time.series/ap/ap.data.2.Gasoline&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;. Once you provide the&lt;/p&gt;

&lt;p&gt;URL&lt;/p&gt;

&lt;p&gt;, click&lt;/p&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;p&gt;, as shown in the following screenshot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_05.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.5: Providing the URL where your data can be found&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Next, you will likely be prompted with an&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Access Web Content&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dialog box. This is important 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;when you are using a data source that requires someone to log in to access it. Since this&lt;/p&gt;

&lt;p&gt;data source does not require a login to find the data, you can simply select anonymous access, which is the default, and then click&lt;/p&gt;

&lt;p&gt;Connect&lt;/p&gt;

&lt;p&gt;, as shown in the following screenshot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_06.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.6: A preview of the data is shown before you import it into Power BI&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Notice on the next screen that Power BI Desktop recognizes the URL you provided as a tab-delimited file. This can now easily be added to any existing data model you have designed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that you&amp;rsquo;ve learned how to connect to various  data sources, it is important to discuss in more depth the different  data storage modes.&lt;/p&gt;

&lt;h1 id=&#34;directquery&#34;&gt;DirectQuery&lt;/h1&gt;

&lt;p&gt;Many of you have likely been trying to envision how you may implement these data imports in your  environment. You may have asked yourself questions such as the  following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If data imported into Power BI uses an  in-memory technology, did my company provide me with a machine that has  enough memory to handle this?&lt;/li&gt;
&lt;li&gt;Am I really going to import my source table with tens of billions of rows into memory?&lt;/li&gt;
&lt;li&gt;How do I handle the requirement of displaying results in real time from the source?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all excellent questions that would have  many negative answers if the only way to connect to your data was by  importing your source into Power BI. Fortunately, there is another way.  Using DirectQuery, Power BI allows you to connect directly to a data  source so that no data is imported or copied into Power BI Desktop.&lt;/p&gt;

&lt;p&gt;Why is this a good thing? Consider the questions  that were asked at the beginning of this section. Since no data is  imported to Power BI Desktop, this means it is less important how  powerful your personal laptop is. This is because all query results  are now processed on the source server instead of your laptop. It also  means that there is no need to refresh the results in Power BI, since  any reports you design are always pointing to a live version of the data source. That&amp;rsquo;s a huge benefit!&lt;/p&gt;

&lt;p&gt;The following screenshot shows a connection to a SQL Server database with the &lt;strong&gt;DirectQuery&lt;/strong&gt; option selected:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_07.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.7: SQL Server Data Connectivity mode allows you to switch to DirectQuery mode&lt;/p&gt;

&lt;p&gt;Earlier in this chapter, the Data Gateway  application was mentioned as a requirement for scheduling data refreshes for sources that used the &lt;strong&gt;Import&lt;/strong&gt; option. This same application is also needed with &lt;strong&gt;DirectQuery&lt;/strong&gt; if your data is an on-premises source. Even though there is no  scheduled data refresh, the Data Gateway application is still required  to push on-premises data into the cloud. Again, this will be discussed  in more depth in &lt;em&gt;Chapter 7&lt;/em&gt;, &lt;em&gt;Using a Cloud Deployment with the Power BI Service&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;So, if DirectQuery is so great, why not choose it every time? Well, with every great feature, you will also find limitations. The first glaring limitation is that not all data sources support  DirectQuery. At the time this book was written, the following data  sources support DirectQuery in Power BI:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amazon Redshift&lt;/li&gt;
&lt;li&gt;AtScale Cubes&lt;/li&gt;
&lt;li&gt;Azure HDInsight Spark&lt;/li&gt;
&lt;li&gt;Azure SQL Database&lt;/li&gt;
&lt;li&gt;Azure SQL Data Warehouse&lt;/li&gt;
&lt;li&gt;BI Connector&lt;/li&gt;
&lt;li&gt;Denodo&lt;/li&gt;
&lt;li&gt;Dremio&lt;/li&gt;
&lt;li&gt;Essbase&lt;/li&gt;
&lt;li&gt;Exasol&lt;/li&gt;
&lt;li&gt;Google BigQuery&lt;/li&gt;
&lt;li&gt;HDInsight Interactive Query&lt;/li&gt;
&lt;li&gt;IBM DB2&lt;/li&gt;
&lt;li&gt;IBM Netezza&lt;/li&gt;
&lt;li&gt;Impala&lt;/li&gt;
&lt;li&gt;Indexima&lt;/li&gt;
&lt;li&gt;Intersystems IRIS&lt;/li&gt;
&lt;li&gt;Jethro ODBC&lt;/li&gt;
&lt;li&gt;Kyligence Enterprise&lt;/li&gt;
&lt;li&gt;MarkLogic ODBC&lt;/li&gt;
&lt;li&gt;Oracle&lt;/li&gt;
&lt;li&gt;PostgreSQL&lt;/li&gt;
&lt;li&gt;Power BI datasets&lt;/li&gt;
&lt;li&gt;QubolePresto&lt;/li&gt;
&lt;li&gt;SAP Business Warehouse Message Server&lt;/li&gt;
&lt;li&gt;SAP Business Warehouse Server&lt;/li&gt;
&lt;li&gt;SAP HANA&lt;/li&gt;
&lt;li&gt;Snowflake&lt;/li&gt;
&lt;li&gt;Spark&lt;/li&gt;
&lt;li&gt;SQL Server&lt;/li&gt;
&lt;li&gt;Teradata Database&lt;/li&gt;
&lt;li&gt;Vertica&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Depending on the data source you choose, there is a chance of slower query performance when using DirectQuery compared to  the default data import option. Keep in mind that when the &lt;strong&gt;Import&lt;/strong&gt; option is selected, it leverages a highly sophisticated in-memory  storage engine. When selecting DirectQuery, performance will depend on  the source type you have chosen from the preceding list.&lt;/p&gt;

&lt;p&gt;Another limitation worth noting is that not all Power BI features are supported when you choose DirectQuery. For example, depending on the selected source, some of the Power Query Editor  features are disabled and could result in the following message: &amp;ldquo;&lt;strong&gt;This step results in a query that is not supported in DirectQuery mode&lt;/strong&gt;.&amp;rdquo; The following screenshot shows this response:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_08.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.8: Certain transforms may force a user out of DirectQuery mode&lt;/p&gt;

&lt;p&gt;The reason for this limitation is because  DirectQuery automatically attempts to convert any Power Query steps into a query in the data source&amp;rsquo;s native language. So, if the source of this solution was SQL Server, then Power BI would attempt to convert this  data transformation into a comparable T-SQL script. Once Power BI  realizes Power Query Editor used a function that is not compatible with  the source, the error is generated.&lt;/p&gt;

&lt;h1 id=&#34;composite-models&#34;&gt;Composite models&lt;/h1&gt;

&lt;p&gt;Occasionally, you may find it helpful for your  data model to take a hybrid approach regarding how it stores data. For  example, you want sales transactions to be displayed in near real time  on your dashboard, so you set your &lt;code&gt;SalesTransaction&lt;/code&gt; table to use DirectQuery. However, your &lt;code&gt;Product&lt;/code&gt; table rarely has values that are added or changed. Having values that  do not change often make it a great candidate for the imported data  storage method to take advantage of the performance benefits.&lt;/p&gt;

&lt;p&gt;This describes a perfect scenario for utilizing a  feature called composite models. Composite models allow a single Power  BI solution to include both DirectQuery and import table connections  within one data model. From the Power BI developer&amp;rsquo;s perspective, you  can take advantage of the best parts of each data storage mode within  your design.&lt;/p&gt;

&lt;p&gt;Another effective use case for  composite models is available due to a feature called aggregations.  Leveraging aggregations is one of the best ways to manage extremely  large datasets in Power BI. You will learn more about designing  aggregations in &lt;em&gt;Chapter 3&lt;/em&gt;, &lt;em&gt;Building the Data Model&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Within Power BI Desktop, it is clear a solution is leveraging composite models if we view the storage mode in the  bottom-right corner of the tool. Clicking this corner, shown in the  following screenshot, will allow you to switch all tables to &lt;strong&gt;Import&lt;/strong&gt; mode instead. Optionally, if you need to change the storage mode of individual tables, this can be accomplished in the &lt;strong&gt;Model&lt;/strong&gt; view by selecting individual tables:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_09.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.9: The bottom-right corner allows you to switch storage modes for the entire model&lt;/p&gt;

&lt;p&gt;While composite models give you the best of DirectQuery and import models, there&amp;rsquo;s a third storage mode that is often used for data sources that are highly groomed by IT.&lt;/p&gt;

&lt;h1 id=&#34;live-connection&#34;&gt;Live connection&lt;/h1&gt;

&lt;p&gt;The basic concept of live connection is very  similar to that of DirectQuery. Just like DirectQuery, when you use a  live connection, no data is actually imported into Power BI. Instead,  your solution points directly to the  underlying data source and leverages Power BI Desktop simply as a data  visualization tool. So, if these two things are so similar, why give  them different names? The answer is because even though the basic  concept is the same, DirectQuery and live connection vary greatly.&lt;/p&gt;

&lt;p&gt;One difference that should quickly be noticeable  is the query performance experience. It was mentioned in a previous  section that DirectQuery can often have poor performance, depending on  the data source type. With live connection, you generally will not have  any performance problem because it is only supported by the following  types of data sources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SQL Server Analysis Services database&lt;/li&gt;
&lt;li&gt;Azure Analysis Services database&lt;/li&gt;
&lt;li&gt;Power BI datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reason performance does not suffer with these  data sources is because they either use the same xVelocity engine that  Power BI does, or another high-performance storage engine. To set up  your own live connection to one of these sources, you can choose the &lt;strong&gt;SQL Server Analysis Services database&lt;/strong&gt; from the list of connectors after selecting &lt;strong&gt;Get data&lt;/strong&gt;. The following screenshot shows that you can specify that the connection should be set to &lt;strong&gt;Connect live&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_10.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.10: SQL Server Analysis Services Data Connectivity mode allows you to switch to Connect live mode&lt;/p&gt;

&lt;p&gt;If a dataset is configured for a Live connection or DirectQuery, then you can expect automatic refreshes to occur approximately each hour by default. You can manually adjust  the refresh frequency in the Scheduled cache refresh option in the Power BI service.&lt;/p&gt;

&lt;p&gt;Of course, these benefits don&amp;rsquo;t come without a cost. Let&amp;rsquo;s discuss some of the limitations of Live connection.&lt;/p&gt;

&lt;h2 id=&#34;limitations-1&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;So far, this sounds great! You have now learned that you can connect directly to your data sources, without importing data into your model, and that you won&amp;rsquo;t have significant  performance consequences. Of course, these benefits don&amp;rsquo;t come without  giving something up, so what are the limitations of a live connection?&lt;/p&gt;

&lt;p&gt;What you will encounter with live connections are  limitations that are generally a result of the fact that Analysis  Services is an Enterprise BI tool. Thus, if you are going to connect to  it, then it has probably already gone through significant data cleansing and modeling by your IT team.&lt;/p&gt;

&lt;p&gt;Modeling capabilities such as defining  relationships are not available because these would be designed in an  Analysis Services Model. Also, Power Query Editor is not available at  all against a Live connection source.  While, at times, this may be frustrating, it does make sense that it  works this way. This is because any of the changes you may desire to  make with relationships or in the Power Query Editor should be done in  Analysis Services, not Power BI.&lt;/p&gt;

&lt;h1 id=&#34;which-should-i-choose&#34;&gt;Which should I choose?&lt;/h1&gt;

&lt;p&gt;Now that you have learned about the three  different ways to connect to your data, you are left wondering which  option is best for you. It&amp;rsquo;s fair to say that the choice you make will  really depend on the requirements of each individual project you have.&lt;/p&gt;

&lt;p&gt;To summarize, some of the considerations that were mentioned in this chapter are listed in the following table:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Consideration&lt;/th&gt;
&lt;th&gt;Import Data&lt;/th&gt;
&lt;th&gt;DirectQuery&lt;/th&gt;
&lt;th&gt;Live connection&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Best performance&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Best design experience&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Best for keeping data up to date&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Data source availability&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Most scalable&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Some of the items you&amp;rsquo;ll consider may be more important than others. So, to make this more personal, try using the &lt;code&gt;Data Connectivity - Decision Matrix&lt;/code&gt; file that is included with this book. In this file, you can rank (from 1 to 10) the importance of each of these considerations to help you  choose which option is best for you.&lt;/p&gt;

&lt;p&gt;Since the &lt;strong&gt;Import Data&lt;/strong&gt; option presents the most available features, going forward, this book primarily uses this option. In &lt;em&gt;Chapter 2&lt;/em&gt;, &lt;em&gt;Data Transformation Strategies&lt;/em&gt;, you will learn how to implement data transformation strategies to  ensure all the necessary business rules are applied to your data.&lt;/p&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Power BI provides users with a variety of methods  for connecting to data sources with natively built-in data connectors.  The connector you choose for your solution will depend on where your  data is located. Once you&amp;rsquo;ve connected to a data source, you can decide  on what type of query mode best suits your needs. Some connectors allow  for little to no latency in your results with options like DirectQuery  or live connection. In this chapter, you learned about the benefits and  disadvantages of each query mode, and you were given a method for  weighting these options using a decision matrix.&lt;/p&gt;

&lt;p&gt;In the next chapter, you will learn more about how data transformations may be applied to your data import process so that incoming data will be properly cleansed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Power BI</title>
      <link>https://www.marcusrb.com/en/en/power-bi/01-intro-power-bi/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/en/power-bi/01-intro-power-bi/</guid>
      <description>

&lt;p&gt;Power BI may very well be one of the most aptly named tools ever developed by Microsoft, giving analysts and developers a powerful business intelligence and analytics playground while still packaging it in a surprisingly lightweight application. Using Microsoft Power BI, the processes of data discovery, data modeling, data visualization, and sharing are made elegantly simple using a single product. These processes are so commonplace when developing Power BI solutions that this book has adopted sections that follow this pattern. However, from your perspective, the really exciting thing may be that development problems that would previously take you weeks to solve in a corporate BI solution can now be accomplished in only hours.&lt;/p&gt;

&lt;p&gt;Power BI is a Software as a Service (SaaS) offering in the Azure cloud and, as such, the Microsoft product team follows a strategy of cloud first as they develop and add new features to the product. However, this does not mean that Power BI is only available in the cloud. Microsoft presents two options for sharing your results with others. The first, most often-utilized, method is the cloud-hosted Power BI service, which is available to users for a low monthly subscription fee. The second option is the on-premises Power BI Report Server, which can be obtained through either your SQL Server Enterprise licensing with Software Assurance or a subscription level known as Power BI Premium. Both solutions require a development tool called Power BI Desktop, which is available for free, and is where you must start to design your solutions.&lt;/p&gt;

&lt;p&gt;Using the Power BI Desktop application enables you to define your data discovery and data preparation steps, organize your data model, and design engaging data visualizations based on your reports. In this first chapter, the development environment will be introduced, and the data discovery process will be explored in depth. The topics detailed in this chapter include the following:&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;[ ] Getting started&lt;/li&gt;
&lt;li&gt;[ ] Importing data&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Direct query&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Live connection&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s first start by learning about what you need on your machine to get started.&lt;/p&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;

&lt;p&gt;Power BI Desktop is available for free and can be found via a direct download link at Power BI (&lt;a href=&#34;https://powerbi.microsoft.com/&#34; target=&#34;_blank&#34;&gt;https://powerbi.microsoft.com/&lt;/a&gt;), or by installing it as an app from Microsoft Store. There are several benefits of using  the Microsoft Store Power BI app, including automatic updates, no  requirement for admin privileges, and making it easier for planned IT  rollout of Power BI.&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;ve downloaded, installed, and launched Power  BI Desktop, you will likely be welcomed by the start up screen, which is designed to help new users find their way. Close this start up screen  so that we can review some of the most commonly used features of the  application:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://learning.oreilly.com/library/view/microsoft-power-bi/9781800561571/Images/B16601_01_01.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.1: First view of Power BI Desktop&lt;/p&gt;

&lt;p&gt;Following the preceding screenshot, let&amp;rsquo;s learn the names and purposes of some of the most important features in Power BI Desktop:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Get data&lt;/strong&gt;: Used for selecting data connectors and configuring data source details.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transform data&lt;/strong&gt;: Launches the Power Query Editor, which is used to apply data transformations to incoming data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Report view&lt;/strong&gt;: Report canvas used for designing data visualizations. This is the default view that&amp;rsquo;s open when Power BI Desktop is launched.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data view&lt;/strong&gt;: Provides a view of the data in your model. This looks similar to a typical Excel spreadsheet, but it is read-only.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model view&lt;/strong&gt;: Primarily used when your data model has multiple tables and relationships that need to be defined between them.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that you have a little familiarity with the  basic controls within Power BI Desktop, let&amp;rsquo;s learn about the options  you have for connecting to your various data sources.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conceptos de Amazon Web Services AWS</title>
      <link>https://www.marcusrb.com/en/courses/cloud-computing/intro-aws/aws101-intro/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/cloud-computing/intro-aws/aws101-intro/</guid>
      <description>

&lt;h2 id=&#34;introducción&#34;&gt;Introducción&lt;/h2&gt;

&lt;p&gt;El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.&lt;/p&gt;

&lt;p&gt;En esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.&lt;/p&gt;

&lt;h3 id=&#34;tus-primeros-comandos-bigquery&#34;&gt;Tus primeros comandos BigQuery&lt;/h3&gt;

&lt;p&gt;Para usar BigQuery, importaremos el paquete de Python a continuación:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.cloud import bigquery
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El primer paso en el flujo de trabajo es crear un objeto &lt;em&gt;Client&lt;/em&gt;. Como pronto verá, este objeto &lt;em&gt;Client&lt;/em&gt; desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a &amp;quot;Client&amp;quot; object
client = bigquery.Client()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trabajaremos con un conjunto de datos de publicaciones en &lt;a href=&#34;https://news.ycombinator.com/&#34; target=&#34;_blank&#34;&gt;Hacker News&lt;/a&gt;, un sitio web que se centra en noticias de informática y seguridad cibernética.&lt;/p&gt;

&lt;p&gt;En BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos &lt;em&gt;hacker_news&lt;/em&gt; está contenido en el proyecto &lt;em&gt;bigquery-public-data&lt;/em&gt;. Para acceder al conjunto de datos,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Comenzamos construyendo una referencia al conjunto de datos con el método &lt;em&gt;dataset()&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;A continuación, utilizamos el método &lt;em&gt;get_dataset()&lt;/em&gt;, junto con la referencia que acabamos de construir, para obtener el conjunto de datos.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Construct a reference to the &amp;quot;hacker_news&amp;quot; dataset
dataset_ref = client.dataset(&amp;quot;hacker_news&amp;quot;, project=&amp;quot;bigquery-public-data&amp;quot;)

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.&lt;/p&gt;

&lt;p&gt;Usamos el método &lt;em&gt;list_tables()&lt;/em&gt; para listar las tablas en el conjunto de datos.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List all the tables in the &amp;quot;hacker_news&amp;quot; dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there are four!)
for table in tables:  
    print(table.table_id)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;OUTPUT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;comments
full
full_201510
stories
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla &lt;em&gt;full&lt;/em&gt; en el conjunto de datos &lt;em&gt;hacker_news&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Construct a reference to the &amp;quot;full&amp;quot; table
table_ref = dataset_ref.table(&amp;quot;full&amp;quot;)

# API request - fetch the table
table = client.get_table(table_ref)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../biYqbUB.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;esquema-de-la-tabla&#34;&gt;Esquema de la tabla&lt;/h2&gt;

&lt;p&gt;La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.&lt;/p&gt;

&lt;p&gt;En este ejemplo, investigaremos la tabla completa &lt;em&gt;full&lt;/em&gt; que obtuvimos anteriormente.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print information on all the columns in the &amp;quot;full&amp;quot; table in the &amp;quot;hacker_news&amp;quot; dataset
table.schema
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;OUTPUT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[SchemaField(&#39;by&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &amp;quot;The username of the item&#39;s author.&amp;quot;, ()),
 SchemaField(&#39;score&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Story score&#39;, ()),
 SchemaField(&#39;time&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Unix time&#39;, ()),
 SchemaField(&#39;timestamp&#39;, &#39;TIMESTAMP&#39;, &#39;NULLABLE&#39;, &#39;Timestamp for the unix time&#39;, ()),
 SchemaField(&#39;title&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story title&#39;, ()),
 SchemaField(&#39;type&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Type of details (comment, comment_ranking, poll, story, job, pollopt)&#39;, ()),
 SchemaField(&#39;url&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story url&#39;, ()),
 SchemaField(&#39;text&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story or comment text&#39;, ()),
 SchemaField(&#39;parent&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Parent comment ID&#39;, ()),
 SchemaField(&#39;deleted&#39;, &#39;BOOLEAN&#39;, &#39;NULLABLE&#39;, &#39;Is deleted?&#39;, ()),
 SchemaField(&#39;dead&#39;, &#39;BOOLEAN&#39;, &#39;NULLABLE&#39;, &#39;Is dead?&#39;, ()),
 SchemaField(&#39;descendants&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Number of story or poll descendants&#39;, ()),
 SchemaField(&#39;id&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &amp;quot;The item&#39;s unique id.&amp;quot;, ()),
 SchemaField(&#39;ranking&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Comment ranking&#39;, ())]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cada &lt;em&gt;SchemaField&lt;/em&gt; nos informa sobre una columna específica (a la que también nos referimos como un campo &lt;strong&gt;field&lt;/strong&gt;). En orden, la información es:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;El nombre de la columna.&lt;/li&gt;
&lt;li&gt;El tipo de campo (o tipo de datos) en la columna&lt;/li&gt;
&lt;li&gt;El modo de la columna (&amp;lsquo;NULLABLE&amp;rsquo; significa que una columna permite valores NULL y es el valor predeterminado)&lt;/li&gt;
&lt;li&gt;Una descripción de los datos en esa columna.&lt;/li&gt;
&lt;li&gt;El primer campo tiene el SchemaField:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;SchemaField (&amp;lsquo;by&amp;rsquo;, &amp;lsquo;string&amp;rsquo;, &amp;lsquo;NULLABLE&amp;rsquo;, &amp;ldquo;El nombre de usuario del autor del elemento&amp;rdquo;, ()&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Esto nos dice:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;el campo (o columna) es llamado por&lt;/li&gt;
&lt;li&gt;los datos en este campo son cadenas,&lt;/li&gt;
&lt;li&gt;Se permiten valores NULL y&lt;/li&gt;
&lt;li&gt;Contiene los nombres de usuario correspondientes al autor de cada elemento.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Podemos usar el método &lt;em&gt;list_rows()&lt;/em&gt; para verificar solo las primeras cinco líneas de la tabla completa &lt;em&gt;full&lt;/em&gt; para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery &lt;em&gt;RowIterator&lt;/em&gt; que se puede convertir rápidamente en un DataFrame de pandas con el método &lt;em&gt;to_dataframe()&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview the first five lines of the &amp;quot;full&amp;quot; table
client.list_rows(table, max_results=5).to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El método &lt;em&gt;list_rows()&lt;/em&gt; también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview the first five entries in the &amp;quot;by&amp;quot; column of the &amp;quot;full&amp;quot; table
client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EXERCISE (Exercise_ Getting Started With SQL and BigQuery)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conceptos de Microsoft Azure</title>
      <link>https://www.marcusrb.com/en/courses/cloud-computing/intro-azure/azure101-intro/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/cloud-computing/intro-azure/azure101-intro/</guid>
      <description>

&lt;h2 id=&#34;introducción&#34;&gt;Introducción&lt;/h2&gt;

&lt;p&gt;El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web&lt;/p&gt;

&lt;p&gt;TBD&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conceptos de Google Cloud Platform GCP</title>
      <link>https://www.marcusrb.com/en/courses/cloud-computing/intro-gcp/gcp101-intro/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/cloud-computing/intro-gcp/gcp101-intro/</guid>
      <description>

&lt;h2 id=&#34;introducción&#34;&gt;Introducción&lt;/h2&gt;

&lt;p&gt;El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web&lt;/p&gt;

&lt;p&gt;TBD&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conceptos de Cloud computing</title>
      <link>https://www.marcusrb.com/en/courses/cloud-computing/intro-cloud/cloud101-intro/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/cloud-computing/intro-cloud/cloud101-intro/</guid>
      <description>

&lt;h2 id=&#34;introducción&#34;&gt;Introducción&lt;/h2&gt;

&lt;p&gt;El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.&lt;/p&gt;

&lt;p&gt;En esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.&lt;/p&gt;

&lt;h3 id=&#34;tus-primeros-comandos-bigquery&#34;&gt;Tus primeros comandos BigQuery&lt;/h3&gt;

&lt;p&gt;Para usar BigQuery, importaremos el paquete de Python a continuación:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.cloud import bigquery
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El primer paso en el flujo de trabajo es crear un objeto &lt;em&gt;Client&lt;/em&gt;. Como pronto verá, este objeto &lt;em&gt;Client&lt;/em&gt; desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a &amp;quot;Client&amp;quot; object
client = bigquery.Client()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trabajaremos con un conjunto de datos de publicaciones en &lt;a href=&#34;https://news.ycombinator.com/&#34; target=&#34;_blank&#34;&gt;Hacker News&lt;/a&gt;, un sitio web que se centra en noticias de informática y seguridad cibernética.&lt;/p&gt;

&lt;p&gt;En BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos &lt;em&gt;hacker_news&lt;/em&gt; está contenido en el proyecto &lt;em&gt;bigquery-public-data&lt;/em&gt;. Para acceder al conjunto de datos,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Comenzamos construyendo una referencia al conjunto de datos con el método &lt;em&gt;dataset()&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;A continuación, utilizamos el método &lt;em&gt;get_dataset()&lt;/em&gt;, junto con la referencia que acabamos de construir, para obtener el conjunto de datos.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Construct a reference to the &amp;quot;hacker_news&amp;quot; dataset
dataset_ref = client.dataset(&amp;quot;hacker_news&amp;quot;, project=&amp;quot;bigquery-public-data&amp;quot;)

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.&lt;/p&gt;

&lt;p&gt;Usamos el método &lt;em&gt;list_tables()&lt;/em&gt; para listar las tablas en el conjunto de datos.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List all the tables in the &amp;quot;hacker_news&amp;quot; dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there are four!)
for table in tables:  
    print(table.table_id)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;OUTPUT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;comments
full
full_201510
stories
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla &lt;em&gt;full&lt;/em&gt; en el conjunto de datos &lt;em&gt;hacker_news&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Construct a reference to the &amp;quot;full&amp;quot; table
table_ref = dataset_ref.table(&amp;quot;full&amp;quot;)

# API request - fetch the table
table = client.get_table(table_ref)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../biYqbUB.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;esquema-de-la-tabla&#34;&gt;Esquema de la tabla&lt;/h2&gt;

&lt;p&gt;La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.&lt;/p&gt;

&lt;p&gt;En este ejemplo, investigaremos la tabla completa &lt;em&gt;full&lt;/em&gt; que obtuvimos anteriormente.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print information on all the columns in the &amp;quot;full&amp;quot; table in the &amp;quot;hacker_news&amp;quot; dataset
table.schema
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;OUTPUT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[SchemaField(&#39;by&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &amp;quot;The username of the item&#39;s author.&amp;quot;, ()),
 SchemaField(&#39;score&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Story score&#39;, ()),
 SchemaField(&#39;time&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Unix time&#39;, ()),
 SchemaField(&#39;timestamp&#39;, &#39;TIMESTAMP&#39;, &#39;NULLABLE&#39;, &#39;Timestamp for the unix time&#39;, ()),
 SchemaField(&#39;title&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story title&#39;, ()),
 SchemaField(&#39;type&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Type of details (comment, comment_ranking, poll, story, job, pollopt)&#39;, ()),
 SchemaField(&#39;url&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story url&#39;, ()),
 SchemaField(&#39;text&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story or comment text&#39;, ()),
 SchemaField(&#39;parent&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Parent comment ID&#39;, ()),
 SchemaField(&#39;deleted&#39;, &#39;BOOLEAN&#39;, &#39;NULLABLE&#39;, &#39;Is deleted?&#39;, ()),
 SchemaField(&#39;dead&#39;, &#39;BOOLEAN&#39;, &#39;NULLABLE&#39;, &#39;Is dead?&#39;, ()),
 SchemaField(&#39;descendants&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Number of story or poll descendants&#39;, ()),
 SchemaField(&#39;id&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &amp;quot;The item&#39;s unique id.&amp;quot;, ()),
 SchemaField(&#39;ranking&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Comment ranking&#39;, ())]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cada &lt;em&gt;SchemaField&lt;/em&gt; nos informa sobre una columna específica (a la que también nos referimos como un campo &lt;strong&gt;field&lt;/strong&gt;). En orden, la información es:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;El nombre de la columna.&lt;/li&gt;
&lt;li&gt;El tipo de campo (o tipo de datos) en la columna&lt;/li&gt;
&lt;li&gt;El modo de la columna (&amp;lsquo;NULLABLE&amp;rsquo; significa que una columna permite valores NULL y es el valor predeterminado)&lt;/li&gt;
&lt;li&gt;Una descripción de los datos en esa columna.&lt;/li&gt;
&lt;li&gt;El primer campo tiene el SchemaField:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;SchemaField (&amp;lsquo;by&amp;rsquo;, &amp;lsquo;string&amp;rsquo;, &amp;lsquo;NULLABLE&amp;rsquo;, &amp;ldquo;El nombre de usuario del autor del elemento&amp;rdquo;, ()&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Esto nos dice:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;el campo (o columna) es llamado por&lt;/li&gt;
&lt;li&gt;los datos en este campo son cadenas,&lt;/li&gt;
&lt;li&gt;Se permiten valores NULL y&lt;/li&gt;
&lt;li&gt;Contiene los nombres de usuario correspondientes al autor de cada elemento.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Podemos usar el método &lt;em&gt;list_rows()&lt;/em&gt; para verificar solo las primeras cinco líneas de la tabla completa &lt;em&gt;full&lt;/em&gt; para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery &lt;em&gt;RowIterator&lt;/em&gt; que se puede convertir rápidamente en un DataFrame de pandas con el método &lt;em&gt;to_dataframe()&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview the first five lines of the &amp;quot;full&amp;quot; table
client.list_rows(table, max_results=5).to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El método &lt;em&gt;list_rows()&lt;/em&gt; también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview the first five entries in the &amp;quot;by&amp;quot; column of the &amp;quot;full&amp;quot; table
client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EXERCISE (Exercise_ Getting Started With SQL and BigQuery)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conceptos de Amazon Web Services AWS</title>
      <link>https://www.marcusrb.com/en/courses/cloud-computing/intro-aws/aws101-servicios/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/cloud-computing/intro-aws/aws101-servicios/</guid>
      <description>

&lt;h2 id=&#34;introducción&#34;&gt;Introducción&lt;/h2&gt;

&lt;p&gt;El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.&lt;/p&gt;

&lt;p&gt;En esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.&lt;/p&gt;

&lt;h3 id=&#34;tus-primeros-comandos-bigquery&#34;&gt;Tus primeros comandos BigQuery&lt;/h3&gt;

&lt;p&gt;Para usar BigQuery, importaremos el paquete de Python a continuación:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.cloud import bigquery
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El primer paso en el flujo de trabajo es crear un objeto &lt;em&gt;Client&lt;/em&gt;. Como pronto verá, este objeto &lt;em&gt;Client&lt;/em&gt; desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a &amp;quot;Client&amp;quot; object
client = bigquery.Client()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trabajaremos con un conjunto de datos de publicaciones en &lt;a href=&#34;https://news.ycombinator.com/&#34; target=&#34;_blank&#34;&gt;Hacker News&lt;/a&gt;, un sitio web que se centra en noticias de informática y seguridad cibernética.&lt;/p&gt;

&lt;p&gt;En BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos &lt;em&gt;hacker_news&lt;/em&gt; está contenido en el proyecto &lt;em&gt;bigquery-public-data&lt;/em&gt;. Para acceder al conjunto de datos,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Comenzamos construyendo una referencia al conjunto de datos con el método &lt;em&gt;dataset()&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;A continuación, utilizamos el método &lt;em&gt;get_dataset()&lt;/em&gt;, junto con la referencia que acabamos de construir, para obtener el conjunto de datos.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Construct a reference to the &amp;quot;hacker_news&amp;quot; dataset
dataset_ref = client.dataset(&amp;quot;hacker_news&amp;quot;, project=&amp;quot;bigquery-public-data&amp;quot;)

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.&lt;/p&gt;

&lt;p&gt;Usamos el método &lt;em&gt;list_tables()&lt;/em&gt; para listar las tablas en el conjunto de datos.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List all the tables in the &amp;quot;hacker_news&amp;quot; dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there are four!)
for table in tables:  
    print(table.table_id)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;OUTPUT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;comments
full
full_201510
stories
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla &lt;em&gt;full&lt;/em&gt; en el conjunto de datos &lt;em&gt;hacker_news&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Construct a reference to the &amp;quot;full&amp;quot; table
table_ref = dataset_ref.table(&amp;quot;full&amp;quot;)

# API request - fetch the table
table = client.get_table(table_ref)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../biYqbUB.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;esquema-de-la-tabla&#34;&gt;Esquema de la tabla&lt;/h2&gt;

&lt;p&gt;La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.&lt;/p&gt;

&lt;p&gt;En este ejemplo, investigaremos la tabla completa &lt;em&gt;full&lt;/em&gt; que obtuvimos anteriormente.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print information on all the columns in the &amp;quot;full&amp;quot; table in the &amp;quot;hacker_news&amp;quot; dataset
table.schema
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;OUTPUT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[SchemaField(&#39;by&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &amp;quot;The username of the item&#39;s author.&amp;quot;, ()),
 SchemaField(&#39;score&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Story score&#39;, ()),
 SchemaField(&#39;time&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Unix time&#39;, ()),
 SchemaField(&#39;timestamp&#39;, &#39;TIMESTAMP&#39;, &#39;NULLABLE&#39;, &#39;Timestamp for the unix time&#39;, ()),
 SchemaField(&#39;title&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story title&#39;, ()),
 SchemaField(&#39;type&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Type of details (comment, comment_ranking, poll, story, job, pollopt)&#39;, ()),
 SchemaField(&#39;url&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story url&#39;, ()),
 SchemaField(&#39;text&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story or comment text&#39;, ()),
 SchemaField(&#39;parent&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Parent comment ID&#39;, ()),
 SchemaField(&#39;deleted&#39;, &#39;BOOLEAN&#39;, &#39;NULLABLE&#39;, &#39;Is deleted?&#39;, ()),
 SchemaField(&#39;dead&#39;, &#39;BOOLEAN&#39;, &#39;NULLABLE&#39;, &#39;Is dead?&#39;, ()),
 SchemaField(&#39;descendants&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Number of story or poll descendants&#39;, ()),
 SchemaField(&#39;id&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &amp;quot;The item&#39;s unique id.&amp;quot;, ()),
 SchemaField(&#39;ranking&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Comment ranking&#39;, ())]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cada &lt;em&gt;SchemaField&lt;/em&gt; nos informa sobre una columna específica (a la que también nos referimos como un campo &lt;strong&gt;field&lt;/strong&gt;). En orden, la información es:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;El nombre de la columna.&lt;/li&gt;
&lt;li&gt;El tipo de campo (o tipo de datos) en la columna&lt;/li&gt;
&lt;li&gt;El modo de la columna (&amp;lsquo;NULLABLE&amp;rsquo; significa que una columna permite valores NULL y es el valor predeterminado)&lt;/li&gt;
&lt;li&gt;Una descripción de los datos en esa columna.&lt;/li&gt;
&lt;li&gt;El primer campo tiene el SchemaField:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;SchemaField (&amp;lsquo;by&amp;rsquo;, &amp;lsquo;string&amp;rsquo;, &amp;lsquo;NULLABLE&amp;rsquo;, &amp;ldquo;El nombre de usuario del autor del elemento&amp;rdquo;, ()&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Esto nos dice:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;el campo (o columna) es llamado por&lt;/li&gt;
&lt;li&gt;los datos en este campo son cadenas,&lt;/li&gt;
&lt;li&gt;Se permiten valores NULL y&lt;/li&gt;
&lt;li&gt;Contiene los nombres de usuario correspondientes al autor de cada elemento.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Podemos usar el método &lt;em&gt;list_rows()&lt;/em&gt; para verificar solo las primeras cinco líneas de la tabla completa &lt;em&gt;full&lt;/em&gt; para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery &lt;em&gt;RowIterator&lt;/em&gt; que se puede convertir rápidamente en un DataFrame de pandas con el método &lt;em&gt;to_dataframe()&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview the first five lines of the &amp;quot;full&amp;quot; table
client.list_rows(table, max_results=5).to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El método &lt;em&gt;list_rows()&lt;/em&gt; también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview the first five entries in the &amp;quot;by&amp;quot; column of the &amp;quot;full&amp;quot; table
client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EXERCISE (Exercise_ Getting Started With SQL and BigQuery)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNIX from Scratch</title>
      <link>https://www.marcusrb.com/en/unix/01-unix-intro/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/unix/01-unix-intro/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Unix&#34; target=&#34;_blank&#34;&gt;Unix&lt;/a&gt; es una familia de sistemas operativos. La primera versión de Linux fue desarrollada a partir de 1969. Unix se caracteriza por ser portable y multitarea.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Unix#/media/File:Unix_history-simple.svg&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://bioinf.comav.upv.es/courses/unix/static/unix_history.png&#34; alt=&#34;Unix history&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hoy en día los sistemas operativos Unix son ampliamente utilizados en multitud de dispositivos que abarcan desde los supercomputadores más capaces hasta los teléfonos móviles más populares, pasando por los ordenadores que utilizamos diariamente en nuestros escritorios. La filosofía de los sistemas Unix se caracteriza por:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;un sistema de ficheros jerárquico,&lt;/li&gt;
&lt;li&gt;una gran colección de pequeños programas que pueden trabajar en serie,&lt;/li&gt;
&lt;li&gt;el uso de ficheros de texto para almacenar los datos,&lt;/li&gt;
&lt;li&gt;tratar los dispositivos como ficheros.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Linux&#34; target=&#34;_blank&#34;&gt;Linux&lt;/a&gt; y MacOS X son ejemplos de sistemas Unix.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Unix&#34; target=&#34;_blank&#34;&gt;Unix&lt;/a&gt; es un sistema operativo portable, multitarea y multiusuario desarrollado a partir de 1969.&lt;/p&gt;

&lt;h2 id=&#34;linux&#34;&gt;Linux&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Linux&#34; target=&#34;_blank&#34;&gt;Linux&lt;/a&gt; es una familia de sistemas operativos de tipo Unix que utilizan el kernel Linux. Linux puede instalarse en prácticamente cualquier ordenador personal además en en teléfonos móviles y supercomputadores.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bioinf.comav.upv.es/courses/unix/static/tux.png&#34; alt=&#34;Tux&#34; /&gt;&lt;/p&gt;

&lt;p&gt;El nombre proviene del programador original, un estudiante llamado Linus Torvals, que en 1991 completando las herramientas GNU desarrolladas por el proyecto GNU de la &lt;a href=&#34;http://www.fsf.org/&#34; target=&#34;_blank&#34;&gt;Fundación del Software Libre&lt;/a&gt;, creó la primera versión de este sistema operativo. El papel fundamental jugado por estas herramientas libres del proyecto GNU hace que este sistema operativo sea denominado también como GNU/Linux, pero en este texto utilizaremos la denominación más sencilla y corta.&lt;/p&gt;

&lt;p&gt;El desarrollo de Linux es uno de los ejemplos más claros de desarrollo de software libre por una comunidad dispersa de programadores. Cualquiera puede usar el sistema operativo, estudiarlo y modificarlo. Estos derechos están protegidos por la licencia &lt;a href=&#34;http://es.wikipedia.org/wiki/GPL&#34; target=&#34;_blank&#34;&gt;GPL&lt;/a&gt; (GNU General Public License).&lt;/p&gt;

&lt;h2 id=&#34;distribuciones&#34;&gt;Distribuciones&lt;/h2&gt;

&lt;p&gt;Linux, como cualquier otro sistema operativo, se compone de un gran número de piezas, que, en este caso, son desarrolladas de forma independiente por miles de programadores y proyectos. Normalmente estas piezas son integradas por un distribuidor y Linux es suministrado como una &lt;a href=&#34;http://es.wikipedia.org/wiki/Distribución_linux&#34; target=&#34;_blank&#34;&gt;distribución Linux&lt;/a&gt;. Las distribuciones Linux incluyen todo el software necesario para instalar un servidor o un escritorio. Algunas de las aplicaciones comúnmente incluidas incluyen: el navegador web Firefox y las aplicaciones de oficina LibreOffice.&lt;/p&gt;

&lt;p&gt;Existen cientos de distribuciones Linux. Estas distribuciones están adaptadas para usuarios o tareas específicas. Algunas de estas distribuciones están desarrolladas o apoyadas por empresas como Fedora (Red Hat) y &lt;a href=&#34;http://www.ubuntu.com/&#34; target=&#34;_blank&#34;&gt;Ubuntu&lt;/a&gt; (Canonical) mientras que otras son mantenidas por la propia comunidad de usuarios como &lt;a href=&#34;http://www.debian.org/&#34; target=&#34;_blank&#34;&gt;Debian&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;software-libre&#34;&gt;Software libre&lt;/h2&gt;

&lt;p&gt;El &lt;a href=&#34;http://es.wikipedia.org/wiki/Software_libre&#34; target=&#34;_blank&#34;&gt;software libre&lt;/a&gt; es software que puede ser utilizado, estudiado, modificado, copiado y redistribuido sin restricciones. Habitualmente el software libre suele ser además gratuito, pero ese no tiene por que ser necesariamente el caso.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bioinf.comav.upv.es/courses/unix/static/gpl_logo.png&#34; alt=&#34;GPL Logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;En la práctica el software libre se distribuye junto al código fuente que lo hace posible y junto a una nota en la que se explican cuales son los derechos y las obligaciones del usuario final. Esta nota se denomina licencia. El movimiento del software libre fue iniciado por Richard Stallman en 1983. Stallman decidió crear un sistema compatible con Unix completamente libre al que llamó GNU (GNU is Not Unix). Con el tiempo este sistema acabaría uniéndose al kernel de Linus para formar un sistema operativo completo.&lt;/p&gt;

&lt;p&gt;Dado que las aplicaciones del software libre suelen ser gratuitas, su modelo de negocio suele basarse en el cobro de los servicios de soporte al usuario y de adaptación del software.&lt;/p&gt;

&lt;h1 id=&#34;introducción-a-ubuntu&#34;&gt;Introducción a Ubuntu&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Ubuntu&#34; target=&#34;_blank&#34;&gt;Ubuntu&lt;/a&gt; es una distribución Linux mantenida por la empresa Canonical. Está orientada a usuarios de escritorio y sus puntos fuertes son su facilidad de uso y de instalación. Aunque el escritorio es algo distinto al de Windows o Mac OS X familiarizarse con él para un usuario acostumbrado a cualquiera de los otros sistemas operativos no debería presentar muchos problemas.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bioinf.comav.upv.es/courses/unix/static/ubuntu_logo.png&#34; alt=&#34;Ubuntu logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Su instalación resulta muy sencilla. Al instalarla, una gran cantidad de software se instala de forma automática para facilitar su uso como escritorio. Ejemplos de estos programas son LibreOffice o Firefox. Además de estos programas instalados por defecto una enorme cantidad de programas se encuentra disponible para ser instalados con unos pocos clicks de ratón.&lt;/p&gt;

&lt;p&gt;Ubuntu está basada en una distribución mantenida por la comunidad de usuarios llamada &lt;a href=&#34;http://www.debian.org/&#34; target=&#34;_blank&#34;&gt;Debian&lt;/a&gt;. El principal objetivo de Debian es crear un sistema operativo robusto que incluya la mayor proporción posible de programas libres.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bioinf.comav.upv.es/courses/unix/static/debian_logo.png&#34; alt=&#34;Debian&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Existen numerosos manuales de utilización de Ubuntu, pero algunas de las guías más completas son la &lt;a href=&#34;https://help.ubuntu.com/lts/ubuntu-help/index.html&#34; target=&#34;_blank&#34;&gt;Ubuntu Desktop Guide&lt;/a&gt; y el manual &lt;a href=&#34;http://ubuntu-manual.org/&#34; target=&#34;_blank&#34;&gt;Getting Started with Ubuntu&lt;/a&gt;. Vamos a recorrer lo principales conceptos de este sistema operativo basandonos en este &lt;a href=&#34;https://bioinf.comav.upv.es/courses/unix/documentacion/Getting Started with Ubuntu 16.04.pdf&#34; target=&#34;_blank&#34;&gt;manual&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;los-usuarios&#34;&gt;Los usuarios&lt;/h2&gt;

&lt;p&gt;Los sistemas Unix son multiusuario, es decir soportan que varios usuarios los utilicen simultáneamente. Todos los usuarios, excepto uno, tienen unos privilegios bastante restringidos y no pueden modificar el sistema. De este modo unos usuarios se ven protegidos de las acciones de los otros.&lt;/p&gt;

&lt;p&gt;Existe un usuario especial llamado &lt;em&gt;root&lt;/em&gt; con privilegios de administración absolutos sobre el sistema. Para realizar las tareas cotidianas nunca hay que acceder al sistema como &lt;em&gt;root&lt;/em&gt;. En Ubuntu este usuario está deshabilitado por defecto y sólo se pueden adquirir los privilegios de administrador temporalmente.&lt;/p&gt;

&lt;h2 id=&#34;el-escritorio&#34;&gt;El escritorio&lt;/h2&gt;

&lt;p&gt;Todos las distribciones basadas en entronos graficos (GUI) suelen tener varios entornos de escritorio para eleguir. Los entornos de escritorio suelen diferir por:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;El estilo y apariencia del entorno&lt;/li&gt;
&lt;li&gt;La forma en la que los diferentes elementos se disponen en la pantalla&lt;/li&gt;
&lt;li&gt;La forma en la que el ususario navega por el escritorio&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;En el caso de Ubuntu el entorno de escritorio por defecto se denomina Unity. Se caracteriza por tener dos barras, la denominada Menu Bar y el Launcher. El Menu Bar incorpora por una lado los menus de las aplicaciones que estań activas y, por otro, un area de indicadores que nos ofrecen informacion actualizada del sistema en todo momento. El Launcher es la barra vertical que facilita el acceso a las aplicaciones mas usadas y a su estado, además de a los discos montados y a la papelera. Además tenemos el selector de escritorios virtuales. En el launcher encontramos varias aplicaciones especiales:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;El menu&lt;/li&gt;
&lt;li&gt;El selector de escritorios virtuales&lt;/li&gt;
&lt;li&gt;La papelera&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Los escritorios virtuales sirven para ampliar la zona de trabajo. Por defecto hay 4 escritorios virtuales que amplían nuestro monitor por cuatro.&lt;/p&gt;

&lt;h2 id=&#34;navegando-por-el-sistema-de-ficheros&#34;&gt;Navegando por el sistema de ficheros&lt;/h2&gt;

&lt;p&gt;Al sistema de ficheros se accede a traves del menu. Tenemos la posibilidad de buscar en la barra de busqueda o navegar directamente por el menu. Una vez que seleccionemos la carpeta se abrirá una ventana del navegador de ficheros con la carpeta seleccionada. Por cierto, los términos carpeta y directorio son sinónimos, al igual que fichero y archivo.&lt;/p&gt;

&lt;p&gt;Desde el launcher también podemos acceder a nuestra carpeta personal. Este directorio personal y sus subdirectorios son los únicos lugares en los que podremos almacenar nuestros archivos personales. El resto del sistema de archivos estará restringido para funciones de administración del sistema.&lt;/p&gt;

&lt;h2 id=&#34;ejercicios&#34;&gt;Ejercicios&lt;/h2&gt;

&lt;p&gt;Para comprobar que no tenemos problemas de manejo del sistema vamos a realizar una serie de tareas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Explorar el escritorio abriendo programas, moviéndote entre escritorios virtuales, minimizando y maximizando las aplicaciones, etc.&lt;/li&gt;
&lt;li&gt;Navegar al directorio “Documentos” y comprobar si tenemos algún archivo guardado.&lt;/li&gt;
&lt;li&gt;Crear un subdirectorio llamado “curso” dentro del directorio “Documentos”.&lt;/li&gt;
&lt;li&gt;Crear un fichero de texto mediante el editor de textos gedit y guardarlo en el directorio que acabamos de crear.&lt;/li&gt;
&lt;li&gt;Copiar el fichero anterior al directorio personal.&lt;/li&gt;
&lt;li&gt;Eliminar el fichero original.&lt;/li&gt;
&lt;li&gt;Reinicia el sistema operativo.&lt;/li&gt;
&lt;li&gt;Bloquea la sesión de tu usuario y vuelve a entrar en ella&lt;/li&gt;
&lt;li&gt;Añade un nuevo usuario al sistema&lt;/li&gt;
&lt;li&gt;Sal de tu usuario actual y entra como el nuevo usuario&lt;/li&gt;
&lt;li&gt;Cambiar el password del nuevo usuario&lt;/li&gt;
&lt;li&gt;Configura el protector de pantalla para que se inicie a los 10 minutos de inactividad&lt;/li&gt;
&lt;li&gt;Modifica los ajustes de la aplicación terminal para que el tipo de letra tenga un tamaño de 11 puntos&lt;/li&gt;
&lt;li&gt;Ancla la aplicación terminal a la barra de aplicaciones y desancla el editor de hojas de cálculo LibreOffice&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;la-terminal-de-unix&#34;&gt;La terminal de UNIX&lt;/h1&gt;

&lt;p&gt;La &lt;a href=&#34;https://en.wikipedia.org/wiki/Command-line_interface&#34; target=&#34;_blank&#34;&gt;Shell&lt;/a&gt; (o terminal) es un interprete de comandos. Es simplemente un modo alternativo de controlar un ordenador basado en una interfaz de texto. La terminal nos permite ejecutar software escribiendo el nombre del programa que queremos ejecutar en la terminal. Podemos pedirle al ordenador que ejecute un programa mediante el ratón ciclando en distintos lugares del escritorio o podemos escribir una orden para conseguir el mismo objetivo. Por ejemplo, para pedirle al ordenador que nos de una lista de los archivos presentes en un directorio podemos abrir un navegador de archivos o podemos escribir en la terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls folder_name
file_1.txt
file_2.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ninguna de las dos formas de comunicarse con el ordenador es mejor que la otra aunque en ciertas ocasiones puede resultar más conveniente utilizar una u otra Las ventajas de la línea de comandos son:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Necesidad. Existe mucho software que está sólo disponible en la terminal. Esto es especialmente cierto en el área de la bioinformática.&lt;/li&gt;
&lt;li&gt;Flexibilidad. Los programas gráficos suelen ser muy adecuados para realizar la tarea para la que han sido creados, pero son difíciles de adaptar para otras tareas. Los programas diseñados para ser usados en la línea de comandos suelen ser muy versátiles.&lt;/li&gt;
&lt;li&gt;Reproducibilidad. Documentar y repetir el proceso seguido para realizar un análisis con un programa gráfico es muy costoso puesto que es difícil describir la secuencia de clicks y doble clicks que hemos realizado. Por el contrario, los procesos realizados mediante la línea de comandos son muy fáciles de documentar puesto que tan sólo debemos guardar el texto que hemos introducido en la pantalla.&lt;/li&gt;
&lt;li&gt;Fiabilidad. Los programas básicos de Unix fueron creados en los años 70 y han sido probados por innumerables usuarios por lo que se han convertido en piezas de código extraordinariamente confiables.&lt;/li&gt;
&lt;li&gt;Recursos. Las interfaces gráficas suelen consumir muchos recursos mientras que los programas que funcionan en línea de comandos suelen ser extraordinariamente livianos y rápidos. Este poco uso de recursos facilita, por ejemplo, que se utilice a través de la red.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;El problema de la terminal es que para poder utilizarla debemos saber previamente qué queremos hacer y cómo. Es habitual descubrir como funciona un programa con una interfaz gráfica sin tener que leer un manual, esto no sucede en la terminal.&lt;/p&gt;

&lt;p&gt;Para usar la línea de comandos hay que abrir una terminal. Se abrirá una terminal con un mensaje similar a:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;usuario $
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Este pequeño mensaje se denomina &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Command-line_interface#Command_prompt&#34; target=&#34;_blank&#34;&gt;prompt&lt;/a&gt;&lt;/em&gt; y el cursor parpadeante que aparece junto al él indica que el ordenador está esperando una orden. El mensaje exacto que aparece en el &lt;em&gt;prompt&lt;/em&gt; puede variar ligeramente, pero en Ubuntu suele ser similar a:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;usuario@ordenador:~/documentos$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En el &lt;em&gt;prompt&lt;/em&gt; de Ubuntu se nos muestra el nombre del usuario, el nombre del ordenador y el directorio en el que nos encontramos actualmente, es decir, el directorio de trabajo actual.&lt;/p&gt;

&lt;p&gt;Cuando el &lt;em&gt;prompt&lt;/em&gt; se muestra podemos ejecutar cualquier cosa, por ejemplo le podemos pedir que liste los ficheros mediante el comando &lt;em&gt;ls&lt;/em&gt; (LiSt)::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;usuario $ ls
lista_libros.txt
rectas_cocina/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;ls&lt;/em&gt;, como cualquier otro comando, es en realidad un programa que el ordenador ejecuta. Cuando escribimos la orden (y pulsamos enter) el programa se ejecuta. Mientras el programa está ejecutándose el &lt;em&gt;prompt&lt;/em&gt; desaparece y no podemos ejecutar ningún otro comando. Pasado el tiempo el programa termina su ejecución y el &lt;em&gt;prompt&lt;/em&gt; vuelve a aparecer. En el caso del comando &lt;em&gt;ls&lt;/em&gt; el tiempo de ejecución es tan pequeño que suele ser imperceptible.&lt;/p&gt;

&lt;p&gt;Los programas suelen tener unas entradas y unas salidas. Dependiendo del caso estas pueden ser ficheros o caracteres introducidos o impresos en la pantalla. Por ejemplo, el resultado de &lt;em&gt;ls&lt;/em&gt; es simplemente una lista impresa de ficheros y directorios en la interfaz de comandos.&lt;/p&gt;

&lt;p&gt;Normalmente el comportamiento de los programas puede ser modificado pasándoles parámetros. Por ejemplo, podríamos pedirle al programa &lt;em&gt;ls&lt;/em&gt; que nos imprima una lista de ficheros más detallada escribiendo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -l
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ayuda&#34;&gt;Ayuda&lt;/h2&gt;

&lt;p&gt;Cada comando tiene unos parámetros y opciones distintos. La forma estándar de pedirles que nos enseñen cuales son estos parámetros suele ser utilizar las opciones ‘–help’, ‘-h’ o ‘-help’, aunque esto puede variar en comandos no estándar.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls --help
Modo de empleo: ls [OPCIÓN]... [FICHERO]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Otro modo de acceder a una documentación más detallada es acceder al manual del programa utilizando el comando &lt;em&gt;man&lt;/em&gt; (MANual):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ man ls
(para terminar pulsar &amp;quot;q&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;man&lt;/em&gt; es un programa interactivo, cuando ejecutamos el comando el programa se abre y el &lt;em&gt;prompt&lt;/em&gt; desaparece. &lt;em&gt;man&lt;/em&gt; es en realidad un visor de ficheros de texto por lo que cuando lo ejecutamos la pantalla se rellena con la ayuda del programa que hemos solicitado. Podemos ir hacia abajo o hacia arriba y podemos buscar en el contenido de la ayuda. El &lt;em&gt;prompt&lt;/em&gt; y la posibilidad de ejecutar otro programa no volverán a aparecer hasta que no cerremos el programa interactivo. En el caso de man para cerrar el programa hay que pulsar la tecla “q”.&lt;/p&gt;

&lt;h2 id=&#34;completado-automático-e-historia&#34;&gt;Completado automático e historia&lt;/h2&gt;

&lt;p&gt;El intérprete de comandos dispone de algunas utilidades para facilitarnos su uso. Una de las más utilizadas es el completado automático. Podemos evitarnos escribir una gran parte de los comandos haciendo uso de la tecla tabulador. Si empezamos a escribir un comando y pulsamos la tecla tabulador el sistema completará el comando por nosotros. Para probarlo creemos los ficheros datos_1.txt, datos_2.txt y tesis.txt::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ touch datos_1.txt
~$ touch datos_2.txt
~$ touch experimento.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si ahora empezamos a escribir &lt;em&gt;cp e&lt;/em&gt; y pulsamos el tabulador dos veces, el intérprete de comandos completará el comando automáticamente::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ cp e
~$ cp experimento.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si el intérprete encuentra varias alternativas completará el comando hasta el punto en el que no haya ambigüedad. Si deseamos que imprima una lista de todas las alternativas disponibles para continuar con el comando deberemos pulsar el tabulador dos veces.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ cp d
$ cp datos_
datos_1.txt  datos_2.txt
~$ cp datos_
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Otra de las funcionalidades que más nos pueden ayudar es la historia. El intérprete recuerda todos los comandos que hemos introducido anteriormente. Si queremos podemos obtener una lista de todo lo que hemos ejecutado utilizando el comando history. Pero lo más socorrido es simplemente utilizar los cursores arriba y abajo para revisar los comandos anteriores. Otra forma de acceder a la historia es utilizar la combinación de teclas control y r. De este modo podemos buscar comandos antiguos sencillamente.&lt;/p&gt;

&lt;h2 id=&#34;ejercicio&#34;&gt;Ejercicio&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Lista todos los comandos que empiezan por apt&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;bibliografía&#34;&gt;Bibliografía&lt;/h1&gt;

&lt;p&gt;Existen numerosas fuentes sobre la historia y la filosofía de Unix, de Linux y del software libre. Entre ellas se encuentran:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Las páginas de la wikipedia sobre: &lt;a href=&#34;http://es.wikipedia.org/wiki/Unix&#34; target=&#34;_blank&#34;&gt;Unix&lt;/a&gt;, &lt;a href=&#34;http://es.wikipedia.org/wiki/Linux&#34; target=&#34;_blank&#34;&gt;Linux&lt;/a&gt;, &lt;a href=&#34;http://es.wikipedia.org/wiki/Ubuntu&#34; target=&#34;_blank&#34;&gt;Ubuntu&lt;/a&gt; y [software libre](&lt;a href=&#34;https://bioinf.comav.upv.es/courses/unix/&#34; target=&#34;_blank&#34;&gt;https://bioinf.comav.upv.es/courses/unix/&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Rebel-Code-Linux-Source-Revolution/dp/0738203335&#34; target=&#34;_blank&#34;&gt;Rebel Code&lt;/a&gt;, un libro de Glyn Moody dedicado a la historia del movimiento del software libre.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://oreilly.com/catalog/9780596001087&#34; target=&#34;_blank&#34;&gt;La catedral y el bazar&lt;/a&gt; de Eric S. Raymond. Un ensayo sobre los beneficios del modelo de desarrollo asociados al software libre.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://catb.org/~esr/writings/taoup/&#34; target=&#34;_blank&#34;&gt;The Art of Unix Programming&lt;/a&gt; (&lt;a href=&#34;https://bioinf.comav.upv.es/documentacion/bibliography/the_art_of_unix_programming.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt; de Eric S. Raymond. Dedicado a la filosofía de los sistemas Unix.&lt;/li&gt;
&lt;li&gt;El excelente [Ubuntu manual](&lt;a href=&#34;https://bioinf.comav.upv.es/courses/unix/&#34; target=&#34;_blank&#34;&gt;https://bioinf.comav.upv.es/courses/unix/&lt;/a&gt; [pdf](&lt;a href=&#34;https://bioinf.comav.upv.es/courses/unix/&#34; target=&#34;_blank&#34;&gt;https://bioinf.comav.upv.es/courses/unix/&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;La documentación &lt;a href=&#34;https://help.ubuntu.com/&#34; target=&#34;_blank&#34;&gt;oficial&lt;/a&gt; y de la &lt;a href=&#34;https://help.ubuntu.com/community&#34; target=&#34;_blank&#34;&gt;comunidad&lt;/a&gt; de Ubuntu.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hay varios cursos para iniciarse en el uso de la línea de comandos de Unix, como:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://en.flossmanuals.net/gnulinux&#34; target=&#34;_blank&#34;&gt;Put Yourself in Command&lt;/a&gt; de la Free Software Fundation, copia en &lt;a href=&#34;https://bioinf.comav.upv.es/courses/unix/documentacion/fsf_cmd_manual.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://linuxcommand.org/learning_the_shell.php&#34; target=&#34;_blank&#34;&gt;Learning the shell&lt;/a&gt; de &lt;a href=&#34;http://linuxcommand.org/lc3_learning_the_shell.php&#34; target=&#34;_blank&#34;&gt;Linuxcommand.org&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rlworkman.net/howtos/rute/&#34; target=&#34;_blank&#34;&gt;Rute User’s Tutorial and Exposition&lt;/a&gt; de Paul Sheer, copia en &lt;a href=&#34;https://bioinf.comav.upv.es/courses/unix/documentacion/rute.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://oreilly.com/catalog/9780596002619&#34; target=&#34;_blank&#34;&gt;Learning the Unix Operating System&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sistema de ficheros</title>
      <link>https://www.marcusrb.com/en/unix/02-sistema-ficheros/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/unix/02-sistema-ficheros/</guid>
      <description>

&lt;p&gt;El &lt;a href=&#34;https://en.wikipedia.org/wiki/File_system&#34; target=&#34;_blank&#34;&gt;sistema de archivos&lt;/a&gt; controla como se almacenan los archivos en el ordenador. Sus dos tareas principales son guardar y leer archivos previamente guardados.&lt;/p&gt;

&lt;h2 id=&#34;sistemas-jerárquicos&#34;&gt;Sistemas jerárquicos&lt;/h2&gt;

&lt;p&gt;Los sistemas de archivos suelen tener &lt;a href=&#34;https://en.wikipedia.org/wiki/Directory_(computing)&#34; target=&#34;_blank&#34;&gt;directorios&lt;/a&gt; en los que organizar los archivos y estos directorios suelen estar organizados jerárquicamente. La jerarquía implica que un directorio puede contener subdirectorios. El directorio más alto en la jerarquía del que cuelgan todos los demás se denomina &lt;em&gt;raíz&lt;/em&gt; (root). En los sistemas Unix el directorio raíz se representa con una barra “*/*” y sólo existe una jerarquía, es decir, sólo existe un directorio raíz, incluso aunque haya distintos discos duros en el ordenador.&lt;/p&gt;

&lt;p&gt;Dentro del directorio raíz podemos encontrar diversos subdirectorios, por ejemplo en Linux existe el directorio &lt;em&gt;home&lt;/em&gt;. &lt;em&gt;home&lt;/em&gt; es por tanto un subdictorio del directorio raíz. Esta relación se representa como:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/home
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;home&lt;/em&gt; es el directorio dónde se encuentran los directorios de los usuarios en un sistema Linux. Imaginemos que tiene los subdirectorios &lt;em&gt;alicia&lt;/em&gt; y &lt;em&gt;juan&lt;/em&gt;. Se representaría como:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/home/alicia
/home/juan
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Existe un estándar, denominado &lt;a href=&#34;https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard&#34; target=&#34;_blank&#34;&gt;Filesystem Hierarchy Standard&lt;/a&gt; que define la estructura de directorios de los sistemas Unix. Los sistemas Unix suelen seguir este estándar, aunque a veces lo violan en algunos aspectos. Por ejemplo en MacOS X el directorio donde se encuentran los direcotorios de los usuarios se denomina &lt;em&gt;Users&lt;/em&gt; y no &lt;em&gt;home&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;En algunos sistemas operativos no UNIX la barra se escribe al revés “&amp;rdquo;, a pesar de que la convención siempre fue la contraria.&lt;/p&gt;

&lt;p&gt;En el directorio raíz hay diversos directorios que, en la mayoría de los casos, sólo deberían interesarnos si estamos administrando el ordenador. Los usuarios normalmente sólo escriben dentro de un directorio de su propiedad localizado dentro de /home y denominado como su nombre de usuario.&lt;/p&gt;

&lt;p&gt;Los usuarios también pueden escribir en /tmp aunque normalmente son los procesos lanzados por estos lo que hacen esta escritura. Es importante revisar el espacio libre en la partición en la que se encuentra /tmp para que no se colapse el sistema. Recuerda que /tmp es borrado habitualmente por el sistema. Normalmente con cada nuevo arranque.&lt;/p&gt;

&lt;h2 id=&#34;rutas-absolutas-relativas-y-directorio-de-trabajo&#34;&gt;Rutas absolutas, relativas y directorio de trabajo&lt;/h2&gt;

&lt;p&gt;Para referirnos a un archivo o a un directorio debemos indicar su ruta (&lt;a href=&#34;https://en.wikipedia.org/wiki/Path_(computing)&#34; target=&#34;_blank&#34;&gt;path&lt;/a&gt;. Un ejemplo de ruta podría ser:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/home/alicia/documentos/tesis.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Este tipo de rutas en las que se especifican todos los subdirectorios empezando desde el directorio raíz se denominan rutas absolutas.&lt;/p&gt;

&lt;p&gt;Para no tener que escribir la ruta absoluta completa cada vez que queremos referirnos a un archivo o a un directorio se crearon los conceptos de &lt;a href=&#34;https://en.wikipedia.org/wiki/Working_directory&#34; target=&#34;_blank&#34;&gt;directorio de trabajo&lt;/a&gt; y de &lt;a href=&#34;https://en.wikipedia.org/wiki/Path_(computing)#Absolute_and_relative_paths&#34; target=&#34;_blank&#34;&gt;ruta relativa&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;El directorio de trabajo es una propiedad del terminal (del shell) en la que estamos trabajando. Siempre que estemos trabajando en una terminal tendremos asignado un directorio de trabajo. Por ejemplo, si nuestro usuario es &lt;em&gt;alicia&lt;/em&gt; sería normal que al abrir un terminal nuestro directorio de trabajo fuese:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/home/alicia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El directorio de trabajo se utiliza para escribir rutas a archivos relativas al mismo. De este modo nos ahorramos escribir bastante. Imaginemos que Alicia tiene en su directorio un documento llamado peliculas.txt. La ruta absoluta sería.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/home/alicia/peliculas.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mientras su directorio de trabajo sea /home/alicia la ruta relativa sería simplemetne:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;peliculas.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Es decir, podemos escribir rutas relativas al directorio de trabajo, rutas que en vez de partir del directorio raíz parten desde el directorio de trabajo. Las rutas relativas se diferencian de las absolutas en los sistemas Unix porque las absolutas empiezan por “y” las relativas no.&lt;/p&gt;

&lt;p&gt;Es común referirse al directorio de trabajo de una terminal como a un lugar en el que nos encontramos mientras estamos trabajando en la terminal. Siempre que estemos en una terminal estaremos dentro de un directorio de trabajo.&lt;/p&gt;

&lt;p&gt;Por ejemplo, cuando abrimos un nuevo terminal el directorio de trabajo se sitúa en /home/nombre_de_usuario. Si ejecutamos el comando &lt;em&gt;ls&lt;/em&gt;, el programa asumirá que queremos listar los archivos presentes en ese directorio y no en otro cualquiera. Existe un comando que nos informa sobre el directorio de trabajo actual, pwd (Print Working Directory):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pwd
/home/alicia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si deseamos podemos modificar el directorio de trabajo “moviéndonos” a otro directorio. Para lograrlo hay que utilizar el comando &lt;em&gt;cd&lt;/em&gt; (Change Directory):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd documentos
$ pwd
/home/alicia/documentos
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A partir de ese momento los comandos asumirán que si no se les indica lo contrario el directorio desde el que deben trabajar es /home/alicia/documentos.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;cd&lt;/em&gt; además tiene algunos parámetros especiales:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd      Ir al directorio $HOME del usuario.
cd -    Ir al directorio de trabajo previo
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;directorio-home&#34;&gt;Directorio $HOME&lt;/h2&gt;

&lt;p&gt;El directorio $HOME en los sistemas Unix, que son sistemas multiusuario, es el directorio en el que el usuario debe mantener sus ficheros y directorios. Fuera de este directorio el usuario tendrá unos permisos restringidos puesto que sus acciones podrían afectar a otros usuarios.&lt;/p&gt;

&lt;p&gt;En Linux los directorios $HOME de los usuarios son subdirectorios del directorio &lt;em&gt;/home&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;El directorio $HOME de un usuario es además el directorio de trabajo por defecto, es decir, el directorio de trabajo que se establece cuando se abre una terminal.&lt;/p&gt;

&lt;h2 id=&#34;moviendo-renombrando-y-copiando-ficheros&#34;&gt;Moviendo, renombrando y copiando ficheros&lt;/h2&gt;

&lt;p&gt;En primer lugar vamos a crear un fichero de prueba:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ touch data.txt
~$ ls
data.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El comando &lt;em&gt;touch&lt;/em&gt;, en este caso, ha creado un fichero vacío.&lt;/p&gt;

&lt;p&gt;Los ficheros se copian con el comando &lt;em&gt;cp&lt;/em&gt; (CoPy):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ cp data.txt data.bak.txt
~$ ls
data.bak.txt  data.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Se mueven y renombran con el &lt;em&gt;mv&lt;/em&gt; (MoVe):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ mv data.txt experimento_1.txt
~$ ls
data.bak.txt  experimento_1.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para crear un nuevo directorio podemos utilizar la orden &lt;em&gt;mkdir&lt;/em&gt; (MaKeDIRectory):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ mkdir exp_1
~$ ls
data.bak.txt  exp_1  experimento_1.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;mv&lt;/em&gt; también sirve para mover ficheros entre directorios:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ mv experimento_1.txt exp_1/
~$ ls
data.bak.txt  exp_1
~$ ls exp_1/
experimento_1.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Los ficheros se eliminan con la orden &lt;em&gt;rm&lt;/em&gt; (ReMove):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ rm data.bak.txt
~$ ls
exp_1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la línea de comandos de los sistemas Unix cuando se borra un fichero se borra definitivamente, no hay papelera. Una vez ejecutado el &lt;em&gt;rm&lt;/em&gt; no podremos recuperar el archivo.&lt;/p&gt;

&lt;p&gt;Los comandos &lt;em&gt;cp&lt;/em&gt; y &lt;em&gt;rm&lt;/em&gt; no funcionarán bien con los directorios a no ser que modifiquemos el comportamiento que muestran por defecto:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ rm exp_1/
rm: cannot remove exp_1/ Is a directory
~$ cp exp_1/ exp_1_bak/
cp: omitting directory exp_1/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esto sucede porque para copiar o borrar un directorio hay que copiar o borrar todos sus contenidos recursivamente y esto podría alterar muchos datos con un sólo comando. Por esta razón se exige que estos dos comandos incluyan un modificador que les indique que sí deben funcionar recursivamente cuando tratan con directorios:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ cp -r exp_1/ exp_1_bak/
~$ ls
exp_1  exp_1_bak
~$ rm -r exp_1_bak/
~$ ls
exp_1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;nombres-de-directorios-y-archivos&#34;&gt;Nombres de directorios y archivos&lt;/h2&gt;

&lt;p&gt;En Unix los archivos pueden tener prácticamente cualquier nombre. Existe la convención de acabar los nombres con un punto y una pequeña extensión que indica el tipo de archivo. Pero esto es sólo una convención, en realidad podríamos no utilizar este tipo de nomenclatura.&lt;/p&gt;

&lt;p&gt;Si deseamos utilizar nombres de archivos que no vayan a causar extraños comportamientos en el futuro lo mejor sería seguir unas cuantas reglas al nombrar un archivo:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Añadir una extensión para recordarnos el tipo de archivo, por ejemplo .txt para los archivos de texto.&lt;/li&gt;
&lt;li&gt;No utilizar en los nombres:

&lt;ul&gt;
&lt;li&gt;espacios,&lt;/li&gt;
&lt;li&gt;caracteres no alfanuméricos,&lt;/li&gt;
&lt;li&gt;ni caracteres no ingleses como letras acentuadas o eñes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Por supuesto, podríamos crear un archivo denominado “$ñ 1.txt” para referirnos a un archivo de sonido, pero esto conllevaría una sería de problemas que aunque son solventables nos dificultarán el trabajo.&lt;/p&gt;

&lt;p&gt;Además es importante recordar que en Unix las mayúsculas y las minúsculas no son lo mismo. Los ficheros “documento.txt”, “Documento.txt” y “DOCUMENTO.TXT” son tres ficheros distintos.&lt;/p&gt;

&lt;p&gt;Otra convención utilizada en los sistema Unix es la de ocultar los archivos cuyos nombres comienzan por punto “.”. Por ejemplo el archivo “.oculto” no aparecerá normalmente cuando pedimos el listado de un directorio. Esto se utiliza normalmente para guardar archivos de configuración que no suelen ser utilizados directamente por los usuarios. Para listar todos los archivos (All), ya sean éstos ocultos o no se puede ejecutar:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -a
.               .fontconfig          .HyperTree           .pki
..               fsm.jpg              .ICEauthority        .recently-used
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta convención de ocultar los ficheros cuyo nombre comienza por un punto se mantiene también en el navegador gráfico de ficheros. En este caso podemos pedir que se muestren estos archivos en el menú Ver -&amp;gt; Mostrar los archivos ocultos.&lt;/p&gt;

&lt;p&gt;Para acelerar el acceso a ciertos directorios existen algunos nombres especiales que son bastante útiles:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;* &amp;quot;..&amp;quot; indica el directorio padre del directorio actual
* &amp;quot;.&amp;quot; indica el directorio actual
* &amp;quot;~&amp;quot; representa la $HOME del usuario
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;wildcards&#34;&gt;WildCards&lt;/h2&gt;

&lt;p&gt;En muchas ocasiones resulta útil tratar los ficheros de un modo conjunto. Por ejemplo, imaginemos que queremos mover todos los ficheros de texto a un directorio y la imágenes a otro. Creemos una pequeña demostración::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ touch exp_1a.txt
~$ touch exp_1b.txt
~$ touch exp_1b.jpg
~$ touch exp_1a.jpg
~$ ls
exp_1  exp_1a.jpg  exp_1a.txt  exp_1b.jpg  exp_1b.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podemos referirnos a todos los archivos que acaban en txt utilizando un asterisco:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ mv *txt exp_1
~$ ls
exp_1  exp_1a.jpg  exp_1b.jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El asterisco sustituye a cualquier texto, por lo que al escribir *txt incluimos a cualquier fichero que tenga un nombre cualquiera, pero que termine con las letras txt. Podríamos por ejemplo referirnos a los ficheros del experimento 1a:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ ls *1a*
exp_1a.jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta herramienta es muy potente y útil, pero tenemos que tener cuidado con ella, sobre todo cuando la combinamos con &lt;em&gt;rm&lt;/em&gt;. Por ejemplo la orden:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rm -r *
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Borraría todos los ficheros y directorios que se encuentren bajo el directorio de trabajo actual, si lo hacemos perderemos todos los ficheros y directorios que cuelgan del actual directorio de trabajo, puede que esto sea lo que queramos, pero hemos de andar con cuidado.&lt;/p&gt;

&lt;h2 id=&#34;ejercicios&#34;&gt;Ejercicios&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;¿Cuáles son los ficheros y directorios presentes en el directorio raíz?&lt;/li&gt;
&lt;li&gt;¿Cuáles son todos los archivos presentes en nuestro directorio de usuario?&lt;/li&gt;
&lt;li&gt;Crea un directorio llamado experimento.&lt;/li&gt;
&lt;li&gt;Crea con touch los archivos datos1.txt y datos2.txt dentro del directorio experimento.&lt;/li&gt;
&lt;li&gt;Vuelve al directorio principal de tu usuario y desde allí lista los archivos presentes en el directorio experimento usando rutas absolutas y relativas&lt;/li&gt;
&lt;li&gt;Haz del directorio ~/Documentos tu directorio de trabajo y repite el ejercicio anterior&lt;/li&gt;
&lt;li&gt;Borra todos los archivos que contengan un 2 en el directorio experimento.&lt;/li&gt;
&lt;li&gt;Copia el directorio experimento a un nuevo directorio llamado exp_seguridad.&lt;/li&gt;
&lt;li&gt;Borra el directorio experimento.&lt;/li&gt;
&lt;li&gt;Renombra el directorio exp_seguridad a experimento.&lt;/li&gt;
&lt;li&gt;Copia el fichero /etc/passwd al directorio ~/Documentos&lt;/li&gt;
&lt;li&gt;Copia el fichero /etc/passwd al directorio ~/Documentos llamándolo usuarios.txt&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;obteniendo-información-sobre-archivos-y-directorios&#34;&gt;Obteniendo información sobre archivos y directorios&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;ls&lt;/em&gt; es un comando capaz de mostrarnos información extra sobre los archivos y directorios que lista. Por ejemplo podemos pedirle, usando la opción &lt;em&gt;-l&lt;/em&gt; (Long), que nos muestre quién es el dueño del archivo y cuanto ocupa y qué permisos tiene además de otras cosas::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ ls
exp_1
~$ ls -l
total 4
drwxr-xr-x 2 usuario usuario 4096 Oct 13 09:48 exp_1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La información sobre la cantidad de disco ocupada la da por defecto en bytes, si la queremos en un formato más inteligible podemos utilizar la opción &lt;em&gt;-h&lt;/em&gt; (Human):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ ls -lh
total 4.0K
drwxr-xr-x 2 usuario usuario 4.0K Oct 13 09:48 exp_1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podemos consultar el tipo de un archivo mediante el comando &lt;em&gt;file&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ file imagen.png
imagen.png: PNG image data, 1920 x 1080, 8-bit/color RGB, non-interlaced
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En principio, el tipo de un archivo no está determinado por la extensión, la extensión es sólo parte del nombre, aunque hay software que viola o complementa este principio. El tipo de archivo está determinado por su &lt;a href=&#34;https://en.wikipedia.org/wiki/Magic_number_(programming)&#34; target=&#34;_blank&#34;&gt;magic number&lt;/a&gt;. El magic number está compuesto por una corta &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_file_signatures&#34; target=&#34;_blank&#34;&gt;serie de bytes&lt;/a&gt; que indican el tipo de archivo.&lt;/p&gt;

&lt;h2 id=&#34;permisos&#34;&gt;Permisos&lt;/h2&gt;

&lt;p&gt;Unix desde su origen ha sido un sistema multiusuario. Para conseguir que cada usuario pueda trabajar en sus archivos, pero que no pueda interferir accidental o deliberadamente con los archivos de otros usuarios se estableció desde el principio un sistema de permisos. Por defecto un usuario tiene permiso para leer y modificar sus propios archivos y directorios, pero no los de los demás. En los sistemas Unix los ficheros pertenecen a un usuario concreto y existen unos permisos diferenciados para este usuario y para el resto. Además el usuario pertenece a un grupo de trabajo. Por ejemplo, imaginemos que la usuaria alicia puede pertenecer al grupo de trabajo “diagnostico”. Si alicia crea un fichero este tendrá unos permisos diferentes para alicia, para el resto de miembros de su grupo y para el resto de usuarios del ordenador. Podemos ver los permisos asociados a los ficheros utilizando el comando &lt;em&gt;ls&lt;/em&gt; con la opción -l (Long)::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~$ ls -l
total 7324
-rw-r--r-- 1 alicia diagnostico    1059 Oct 20 12:42 busqueda_leukemia_100.txt
-rw-r--r-- 1 alicia diagnostico       0 Oct 13 10:53 datos_1.txt
drwxr-xr-x 2 alicia diagnostico    4096 Oct 13 10:29 experimento
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En este caso, los ficheros listados pertenecen Alicia y al grupo diagnostico. Los permisos asignados al usuario, a los miembros del grupo y al resto de usuarios están resumidos en la primeras letras de cada línea::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drwxr-x---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La primera letra indica el tipo de fichero listado: (d) directorio, (-) fichero u otro tipo especial. Las siguientes nueve letras muestran, en grupos de tres, los permisos para el usuario, para el grupo y para el resto de usuarios del ordenador. Cada grupo de tres letras indica los permisos de lectura (Read), escritura (Write) y ejecución (eXecute). En el caso anterior el usuario tiene permiso de lectura, escritura y ejecución (rwx), el grupo tiene permiso de lectura y ejecución (r-x), es decir no puede modificar el fichero o el directorio, y el resto de usuarios no tienen ningún permiso (—).&lt;/p&gt;

&lt;p&gt;En los ficheros normales el permiso de lectura indica si el fichero puede ser leído, el de escritura si puede ser modificado y el de ejecución si puede ser ejecutado. En el caso de los directorios el de escritura indica si podemos añadir o borrar ficheros del directorio y el de ejecución si podemos listar los contenidos del directorio.&lt;/p&gt;

&lt;p&gt;Estos permisos pueden ser modificados con la orden &lt;em&gt;chmod&lt;/em&gt;. En chmod cada grupo de usuarios se representa por una letra:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;u: usuario dueño del fichero&lt;/li&gt;
&lt;li&gt;g: grupo de usuarios del dueño del fichero&lt;/li&gt;
&lt;li&gt;o: todos los otros usuarios&lt;/li&gt;
&lt;li&gt;a: todos los tipos de usuario (dueño, grupo y otros)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Los tipos de permisos también están abreviados por letras:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;r: lectura&lt;/li&gt;
&lt;li&gt;w: escritura&lt;/li&gt;
&lt;li&gt;x: ejecución&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Con estas abreviaturas podemos modificar los permisos existentes.&lt;/p&gt;

&lt;p&gt;Hacer un fichero ejecutable:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chmod u+x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chmod a+x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;También podemos mediante chmod indicar los permisos para un tipo de usuario determinado.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chmod a=rwx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Un modo algo menos intuitivo, pero más útil de utilizar chmod es mediante los números octales que representan los permisos.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- lectura: 4
- escritura: 2
- ejecución: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para modificar los permisos de este modo debemos indicar el número octal que queremos que represente los permisos del fichero. La primera cifra representará al dueño, la segunda al grupo y la tercera al resto de usuarios. Por ejemplo si queremos que único permiso para el dueño y su grupo sea la lectura y que no haya ningún permiso para el resto de usuarios:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chmod 110 fichero.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;También podemos combinar permisos sumando los números anteriores. Por ejemplo, permiso para leer y escribir para el dueño y ningún permiso para el resto.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chmod 300 fichero.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Permisos de lectura, escritura y ejecución para el dueño y su grupo y ninguno para el resto.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chmod 770 fichero.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Las restricciones para los permisos no afectan al usuario root, al administrador del sistema. root también puede modificar quien el dueño y el grupo al que pertenecen los ficheros mediante los comando chown y chgrp.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chown alicia fichero.txt
$ chown diagnostico fichero.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;obteniendo-información-sobre-el-sistema-de-archivos&#34;&gt;Obteniendo información sobre el sistema de archivos&lt;/h2&gt;

&lt;p&gt;El sistema de archivos puede abarcar una o más &lt;a href=&#34;https://en.wikipedia.org/wiki/Disk_partitioning&#34; target=&#34;_blank&#34;&gt;particiones&lt;/a&gt;. Una partición es una región de un disco o de cualquier otro medio de almacenamiento. Las instalaciones de Windows tienen normalmente una partición por disco, pero en Linux esto no es tan habitual. Cada partición tiene un sistema de archivos propio, pero en Unix estos sistemas deben estar montados en algún lugar dentro de la jerarquía que cuelga de la raíz. En Windows cada partición tiene por defecto una jerarquía independiente.&lt;/p&gt;

&lt;p&gt;Podemos pedir información sobre el espacio ocupado por las distintas particiones que tenemos actualmente montadas usando el comando &lt;em&gt;df&lt;/em&gt; (Disk Free).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ df -h
S.ficheros     Tamaño Usados  Disp Uso% Montado en
udev             7,8G      0  7,8G   0% /dev
tmpfs            1,6G   9,8M  1,6G   1% /run
/dev/nvme0n1p2    25G   8,1G   16G  35% /
tmpfs            7,8G   5,3M  7,8G   1% /dev/shm
tmpfs            5,0M   4,0K  5,0M   1% /run/lock
tmpfs            7,8G      0  7,8G   0% /sys/fs/cgroup
/dev/nvme0n1p4   206G    18G  178G   9% /home
/dev/nvme0n1p1   511M   3,6M  508M   1% /boot/efi
/dev/sda1        2,7T   117G  2,5T   5% /home/jose/magnet
tmpfs            1,6G    64K  1,6G   1% /run/user/1000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Algunos de los sistemas de archivos montados puede que no se correspondan con particiones en un disco físico sino con espacios de la memoria RAM que son utilizados como sistemas de archivos especiales.&lt;/p&gt;

&lt;p&gt;El commando &lt;em&gt;du&lt;/em&gt; (disk usage) informa sobre el espacio que ocupa un árbol de directorios. Este comando tiene equivalentes gráficos como Baobab o xdiskusage. Podemos pedir a &lt;em&gt;du&lt;/em&gt; que nos muestre cuanto espacio ocupan los directorios bajo el directorio analysis:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ du -h analyses/
36K	analyses/alicia/cache
204K	analyses/alicia/differential_snps/differential
252K	analyses/alicia/differential_snps/non_differentia
919M	analyses/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si sólo queremos obtener el resultado para el directorio que le hemos dado y no para sus subdirectorios podemos utilizar el parámetro &lt;em&gt;-s&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ du -sh analyses/
919M	analyses/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si queremos información sobre todos los archivos y no sólo los directorios podemos usar &lt;em&gt;-a&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ du -ha analyses/
32K	analyses/alicia/cache/min_called_rate_samples_cache.pickle
36K	analyses/alicia/cache
8,0K	analyses/alicia/look_for_matching_accessions.py
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ejercicios-1&#34;&gt;Ejercicios&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;¿Cuáles son los permisos de los directorios presentes en el directorio raíz y en nuestro directorio de usuario? ¿A quién pertenecen los ficheros y qué permisos tienen los distintos usuarios del ordenador?&lt;/li&gt;
&lt;li&gt;Crea un directorio en tu home y muestra los permisos que tiene.&lt;/li&gt;
&lt;li&gt;Cambia los permisos para que sólo tu usuario pueda acceder al nuevo directorio&lt;/li&gt;
&lt;li&gt;Crea un fichero nuevo y dale permisos de ejecución para todos los usuarios&lt;/li&gt;
&lt;li&gt;Último fichero modificado en el directorio /etc.&lt;/li&gt;
&lt;li&gt;Lista los ficheros de /etc con su tamaño y ordénalos por tamaño.&lt;/li&gt;
&lt;li&gt;Copia todos los ficheros y directorios del directorio /etc cuyo nombre comience por s. ¿Has podido copiarlos todos?&lt;/li&gt;
&lt;li&gt;¿Cuánto espacio libre queda en las distintas particiones del sistema?&lt;/li&gt;
&lt;li&gt;¿Cuánto espacio ocupan todos los ficheros y subdirectorios de tu $HOME?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;compresion-y-descompresión-de-ficheros&#34;&gt;Compresion y descompresión de ficheros&lt;/h2&gt;

&lt;p&gt;Existen distintos formatos de compresión de ficheros como: gzip, bzip, zip o rar. Los formatos más utilizados en Unix son gzip y bzip.&lt;/p&gt;

&lt;p&gt;Comprimir un fichero con gzip o bzip:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gzip informacion_snps.txt
$ ls
informacion_snps.txt.gz

$ bzip2 accs.txt
$ ls
accs.txt.bz2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bzip2 comprime más que gzip, pero es más lento. gzip también dispone de varios niveles de compresión, cuanto más comprime más lenta suele ser la compresión.&lt;/p&gt;

&lt;p&gt;Podemos descomprimir cualquier fichero utilizando la línea de comandos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gunzip informacion_snps.txt.gz
$ ls
informacion_snps.txt
$ bunzip2 accs.txt.bz2
$ ls
accs.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Muchos estamos acostumbrados al formato zip. Un fichero zip no se corresponde en realidad con un sólo fichero comprimido sino con varios. Un fichero zip hace dos cosas: unir varios ficheros en uno y comprimir el resultado. Los comandos que hemos visto (gzip y bzip2) son capaces de comprimir un sólo archivo, pero no pueden unir varios archivos en uno. &lt;em&gt;tar&lt;/em&gt; es el comando capaz de unir varios archivos en uno.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls
seq1.fasta  seq2.fasta
$ tar -cvf secuencias.tar seq*
seq1.fasta
seq2.fasta
$ ls
secuencias.tar  seq1.fasta  seq2.fasta
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;tar&lt;/em&gt; también es capaz de desempaquetar los archivos que habíamos unido.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls
secuencias.tar
$ rm seq1.fasta seq2.fasta
$ tar -xvf secuencias.tar
seq1.fasta
seq2.fasta
$ ls
secuencias.tar  seq1.fasta  seq2.fasta
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El problema es que utilizando el comando &lt;em&gt;tar&lt;/em&gt; tal y como lo hemos hecho hemos conseguido unir y separar archivos, pero no hemos comprimido el fichero unido. Para hacerlo podríamos utilizar los comandos &lt;em&gt;gzip&lt;/em&gt; o &lt;em&gt;bzip2&lt;/em&gt;, pero este no es el modo habitula de hacerlo. Dado que casi siempre que unamos archivos en un archivo tar también querremos comprimir el resultado el comando tar tiene también la capacidad de comprimir y descomprimir utilizando los algoritmos gzip y bzip2. Unir y comprimir con gzip varios archivos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tar -cvzf secuencias.tar.gz seq*
seq1.fasta
seq2.fasta
$ ls
secuencias.tar.gz  seq1.fasta  seq2.fasta
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Descomprimir un archivo tar.gz:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tar -xvzf secuencias.tar.gz
seq1.fasta
seq2.fasta
$ ls
secuencias.tar.gz  seq1.fasta  seq2.fasta
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;También podemos descomprimir el contenido de un fichero de texto y enviar el resultado a la terminal con el comando &lt;em&gt;zcat&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;$ zcat fichero.txt.gz&lt;/p&gt;

&lt;p&gt;Con bzip2.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tar -cvjf secuencias.tar.bz seq*
seq1.fasta
seq2.fasta
$ ls
secuencias.tar.bz  seq1.fasta  seq2.fasta

$ tar -xvjf secuencias.tar.bz
seq1.fasta
seq2.fasta
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ejercicios-2&#34;&gt;Ejercicios&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Crea un fichero de texto en el directorio ~/Documentos y comprimelo con gzip&lt;/li&gt;
&lt;li&gt;Muestra el contenido del fichero anterior en pantalla sin descomprimirlo previamente&lt;/li&gt;
&lt;li&gt;Crea un archivo tar de todo el contenido del directorio ~/Documentos&lt;/li&gt;
&lt;li&gt;Comprime el fichero tar anterior&lt;/li&gt;
&lt;li&gt;Vuelve a hacer los ejercicios 2 y 3, pero en un sólo paso&lt;/li&gt;
&lt;li&gt;Descomprime el fichero tar.gz anterior en un nuevo directorio llamado Documentos2&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;enlaces-duros-y-blandos&#34;&gt;Enlaces duros y blandos&lt;/h2&gt;

&lt;p&gt;Podemos pensar en el nombre de un fichero como en una etiqueta que apunta a una posición concreta en el disco duro, en realidad es un puntero a un &lt;a href=&#34;https://en.wikipedia.org/wiki/Inode&#34; target=&#34;_blank&#34;&gt;inodo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Podmeos pensar en un &lt;a href=&#34;https://en.wikipedia.org/wiki/Hard_link&#34; target=&#34;_blank&#34;&gt;enlace duro&lt;/a&gt; como en un nombre adicional para un archivo. Si tenemos un archivo en el disco y creamos un enlace duro tendremos dos nombres para ese único archivo.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls
archivo1.txt
$ ln archivo1.txt nombre2.txt
$ ls
archivo1.txt nombre2.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Las dos referencias, nombres, al archivo serán indistinguibles. Si borramos un nombre quedará el otro. Si modificamos un archivo se modifica independientemente del nombre por el cual estemos accediendo a él. No es muy común utilizar enlaces duro salvo en aplicaciones muy concretas, por ejemplo en versiones de copias de seguridad.&lt;/p&gt;

&lt;p&gt;Un enlace blando, más comumente conocido como un enlace simbólico, es una referencia al nombre de un archivo, no al archivo en sí.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls
archivo1.txt
$ ln -s archivo1.txt nombre3.txt
$ ls -l
-rw-rw-r-- 1 jose jose  0 sep 27 15:16 archivo1.txt
lrwxrwxrwx 1 jose jose 12 sep 27 15:16 nombre3.txt -&amp;gt; archivo1.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si eliminamos el archivo original el enlace quedará roto.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rm archivo1.txt
$ cat nombre3.txt
cat: nombre3.txt: No existe el archivo o el directorio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El comportamiento de ambos tipos de enlaces cambia si sobreescribimos el fichero.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x
$ echo &amp;quot;hola&amp;quot; &amp;gt; hola.txt
$ cat hola.txt
hola
$ ln hola.txt hola2.txt
$ ln -s hola.txt hola3.txt
$ ls -l
-rw-rw-r-- 2 jose jose 5 sep 27 15:23 hola2.txt
lrwxrwxrwx 1 jose jose 8 sep 27 15:25 hola3.txt -&amp;gt; hola.txt
-rw-rw-r-- 2 jose jose 5 sep 27 15:23 hola.txt
$ echo &amp;quot;adios&amp;quot; &amp;gt; adios.txt
$ mv adios.txt hola.txt
$ cat hola.txt
adios
$ cat hola2.txt
hola
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Los enlaces blandos funcionan incluso entre distintos sistemas de archivos o particiones, los duros no.&lt;/p&gt;

&lt;h2 id=&#34;ejercicios-3&#34;&gt;Ejercicios&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Crea un enlace simbólico a un fichero de texto dentro del direcotorio ~/Documentos&lt;/li&gt;
&lt;li&gt;Crea un enlace duro al mismo fichero.&lt;/li&gt;
&lt;li&gt;Edita el fichero de texto y observa como cambian ambos enlaces&lt;/li&gt;
&lt;li&gt;Crea un nuevo fichero de texto con otro contenido. Sustituye el primer fichero con el segundo y observa el resultado en ambos enlaces&lt;/li&gt;
&lt;li&gt;Crea dos enlaces, uno simbólico y otro duro, a un fichero. Elimina el fichero y observa el resultado en ambos enlaces&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;acceso-remoto&#34;&gt;Acceso remoto&lt;/h2&gt;

&lt;p&gt;Una de las grandes ventajas de utilizar la terminal es que podemos acceder a terminales en otros ordenadores muy fácilmente. El protocolo más utilizado para acceder a terminales de forma remota es &lt;a href=&#34;https://en.wikipedia.org/wiki/Secure_Shell&#34; target=&#34;_blank&#34;&gt;ssh&lt;/a&gt; (Secure Shell). ssh tiene un gran número de posibilidades, pero el uso más habitual es utilizarlo para abrir terminales en ordenadores remotos que tienen un servicio ssh. ssh es seguro porque cifra las comunicaciones entre el cliente y el servidor. ssh se diseñó como una alternativa segura a telnet. No debemos usar el protocolo telnet porque las comunicaciones en telnet, incluidas las claves de acceso, no están cifradas y cualquiera puede tener acceso a ellas.&lt;/p&gt;

&lt;p&gt;Para acceder a una computadora que implemente el protocolo ssh podemos usar el programa ssh, pero previamente tenemos que tener una cuenta en esa computadora. Imaginemos que alicia tiene una cuenta en un ordenador que tiene un servicio ssh. Para conectarse puede hacer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh alicia@ordenador.upv.es
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si el nombre de la cuenta de usuario en el ordenador cliente y en el servidor es el mismo puede obviar el nombre de usuario.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh ordenador.upv.es
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación el servidor le pedirá la clave correspondiente a ese usuario.&lt;/p&gt;

&lt;p&gt;Existen clientes ssh para windows con los que nos podemos conectar a servidores ssh. Uno muy común es &lt;a href=&#34;http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html&#34; target=&#34;_blank&#34;&gt;putty&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Una tarea muy habitual cuando estamos trabajando en un ordenador remoto es enviar o traer ficheros desde el mismo. Esto también lo podemos hacer utilizando el protocolo ssh por lo que podremos hacerlo de un modo seguro en cualquier ordenador que no de acceso ssh. El programa más sencillo para hacerlo desde Unix es &lt;a href=&#34;https://en.wikipedia.org/wiki/Secure_copy&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;scp&lt;/em&gt;&lt;/a&gt; (Secure CoPy). &lt;em&gt;scp&lt;/em&gt; tiene una interfaz muy similar a &lt;em&gt;cp&lt;/em&gt; pero acepta que los ficheros de origen y destino estén en distintos ordenadores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scp alicia@remotehost.edu:/remote/directory/seq.txt /some/local/directory
$ scp /some/local/directory/seq.txt alicia@remotehost.edu:/remote/directory/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En windows también hay distintos clientes scp, uno de ellos es &lt;a href=&#34;https://winscp.net/eng/download.php&#34; target=&#34;_blank&#34;&gt;winscp&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Una alternativa a scp que tiene más capacidades, como enviar fragmentos de ficheros, es &lt;a href=&#34;https://en.wikipedia.org/wiki/Rsync&#34; target=&#34;_blank&#34;&gt;rsync&lt;/a&gt;. &lt;em&gt;rsync&lt;/em&gt; está diseñado para mantener varios archivos sincronizados entre dos ordenadores, pero también ser puede utilizar para copiar archivos de un ordenador a otro como scp. &lt;em&gt;rsync&lt;/em&gt; puede establecer la conexión utilizando distintos protocolos, pero uno de ellos es ssh por lo que funcionará también con cualquier servidor ssh.&lt;/p&gt;

&lt;p&gt;Si lo que queremos es descargar un fichero desde un servidor en internet, por ejemplo desde una página web, al ordenador remoto en el que estamos trabajando en una sesión ssh podemos utilizar el comando &lt;em&gt;wget&lt;/em&gt; o su alternativa &lt;em&gt;curl&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://http://ncbi.nlm.nih.gov/una_secuencia.fasta
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ejercicios-4&#34;&gt;Ejercicios&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Contectate a un servidor remoto usando ssh&lt;/li&gt;
&lt;li&gt;Transfiere un fichero desde tu ordenador al servidor&lt;/li&gt;
&lt;li&gt;Descarga el fichero &lt;a href=&#34;https://www.gnu.org/licenses/gpl.txt&#34; target=&#34;_blank&#34;&gt;https://www.gnu.org/licenses/gpl.txt&lt;/a&gt; directamente en el ordenador remoto&lt;/li&gt;
&lt;li&gt;Copia el fichero gpl.txt a tu ordenador&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Python Tutorial - Linear Regression in Python</title>
      <link>https://www.marcusrb.com/en/resources/tutorials/python/pytut-linear/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/resources/tutorials/python/pytut-linear/</guid>
      <description>

&lt;p&gt;Vivimos en la era de grandes cantidades de datos, computadoras potentes e inteligencia artificial. Este es solo el comienzo. La ciencia de datos y el aprendizaje automático están impulsando el reconocimiento de imágenes, el desarrollo de vehículos autónomos, las decisiones en los sectores financiero y energético, los avances en medicina, el auge de las redes sociales y más. La regresión lineal es una parte importante de esto.&lt;/p&gt;

&lt;p&gt;La regresión lineal es una de las técnicas estadísticas y de aprendizaje automático fundamentales. Ya sea que desee hacer estadísticas, aprendizaje automático o computación científica, hay buenas posibilidades de que lo necesite. Es recomendable aprenderlo primero y luego proceder hacia métodos más complejos.&lt;/p&gt;

&lt;p&gt;Al final de este artículo, habrás aprendido:&lt;/p&gt;

&lt;p&gt;¿Qué es la regresión lineal?
Para qué regresión lineal se utiliza
Cómo funciona la regresión lineal
Cómo implementar la regresión lineal en Python, paso a paso&lt;/p&gt;

&lt;h2 id=&#34;regresión&#34;&gt;Regresión&lt;/h2&gt;

&lt;p&gt;El análisis de regresión es uno de los campos más importantes en estadística y aprendizaje automático. Hay muchos métodos de regresión disponibles. La regresión lineal es una de ellas.&lt;/p&gt;

&lt;p&gt;¿Qué es la regresión?
La regresión busca relaciones entre variables.&lt;/p&gt;

&lt;p&gt;Por ejemplo, puede observar a varios empleados de alguna compañía e intentar comprender cómo sus salarios dependen de las características, como la experiencia, el nivel de educación, el rol, la ciudad en la que trabajan, etc.&lt;/p&gt;

&lt;p&gt;Este es un problema de regresión en el que los datos relacionados con cada empleado representan una observación. La presunción es que la experiencia, la educación, el rol y la ciudad son las características independientes, mientras que el salario depende de ellas.&lt;/p&gt;

&lt;p&gt;Del mismo modo, puede intentar establecer una dependencia matemática de los precios de las casas en sus áreas, número de dormitorios, distancias al centro de la ciudad, etc.&lt;/p&gt;

&lt;p&gt;En general, en el análisis de regresión, generalmente considera algún fenómeno de interés y tiene una serie de observaciones. Cada observación tiene dos o más características. Siguiendo el supuesto de que (al menos) una de las características depende de las otras, intenta establecer una relación entre ellas.&lt;/p&gt;

&lt;p&gt;En otras palabras, debe encontrar una función que asigne algunas características o variables a otras lo suficientemente bien.&lt;/p&gt;

&lt;p&gt;Las características dependientes se denominan variables dependientes, salidas o respuestas.&lt;/p&gt;

&lt;p&gt;Las características independientes se denominan variables independientes, entradas o predictores.&lt;/p&gt;

&lt;p&gt;Los problemas de regresión generalmente tienen una variable dependiente continua y sin límites. Sin embargo, las entradas pueden ser datos continuos, discretos o incluso categóricos, como género, nacionalidad, marca, etc.&lt;/p&gt;

&lt;p&gt;Es una práctica común denotar las salidas con 𝑦 y las entradas con 𝑥. Si hay dos o más variables independientes, se pueden representar como el vector 𝐱 = (𝑥₁, &amp;hellip;, 𝑥ᵣ), donde 𝑟 es el número de entradas.&lt;/p&gt;

&lt;h2 id=&#34;cuándo-necesitas-regresión&#34;&gt;¿Cuándo necesitas regresión?&lt;/h2&gt;

&lt;p&gt;Por lo general, se necesita una regresión para responder si un fenómeno influye en el otro y cómo se relacionan varias variables. Por ejemplo, puede usarlo para determinar si y en qué medida la experiencia o el género afectan los salarios.&lt;/p&gt;

&lt;p&gt;La regresión también es útil cuando desea pronosticar una respuesta utilizando un nuevo conjunto de predictores. Por ejemplo, podría intentar predecir el consumo de electricidad de un hogar para la próxima hora dada la temperatura exterior, la hora del día y el número de residentes en ese hogar.&lt;/p&gt;

&lt;p&gt;La regresión se usa en muchos campos diferentes: economía, ciencias de la computación, ciencias sociales, etc. Su importancia aumenta cada día con la disponibilidad de grandes cantidades de datos y una mayor conciencia del valor práctico de los datos.&lt;/p&gt;

&lt;p&gt;Regresión lineal
La regresión lineal es probablemente una de las técnicas de regresión más importantes y ampliamente utilizadas. Es uno de los métodos de regresión más simples. Una de sus principales ventajas es la facilidad de interpretación de los resultados.&lt;/p&gt;

&lt;p&gt;Formulación del problema
Al implementar la regresión lineal de alguna variable dependiente 𝑦 en el conjunto de variables independientes 𝐱 = (𝑥₁, &amp;hellip;, 𝑥ᵣ), donde 𝑟 es el número de predictores, se supone una relación lineal entre 𝑦 y 𝐱: 𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽ᵣ𝑥ᵣ + 𝜀. Esta ecuación es la ecuación de regresión. 𝛽₀, 𝛽₁, &amp;hellip;, 𝛽ᵣ son los coeficientes de regresión, y 𝜀 es el error aleatorio.&lt;/p&gt;

&lt;p&gt;La regresión lineal calcula los estimadores de los coeficientes de regresión o simplemente los pesos predichos, denotados con 𝑏₀, 𝑏₁, &amp;hellip;, 𝑏ᵣ. Definen la función de regresión estimada 𝑓 (𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ. Esta función debe capturar las dependencias entre las entradas y salidas lo suficientemente bien.&lt;/p&gt;

&lt;p&gt;La respuesta estimada o pronosticada, 𝑓 (𝐱ᵢ), para cada observación 𝑖 = 1, &amp;hellip;, 𝑛, debe estar lo más cerca posible de la respuesta real correspondiente 𝑦ᵢ. Las diferencias 𝑦ᵢ - 𝑓 (𝐱ᵢ) para todas las observaciones 𝑖 = 1, &amp;hellip;, 𝑛, se denominan residuales. La regresión se trata de determinar los mejores pesos pronosticados, es decir, los pesos correspondientes a los residuos más pequeños.&lt;/p&gt;

&lt;p&gt;Para obtener los mejores pesos, generalmente minimiza la suma de los residuos cuadrados (SSR) para todas las observaciones 𝑖 = 1, &amp;hellip;, 𝑛: SSR = Σᵢ (𝑦ᵢ - 𝑓 (𝐱ᵢ)) ². Este enfoque se llama el método de mínimos cuadrados ordinarios.&lt;/p&gt;

&lt;h2 id=&#34;rendimiento-de-regresión&#34;&gt;Rendimiento de regresión&lt;/h2&gt;

&lt;p&gt;La variación de las respuestas reales 𝑦ᵢ, 𝑖 = 1, &amp;hellip;, 𝑛, se debe en parte a la dependencia de los predictores 𝐱ᵢ. Sin embargo, también hay una variación inherente adicional de la salida.&lt;/p&gt;

&lt;p&gt;El coeficiente de determinación, denotado como 𝑅², le indica qué cantidad de variación en 𝑦 puede explicarse por la dependencia de 𝐱 utilizando el modelo de regresión particular. Mayor 𝑅² indica un mejor ajuste y significa que el modelo puede explicar mejor la variación de la salida con diferentes entradas.&lt;/p&gt;

&lt;p&gt;El valor 𝑅² = 1 corresponde a SSR = 0, es decir, al ajuste perfecto ya que los valores de las respuestas pronosticadas y reales se ajustan completamente entre sí.&lt;/p&gt;

&lt;p&gt;Regresión lineal simple
La regresión lineal simple o de una sola variable es el caso más simple de regresión lineal con una sola variable independiente, 𝐱 = 𝑥.&lt;/p&gt;

&lt;p&gt;La siguiente figura ilustra la regresión lineal simple:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../fig-lin-reg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducción</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/advanced-r/r201-tidy/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/advanced-r/r201-tidy/</guid>
      <description>

&lt;p&gt;La ciencia de datos es una disciplina emocionante que le permite convertir datos sin procesar en comprensión, conocimiento y conocimiento. El objetivo de &amp;ldquo;R for Data Science&amp;rdquo; es ayudarlo a aprender las herramientas más importantes en R que le permitirán hacer ciencia de datos. Después de leer este libro, tendrá las herramientas para abordar una amplia variedad de desafíos de la ciencia de datos, utilizando las mejores partes de R.&lt;/p&gt;

&lt;h2 id=&#34;lo-que-vas-a-aprender&#34;&gt;Lo que vas a aprender&lt;/h2&gt;

&lt;p&gt;La ciencia de datos es un campo enorme, y no hay forma de que puedas dominarlo leyendo un solo libro. El objetivo de este libro es brindarle una base sólida en las herramientas más importantes. Nuestro modelo de las herramientas necesarias en un proyecto típico de ciencia de datos se parece a esto:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;`{r echo = FALSE, out.width =&amp;quot; 75% &amp;quot;}
knitr :: include_graphics (&amp;quot;diagrams / data-science.png&amp;quot;)
&lt;/code&gt; `&lt;/p&gt;

&lt;p&gt;Primero debe &lt;strong&gt;importar&lt;/strong&gt; sus datos en R. Esto generalmente significa que toma los datos almacenados en un archivo, base de datos o API web, y los carga en un marco de datos en R. Si no puede ingresar sus datos en R, puede ¡No hagas ciencia de datos!&lt;/p&gt;

&lt;p&gt;Una vez que haya importado sus datos, es una buena idea ponerlos en orden. Poner en orden sus datos significa almacenarlos en una forma consistente que coincida con la semántica del conjunto de datos con la forma en que se almacenan. En resumen, cuando sus datos están ordenados, cada columna es una variable y cada fila es una observación. Los datos ordenados son importantes porque la estructura consistente le permite enfocar su lucha en preguntas sobre los datos, no en luchar para obtener los datos en la forma correcta para diferentes funciones.&lt;/p&gt;

&lt;p&gt;Una vez que tenga datos ordenados, un primer paso común es &lt;strong&gt;transformar&lt;/strong&gt;. La transformación incluye reducir las observaciones de interés (como todas las personas en una ciudad, o todos los datos del año pasado), crear nuevas variables que son funciones de variables existentes (como calcular la velocidad a partir de la distancia y el tiempo) y calcular un conjunto de resumen estadísticas (como recuentos o medios). Juntos, ordenar y transformar se llaman &lt;strong&gt;wrangling&lt;/strong&gt;, ¡porque obtener sus datos en una forma natural para trabajar a menudo se siente como una pelea!&lt;/p&gt;

&lt;p&gt;Una vez que tenga datos ordenados con las variables que necesita, hay dos motores principales de generación de conocimiento: visualización y modelado. Estos tienen fortalezas y debilidades complementarias, por lo que cualquier análisis real se repetirá entre ellos muchas veces.&lt;/p&gt;

&lt;p&gt;La visualización es una actividad fundamentalmente humana. Una buena visualización le mostrará cosas que no esperaba o planteará nuevas preguntas sobre los datos. Una buena visualización también puede indicar que está haciendo una pregunta incorrecta o que necesita recopilar datos diferentes. Las visualizaciones pueden sorprenderte, pero no escales particularmente bien porque requieren que un humano las interprete.&lt;/p&gt;

&lt;p&gt;Los &lt;strong&gt;Modelos&lt;/strong&gt; son herramientas complementarias para la visualización. Una vez que haya hecho sus preguntas lo suficientemente precisas, puede usar un modelo para responderlas. Los modelos son una herramienta fundamentalmente matemática o computacional, por lo que generalmente escalan bien. Incluso cuando no lo hacen, ¡generalmente es más barato comprar más computadoras que comprar más cerebros! Pero cada modelo hace suposiciones y, por su propia naturaleza, un modelo no puede cuestionar sus propias suposiciones. Eso significa que un modelo no puede sorprenderte fundamentalmente.&lt;/p&gt;

&lt;p&gt;El último paso de la ciencia de datos es &lt;strong&gt;comunicación&lt;/strong&gt;, una parte absolutamente crítica de cualquier proyecto de análisis de datos. No importa qué tan bien sus modelos y visualización lo hayan llevado a comprender los datos, a menos que también pueda comunicar sus resultados a otros.&lt;/p&gt;

&lt;p&gt;Alrededor de todas estas herramientas está &lt;strong&gt;programación&lt;/strong&gt;. La programación es una herramienta transversal que utiliza en cada parte del proyecto. No es necesario ser un programador experto para ser un científico de datos, pero aprender más sobre programación vale la pena porque convertirse en un mejor programador le permite automatizar tareas comunes y resolver nuevos problemas con mayor facilidad.&lt;/p&gt;

&lt;p&gt;Utilizará estas herramientas en cada proyecto de ciencia de datos, pero para la mayoría de los proyectos no son suficientes. Hay una regla aproximada de 80-20 en juego; puede abordar aproximadamente el 80% de cada proyecto utilizando las herramientas que aprenderá en este libro, pero necesitará otras herramientas para abordar el 20% restante. A lo largo de este libro, le indicaremos los recursos donde puede obtener más información.&lt;/p&gt;

&lt;h2 id=&#34;cómo-está-organizado-este-libro&#34;&gt;Cómo está organizado este libro&lt;/h2&gt;

&lt;p&gt;La descripción anterior de las herramientas de la ciencia de datos está organizada de manera aproximada de acuerdo con el orden en que las usa en un análisis (aunque, por supuesto, las repetirá varias veces). En nuestra experiencia, sin embargo, esta no es la mejor manera de aprenderlos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Comenzar con la ingesta de datos y el orden es subóptimo porque el 80% del tiempo
  es rutinario y aburrido, y el otro 20% del tiempo es extraño y
  frustrante. ¡Ese es un mal lugar para comenzar a aprender un nuevo tema! En lugar,
  comenzaremos con la visualización y transformación de datos que ya han sido
  importado y ordenado. De esa manera, cuando ingiere y ordena sus propios datos, su
  la motivación se mantendrá alta porque sabes que el dolor lo vale.
  &lt;/li&gt;
&lt;li&gt;Algunos temas se explican mejor con otras herramientas. Por ejemplo, creemos que
  Es más fácil entender cómo funcionan los modelos si ya conoce
  visualización, datos ordenados y programación.
  &lt;/li&gt;
&lt;li&gt;Las herramientas de programación no son necesariamente interesantes por derecho propio,
  pero le permiten abordar problemas considerablemente más desafiantes. Bien
  darle una selección de herramientas de programación en el medio del libro, y
  entonces verás cómo se pueden combinar con las herramientas de ciencia de datos para abordar
  Problemas interesantes de modelado.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dentro de cada capítulo, tratamos de seguir un patrón similar: comience con algunos ejemplos motivadores para que pueda ver la imagen más grande y luego profundice en los detalles. Cada sección del libro se combina con ejercicios para ayudarlo a practicar lo que ha aprendido. Si bien es tentador saltarse los ejercicios, no hay mejor manera de aprender que practicar en problemas reales.&lt;/p&gt;

&lt;h2 id=&#34;lo-que-no-aprenderás&#34;&gt;Lo que no aprenderás&lt;/h2&gt;

&lt;p&gt;Hay algunos temas importantes que este libro no cubre. Creemos que es importante mantenerse centrado sin piedad en lo esencial para que pueda comenzar a trabajar lo más rápido posible. Eso significa que este libro no puede cubrir todos los temas importantes.&lt;/p&gt;

&lt;h3 id=&#34;big-data&#34;&gt;Big data&lt;/h3&gt;

&lt;p&gt;Este libro se centra orgullosamente en pequeños conjuntos de datos en memoria. Este es el lugar correcto para comenzar porque no puede abordar big data a menos que tenga experiencia con datos pequeños. Las herramientas que aprende en este libro manejarán fácilmente cientos de megabytes de datos, y con un poco de cuidado, generalmente puede usarlas para trabajar con 1-2 Gb de datos. Si trabaja habitualmente con datos más grandes (10-100 Gb, por ejemplo), debe obtener más información sobre &lt;a href=&#34;https://github.com/Rdatatable/data.table&#34; target=&#34;_blank&#34;&gt;data.table&lt;/a&gt;. Este libro no enseña data.table porque tiene una interfaz muy concisa que hace que sea más difícil de aprender ya que ofrece menos claves lingüísticas. Pero si está trabajando con datos de gran tamaño, la rentabilidad del rendimiento vale la pena el esfuerzo adicional requerido para aprenderlo.&lt;/p&gt;

&lt;p&gt;Si sus datos son más grandes que esto, considere cuidadosamente si su problema de big data podría ser un pequeño problema de datos disfrazado. Si bien los datos completos pueden ser grandes, a menudo los datos necesarios para responder una pregunta específica son pequeños. Es posible que pueda encontrar un subconjunto, una submuestra o un resumen que se ajuste a la memoria y que aún le permita responder la pregunta que le interesa. El desafío aquí es encontrar los datos pequeños correctos, que a menudo requieren mucha iteración.&lt;/p&gt;

&lt;p&gt;Otra posibilidad es que su problema de big data sea en realidad una gran cantidad de problemas de data pequeña. Cada problema individual puede caber en la memoria, pero tiene millones de ellos. Por ejemplo, es posible que desee ajustar un modelo a cada persona en su conjunto de datos. Eso sería trivial si solo tuvieras 10 o 100 personas, pero en cambio tienes un millón. Afortunadamente, cada problema es independiente de los demás (una configuración que a veces se llama vergonzosamente paralela), por lo que solo necesita un sistema (como Hadoop o Spark) que le permita enviar diferentes conjuntos de datos a diferentes computadoras para su procesamiento. Una vez que haya descubierto cómo responder la pregunta para un solo subconjunto utilizando las herramientas descritas en este libro, aprenderá nuevas herramientas como sparklyr, rhipe y ddr para resolverlo para el conjunto de datos completo.&lt;/p&gt;

&lt;h3 id=&#34;python-julia-y-amigos&#34;&gt;Python, Julia y amigos&lt;/h3&gt;

&lt;p&gt;En este libro, no aprenderá nada sobre Python, Julia o cualquier otro lenguaje de programación útil para la ciencia de datos. Esto no es porque pensemos que estas herramientas son malas. ¡Ellos no están! Y en la práctica, la mayoría de los equipos de ciencia de datos usan una combinación de lenguajes, a menudo al menos R y Python.&lt;/p&gt;

&lt;p&gt;Sin embargo, creemos firmemente que es mejor dominar una herramienta a la vez. Mejorará más rápido si bucea profundamente, en lugar de extenderse poco a poco sobre muchos temas. Esto no significa que solo deba saber una cosa, solo que generalmente aprenderá más rápido si se apega a una cosa a la vez. Debes esforzarte por aprender cosas nuevas a lo largo de tu carrera, pero asegúrate de que tu comprensión sea sólida antes de pasar a la siguiente cosa interesante.&lt;/p&gt;

&lt;p&gt;Creemos que R es un gran lugar para comenzar su viaje de ciencia de datos porque es un entorno diseñado desde cero para apoyar la ciencia de datos. R no es solo un lenguaje de programación, sino que también es un entorno interactivo para hacer ciencia de datos. Para apoyar la interacción, R es un lenguaje mucho más flexible que muchos de sus pares. Esta flexibilidad viene con sus desventajas, pero la gran ventaja es lo fácil que es desarrollar gramáticas adaptadas para partes específicas del proceso de ciencia de datos. Estos mini idiomas lo ayudan a pensar en problemas como científico de datos, al tiempo que respaldan una interacción fluida entre su cerebro y la computadora.&lt;/p&gt;

&lt;h3 id=&#34;datos-no-rectangulares&#34;&gt;Datos no rectangulares&lt;/h3&gt;

&lt;p&gt;Este libro se enfoca exclusivamente en datos rectangulares: colecciones de valores que están asociados con una variable y una observación. Hay muchos conjuntos de datos que no encajan naturalmente en este paradigma: incluyendo imágenes, sonidos, árboles y texto. Pero los marcos de datos rectangulares son extremadamente comunes en la ciencia y la industria, y creemos que son un gran lugar para comenzar su viaje de ciencia de datos.&lt;/p&gt;

&lt;h3 id=&#34;confirmación-de-hipótesis&#34;&gt;Confirmación de hipótesis&lt;/h3&gt;

&lt;p&gt;Es posible dividir el análisis de datos en dos campos: generación de hipótesis y confirmación de hipótesis (a veces llamado análisis confirmatorio). El objetivo de este libro es descaradamente la generación de hipótesis o la exploración de datos. Aquí observará profundamente los datos y, en combinación con el conocimiento de su materia, generará muchas hipótesis interesantes para ayudar a explicar por qué los datos se comportan de la manera en que lo hacen. Evalúa las hipótesis de manera informal, utilizando su escepticismo para desafiar los datos de múltiples maneras.&lt;/p&gt;

&lt;p&gt;El complemento de la generación de hipótesis es la confirmación de hipótesis. La confirmación de la hipótesis es difícil por dos razones:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Necesita un modelo matemático preciso para generar datos falsificables
    predicciones Esto a menudo requiere una considerable sofisticación estadística.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Solo puede usar una observación una vez para confirmar una hipótesis. Tan pronto como
    lo usa más de una vez que vuelve a hacer análisis exploratorios.
    Esto significa hacer una confirmación de hipótesis que necesita &amp;ldquo;preregistrarse&amp;rdquo;
    (escriba de antemano) su plan de análisis, y no se desvíe de él
    incluso cuando has visto los datos. Hablaremos un poco sobre algunos
    estrategias que puede usar para facilitar esto en &lt;a href=&#34;# model-intro&#34;&gt;modelado&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Es común pensar en el modelado como una herramienta para la confirmación de hipótesis y la visualización como una herramienta para la generación de hipótesis. Pero esa es una falsa dicotomía: los modelos a menudo se usan para exploración, y con un poco de cuidado puede usar la visualización para confirmar. La diferencia clave es con qué frecuencia mira cada observación: si mira solo una vez, es una confirmación; si miras más de una vez, es exploración.&lt;/p&gt;

&lt;h2 id=&#34;prerrequisitos&#34;&gt;Prerrequisitos&lt;/h2&gt;

&lt;p&gt;Hemos hecho algunas suposiciones sobre lo que ya sabes para aprovechar al máximo este libro. En general, debe tener conocimientos numéricos y es útil si ya tiene experiencia en programación. Si nunca ha programado antes, es posible que Garrett &lt;a href=&#34;http://amzn.com/1449359019&#34; target=&#34;_blank&#34;&gt;Hands on Programming with R&lt;/a&gt; de Garrett sea un complemento útil de este libro.&lt;/p&gt;

&lt;p&gt;Hay cuatro cosas que necesita para ejecutar el código en este libro: R, RStudio, una colección de paquetes R llamada &lt;strong&gt;tidyverse&lt;/strong&gt;, y un puñado de otros paquetes. Los paquetes son las unidades fundamentales del código R reproducible. Incluyen funciones reutilizables, la documentación que describe cómo usarlas y datos de muestra.&lt;/p&gt;

&lt;h3 id=&#34;r&#34;&gt;R&lt;/h3&gt;

&lt;p&gt;Para descargar R, vaya a CRAN, el ** c ** omprehensive ** R ** ** a ** rchive ** n ** etwork. CRAN se compone de un conjunto de servidores espejo distribuidos en todo el mundo y se utiliza para distribuir paquetes R y R. No intente elegir un espejo que esté cerca de usted: en su lugar, use el espejo en la nube, &lt;a href=&#34;https://cloud.r-project.org&#34; target=&#34;_blank&#34;&gt;https://cloud.r-project.org&lt;/a&gt;, que automáticamente lo resuelve por usted.&lt;/p&gt;

&lt;p&gt;Una nueva versión principal de R sale una vez al año, y hay 2-3 lanzamientos menores cada año. Es una buena idea actualizar regularmente. La actualización puede ser una molestia, especialmente para las versiones principales, que requieren que reinstales todos tus paquetes, pero posponerlo solo lo empeora.&lt;/p&gt;

&lt;h3 id=&#34;rstudio&#34;&gt;RStudio&lt;/h3&gt;

&lt;p&gt;RStudio es un entorno de desarrollo integrado, o IDE, para la programación R. Descargue e instálelo desde &lt;a href=&#34;http://www.rstudio.com/download&#34; target=&#34;_blank&#34;&gt;http://www.rstudio.com/download&lt;/a&gt;. RStudio se actualiza un par de veces al año. Cuando hay una nueva versión disponible, RStudio se lo informará. Es una buena idea actualizar regularmente para que pueda aprovechar las últimas y mejores funciones. Para este libro, asegúrese de tener RStudio 1.0.0.&lt;/p&gt;

&lt;p&gt;Cuando inicie RStudio, verá dos regiones clave en la interfaz:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;`{r echo = FALSE, out.width =&amp;quot; 75% &amp;quot;}
knitr :: include_graphics (&amp;quot;diagrams / rstudio-console.png&amp;quot;)
&lt;/code&gt; `&lt;/p&gt;

&lt;p&gt;Por ahora, todo lo que necesita saber es que escribe el código R en el panel de la consola y presiona Intro para ejecutarlo. ¡Aprenderás más a medida que avanzamos!&lt;/p&gt;

&lt;h3 id=&#34;el-tidyverse&#34;&gt;El tidyverse&lt;/h3&gt;

&lt;p&gt;También necesitará instalar algunos paquetes de R. Un R &lt;strong&gt;paquete&lt;/strong&gt; es una colección de funciones, datos y documentación que amplía las capacidades de la base R. El uso de paquetes es clave para el uso exitoso de R. La mayoría de los paquetes que aprenderá en este libro son parte de llamado tidyverse. Los paquetes en el tidyverse comparten una filosofía común de datos y programación R, y están diseñados para trabajar juntos de forma natural.&lt;/p&gt;

&lt;p&gt;Puede instalar el tidyverse completo con una sola línea de código:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;`{r, eval = FALSO}
install.packages (&amp;quot;tidyverse&amp;quot;)
&lt;/code&gt; `&lt;/p&gt;

&lt;p&gt;En su propia computadora, escriba esa línea de código en la consola y luego presione Intro para ejecutarla. R descargará los paquetes de CRAN y los instalará en su computadora. Si tiene problemas para instalar, asegúrese de estar conectado a Internet y de que &lt;a href=&#34;https://cloud.r-project.org/&#34; target=&#34;_blank&#34;&gt;https://cloud.r-project.org/&lt;/a&gt; no esté bloqueado por su firewall o proxy.&lt;/p&gt;

&lt;p&gt;No podrá utilizar las funciones, objetos y archivos de ayuda en un paquete hasta que lo cargue con &lt;code&gt;library ()&lt;/code&gt;. Una vez que haya instalado un paquete, puede cargarlo con la función &lt;code&gt;library ()&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;{r}
biblioteca (tidyverse)
&lt;/code&gt; `&lt;/p&gt;

&lt;p&gt;Esto le indica que tidyverse está cargando los paquetes ggplot2, tibble, tidyr, readr, purrr y dplyr. Estos se consideran el &lt;strong&gt;core&lt;/strong&gt; del tidyverse porque los usará en casi todos los análisis.&lt;/p&gt;

&lt;p&gt;Los paquetes en el tidyverse cambian con bastante frecuencia. Puede ver si hay actualizaciones disponibles y, opcionalmente, instalarlas ejecutando &lt;code&gt;tidyverse_update ()&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;otros-paquetes&#34;&gt;Otros paquetes&lt;/h3&gt;

&lt;p&gt;Hay muchos otros paquetes excelentes que no forman parte del tidyverse, porque resuelven problemas en un dominio diferente o están diseñados con un conjunto diferente de principios subyacentes. Esto no los hace mejores o peores, solo diferentes. En otras palabras, el complemento al tidyverse no es el messyverse, sino muchos otros universos de paquetes interrelacionados. A medida que aborde más proyectos de ciencia de datos con R, aprenderá nuevos paquetes y nuevas formas de pensar sobre los datos.&lt;/p&gt;

&lt;p&gt;En este libro usaremos tres paquetes de datos externos al tidyverse:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;`{r, eval = FALSO}
install.packages (c (&amp;quot;nycflights13&amp;quot;, &amp;quot;gapminder&amp;quot;, &amp;quot;Lahman&amp;quot;))
&lt;/code&gt; `&lt;/p&gt;

&lt;p&gt;Estos paquetes proporcionan datos sobre vuelos de aerolíneas, desarrollo mundial y béisbol que usaremos para ilustrar ideas clave de ciencia de datos.&lt;/p&gt;

&lt;h2 id=&#34;ejecutando-código-r&#34;&gt;Ejecutando código R&lt;/h2&gt;

&lt;p&gt;La sección anterior le mostró un par de ejemplos de ejecución de código R. El código en el libro se ve así:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;`{r, eval = TRUE}
1 + 2
#&amp;gt; [1] 3
&lt;/code&gt; `&lt;/p&gt;

&lt;p&gt;Si ejecuta el mismo código en su consola local, se verá así:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;`
&amp;gt; 1 + 2
[1] 3
&lt;/code&gt; `&lt;/p&gt;

&lt;p&gt;Hay dos diferencias principales. En su consola, escribe después del &lt;code&gt;&amp;gt;&lt;/code&gt;, llamado &lt;strong&gt;prompt&lt;/strong&gt;; No mostramos el aviso en el libro. En el libro, la salida se comenta con &lt;code&gt;#&amp;gt;&lt;/code&gt;; en tu consola aparece directamente después de tu código. Estas dos diferencias significan que si está trabajando con una versión electrónica del libro, puede copiar fácilmente el código del libro y en la consola.&lt;/p&gt;

&lt;p&gt;A lo largo del libro usamos un conjunto consistente de convenciones para referirnos al código:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Las funciones están en una fuente de código y seguidas de paréntesis, como &lt;code&gt;sum ()&lt;/code&gt;,
  o &lt;code&gt;mean ()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Otros objetos R (como datos o argumentos de funciones) están en una fuente de código,
  sin paréntesis, como &lt;code&gt;vuelos&lt;/code&gt; o&lt;code&gt;x&lt;/code&gt;.
  &lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Si queremos dejar en claro de qué paquete proviene un objeto, usaremos
  el nombre del paquete seguido de dos puntos, como &lt;code&gt;dplyr :: mutate ()&lt;/code&gt;, o
  &lt;code&gt;nycflights13 :: vuelos&lt;/code&gt;. Este también es un código R válido.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;obteniendo-ayuda-y-aprendiendo-más&#34;&gt;Obteniendo ayuda y aprendiendo más&lt;/h2&gt;

&lt;p&gt;Este libro no es una isla; no existe un recurso único que le permita dominar R. Cuando comience a aplicar las técnicas descritas en este libro a sus propios datos, pronto encontrará preguntas que no contesto. Esta sección describe algunos consejos sobre cómo obtener ayuda y para ayudarlo a seguir aprendiendo.&lt;/p&gt;

&lt;p&gt;Si te quedas atascado, comienza con Google. Normalmente, agregar &amp;ldquo;R&amp;rdquo; a una consulta es suficiente para restringirla a resultados relevantes: si la búsqueda no es útil, a menudo significa que no hay resultados específicos de R disponibles. Google es particularmente útil para mensajes de error. Si recibe un mensaje de error y no tiene idea de lo que significa, intente buscarlo en Google. Lo más probable es que alguien más haya estado confundido en el pasado, y habrá ayuda en algún lugar de la web. (Si el mensaje de error no está en inglés, ejecute &lt;code&gt;Sys.setenv (LANGUAGE =&amp;quot; en &amp;quot;)&lt;/code&gt; y vuelva a ejecutar el código; es más probable que encuentre ayuda para los mensajes de error en inglés).&lt;/p&gt;

&lt;p&gt;Si Google no ayuda, intente &lt;a href=&#34;http://stackoverflow.com&#34; target=&#34;_blank&#34;&gt;stackoverflow&lt;/a&gt;. Comience por pasar un poco de tiempo buscando una respuesta existente, incluyendo &lt;code&gt;[R]&lt;/code&gt; para restringir su búsqueda a preguntas y respuestas que usen R. Si no encuentra nada útil, prepare un ejemplo reproducible mínimo o &lt;strong&gt;reprex&lt;/strong&gt;. Un buen reprex hace que sea más fácil para otras personas ayudarte, y a menudo descubrirás el problema tú mismo mientras lo haces.&lt;/p&gt;

&lt;p&gt;Hay tres cosas que debe incluir para que su ejemplo sea reproducible: paquetes, datos y código requeridos.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;** Los paquetes ** deben cargarse en la parte superior del script, por lo que es fácil
    ver cuáles necesita el ejemplo. Este es un buen momento para comprobar que estás
    usando la última versión de cada paquete; es posible que hayas descubierto
    Un error que se ha solucionado desde que instaló el paquete. Para paquetes
    en tidyverse, la forma más fácil de verificar es ejecutar &lt;code&gt;tidyverse_update ()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;La forma más fácil de incluir ** datos ** en una pregunta es usar &lt;code&gt;dput ()&lt;/code&gt; para
    generar el código R para recrearlo. Por ejemplo, para recrear los &lt;code&gt;mtcars&lt;/code&gt;
    conjunto de datos en R, realizaría los siguientes pasos:
  
    1. Ejecute &lt;code&gt;dput (mtcars)&lt;/code&gt; en R
    2. Copie la salida
    3. En mi script reproducible, escriba &lt;code&gt;mtcars &amp;lt;-&lt;/code&gt; y luego pegue.
    
    Intenta encontrar el subconjunto más pequeño de tus datos que aún revela
    el problema.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dedica un poco de tiempo a asegurarte de que tu ** código ** sea fácil para otros
    leer:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;    * Asegúrese de haber usado espacios y que sus nombres de variables sean concisos, aún
      informativo.
    
    * Use comentarios para indicar dónde radica su problema.
    
    * Haga todo lo posible para eliminar todo lo que no esté relacionado con el problema.
      Cuanto más corto sea su código, más fácil será comprenderlo y
      Más fácil es arreglarlo.&lt;/p&gt;

&lt;p&gt;Termine comprobando que realmente ha hecho un ejemplo reproducible comenzando una nueva sesión de R y copiando y pegando su script.&lt;/p&gt;

&lt;p&gt;También debe pasar algún tiempo preparándose para resolver los problemas antes de que ocurran. Invertir un poco de tiempo en aprender R cada día dará buenos resultados a largo plazo. Una forma es seguir lo que Hadley, Garrett y todos los demás en RStudio están haciendo en el &lt;a href=&#34;https://blog.rstudio.org&#34; target=&#34;_blank&#34;&gt;blog RStudio&lt;/a&gt;. Aquí es donde publicamos anuncios sobre nuevos paquetes, nuevas funciones de IDE y cursos presenciales. También puede seguir a Hadley (&lt;a href=&#34;https://twitter.com/hadleywickham&#34; target=&#34;_blank&#34;&gt;\ @hadleywickham&lt;/a&gt;) o Garrett (&lt;a href=&#34;https://twitter.com/statgarrett&#34; target=&#34;_blank&#34;&gt;\ @statgarrett&lt;/a&gt;) en Twitter, o seguir &lt;a href=&#34;https://twitter.com/rstudiotips&#34; target=&#34;_blank&#34;&gt;\ @rstudiotips&lt;/a&gt; para mantenerse al día con las nuevas funciones en el IDE.&lt;/p&gt;

&lt;p&gt;Para mantenerse al día con la comunidad R en general, recomendamos leer &lt;a href=&#34;http://www.r-bloggers.com&#34; target=&#34;_blank&#34;&gt;http://www.r-bloggers.com&lt;/a&gt;: agrega más de 500 blogs sobre R de todo el mundo. Si eres un usuario activo de Twitter, sigue el hashtag &lt;code&gt;# rstats&lt;/code&gt;. Twitter es una de las herramientas clave que Hadley utiliza para mantenerse al día con los nuevos desarrollos en la comunidad.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started with SQL y Big Query</title>
      <link>https://www.marcusrb.com/en/courses/business-analytics/intro-sql/sql101-0-start/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/business-analytics/intro-sql/sql101-0-start/</guid>
      <description>

&lt;h2 id=&#34;introducción&#34;&gt;Introducción&lt;/h2&gt;

&lt;p&gt;El lenguaje de consulta estructurado, o SQL, es el lenguaje de programación utilizado con las bases de datos, y es una habilidad importante para cualquier científico de datos. En este curso, desarrollará sus habilidades de SQL utilizando BigQuery, un servicio web que le permite aplicar SQL a grandes conjuntos de datos.&lt;/p&gt;

&lt;p&gt;En esta lección, aprenderá los conceptos básicos para acceder y examinar los conjuntos de datos de BigQuery. Después de que tenga una idea de estos conceptos básicos, volveremos a desarrollar sus habilidades de SQL.&lt;/p&gt;

&lt;h3 id=&#34;tus-primeros-comandos-bigquery&#34;&gt;Tus primeros comandos BigQuery&lt;/h3&gt;

&lt;p&gt;Para usar BigQuery, importaremos el paquete de Python a continuación:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.cloud import bigquery
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El primer paso en el flujo de trabajo es crear un objeto &lt;em&gt;Client&lt;/em&gt;. Como pronto verá, este objeto &lt;em&gt;Client&lt;/em&gt; desempeñará un papel central en la recuperación de información de los conjuntos de datos de BigQuery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a &amp;quot;Client&amp;quot; object
client = bigquery.Client()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trabajaremos con un conjunto de datos de publicaciones en &lt;a href=&#34;https://news.ycombinator.com/&#34; target=&#34;_blank&#34;&gt;Hacker News&lt;/a&gt;, un sitio web que se centra en noticias de informática y seguridad cibernética.&lt;/p&gt;

&lt;p&gt;En BigQuery, cada conjunto de datos está contenido en un proyecto correspondiente. En este caso, nuestro conjunto de datos &lt;em&gt;hacker_news&lt;/em&gt; está contenido en el proyecto &lt;em&gt;bigquery-public-data&lt;/em&gt;. Para acceder al conjunto de datos,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Comenzamos construyendo una referencia al conjunto de datos con el método &lt;em&gt;dataset()&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;A continuación, utilizamos el método &lt;em&gt;get_dataset()&lt;/em&gt;, junto con la referencia que acabamos de construir, para obtener el conjunto de datos.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Construct a reference to the &amp;quot;hacker_news&amp;quot; dataset
dataset_ref = client.dataset(&amp;quot;hacker_news&amp;quot;, project=&amp;quot;bigquery-public-data&amp;quot;)

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cada conjunto de datos es solo una colección de tablas. Puede pensar en un conjunto de datos como un archivo de hoja de cálculo que contiene varias tablas, todas compuestas de filas y columnas.&lt;/p&gt;

&lt;p&gt;Usamos el método &lt;em&gt;list_tables()&lt;/em&gt; para listar las tablas en el conjunto de datos.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List all the tables in the &amp;quot;hacker_news&amp;quot; dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there are four!)
for table in tables:  
    print(table.table_id)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;OUTPUT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;comments
full
full_201510
stories
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De forma similar a cómo obtuvimos un conjunto de datos, podemos obtener una tabla. En la celda de código a continuación, buscamos la tabla &lt;em&gt;full&lt;/em&gt; en el conjunto de datos &lt;em&gt;hacker_news&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Construct a reference to the &amp;quot;full&amp;quot; table
table_ref = dataset_ref.table(&amp;quot;full&amp;quot;)

# API request - fetch the table
table = client.get_table(table_ref)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la siguiente sección, explorará el contenido de esta tabla con más detalle. Por ahora, tómese el tiempo de usar la imagen a continuación para consolidar lo que ha aprendido hasta ahora.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../biYqbUB.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;esquema-de-la-tabla&#34;&gt;Esquema de la tabla&lt;/h2&gt;

&lt;p&gt;La estructura de una tabla se llama esquema. Necesitamos entender el esquema de una tabla para extraer efectivamente los datos que queremos.&lt;/p&gt;

&lt;p&gt;En este ejemplo, investigaremos la tabla completa &lt;em&gt;full&lt;/em&gt; que obtuvimos anteriormente.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print information on all the columns in the &amp;quot;full&amp;quot; table in the &amp;quot;hacker_news&amp;quot; dataset
table.schema
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;OUTPUT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[SchemaField(&#39;by&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &amp;quot;The username of the item&#39;s author.&amp;quot;, ()),
 SchemaField(&#39;score&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Story score&#39;, ()),
 SchemaField(&#39;time&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Unix time&#39;, ()),
 SchemaField(&#39;timestamp&#39;, &#39;TIMESTAMP&#39;, &#39;NULLABLE&#39;, &#39;Timestamp for the unix time&#39;, ()),
 SchemaField(&#39;title&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story title&#39;, ()),
 SchemaField(&#39;type&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Type of details (comment, comment_ranking, poll, story, job, pollopt)&#39;, ()),
 SchemaField(&#39;url&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story url&#39;, ()),
 SchemaField(&#39;text&#39;, &#39;STRING&#39;, &#39;NULLABLE&#39;, &#39;Story or comment text&#39;, ()),
 SchemaField(&#39;parent&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Parent comment ID&#39;, ()),
 SchemaField(&#39;deleted&#39;, &#39;BOOLEAN&#39;, &#39;NULLABLE&#39;, &#39;Is deleted?&#39;, ()),
 SchemaField(&#39;dead&#39;, &#39;BOOLEAN&#39;, &#39;NULLABLE&#39;, &#39;Is dead?&#39;, ()),
 SchemaField(&#39;descendants&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Number of story or poll descendants&#39;, ()),
 SchemaField(&#39;id&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &amp;quot;The item&#39;s unique id.&amp;quot;, ()),
 SchemaField(&#39;ranking&#39;, &#39;INTEGER&#39;, &#39;NULLABLE&#39;, &#39;Comment ranking&#39;, ())]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cada &lt;em&gt;SchemaField&lt;/em&gt; nos informa sobre una columna específica (a la que también nos referimos como un campo &lt;strong&gt;field&lt;/strong&gt;). En orden, la información es:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;El nombre de la columna.&lt;/li&gt;
&lt;li&gt;El tipo de campo (o tipo de datos) en la columna&lt;/li&gt;
&lt;li&gt;El modo de la columna (&amp;lsquo;NULLABLE&amp;rsquo; significa que una columna permite valores NULL y es el valor predeterminado)&lt;/li&gt;
&lt;li&gt;Una descripción de los datos en esa columna.&lt;/li&gt;
&lt;li&gt;El primer campo tiene el SchemaField:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;SchemaField (&amp;lsquo;by&amp;rsquo;, &amp;lsquo;string&amp;rsquo;, &amp;lsquo;NULLABLE&amp;rsquo;, &amp;ldquo;El nombre de usuario del autor del elemento&amp;rdquo;, ()&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Esto nos dice:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;el campo (o columna) es llamado por&lt;/li&gt;
&lt;li&gt;los datos en este campo son cadenas,&lt;/li&gt;
&lt;li&gt;Se permiten valores NULL y&lt;/li&gt;
&lt;li&gt;Contiene los nombres de usuario correspondientes al autor de cada elemento.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Podemos usar el método &lt;em&gt;list_rows()&lt;/em&gt; para verificar solo las primeras cinco líneas de la tabla completa &lt;em&gt;full&lt;/em&gt; para asegurarnos de que esto sea correcto. (A veces las bases de datos tienen descripciones desactualizadas, por lo que es bueno verificarlo). Esto devuelve un objeto BigQuery &lt;em&gt;RowIterator&lt;/em&gt; que se puede convertir rápidamente en un DataFrame de pandas con el método &lt;em&gt;to_dataframe()&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview the first five lines of the &amp;quot;full&amp;quot; table
client.list_rows(table, max_results=5).to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El método &lt;em&gt;list_rows()&lt;/em&gt; también nos permitirá ver solo la información en una columna específica. Si queremos ver las primeras cinco entradas en la columna por, por ejemplo, ¡podemos hacerlo!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview the first five entries in the &amp;quot;by&amp;quot; column of the &amp;quot;full&amp;quot; table
client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EXERCISE (Exercise_ Getting Started With SQL and BigQuery)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Instalación de R</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/instalacion-r/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/instalacion-r/</guid>
      <description>

&lt;p&gt;R es un programa de código abierto del que existen varias distribuciones, que se pueden descargar libremente. Veamos los procedimientos para instalarlo y actualizarlo.&lt;/p&gt;

&lt;h3 id=&#34;download&#34;&gt;Download&lt;/h3&gt;

&lt;p&gt;Primero, por supuesto, debe descargar el paquete básico (70 Mb para la versión 3.3), elegir un &amp;ldquo;espejo&amp;rdquo; en &lt;a href=&#34;http://cran.r-project.org/mirrors.html&#34; target=&#34;_blank&#34;&gt;http://cran.r-project.org/mirrors.html&lt;/a&gt;, o ir directamente a:&lt;/p&gt;

&lt;p&gt;espejo para sistemas Windows;
espejos para sistemas Mac-Os;
réplicas para sistemas Linux (Debian, Redhat, Suse, Ubuntu).
Recientemente, Microsoft ha puesto a disposición una versión de R, de código abierto, gratuita y que tiene algunas características adicionales que facilitan la reproducibilidad de la búsqueda y el cálculo en paralelo (&lt;a href=&#34;https://mran.microsoft.com/open&#34; target=&#34;_blank&#34;&gt;https://mran.microsoft.com/open&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;r-portátil&#34;&gt;R portátil&lt;/h3&gt;

&lt;p&gt;En Sourgeforce.net está disponible una versión portátil de R, que se puede instalar, con todas sus características, en un soporte de memoria externa (disco duro externo, unidad flash USB, etc.). Esta versión de R se puede integrar en la suite PortableApps.&lt;/p&gt;

&lt;h2 id=&#34;instalación&#34;&gt;Instalación&lt;/h2&gt;

&lt;p&gt;En Windows, el software se instala ejecutando el archivo ejecutable (exe) descargado.&lt;/p&gt;

&lt;p&gt;Para usar RCommander y RExcel, es preferible personalizar la instalación:&lt;/p&gt;

&lt;p&gt;Ejecute el archivo ejecutable en modo administrador (haga clic en el archivo con el botón derecho del mouse y elija esta opción);
Continúe con la instalación hasta la siguiente pantalla: ¿Desea configurar las opciones de arranque? Elige &amp;ldquo;Sí&amp;rdquo;:
 |
Modo de visualización: elija SDI (varias ventanas)
Estilo de ayuda: HTML (por defecto)
Selección de procesos adicionales: almacena el número de versión en el registro:
 |&lt;/p&gt;

&lt;h2 id=&#34;actualización&#34;&gt;Actualización&lt;/h2&gt;

&lt;p&gt;La forma más fácil de actualizar R y mantener bibliotecas es:&lt;/p&gt;

&lt;p&gt;instale la nueva versión (se instalará en una nueva carpeta)
copie los paquetes instalados desde la carpeta de la biblioteca anterior a la carpeta correspondiente de la nueva instalación
iniciar R en modo administrador (en Windows Vista y Windows7)
ejecutar - dentro de la nueva R - el comando
paquetes de actualización (checkBuilt = TRUE, ask = FALSE)
desinstale la versión anterior y elimine el directorio anterior.
También puede actualizar R y paquetes con el paquete de instalación.&lt;/p&gt;

&lt;h2 id=&#34;r-en-la-web&#34;&gt;R en la web&lt;/h2&gt;

&lt;p&gt;Finalmente, existe la posibilidad de ejecutar R en línea (con diferentes interfaces), a través de sitios web que proporcionan una instalación de servidor R, como: &lt;a href=&#34;https://www.tutorialspoint.com/execute_r_online.php&#34; target=&#34;_blank&#34;&gt;https://www.tutorialspoint.com/execute_r_online.php&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducción Math for Data Science</title>
      <link>https://www.marcusrb.com/en/courses/data-science/math-data-science/math-intro/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/data-science/math-data-science/math-intro/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Python 101 - Introducción</title>
      <link>https://www.marcusrb.com/en/courses/python/py101/py101-1-intro/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/python/py101/py101-1-intro/</guid>
      <description>

&lt;h1 id=&#34;introducción&#34;&gt;Introducción&lt;/h1&gt;

&lt;p&gt;Este curso cubre las habilidades clave de Python que necesitará para que pueda comenzar a usar Python para la ciencia de datos. El curso es ideal para alguien con experiencia previa en codificación que quiera agregar Python a su repertorio o subir de nivel sus habilidades básicas de Python. (Si es un programador por primera vez, puede consultar estos recursos de aprendizaje &amp;ldquo;Python para no programadores&amp;rdquo;).&lt;/p&gt;

&lt;p&gt;Comenzaremos con una breve descripción general de la sintaxis de Python, la asignación de variables y los operadores aritméticos. Si tiene experiencia previa en Python, puede pasar directamente al ejercicio práctico.&lt;/p&gt;

&lt;p&gt;## Hello, Python!
Python fue nombrado por la compañía de comedia británica Monty Python, por lo que haremos de nuestro primer programa Python un homenaje a su parodia sobre el spam.&lt;/p&gt;

&lt;p&gt;Solo por diversión, intente leer el código a continuación y predecir lo que hará cuando se ejecute. (Si no tienes idea, ¡está bien!)&lt;/p&gt;

&lt;p&gt;Luego haga clic en el botón &amp;ldquo;salida&amp;rdquo; para ver los resultados de nuestro programa.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spam_amount = 0
print(spam_amount)

# Ordering Spam, egg, Spam, Spam, bacon and Spam (4 more servings of Spam)
spam_amount = spam_amount + 4

if spam_amount &amp;gt; 0:
    print(&amp;quot;But I don&#39;t want ANY spam!&amp;quot;)

viking_song = &amp;quot;Spam &amp;quot; * spam_amount
print(viking_song)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0
But I don&#39;t want ANY spam!
Spam Spam Spam Spam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;¡Hay mucho que desempacar aquí! Este programa tonto demuestra muchos aspectos importantes de cómo se ve el código Python y cómo funciona. Revisemos el código de arriba a abajo.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spam_amount = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Asignación de variables: aquí creamos una variable llamada spam_amount y le asignamos el valor de 0 usando =, que se llama operador de asignación.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nota&lt;/strong&gt;: si ha programado en ciertos otros lenguajes (como Java o C ++), puede estar notando algunas cosas que Python no requiere que hagamos aquí:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;no necesitamos &amp;ldquo;declarar&amp;rdquo; spam_amount antes de asignarle&lt;/li&gt;
&lt;li&gt;no necesitamos decirle a Python a qué tipo de valor se referirá spam_amount. De hecho, incluso podemos reasignar spam_amount para referirnos a un tipo diferente de cosas como una cadena o un booleano.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(spam_amount)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Llamadas de función&lt;/strong&gt;: &lt;em&gt;print&lt;/em&gt; es una función de Python que muestra el valor que se le pasa en la pantalla. Llamamos a las funciones poniendo paréntesis después de su nombre y poniendo las entradas (o argumentos) a la función en esos paréntesis.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Ordering Spam, egg, Spam, Spam, bacon and Spam (4 more servings of Spam)
spam_amount = spam_amount + 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La primera línea de arriba es un &lt;strong&gt;comentario&lt;/strong&gt;. En Python, los comentarios comienzan con el símbolo &lt;em&gt;#&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A continuación vemos un ejemplo de reasignación. La reasignación del valor de una variable existente tiene el mismo aspecto que la creación de una variable: todavía utiliza el operador de asignación &lt;em&gt;=&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;En este caso, el valor que estamos asignando a _spam&lt;em&gt;amount&lt;/em&gt; implica una aritmética simple en su valor anterior. Cuando encuentra esta línea, Python evalúa la expresión en el lado derecho de &lt;em&gt;=&lt;/em&gt; (0 + 4 = 4), y luego asigna ese valor a la variable en el lado izquierdo.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if spam_amount &amp;gt; 0:
    print(&amp;quot;But I don&#39;t want ANY spam!&amp;quot;)

viking_song = &amp;quot;Spam Spam Spam&amp;quot;
print(viking_song)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;But I don&#39;t want ANY spam!
Spam Spam Spam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No hablaremos mucho sobre &amp;ldquo;condicionales&amp;rdquo; hasta más tarde, pero, incluso si nunca ha codificado antes, probablemente pueda adivinar lo que hace. Python es apreciado por su legibilidad y simplicidad.&lt;/p&gt;

&lt;p&gt;Observe cómo indicamos qué código pertenece al &lt;em&gt;if. &amp;ldquo;¡Pero no quiero NINGÚN spam!&amp;rdquo;&lt;/em&gt; solo se supone que se imprime si _spam&lt;em&gt;amount&lt;/em&gt; es positivo. Pero el código posterior (como _print (viking&lt;em&gt;song)&lt;/em&gt;) debe ejecutarse sin importar qué. ¿Cómo lo sabemos (y Python)?&lt;/p&gt;

&lt;p&gt;Los dos puntos (:) al final de la línea if indican que se está iniciando un nuevo &amp;ldquo;bloque de código&amp;rdquo;. Las líneas posteriores que están sangradas son parte de ese bloque de código. Algunos otros idiomas usan {llaves &amp;ldquo;para marcar el comienzo y el final de los bloques de código. El uso de espacios en blanco significativos por Python puede ser sorprendente para los programadores que están acostumbrados a otros lenguajes, pero en la práctica puede conducir a un código más coherente y legible que los lenguajes que no imponen sangría de bloques de código.&lt;/p&gt;

&lt;p&gt;Las líneas posteriores que tratan con _viking&lt;em&gt;song&lt;/em&gt; no están sangradas con 4 espacios adicionales, por lo que no forman parte del bloque de código if. Veremos más ejemplos de bloques de código sangrados más adelante cuando definamos funciones y usemos bucles.&lt;/p&gt;

&lt;p&gt;Este fragmento de código también es nuestro primer avistamiento de una cadena &lt;strong&gt;string&lt;/strong&gt; en Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;But I don&#39;t want ANY spam!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;But I don&#39;t want ANY spam!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Las cadenas se pueden marcar con comillas dobles o simples. (Pero debido a que esta cadena en particular contiene un carácter de comillas simples, podríamos confundir a Python tratando de rodearla con comillas simples, a menos que tengamos cuidado).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;viking_song = &amp;quot;Spam &amp;quot; * spam_amount
print(viking_song)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Spam Spam Spam Spam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El operador &lt;em&gt;*&lt;/em&gt; se puede usar para multiplicar dos números (3 * 3 se evalúa como 9), pero de manera bastante divertida, también podemos multiplicar una cadena por un número, para obtener una versión que se ha repetido tantas veces. Python ofrece una serie de trucos descarados y pequeños que ahorran tiempo como este, donde los operadores como * y + tienen un significado diferente según el tipo de cosas a las que se aplican. (El término técnico para esto es &lt;a href=&#34;https://en.wikipedia.org/wiki/Operator_overloading&#34; target=&#34;_blank&#34;&gt;operador de sobrecarga&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&#34;números-y-aritmética-en-python&#34;&gt;Números y aritmética en Python&lt;/h2&gt;

&lt;p&gt;Ya hemos visto un ejemplo de una variable que contiene un número arriba:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spam_amount = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;ldquo;Número&amp;rdquo; es un buen nombre informal para el tipo de cosas, pero si quisiéramos ser más técnicos, podríamos preguntarle a Python cómo describiría el tipo de cosas que es &lt;em&gt;spam_amount&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;type(spam_amount)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Es un &lt;em&gt;int&lt;/em&gt; - abreviatura de entero. Hay otro tipo de número que comúnmente encontramos en Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib import pyplot as plt
%matplotlib inline
import seaborn as sns

df = sns.load_dataset(&#39;iris&#39;)


sns.lmplot(x = &#39;petal_length&#39;, y = &#39;petal_width&#39;, data = df
           , hue = &#39;species&#39;
           , fit_reg = False)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;seaborn.axisgrid.FacetGrid at 0x1a16f2d908&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../output_21_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prácticas 1 - Introducción</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-intro/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-intro/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Da clic a &amp;lsquo;Submit Answer&amp;rsquo; y date cuenta como la consola ejecuta el código de R del editor: la solución 7 aparece como la suma de 3 y 4. El uso más simple de R es como una calculadora y graficadora, pero por supuesto hay mucho, mucho, mucho más!&lt;/li&gt;
&lt;li&gt;Añade una linea que haga el calculo de 6 y 12.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Este es el editor y la parte de abajo es la consola

# El símbolo de numeral # es utilizada para hacer comentarios

# Calcula 3 + 4
3 + 4

# Calcula 6 + 12
6 + 12

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;aritmética-con-r&#34;&gt;Aritmética con R&lt;/h2&gt;

&lt;p&gt;Como vimos el uso más simple que se le puede dar a R es el de una calculadora. Consideremos las siguientes operaciones:&lt;/p&gt;

&lt;p&gt;Adición: +
Resta: -
Multiplicación: *
División: /
Exponenciación: ^
Modulo: %%
Los últimos dos necesitan una breve explicación:&lt;/p&gt;

&lt;p&gt;El operador ^ eleva el número a la izquierda a la potencia a la derecha 3^2 es 9
El módulo (mod) %% calcula el residuo de la división del número a la izquierda por el número a la derecha, por ejemplo 5 mod 3 o 5%%3 es 2.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Escribe 2^5 en el editor para calcular 2 a la quinta potencia.&lt;/li&gt;
&lt;li&gt;Escribe 28 %% 6 para calcular el 28 módulo 6.
Da clic a &amp;lsquo;Submit Answer&amp;rsquo; para ver el resultado en la consola.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Adición
5 + 5

# Resta
5 - 5

# Multiplicación
3 * 5

 # División
(5 + 5) / 2

# Exponenciación
2^5

# Modulo
28 %% 6
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;variables-y-asignaciones&#34;&gt;Variables y asignaciones&lt;/h2&gt;

&lt;p&gt;Un concepto básico en programación es el de variable.&lt;/p&gt;

&lt;p&gt;Una variable nos permite guardar valores (por ejemplo el número 4) o algún objeto (veremos mas adelante de que se trata) en R. Luego puedes acceder al valor guardado en la variable por medio del nombre de la misma.&lt;/p&gt;

&lt;p&gt;Podemos asignar el valor de 4 a la variable mi_variable con el siguiente comando: mi_variable &amp;lt;- 4&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Completa el código en el editor de tal manera que el valor de 42 quede asignado a la variable x. Da clic en &amp;lsquo;Submit Answer&amp;rsquo;. Nota al escribir x en R, se imprime el valor de 42 en la consola.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Asigna 42 a x
x &amp;lt;- 42

# Imprime el valor de la variable x
x

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;variables-y-asignaciones-2&#34;&gt;Variables y asignaciones (2)&lt;/h2&gt;

&lt;p&gt;Supongamos que tienes una canasta con cinco manzanas. Para recordarlo, quizás quieras asignar el número de manzanas en una variable llamada numero_manzanas&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Escribe el siguiente código: numero_manzanas &amp;lt;- 5 para asignar el numero 5 a la variable numero_manzanas.&lt;/li&gt;
&lt;li&gt;Escribe: numero_manzanas abajo del segundo comentario.
Da clic a &amp;lsquo;Submit Answer&amp;rsquo;, ve la consola: el numero que sea ha impreso es 5.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Asigna el valor de 5 a la variable numero_manzanas
numero_manzanas &amp;lt;- 5


# Imprime el valor de la variable numero_manzanas
numero_manzanas

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;variables-y-asignaciones-3&#34;&gt;Variables y asignaciones (3)&lt;/h2&gt;

&lt;p&gt;Supongamos que ahora tienes 6 naranjas. De nuevo, para no olvidarlo se te ocurre crear una variable llamada numero_naranjas y asignar el valor de 6 a esa variable. Ahora podemos a empezar a utilizar las variables creadas para hacer algo con ellas. Usemos R para saber el número total de frutas con las que contamos, pidamos que haga la cuenta por nosotros: numero_manzanas + numero_naranjas. Al leer este codigo nos damos cuenta de la importancia de dar nombres útiles a nuestras variables.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Asigna a numero_naranjas el valor de 6.&lt;/li&gt;
&lt;li&gt;R permite combinar estas variables numero_manzanas y numero_naranjas en una nueva variable numero_frutas. Crea la variable numero_frutas y asigna el valor del total de frutas que tenemos.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Asigna el valor de 5 y 6 a las variables numero_manzanas y numero_naranjas respectivamente
numero_manzanas &amp;lt;- 5
numero_naranjas &amp;lt;- 6

# Suma estas dos variables e imprime el resultado.
numero_manzanas + numero_manzanas

#Crea la variable numero_frutas y asigna el resultado de la suma anterior.
numero_frutas &amp;lt;- (numero_manzanas + numero_naranjas)
numero_frutas
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;manzanas-y-naranjas&#34;&gt;Manzanas y naranjas&lt;/h2&gt;

&lt;p&gt;En la escuela primaria nos decían que no sumáramos manzanas con naranjas, pero es lo que acabamos de hacer :) \n Sin embargo numero_manzanas y numero_naranjas son dos variables que contienen el mismo tipo de dato: un dato de tipo numérico. El operador + en R funciona con variables de este tipo. Si alguna de nuestras variables no es numérica sino por ejemplo caracter (ver el editor), entonces estaríamos tratando de asignar la suma de un caracter y un número a la variable numero_frutas, lo cual no es posible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Da clic a &amp;lsquo;Submit Answer&amp;rsquo; lee el mensaje de error, asegúrate de entender que dice (puedes copiar y pegar el texto en ingles en un traductor)&lt;/li&gt;
&lt;li&gt;Ajusta el código para que R deje de mostrar ese error.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tipos-de-datos-básicos-en-r&#34;&gt;Tipos de datos básicos en R&lt;/h2&gt;

&lt;p&gt;R trabaja con muchos tipos de datos. Para empezar, algunos de los más básicos son:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Decimales como 4.5 son llamados numeric (numéricos).&lt;/li&gt;
&lt;li&gt;Números enteros como 4 son llamados (sorpresa!) integer (enteros).&lt;/li&gt;
&lt;li&gt;Valores Booleanos (TRUE (Verdadero) o FALSE (Falso)) logical (lógicos).&lt;/li&gt;
&lt;li&gt;Texto (cadenas de caracteres) son characters (caracteres).
Nota como utilizamos las comillas para denotar el texto en el editor.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cambia los valores de:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;mi_numerica a 42.&lt;/li&gt;
&lt;li&gt;mi_caracter a &amp;ldquo;cuarenta_y_dos&amp;rdquo;. Nota como utilizamos las comillas.&lt;/li&gt;
&lt;li&gt;mi_logica a FALSE (Falso).
Ten en cuenta que R distingue entre mayúsculas y minúsculas!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Un valor numérico
mi_numerica &amp;lt;- 42

# Asignando una cadena de caracteres (o simplemente caracteres) nota el uso de las comillas
mi_caracter &amp;lt;- &amp;quot;cuarenta_y_dos&amp;quot;

# Asignando un valor lógico verdadero
mi_logica &amp;lt;- FALSE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;## ¿Cómo sé el tipo de dato?
¿Recuerdas que cuando añadiste 5 + &amp;ldquo;seis&amp;rdquo; obtuviste un error debido a que los tipos de datos no coincidian? Para evitar estas penosas situaciones :\ puedes saber de antemano el tipo de dato que tienen tus variables utilizando el código class(nombre_variable)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Completa el código en el editor para imprimir a la consola el tipo de dato de las variables mi_numerica, mi_caracter y mi_logica.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Declarando las variables de diferentes tipos
mi_numerica &amp;lt;- 40
mi_caracter &amp;lt;- &amp;quot;cuarenta&amp;quot;
mi_logica &amp;lt;- FALSE

# Escribe el código para averiguar el tipo de dato de cada variable
class(mi_numerica)
class(mi_caracter)
class(mi_logica)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Reglas del aprendizaje automático - Fase I</title>
      <link>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-0-reglas1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-0-reglas1/</guid>
      <description>

&lt;p&gt;El objetivo de este documento es ayudar a aquellas personas con un conocimiento básico sobre aprendizaje automático a aprovechar las recomendaciones de Google para aprendizaje automático. Presenta un estilo para aprendizaje automático, similar a la guía de estilo de Google para C++ y otras guías populares para la programación práctica. Si tomaste clases de aprendizaje automático, desarrollaste un modelo de aprendizaje automático o trabajaste en uno, tienes el conocimiento necesario para leer este documento.&lt;/p&gt;

&lt;h2 id=&#34;terminología&#34;&gt;Terminología&lt;/h2&gt;

&lt;p&gt;Los siguientes términos se mencionarán con frecuencia en nuestro debate sobre aprendizaje automático eficaz:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Instancia&lt;/strong&gt;: El aspecto sobre el que deseas hacer una predicción. Por ejemplo, la instancia puede ser una página web que deseas clasificar como &amp;ldquo;sobre gatos&amp;rdquo; o &amp;ldquo;no sobre gatos&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Etiqueta&lt;/strong&gt;: Una respuesta a la tarea de predicción, que se puede generar mediante un sistema de aprendizaje automático o a partir de los datos de entrenamiento. Por ejemplo, la etiqueta sobre una página web puede ser &amp;ldquo;sobre gatos&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Atributo&lt;/strong&gt;: Una propiedad de una instancia utilizada en una tarea de predicción. Por ejemplo, una página web puede tener un atributo &amp;ldquo;contiene la palabra gato&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Columna de atributos&lt;/strong&gt;: Un conjunto de atributos relacionados, como el conjunto de todos los países posibles donde es posible que vivan los usuarios. Un ejemplo puede tener uno o más atributos presentes en una columna de atributos. &amp;ldquo;Columna de atributos&amp;rdquo; es terminología específica de Google. Una columna de atributos se conoce como un &amp;ldquo;espacio de nombres&amp;rdquo; en el sistema de VW (en Yahoo/Microsoft) o como un campo.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;: Una instancia (con sus atributos) y una etiqueta.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modelo&lt;/strong&gt;: Una representación estadística de una tarea de predicción. Entrenas un modelo basado en ejemplos y, luego, lo usas para hacer predicciones.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Métrica&lt;/strong&gt;: El número que importa. Se puede o no optimizar directamente.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Objetivo&lt;/strong&gt;: Una métrica que tu algoritmo intenta optimizar.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Canalización&lt;/strong&gt;: La infraestructura que rodea al algoritmo de aprendizaje automático. Incluye la recopilación de datos del frontend, su incorporación a los archivos de datos de entrenamiento, el entrenamiento de uno o más modelos y la exportación de los modelos para la producción.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tasa de clics&lt;/strong&gt;: El porcentaje de visitantes a una página web que hacen clic en un vínculo en un anuncio.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;resumen&#34;&gt;Resumen&lt;/h3&gt;

&lt;p&gt;Para desarrollar buenos productos:&lt;/p&gt;

&lt;p&gt;En aprendizaje automático, implemente todos los conocimientos de ingeniería que tiene, no los conocimientos de aprendizaje automático que no tiene.&lt;/p&gt;

&lt;p&gt;De hecho, la mayoría de los problemas que debes resolver son de ingeniería. Incluso con todos los recursos de un experto en aprendizaje automático, la mayoría de los beneficios provienen de los atributos geniales, no de los algoritmos de aprendizaje automático geniales. Por lo tanto, la estrategia básica es la siguiente:&lt;/p&gt;

&lt;p&gt;Asegúrate de que tu canalización sea completamente estable.
Comienza con un objetivo razonable.
Agrega atributos de sentido común de una forma sencilla.
Asegúrate de que la canalización siga siendo estable.
Esta estrategia dará resultados positivos por un tiempo prolongado. Modifica esta estrategia solo cuando no queda otra opción para probar. Si sumas complejidad, se retrasan las siguientes versiones.&lt;/p&gt;

&lt;p&gt;Una vez que hayas agotado los trucos simples, puedes pasar a aprendizaje automático de vanguardia. Consulta la sección en los proyectos de aprendizaje automático de la Fase III.&lt;/p&gt;

&lt;p&gt;Este documento se organiza de la siguiente manera:&lt;/p&gt;

&lt;p&gt;La primera parte te permite determinar si es el momento indicado para desarrollar un sistema de aprendizaje automático.
La segunda parte se trata de implementar la primera canalización.
La tercera parte cubre el lanzamiento y la iteración mientras se agregan atributos nuevos a la canalización, la evaluación de los modelos y la desviación entre el entrenamiento y la publicación.
La parte final trata sobre qué hacer cuando alcanzas una meseta.
Luego, hay una lista del trabajo relacionado y un apéndice con información sobre los sistemas que se usan comúnmente como ejemplos en este documento.&lt;/p&gt;

&lt;h2 id=&#34;antes-de-aprendizaje-automático&#34;&gt;Antes de aprendizaje automático&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 1: No tengas miedo de lanzar un producto sin aprendizaje automático&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;El aprendizaje automático es genial, pero necesita datos. En teoría, puedes obtener datos de un problema diferente y, luego, modificar un poco el modelo para un producto nuevo, pero es probable que la heurística básica no funcione como es debido. Si piensas que aprendizaje automático te brindará un aumento del 100%, entonces una heurística te permitirá alcanzar el 50% de ese camino.&lt;/p&gt;

&lt;p&gt;Por ejemplo, si clasificas apps en un mercado de apps, puedes usar la tasa de instalación o la cantidad de instalaciones como la heurística. Si detectas spam, filtra los editores que enviaron spam antes. No tengas miedo de usar la edición humana. Si necesitas clasificar contactos, clasifica el que se usó más recientemente como el primero (o puedes clasificarlos por orden alfabético). Si el aprendizaje automático no es indispensable para tu producto, no lo uses hasta que tengas datos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 2: Primero, diseña métricas e impleméntalas&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Antes de establecer las tareas de tu sistema de aprendizaje automático, realiza un seguimiento del sistema actual. Debes hacerlo por las siguientes razones:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Es más fácil obtener permiso de los usuarios del sistema al comienzo.&lt;/li&gt;
&lt;li&gt;Si consideras que algo puede llegar a ser un problema en el futuro, es mejor obtener datos históricos ahora.&lt;/li&gt;
&lt;li&gt;Si diseñas el sistema con la instrumentación de métricas en mente, las cosas irán mejorando en el futuro. En especial, si no deseas realizar búsquedas globales de strings en registros para instrumentar las métricas.&lt;/li&gt;
&lt;li&gt;Notarás qué aspectos cambian y cuáles permanecen iguales. Por ejemplo, supongamos que deseas optimizar directamente los usuarios activos por un día. Sin embargo, durante las primeras manipulaciones del sistema, es posible que notes que los cambios drásticos en la experiencia del usuario no afectan de forma notoria esta métrica.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;El equipo de Google Plus mide las expansiones por lectura, las veces que se compartió el contenido por lectura, las veces que alguien indico +1 por lectura, los comentarios por lectura, los comentarios por usuario, las veces que se compartió el contenido por usuario, etc. para calcular el desempeño de una publicación durante el tiempo de publicación. También ten en cuenta que es importante el marco de trabajo de un experimento, en el que puedes agrupar usuarios y sumar estadísticas por experimento. Consulta la regla n.º 12.&lt;/p&gt;

&lt;p&gt;Si recopilas métricas con mayor flexibilidad, puedes lograr una imagen más amplia del sistema. ¿Notas un problema? ¡Agrega una métrica para realizar un seguimiento! ¿Te entusiasma algún cambio cuantitativo en la actualización más reciente? ¡Agrega una métrica para realizar un seguimiento!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 3: Elige aprendizaje automático antes que una heurística compleja&lt;/strong&gt;.
Una heurística sencilla puede ser la clave para lanzar el producto, pero una heurística compleja no es sostenible. Una vez que tengas los datos y una idea básica de lo que intentas lograr, pasa al aprendizaje automático. Como en la mayoría de las tareas de ingeniería de software, buscas actualizar la estrategia de forma constante, ya sea con un modelo heurístico o de aprendizaje automático. Notarás que el modelo de aprendizaje automático es más fácil de actualizar y mantener (consulta la regla n.º 16).&lt;/p&gt;

&lt;h2 id=&#34;fase-i-de-aprendizaje-automático-tu-primera-canalización&#34;&gt;Fase I de aprendizaje automático: Tu primera canalización&lt;/h2&gt;

&lt;p&gt;Concéntrate en la infraestructura de tu sistema para la primera canalización. Si bien es divertido pensar en todo aprendizaje automático teórico que implementarás, es difícil determinar qué sucede si no usas una canalización confiable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 4: Procura que el primer modelo se mantenga simple y acierta con la infraestructura&lt;/strong&gt;.
El primer modelo proporciona el mayor impulso para tu producto, por lo que no necesita ser muy sofisticado. Sin embargo, te encontrarás con más problemas de infraestructura de lo que esperas. Antes de que alguien pueda usar tu novedoso sistema de aprendizaje automático, debes determinar lo siguiente:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cómo obtener ejemplos para tu algoritmo de aprendizaje.&lt;/li&gt;
&lt;li&gt;Un primer borrador para definir qué es &amp;ldquo;bueno&amp;rdquo; y qué es &amp;ldquo;malo&amp;rdquo; para tu sistema.&lt;/li&gt;
&lt;li&gt;Cómo integrar tu modelo a la aplicación. Puedes aplicar el modelo en vivo o calcularlo previamente con ejemplos sin conexión y guardar los resultados en una tabla. Por ejemplo, tal vez deseas preclasificar páginas web y almacenar los resultados en una tabla, pero deseas clasificar los mensajes de chat en vivo.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Si eliges atributos simples, es más sencillo garantizar lo siguiente:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Que los atributos se comuniquen con tu algoritmo de aprendizaje correctamente.&lt;/li&gt;
&lt;li&gt;Que el modelo aprenda ponderaciones razonables.&lt;/li&gt;
&lt;li&gt;Que los atributos se comuniquen con tu modelo en el servidor correctamente.
Si logras que el sistema cumpla con estas tres condiciones, habrás completado la mayor parte del trabajo. El modelo simple brinda métricas y un comportamiento de punto de referencia que puedes usar para probar modelos más complejos. Algunos equipos apuntan a un primer lanzamiento &amp;ldquo;neutral&amp;rdquo;: un primer lanzamiento que desprioriza de forma explícita las ganancias de aprendizaje, para no distraerse.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 5: Prueba la infraestructura separada de aprendizaje automático&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Asegúrate de poder probar la infraestructura y de que las partes de aprendizaje del sistema están aisladas, para que puedas probar las partes a su alrededor. De forma específica:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Prueba obtener datos para el algoritmo. Comprueba que se completen las columnas de atributos que deben completarse. Cuando la privacidad lo permite, inspecciona de forma manual la entrada al algoritmo de entrenamiento. Si es posible, comprueba las estadísticas en tu canalización para compararlas con las estadísticas de los mismos datos procesados en otro lugar.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Prueba obtener modelos a partir del algoritmo de entrenamiento. Asegúrate de que el modelo de tu entorno de entrenamiento muestre el mismo resultado que el modelo del entorno de publicación (consulta la regla n.º 37).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;El aprendizaje automático es un tanto impredecible; por lo tanto, asegúrate de probar el código para crear ejemplos en el entrenamiento y la publicación. Asegúrate también de poder cargar y usar un modelo fijo durante la publicación. Además, es importante comprender los datos: consulta Consejos prácticos para el análisis de conjuntos de datos grandes y complejos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 6: Ten cuidado con la pérdida de datos al copiar canalizaciones&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A menudo, para crear una canalización, copiamos una existente (es decir, la programación a ciegas), y la canalización anterior pierde datos que necesitamos en la canalización nueva. Por ejemplo, la canalización para la vista Lo más interesante de Google+ pierde las publicaciones anteriores (porque intenta calificar las publicaciones nuevas). Esta canalización se copió para usar las Novedades de Google+, donde las publicaciones anteriores todavía tienen valor. Sin embargo, la canalización sigue perdiendo publicaciones antiguas. Otro patrón común es solo registrar datos que vio el usuario. Estos datos no tienen valor si queremos desarrollar un modelo sobre por qué el usuario no vio una publicación específica, ya que se perdieron todos los ejemplos negativos. Ocurrió un problema similar en Play. Durante el trabajo en la página de inicio de Play Apps, se creó una canalización nueva que también contenía ejemplos de la página de destino de Play Juegos sin ningún atributo para que quedara claro de dónde provenía cada ejemplo.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 7: Convierte la heurística en atributos o gestiónalos de forma externa&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Por lo general, los problemas que el aprendizaje automático intenta resolver no son completamente nuevos. Ya existe un sistema para la clasificación, el ordenamiento o el problema que intentes resolver. Esto significa que existe una gran cantidad de reglas y heurísticas. Estas mismas heurísticas pueden ser de gran ayuda cuando se modifican con aprendizaje automático. Debes recolectar toda la información que tengan tus heurísticas por dos razones. Primero, facilitará la transición a un sistema de aprendizaje automático. Segundo, por lo general, estas reglas contienen mucha de la intuición del sistema de la que no conviene deshacerse. Existen cuatro formas de usar una heurística existente:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Realiza un preprocesamiento con la heurística. Si el atributo es increíblemente asombroso, entonces esto es una opción. Por ejemplo, si, en un filtro de spam, el destinatario ya está en la lista negra, no intentes volver a aprender qué significa &amp;ldquo;estar en la lista negra&amp;rdquo;. Bloquea el mensaje. Esta estrategia tiene más sentido en tareas de clasificación binaria.&lt;/li&gt;
&lt;li&gt;Crea un atributo. Crear directamente un atributo a partir de la heurística es genial. Por ejemplo, si usas una heurística para calcular la calificación de relevancia para el resultado de una consulta, puedes incluir la calificación como el valor de un atributo. Más tarde, es posible que desees usar técnicas de aprendizaje automático para adaptar el valor (por ejemplo, al convertir el valor en uno de un conjunto finito de valores discretos o al combinarlo con otros atributos), pero comienza usando los valores sin procesar que produce la heurística.&lt;/li&gt;
&lt;li&gt;Recolecta las entradas sin procesar de la heurística. Si existe una heurística para apps que combine la cantidad de instalaciones, la cantidad de caracteres en el texto y el día de la semana, considera separar estos datos y enviar estas entradas al aprendizaje de forma separada. En este caso, puedes usar algunas técnicas que usan los conjuntos (consulta la regla n.º 40).&lt;/li&gt;
&lt;li&gt;Modifica la etiqueta. Esta es una opción cuando consideras que la heurística captura información que no está contenida actualmente en la etiqueta. Por ejemplo, si intentas maximizar la cantidad de descargas, pero también deseas obtener contenido de calidad, la solución puede ser multiplicar la etiqueta por la cantidad promedio de estrellas que recibió la app. Esta opción permite mucha libertad. Consulta la sección &amp;ldquo;Tu primer objetivo&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ten en cuenta la mayor complejidad que implica usar heurísticas en un sistema de aprendizaje automático. El uso de heurísticas antiguas en tu algoritmo de aprendizaje automático nuevo puede contribuir a una transición sin sobresaltos, pero piensa si existe una forma más sencilla de lograr el mismo efecto.&lt;/p&gt;

&lt;h3 id=&#34;supervisión&#34;&gt;Supervisión&lt;/h3&gt;

&lt;p&gt;En general, mantén el sistema en buen estado, como usar alertas con opciones y contar con una página de panel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 8: Conoce los requisitos de antigüedad de tu sistema&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;¿Cuánto se degrada el rendimiento si el modelo tiene un día de antigüedad? ¿Y una semana? ¿Un trimestre? Esta información te puede ayudar a establecer las prioridades de la supervisión. Si pierdes una calidad significativa de los productos cuando el modelo no se actualiza por un día, se justifica contar con un ingeniero que haga un seguimiento constante. La mayoría de los sistemas de publicación de anuncios tiene que gestionar anuncios nuevos cada día, por lo que deben actualizarse. Por ejemplo, si el modelo de AA para Búsqueda de Google Play no está actualizado, puede tener un efecto negativo en menos de un mes. Algunos modelos de la sección Lo más interesante en Google+ no tienen un identificador de publicaciones, por lo que pueden exportar estos modelos de forma poco frecuente. Otros modelos tienen identificadores que se actualizan con una frecuencia mucho mayor. Ten en cuenta que la antigüedad puede cambiar con el tiempo, en especial cuando se agregan o quitan columnas de atributos en el modelo.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 9: Detecta los problemas antes de exportar los modelos&lt;/strong&gt;.
Existe una etapa en muchos sistemas de aprendizaje automático en la que exportas el modelo para la publicación. Si existe un problema con un modelo exportado, es un problema que notará el usuario.&lt;/p&gt;

&lt;p&gt;Comprueba el estado del modelo antes de exportarlo. Específicamente, asegúrate de que el rendimiento del modelo sea consistente con los datos existentes. Si tienes problemas persistentes con los datos, no exportes el modelo. Muchos equipos que implementan continuamente modelos comprueban el área bajo la curva ROC (o AUC) antes de la exportación &lt;strong&gt;Los problemas sobre modelos que no se han exportado requieren de una alerta de correo electrónico; los problemas de un modelo para el usuario requieren de una página. Por lo tanto, lo mejor es esperar y estar seguro, antes de hacer algo que afecte a los usuarios&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 10: Busca los fallos silenciosos&lt;/strong&gt;.
Este es un problema que ocurre con más frecuencia en los sistemas de aprendizaje automático que en otros tipos de sistemas. Supongamos que una tabla particular que se une ya no se actualiza. El sistema de aprendizaje automático se ajustará y el comportamiento continuará siendo razonablemente bueno, con una degradación paulatina. A veces, hay tablas con meses de atraso y basta con una simple actualización en lugar de otro lanzamiento para mejorar el rendimiento en ese trimestre. La cobertura de un atributo puede cambiar debido a las modificaciones en la implementación: por ejemplo, una columna de atributo puede estar completa en un 90% con ejemplos y, de pronto, reducir la cantidad de ejemplos a un 60%. Una vez, había una tabla en Play con un atraso de 6 meses. Una mera actualización aportó un aumento del 2% en la tasa de instalación. Si realizas un seguimiento de las estadísticas de los datos e inspeccionas los datos manualmente cada tanto, puedes reducir este tipo de fallos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 11: Documenta y asigna propietarios para las columnas de atributos&lt;/strong&gt;.
Si el sistema es grande y existen muchas columnas de funciones, debes saber quién creó o mantiene cada columna de atributos. Si descubres que la persona que se encarga de una columna de atributos se va, asegúrate de que alguien reciba esa información. Si bien las columnas de atributos tienen nombres descriptivos, es recomendable tener una descripción más detallada de lo que hace un atributo, de dónde proviene y cuál es la contribución esperada.&lt;/p&gt;

&lt;h3 id=&#34;tu-primer-objetivo&#34;&gt;Tu primer objetivo&lt;/h3&gt;

&lt;p&gt;Si bien te importan varias métricas o mediciones sobre el sistema, el algoritmo de aprendizaje automático a menudo requiere un objetivo único, un número que el algoritmo &amp;ldquo;intenta&amp;rdquo; optimizar. Hago una diferencia entre objetivos y métricas: una métrica es cualquier número que genera el sistema, que puede o no ser importante. Consulta también la regla n.º 2.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 12: No pienses demasiado qué objetivo debes optimizar directamente&lt;/strong&gt;.
Quieres ganar dinero, que tus usuarios estén contentos y que el mundo sea un lugar mejor. Hay cientos de métricas para tener en cuenta y debes medirlas a todas (consulta la regla n.º 2). Sin embargo, en el comienzo del proceso de aprendizaje automático, notarás que todas aumentan, incluso aquellas que no optimizaste directamente. Por ejemplo, supongamos que te importan la cantidad de clics y el tiempo de visita en el sitio. Si optimizas la cantidad de clics, es probable que también aumente el tiempo de visita.&lt;/p&gt;

&lt;p&gt;Por lo tanto, hay que simplificar y no pensar demasiado en el equilibrio entre las diferentes métricas cuando puedes aumentar fácilmente todas las métricas. Tampoco fuerces esta regla, es decir, no confundas tu objetivo con el estado general del sistema (consulta la regla n.º 39). Además, si aumentas la métrica optimizada directamente, pero decides no ejecutar el sistema, es posible que debas revisar los objetivos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 13: Elige una métrica simple, observable y con atributos para tu primer objetivo&lt;/strong&gt;.
Por lo general, no sabes cuál es el verdadero objetivo. Crees que sí, pero luego, al observar los datos y comparar el sistema anterior con el nuevo sistema de aprendizaje automático, te das cuenta de que debes modificar el objetivo. Además, muchas veces, los miembros del equipo no se ponen de acuerdo con el objetivo verdadero. El objetivo del aprendizaje automático debe ser algo que sea fácil de medir y una representación del &amp;ldquo;verdadero&amp;rdquo; objetivo. De hecho, a menudo no existe un &amp;ldquo;verdadero&amp;rdquo; objetivo (consulta la regla n.º 39). Por lo tanto, implementa el entrenamiento para un objetivo sencillo de aprendizaje automático y considera contar con una &amp;ldquo;capa de políticas&amp;rdquo; que te permita agregar más lógica (con suerte, una lógica muy sencilla) para la calificación final.&lt;/p&gt;

&lt;p&gt;La forma más sencilla de lograr un modelo es el comportamiento del usuario que se observa directamente y se atribuye a una acción en el sistema:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;¿El usuario hizo clic en este vínculo clasificado?&lt;/li&gt;
&lt;li&gt;¿El usuario descargó este objeto clasificado?&lt;/li&gt;
&lt;li&gt;¿El usuario reenvió, respondió o envió por correo electrónico este objeto clasificado?&lt;/li&gt;
&lt;li&gt;¿El usuario calificó este objeto clasificado?&lt;/li&gt;
&lt;li&gt;¿El usuario denunció este objeto mostrado como spam, pornografía u ofensivo?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Al comienzo, evita los efectos indirectos del modelado:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;¿El usuario realizó otra visita al día siguiente?&lt;/li&gt;
&lt;li&gt;¿Cuánto tiempo duró la visita al sitio del usuario?&lt;/li&gt;
&lt;li&gt;¿Cuáles fueron los usuarios activos por día?
Los efectos indirectos logran excelentes métricas y se pueden usar en pruebas A/B y decisiones de lanzamiento.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Por último, no intentes hacer que el aprendizaje automático responda a estas preguntas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;¿El usuario está feliz con el producto?&lt;/li&gt;
&lt;li&gt;¿El usuario está satisfecho con la experiencia?&lt;/li&gt;
&lt;li&gt;¿El producto mejora el bienestar general del usuario?&lt;/li&gt;
&lt;li&gt;¿Cómo afectará el estado general de la empresa?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Estas preguntas son importantes, pero muy difíciles de medir. En su lugar, usa representantes: si el usuario está feliz, permanecerá en el sitio por más tiempo. Si el usuario está satisfecho, volverá a visitar el sitio mañana. En cuanto al bienestar y el estado de la empresa, se requiere el criterio humano para conectar el objetivo de aprendizaje automático con la naturaleza del producto que vendes y tu plan comercial.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regla n.º 14: Comenzar con un modelo interpretativo facilita la depuración&lt;/strong&gt;.
La regresión lineal, la regresión logística y la regresión de Poisson están directamente relacionadas con un modelo probabilístico. Cada predicción se interpreta como una probabilidad o un valor esperado. Esto facilita la depuración, en comparación con los modelos que usan objetivos (pérdida de cero uno, diferentes pérdidas de bisagra y más) que intentan optimizar directamente el rendimiento o la precisión de la clasificación. Por ejemplo, si las probabilidades en la capacitación son diferentes de las probabilidades predichas en la comparación o la inspección del sistema de producción, es posible que esta diferencia indique un problema.&lt;/p&gt;

&lt;p&gt;Por ejemplo, en la regresión lineal, logística o de Poisson, existen subconjuntos de datos donde la expectativa promedio de las predicciones equivale a la etiqueta promedio (calibrado con un momento o simplemente calibrado). Esto es verdadero en líneas generales siempre y cuando no tengas una regularización y el algoritmo se haya convergido. Si tienes un atributo que es 1 o 0 para cada ejemplo, significa que el conjunto de 3 ejemplos donde el atributo es 1 está calibrado. Además, si tienes un atributo que es 1 para cada ejemplo, entonces el conjunto de todos los ejemplos está calibrado.&lt;/p&gt;

&lt;p&gt;Con modelos simples, es más fácil lidiar con ciclos de reacción (consulta la regla n.º 36). A menudo, usamos estas predicciones probabilísticas para tomar una decisión: por ejemplo, calificar publicaciones según el valor esperado decreciente (es decir, la probabilidad de hacer clic, descargar, etc.). Sin embargo, recuerda que cuando debes elegir qué modelo usar, la decisión importa más que la probabilidad de los datos según el modelo (consulta la regla n.° 27).&lt;/p&gt;

&lt;p&gt;Regla n.º 15: Separa el filtro de spam y la clasificación de calidad en una capa de política.
La clasificación de calidad es un arte delicado, pero el filtro de spam es una guerra. Las señales que usas para determinar las publicaciones de calidad alta serán obvias para los usuarios de tu sistema; ellos podrán modificar sus publicaciones para que tengan estas propiedades. Además, tu clasificación de calidad debe centrarse en calificar el contenido que se publica de buena fe. No subestimes al modelo por darle una clasificación demasiado alta al spam. De forma similar, el contenido &amp;ldquo;subido de tono&amp;rdquo; debe separarse de la clasificación de calidad. El filtro de spam es una historia diferente. Es esperable que los atributos que debes generar cambiarán constantemente. A menudo, incluyes reglas obvias en el sistema (por ejemplo, si una publicación tiene más de tres votos de spam, no hay que recuperarla). Los modelos aprendidos deben actualizarse a diario o de forma más frecuente. La reputación del creador del contenido tiene un peso importante.&lt;/p&gt;

&lt;p&gt;En algún nivel, el resultado de estos dos sistemas debe integrarse. Ten en cuenta que el filtro de spam en resultados de la búsqueda debe ser más agresivo que en mensajes de correo electrónico. Esto es así si no tienes ninguna regularización y el algoritmo está convergido. En general, es de este modo. Además, es una práctica estándar para quitar el spam de los datos de entrenamiento para el clasificador de calidad.&lt;/p&gt;

&lt;p&gt;Fase II de aprendizaje automático: Ingeniería de atributos
En la primera fase del ciclo de vida de un sistema de aprendizaje automático, la prioridad es mandar los datos de entrenamiento al sistema de aprendizaje, lograr instrumentar las métricas de interés y crear una infraestructura de publicación. Una vez que cuentas con un sistema integral en funcionamiento con pruebas de unidades y del sistema instrumentadas, comienza la fase II.&lt;/p&gt;

&lt;p&gt;En la segunda fase, hay muchas recompensas a corto plazo. Existe una variedad de atributos obvios que se pueden agregar al sistema. Además, la segunda fase de aprendizaje automático implica agregar tantos atributos como sea posible y combinarlos de formas intuitivas. Durante esta fase, todas las métricas deben continuar subiendo. Habrá muchos lanzamientos, y es una excelente oportunidad para incorporar muchos ingenieros que puedan recopilar todos los datos que necesitas para crear un sistema de aprendizaje verdaderamente sorprendente.&lt;/p&gt;

&lt;p&gt;Regla n.º 16: Planifica el lanzamiento y la iteración.
No esperes que el modelo en el que trabajas ahora sea el último que lanzarás o, incluso, que dejarás de lanzar modelos. Por lo tanto, ten en cuenta que la complejidad de este lanzamiento retrasará los lanzamientos futuros. Muchos equipos han lanzado uno o más modelos por trimestre durante años. Existen tres razones básicas para lanzar modelos nuevos:&lt;/p&gt;

&lt;p&gt;Tienes atributos nuevos.
Deseas ajustar la regularización y combinar atributos antiguos de formas nuevas.
Deseas ajustar el objetivo.
Independientemente de la razón, es recomendable poner atención en el modelo: analizar los datos que se ingresan en el ejemplo te permite encontrar señales nuevas y detectar señales antiguas con problemas. Entonces, a medida que desarrollas el modelo, piensa en lo fácil que es agregar, quitar o recombinar atributos. Piensa en lo fácil que es crear una copia nueva de la canalización y verifica que sea correcta. Piensa en si es posible ejecutar dos o tres copias en paralelo. Por último, no te preocupes si no logras incluir todos los atributos en esta versión de la canalización. Las incluirás el próximo trimestre.&lt;/p&gt;

&lt;p&gt;Regla n.º 17: Comienza con los atributos directamente observados e informados, en lugar de los atributos aprendidos.
Este puede ser un punto controversial, pero evita muchos problemas. Primero, describamos qué es un atributo aprendido. Un atributo aprendido es un atributo generado por un sistema externo (como un sistema de agrupación en clústeres sin supervisar) o el mismo modelo (por ejemplo, mediante un modelo factorizado o aprendizaje profundo). Ambas opciones pueden ser útiles, pero tienen muchos problemas, por lo que no es conveniente incluirlas en el primer modelo.&lt;/p&gt;

&lt;p&gt;Si usas un sistema externo para crear un atributo, recuerda que ese sistema tiene su propio objetivo. Es posible que el objetivo del sistema externo no tenga mucha relación con tu objetivo actual. Si realizas una instantánea del sistema externo, es posible que esté desactualizada. Si actualizas los atributos desde el sistema externo, es posible que los significados cambien. Si usas un sistema externo para proporcionar un atributo, debes tener mucho cuidado con ese enfoque.&lt;/p&gt;

&lt;p&gt;El principal problema de los modelos multiplicados y los modelos profundos es que no son convexos. Por lo tanto, no hay garantía de encontrar una solución óptima (o de aproximarse a esta). Además, el mínimo local de cada iteración puede ser diferente. Esta variación no permite juzgar con exactitud si el impacto de un cambio en tu sistema es relevante o contingente. Si creas un modelo sin atributos profundos, obtienes un sistema de referencia con buen rendimiento. Una vez que alcanzas este punto de referencia, puedes probar enfoques menos ortodoxos.&lt;/p&gt;

&lt;p&gt;Regla n.º 18: Prueba atributos de contenido que generalicen en contextos.
A menudo, un sistema de aprendizaje automático es una parte pequeña de una entidad mucho más grande. Por ejemplo, si imaginas una publicación que se puede usar en Lo Más Interesante, mucha gente hará +1, la compartirá o escribirá un comentario en ella antes de que llegue a mostrarse en Lo más interesante. Si proporcionas esas estadísticas al modelo, este puede promocionar publicaciones nuevas para las que no tiene datos en el contexto que está optimizando. YouTube Watch Next puede usar la cantidad de videos que miraste, o que miraste de forma secuencial (la cantidad de veces que se miró un video después de otro) en la búsqueda de YouTube. También puedes usar clasificaciones de usuarios explícitas. Por último, si usas una acción de usuario como etiqueta, ver esa acción en el documento en un contexto diferente puede ser un excelente atributo. Todos estos atributos te permiten aportar contenido nuevo al contexto. Ten en cuenta que esto no se trata de personalización: primero, descubre si a alguien le gusta el contenido en este contexto; luego, descubre a quién le gusta más o menos.&lt;/p&gt;

&lt;p&gt;Regla n.º 19: Usa atributos muy específicos cuando sea posible.
Gracias a la gran cantidad de datos, es más fácil aprender millones de atributos sencillos que unos pocos atributos complejos. Los identificadores de documentos que se obtienen y las consultas canónicas no brindan mucha generalización, pero alinean las clasificaciones con las etiquetas en las consultas principales. Por lo tanto, no temas agrupar atributos si cada uno se aplica a una fracción muy pequeña de datos, pero la cobertura total es de más del 90%. Puedes usar la regularización para eliminar los atributos que se aplican a pocos ejemplos.&lt;/p&gt;

&lt;p&gt;Regla n.º 20: Combina y modifica los atributos existentes para crear atributos nuevos de una forma legible.
Hay diferentes formas de combinar y modificar atributos. Los sistemas de aprendizaje automático como TensorFlow te permiten preprocesar los datos mediante transformaciones. Los dos enfoques estándar son las &amp;ldquo;discretizaciones&amp;rdquo; y las &amp;ldquo;combinaciones&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;La discretización consiste en tomar un atributo continuo y crear varios atributos discretos. Considera un atributo continuo como la edad. Puedes crear un atributo que es 1 cuando la edad es menor de 18, otro atributo que es 1 cuando la edad es entre 18 y 35, etc. No pienses demasiado en los límites de estos histogramas; los cuantiles básicos serán los más eficaces.&lt;/p&gt;

&lt;p&gt;Las combinaciones unen dos o más columnas de atributos. En la terminología de TensorFlow, una columna de atributos es un conjunto de atributos homogéneos (p. ej., {masculino, femenino}, {EE.UU., Canadá, México}, etc.). Una combinación es una nueva columna de atributos que incluye, p. ej., {masculino, femenino} × {EE.UU., Canadá, México}. Esta nueva columna de atributos contendrá el atributo (masculino, Canadá). Si usas TensorFlow y le indicas que cree esta combinación, este atributo (masculino, Canadá) aparecerá en los ejemplos que representan canadienses masculinos. Ten en cuenta que se necesita una enorme cantidad de datos para aprender modelos con combinaciones de tres, cuatro o más columnas de atributos básicos.&lt;/p&gt;

&lt;p&gt;Las combinaciones que producen columnas de atributos muy grandes pueden producir un sobreajuste. Por ejemplo, imagina que haces algún tipo de búsqueda y tienes una columna de atributos con palabras en la consulta y otra con palabras en el documento. Si unes estas columnas con una combinación, terminarás con muchos atributos (consulta la regla n.º 21).&lt;/p&gt;

&lt;p&gt;Cuando trabajas con texto, existen dos alternativas. La más rigurosa es un producto escalar. En su forma más simple, un producto escalar simplemente cuenta la cantidad de palabras en común entre la consulta y el documento. Por lo tanto, este atributo se puede discretizar. Otro enfoque es una intersección: tenemos un atributo que está presente solo si la palabra &amp;ldquo;poni&amp;rdquo; aparece en el documento y en la consulta, y otro atributo que está presente solo si la palabra &amp;ldquo;el&amp;rdquo; aparece en el documento y en la consulta.&lt;/p&gt;

&lt;p&gt;Regla n.º 21: La cantidad de ponderaciones de atributos que puedes aprender en un modelo lineal es casi proporcional a la cantidad de datos que tienes.
Existen resultados teóricos fascinantes sobre aprendizaje estadístico relacionados con el nivel de complejidad correspondiente de un modelo, pero esta regla es todo lo que necesitas saber. Tuve discusiones con personas que ponían en duda que se pudiera aprender algo con mil ejemplos o que nunca se necesitan más de un millón de ejemplos, porque están empecinados con un determinado método de aprendizaje. La clave es escalar el aprendizaje a la medida del tamaño de los datos:&lt;/p&gt;

&lt;p&gt;Si trabajas con un sistema de ranking de búsquedas, existen millones de palabras diferentes en los documentos y la consulta, y tienes 1000 ejemplos etiquetados, debes usar un producto escalar entre los atributos de consultas y de documentos, TF-IDF y una media docena de otros atributos desarrollados por humanos. 1000 ejemplos, una docena de atributos.
Si tienes un millón de ejemplos, intersecta la columna de atributos de consultas con la de documentos, y aplica regularización y, posiblemente, selección de atributos. Esto te brindará un millón de atributos, pero, con la regularización, tendrás menos. 10 millones de ejemplos, tal vez 100,000 atributos.
Si tienes miles o cientos de miles de millones, puedes combinar las columnas de atributos con tokens de consultas y de documentos mediante la regularización y la selección de atributos. Tendrás miles de millones de ejemplos y 10 millones de atributos. La teoría de aprendizaje estadístico raramente establece límites rígidos, pero ofrece un buen punto de partida.
En última instancia, usa la regla n.º 28 para decir qué atributos usarás.&lt;/p&gt;

&lt;p&gt;Regla n.º 22: Quita los atributos que ya no uses.
Los atributos sin usar crean deuda técnica. Si descubres que no estás usando un atributo y que no sirve combinarlo con otros atributos, quítalo de la infraestructura. Debes mantener limpia tu infraestructura para que puedas probar los atributos más prometedores tan rápido como sea posible. Si es necesario, tu atributo se puede volver a agregar en cualquier momento.&lt;/p&gt;

&lt;p&gt;Ten en cuenta la cobertura cuando analices qué atributos agregarás o conservarás. ¿Cuántos ejemplos cubre el atributo? Por ejemplo, si tienes algunos atributos de personalización, pero solo el 8% de los usuarios tiene atributos de personalización, eso no será muy eficaz.&lt;/p&gt;

&lt;p&gt;Al mismo tiempo, algunos atributos te sorprenden gratamente. Por ejemplo, si tienes un atributo que cubre solo el 1% de los datos, pero el 90% de los ejemplos de este atributo son positivos, es un excelente atributo que agregar.&lt;/p&gt;

&lt;p&gt;Análisis humano del sistema
Antes de avanzar a la tercera fase de aprendizaje automático, es importante hablar de algo que no se enseña en ninguna clase de aprendizaje automático: cómo analizar un modelo existente y mejorarlo. Esto es más un arte que una ciencia. Aun así, existen muchos antipatrones que es conveniente evitar.&lt;/p&gt;

&lt;p&gt;Regla n.º 23: No eres el típico usuario final.
Probablemente, esta es la razón más común por la que un equipo no progresa. Si bien usar un prototipo con tu equipo o usar un prototipo en tu empresa tiene muchos beneficios, los empleados deben analizar si el rendimiento es correcto. Si bien un cambio que sea evidentemente malo no debe usarse, cualquier función que parezca lista para la producción debe probarse aún más, ya sea contratando a gente común para que responda preguntas en una plataforma de participación colectiva o mediante un experimento en vivo con usuarios reales.&lt;/p&gt;

&lt;p&gt;Hay dos razones para ello. Estás demasiado familiarizado con el código. Es posible que busques un aspecto específico de las publicaciones, o que estés muy involucrado (p. ej., sesgo de confirmación). La segunda razón es que tu tiempo es demasiado valioso. Considera el costo de nueve ingenieros en una reunión de una hora de duración y cuántas etiquetas logradas con trabajo humano puedes obtener en una plataforma de participación colectiva.&lt;/p&gt;

&lt;p&gt;Si realmente quieres comentarios de usuarios, implementa metodologías de experiencia de usuario. Crea usuarios persona (puedes encontrar una descripción en el libro Sketching User Experiences [Cómo diseñar experiencias de usuario] de Bill Buxton) al comienzo del proceso y, luego, implementa una prueba de usabilidad (puedes encontrar una descripción en el libro Don’t Make Me Think [No me hagas pensar] de Steve Krug). Los usuarios persona implican crear un usuario hipotético. Por ejemplo, si tu equipo se compone solo por hombres, será beneficioso diseñar un usuario persona femenina de 35 años (completa con atributos de usuario) y observar los resultados que genera, en lugar de los 10 resultados para hombres de 25 a 40 años. También puedes lograr una nueva perspectiva al evaluar cómo reaccionan las personas reales a tu sitio (de forma local o remota) en una prueba de usabilidad.&lt;/p&gt;

&lt;p&gt;Regla n.º 24: Mide el delta entre los modelos.
Una de las formas más sencillas y, a veces, más útiles de realizar mediciones que puedes usar antes de que los usuarios vean tu nuevo modelo es calcular la diferencia entre los nuevos resultados y los obtenidos con el sistema en producción. Por ejemplo, si tienes un problema de ranking, ejecuta ambos modelos con una muestra de consultas en todo el sistema y observa el tamaño de la diferencia simétrica de los resultados (ponderados según la posición en el ranking). Si la diferencia es muy pequeña, entonces puedes deducir que habrá poco cambio, sin necesidad de ejecutar un experimento. Si la diferencia es muy grande, entonces debes asegurarte de que el cambio sea positivo. Analizar las consultas donde la diferencia simétrica es alta te puede ayudar a comprender de forma cualitativa cómo fue el cambio. Sin embargo, asegúrate de que el sistema sea estable. Cuando compares un modelo consigo mismo, asegúrate de que tenga una diferencia simétrica baja (idealmente cero).&lt;/p&gt;

&lt;p&gt;Regla n.º 25: Cuando elijas un modelo, el rendimiento utilitario predomina por sobre el poder de predicción.
Tu modelo puede intentar predecir la tasa de clics. Sin embargo, en última instancia, la pregunta clave es lo que haces con esa predicción. Si la usas para clasificar documentos, entonces la calidad del ranking final importa más que la predicción en sí misma. Si predices la probabilidad de que un documento sea spam y, luego, tienes un punto límite sobre lo que se bloquea, entonces la precisión de lo que se permite tiene más importancia. La mayoría de las veces, estos dos aspectos coinciden; cuando no es así, es probable que haya una pequeña ganancia. Además, si hay algún cambio que mejore la pérdida logística, pero que reduzca el rendimiento del sistema, busca otro atributo. Si esto comienza a suceder con más frecuencia, es hora de volver a evaluar el objetivo del modelo.&lt;/p&gt;

&lt;p&gt;Regla n.º 26: Busca patrones en los errores observados y crea atributos nuevos.
Supongamos que ves un ejemplo de entrenamiento que el modelo &amp;ldquo;no entendió&amp;rdquo;. En una tarea de clasificación, este error puede ser un falso positivo o un falso negativo. En una tarea de clasificación, el error puede ser un par donde un positivo tiene un ranking menor que un negativo. El punto más importante es que sea un ejemplo que el sistema de aprendizaje automático sepa que no lo entendió y que lo corrija si tiene la oportunidad. Si le agregas un atributo al modelo para que pueda corregir el error, el modelo intentará usarlo.&lt;/p&gt;

&lt;p&gt;Por otro lado, si intentas crear un atributo basado en ejemplos que el sistema no considera errores, el atributo se ignorará. Por ejemplo, supongamos que, en la búsqueda de apps de Play, alguien busca &amp;ldquo;juegos gratuitos&amp;rdquo;. Supongamos que uno de los primeros resultados es una app de bromas menos relevante. Entonces, creas un atributo para &amp;ldquo;apps de bromas&amp;rdquo;. Sin embargo, si maximizas la cantidad de instalaciones y las personas instalan una app de bromas cuando buscan juegos gratuitos, el atributo para &amp;ldquo;apps de bromas&amp;rdquo; no tendrá el efecto deseado.&lt;/p&gt;

&lt;p&gt;Una vez que tengas ejemplos que el modelo no haya entendido, busca las tendencias que estén fuera del conjunto de atributos actual. Por ejemplo, si parece que el sistema penaliza las publicaciones más largas, agrega la longitud de la publicación. No seas demasiado específico sobre los atributos que agregues. Si agregas la longitud de la publicación, no intentes adivinar qué significa &amp;ldquo;largo&amp;rdquo;, solo agrega una decena de atributos y permite que el modelo descubra qué hacer con ellos (consulta la regla n.º 21). Esta es la forma más fácil de obtener el resultado deseado.&lt;/p&gt;

&lt;p&gt;Regla n.º 27: Intenta cuantificar el comportamiento no deseado que observes.
Algunos miembros de tu equipo comenzarán a frustrarse con las propiedades del sistema que no les gusten, ya que la función de pérdida existente no las captura. En este punto, deben hacer lo que sea necesario para convertir sus quejas en números sólidos. Por ejemplo, si consideran que se muestran demasiadas &amp;ldquo;apps de bromas&amp;rdquo; en la búsqueda de Play, se pueden contratar a evaluadores humanos para que identifiquen las apps de bromas. (Puedes usar datos etiquetados por humanos en este caso, ya que una fracción relativamente pequeña de consultas representan una gran fracción del tráfico). Si los problemas se pueden medir, puedes comenzar a usarlos como atributos, objetivos o métricas. La regla general es &amp;ldquo;medir primero, optimizar después&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Regla n.º 28: Ten en cuenta que el comportamiento idéntico a corto plazo no implica un comportamiento idéntico a largo plazo.
Imagina que tienes un sistema nuevo que analiza cada doc_id y exact_query, y luego calcula la probabilidad de clic para cada documento y cada consulta. Descubres que este comportamiento es casi idéntico a tu sistema actual en la comparación y la prueba A/B; por lo tanto, dado su simplicidad, lo ejecutas. Sin embargo, notas que no se muestran apps nuevas. ¿Por qué? Bueno, dado que tu sistema solo muestra un documento basado en su propio historial con esa consulta, no hay forma de aprender qué documento nuevo debe mostrar.&lt;/p&gt;

&lt;p&gt;La única forma de entender cómo debe funcionar un sistema a largo plazo es entrenarlo solo con datos adquiridos cuando el modelo está publicado. Esto es muy difícil.&lt;/p&gt;

&lt;p&gt;Desviación entre el entrenamiento y la publicación
La desviación entre el entrenamiento y la publicación es la diferencia entre el rendimiento del entrenamiento y el de la publicación. Existen diferentes razones para esta desviación:&lt;/p&gt;

&lt;p&gt;una discrepancia entre cómo manipulas los datos en las canalizaciones de entrenamiento y del servidor
un cambio en los datos entre el momento del entrenamiento y el del servidor
un ciclo de retroalimentación entre el modelo y el algoritmo
Hemos observado sistemas de aprendizaje automático de producción en Google con una desviación entre el entrenamiento y la publicación que afecta negativamente el rendimiento. La mejor solución es supervisarlo de forma explícita para que los cambios en el sistema y en los datos no generen una desviación inadvertida.&lt;/p&gt;

&lt;p&gt;Regla n.º 29: La mejor manera de asegurarte de que el entrenamiento se asemeja a la publicación es guardar el conjunto de atributos que usas en la publicación y canalizar esos atributos en un registro para luego usarlos en el entrenamiento.
Incluso si no lo puedes hacer para cada ejemplo, hazlo para una fracción pequeña, de forma tal que puedas verificar la coherencia entre la publicación y el entrenamiento (consulta la regla n.º 37). En Google, los equipos que hicieron esta medición se sorprendieron a menudo por los resultados. La página de inicio de YouTube cambió a atributos del registro en el servidor, lo que generó mejoras de calidad importantes y redujo la complejidad del código. Ahora, muchos equipos están cambiando sus infraestructuras.&lt;/p&gt;

&lt;p&gt;Regla n.º 30: ¡Realiza muestras con datos con ponderación por importancia, no los quites!
Cuando tienes muchos datos, es tentador usar los archivos del 1 al 12 e ignorar los archivos del 13 al 99. Esto es un error. Si bien se pueden quitar los datos que nunca se mostraron al usuario, la ponderación por importancia es la mejor opción para el resto. La ponderación por importancia implica que, si decides hacer una muestra con el ejemplo X con una probabilidad del 30%, la ponderación será de &lt;sup&gt;10&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;. Con la ponderación por importancia, se mantienen todas las propiedades de calibración que analizamos en la regla n.º 14.&lt;/p&gt;

&lt;p&gt;Regla n.º 31: Ten en cuenta que, si cruzas datos de una tabla durante el entrenamiento y la publicación, los datos en la tabla pueden cambiar.
Digamos que cruzas ID de documentos a una tabla que contiene atributos para esos documentos (como la cantidad de comentarios o clics). Entre el entrenamiento y el servidor, es posible que cambien los atributos en la tabla. Por lo tanto, puede cambiar predicción del modelo para el mismo documento entre el entrenamiento y la publicación. La forma más sencilla de evitar este tipo de problemas es registrar los atributos durante la publicación (consulta la regla n.º 32). Si la tabla cambia lentamente, puedes tomar una instantánea de la tabla a cada hora o cada día para obtener datos razonablemente cercanos. Ten en cuenta que esto no resuelve completamente el problema.&lt;/p&gt;

&lt;p&gt;Regla n.º 32: Reutiliza el código entre la canalización de entrenamiento y la canalización del servidor, siempre que sea posible.
El procesamiento por lotes es diferente al procesamiento en línea. En el procesamiento en línea, debes responder cada solicitud a medida que llega (p. ej., debes realizar una búsqueda separada para cada consulta). En el procesamiento por lotes, puedes combinar tareas (p. ej., unir funciones). En la publicación, implementas el procesamiento en línea, mientras que el entrenamiento es una tarea de procesamiento por lotes. Sin embargo, hay varias formas de reutilizar el código. Por ejemplo, puedes crear un objeto que sea específico para tu sistema, donde el resultado de cualquier consulta o unión se puede almacenar de una forma legible y donde los errores se pueden probar fácilmente. Luego, una vez que hayas reunido toda la información, durante el entrenamiento o la publicación, ejecutas un método común para conectar el objeto legible específico del sistema y el formato que espera el sistema de aprendizaje automático. Esto elimina una fuente de desviación entre el entrenamiento y la publicación. Como corolario, intenta no usar dos lenguajes de programación diferentes entre el entrenamiento y la publicación. Si haces esto, será casi imposible compartir el código.&lt;/p&gt;

&lt;p&gt;Regla n.º 33: Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero.
En general, mide el rendimiento de un modelo con datos reunidos en forma posterior a aquellos con los que se ha entrenado el modelo, ya que refleja de forma más precisa qué hará el sistema en la producción. Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. El rendimiento no debería ser tan bueno en los datos nuevos, pero no debería ser mucho peor. Como puede haber efectos diarios, es posible que no predigas la tasa de clics promedio o la tasa de conversión, pero el área bajo la curva, que representa la posibilidad de darle una puntuación más alta al ejemplo positivo que al ejemplo negativo, debería ser razonablemente parecida.&lt;/p&gt;

&lt;p&gt;Regla n.º 34: En la clasificación binaria para filtrado (como la detección de spam o la identificación de correos electrónicos de interés), realiza pequeños sacrificios a corto plazo en el rendimiento para lograr datos más claros.
En la tarea de filtrado, los ejemplos que se marcan como negativos no se muestran al usuario. Supongamos que tienes un filtro que bloquea el 75% de los ejemplos negativos durante la publicación. Puede surgir la tentación de obtener más datos de entrenamiento a partir de las instancias que se muestran a los usuarios. Por ejemplo, si un usuario marca como spam un correo electrónico que permitió tu filtro, se puede aprender de esta acción.&lt;/p&gt;

&lt;p&gt;Pero este enfoque introduce un sesgo en la muestra. Puedes obtener datos más claros si etiquetas el 1% de todo el tráfico como &amp;ldquo;retenido&amp;rdquo; durante la publicación y envías todos los ejemplos retenidos al usuario. Ahora, el filtro bloqueará al menos el 74% de los ejemplos negativos. Los ejemplos retenidos se convertirán en los datos de entrenamiento.&lt;/p&gt;

&lt;p&gt;Si el filtro bloquea el 95% o más de los ejemplos negativos, este enfoque se hace menos viable. Aun así, si deseas medir el rendimiento en la publicación, puedes hacer una muestra todavía más pequeña (p. ej., 0.1% o 0.001%). Diez mil ejemplos son suficientes para estimar el rendimiento de forma precisa.&lt;/p&gt;

&lt;p&gt;Regla n.º 35: Ten en cuenta la desviación inherente a los problemas de ranking.
Si cambias el algoritmo de clasificación lo suficiente como para ver resultados diferentes, habrás logrado modificar los datos que el algoritmo verá en el futuro. Este tipo de desviación aparecerá, y debes tenerla en cuenta al diseñar el modelo. Existen varias estrategias diferentes que sirven para favorecer los datos que tu modelo ya vio.&lt;/p&gt;

&lt;p&gt;Permite tener una regularización más alta en los atributos que cubren más consultas, a diferencia de esos atributos que solo abarcan una consulta. De esta forma, el modelo favorecerá los atributos que son específicos a una o pocas consultas por sobre los atributos que se generalizan a todas las consultas. Esta estrategia permite evitar que los resultados muy populares acaben en consultas irrelevantes. Este enfoque es contrario a la sugerencia más tradicional de contar con más regularización en columnas de funciones con más valores únicos.
Permite que los atributos solo tengan ponderaciones positivas. Además, cualquier atributo bueno será mejor que uno &amp;ldquo;desconocido&amp;rdquo;.
No uses atributos asociados solo al documentos. Esto es una versión extrema de la regla n.º 1. Por ejemplo, incluso si una app determinada es una descarga popular, más allá de la consulta, no es necesario mostrarla en todos lados. Esto se simplifica al no tener atributos solo de documentos. La razón por la que no deseas mostrar una app popular específica en todos lados está relacionada con la importancia de lograr que las apps que deseas estén disponibles. Por ejemplo, si alguien busca &amp;ldquo;apps para observar pájaros&amp;rdquo;, es posible que descarguen &amp;ldquo;Angry birds&amp;rdquo;, pero de forma claramente accidental. Si se muestra esta app, es posible que mejore la tasa de descarga, pero no se cumplen con las necesidades del usuario.
Regla n.º 36: Evita los ciclos de retroalimentación con atributos posicionales.
La posición del contenido afecta enormemente la probabilidad de que el usuario interactúe con este. Si ubicas una app en la primera posición, se seleccionará con más frecuencia y te dará la impresión de que es más probable que se seleccione. Una forma de lidiar con eso es agregar atributos de posición, es decir, atributos sobre la posición del contenido en la página. Entrena el modelo con atributos de posición para que aprenda a darle una mayor ponderación al atributo &amp;ldquo;primera posición&amp;rdquo;, por ejemplo. Así, el modelo les asigna una ponderación menor a otros factores con ejemplos de &amp;ldquo;primera posición=verdadero&amp;rdquo;. Entonces, en la publicación, no asignas ninguna instancia al atributo de posición (o le asignas el mismo atributo predeterminado), porque calificas candidatos antes de que hayas decidido el orden en el que se mostrarán.&lt;/p&gt;

&lt;p&gt;Ten en cuenta que es importante mantener cualquier atributo de posición separado del resto del modelo, debido a la asimetría entre el entrenamiento y la prueba. Lo ideal es que el modelo sea la suma de una función de los atributos posicionales y una función del resto de atributos. Por ejemplo, no cruces los atributos de posición con cualquier atributo de documentos.&lt;/p&gt;

&lt;p&gt;Regla n.º 37: Mide la desviación entre el entrenamiento y la publicación.
En el sentido más general, existen diversas razones para la desviación. Además, se puede dividir en varias partes:&lt;/p&gt;

&lt;p&gt;La diferencia entre el rendimiento en los datos de entrenamiento y los datos retenidos. En general, esta diferencia siempre existe y no siempre es negativa.
La diferencia entre el rendimiento en los datos retenidos y los datos &amp;ldquo;del día siguiente&amp;rdquo;. De nuevo, esta diferencia siempre existe. Debes ajustar la regularización para maximizar el rendimiento del día siguiente. Sin embargo, caídas notables en el rendimiento entre los datos retenidos y los datos del día siguiente pueden ser un indicador de que algunos atributos dependen del tiempo y posiblemente afecten al rendimiento del modelo de forma negativa.
La diferencia entre el rendimiento en los datos del día siguiente y los datos en vivo. Si aplicas un modelo a un ejemplo en los datos de entrenamiento y el mismo ejemplo en la publicación, deberías obtener el mismo resultado (consulta la regla n.º 5). Por lo tanto, si aparece una discrepancia, probablemente indique un error de ingeniería.
Fase III de aprendizaje automático: Crecimiento reducido, refinamiento de la optimización y modelos complejos
Existen ciertos indicios de que la segunda fase llega a su fin. Primero, las ganancias mensuales comienzan a disminuir. Las métricas comenzarán a emparejarse: en algunos experimentos, verás que algunas aumentan y otras disminuyen. Este es un momento interesante. Dado que las ganancias son más difíciles de obtener, el aprendizaje automático debe sofisticarse aún más. Una advertencia: esta sección contiene más reglas especulativas que las secciones anteriores. Hemos visto a muchos equipos realizar grandes progresos en la Fase I y la Fase II de aprendizaje automático. Una vez que alcanzan la Fase III, los equipos deben buscar su propio camino.&lt;/p&gt;

&lt;p&gt;Regla n.º 38: No pierdas tiempo en nuevos atributos si los objetivos no alineados son un problema.
Cuando las mediciones comienzan a estabilizarse, el equipo comenzará a buscar problemas fuera del alcance de los objetivos de tu sistema de aprendizaje automático actual. Como indicamos anteriormente, si el objetivo algorítmico existente no abarca los propósitos del producto, debes cambiar el objetivo o los propósitos. Por ejemplo, puedes optimizar clics, +1 o descargas, pero toma las decisiones de lanzamiento según los evaluadores humanos.&lt;/p&gt;

&lt;p&gt;Regla n.º 39: Las decisiones de lanzamiento representan los objetivos a largo plazo del producto.
Alice tiene una idea para reducir la pérdida logística de las instalaciones predichas. Agrega un atributo. Se reduce la pérdida logística. Cuando hace un experimento en vivo, observa un aumento en la tasa de instalación. Sin embargo, cuando realiza una reunión para evaluar el lanzamiento, alguien menciona que la cantidad de usuarios activos diarios cayó un 5%. El equipo decide no lanzar el modelo. Alice está decepcionada, pero ahora se da cuenta de que las decisiones de lanzamiento dependen de varios factores y solo algunos de ellos se pueden optimizar directamente con AA.&lt;/p&gt;

&lt;p&gt;La verdad es que el mundo real no es un juego de rol; no hay &amp;ldquo;puntos de ataque&amp;rdquo; que indican el estado de tu producto. El equipo debe usar las estadísticas que reúne para intentar predecir de forma eficaz qué tan bueno será el sistema en el futuro. Deben preocuparse por la participación, el número de usuarios activos en un día (DAU), el número de usuarios activos en 30 días (30 DAU), la ganancia y el retorno de la inversión del anunciante. Estas métricas que se miden en las pruebas A/B representan los objetivos a largo plazo: satisfacer a los usuarios, aumentar la cantidad de usuarios, satisfacer a los socios y obtener ganancias. A su vez, puedes considerar estos propósitos como representantes de otros propósitos: lograr un producto útil y de calidad, y que la empresa prospere de aquí a cinco años.&lt;/p&gt;

&lt;p&gt;Las únicas decisiones de lanzamiento fáciles son cuando todas las métricas mejoran (o al menos no empeoran). Si el equipo puede elegir entre un algoritmo de aprendizaje automático sofisticado o una simple heurística (que funciona mejor en todas las métricas), debe elegir la heurística. Además, no existe una clasificación explícita para todos los valores de métricas posibles. En especial, considera estos dos escenarios siguientes:&lt;/p&gt;

&lt;p&gt;Experimento Usuarios activos por día    Ganancia/día
A   1 millón    $4 millones
B   2 millones  $2 millones
Si el sistema actual es A, entonces es poco probable que el equipo cambie a B. Si el sistema actual es B, entonces es poco probable que el equipo cambie a A. Esto parece contradecir el comportamiento racional; sin embargo, las predicciones de las métricas cambiantes pueden o no cumplirse. Por lo tanto, cada cambio conlleva un riesgo grande. Cada métrica abarca una cierta cantidad de riesgo que preocupa al equipo.&lt;/p&gt;

&lt;p&gt;Además, ninguna métrica representa la preocupación máxima del equipo, &amp;ldquo;¿dónde estará mi producto de aquí a cinco años?&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Por otro lado, las personas tienden a favorecer un objetivo que pueden optimizar directamente. La mayoría de las herramientas de aprendizaje automático favorecen dicho entorno. Un ingeniero agregando atributos nuevos puede lograr un flujo constante de lanzamiento en dicho entorno. Existe un tipo de aprendizaje automático, el aprendizaje de multiobjetivo, que comienza a resolver este problema. Por ejemplo, se puede formular un problema de satisfacción de restricciones con límites inferiores en cada métrica y optimiza alguna combinación lineal de las métricas. Pero, aun así, no todas las métricas se enmarcan fácilmente como objetivos de aprendizaje automático: si el usuario hace clic en un documento o instala una app, se debe a que se mostró el contenido. Pero es mucho más difícil determinar la razón por la que un usuario visita tu sitio. Cómo predecir el éxito futuro de un sitio de forma integral depende IA-completo: es tan difícil como la visión por computadora o el procesamiento de lenguajes naturales.&lt;/p&gt;

&lt;p&gt;Regla n.º 40: Mantén las combinaciones simples.
Los modelos unificados que aceptan atributos sin procesar y clasifican contenido directamente son los modelos más sencillos de depurar y comprender. Sin embargo, una combinación de modelos (un modelo que combina los resultados de otros modelos) puede funcionar mejor. Para mantener las cosas simples, cada modelo debe ser una combinación de modelos que solo acepta como entrada el resultado de otros modelos o un modelo básico que acepta muchos atributos, pero no ambos. Si tienes modelos sobre otros modelos que se entrenan de forma separada y los combinas, se puede generar un comportamiento erróneo.&lt;/p&gt;

&lt;p&gt;Usa un modelo simple como combinación, que solo acepte los resultados de los modelos &amp;ldquo;básicos&amp;rdquo; como entradas. También debes implementar propiedades en esos modelos de conjuntos. Por ejemplo, un aumento en el resultado generado por un modelo básico no debe reducir el resultado del combinado. Además, es mejor que los modelos entrantes se puedan interpretar de forma semántica (p. ej., calibrados), para que los cambios en los modelos subyacentes no confundan al modelo combinado. Asegúrate también que un aumento en la probabilidad predicha de un clasificador subyacente no reduzca la probabilidad predicha del conjunto.&lt;/p&gt;

&lt;p&gt;Regla n.º 41: Cuando el rendimiento se estanque, busca nuevas fuentes de información de forma cualitativa para agregar, en lugar de refinar las señales existentes.
Agregaste cierta información demográfica sobre el usuario. Agregaste cierta información sobre las palabras en el documento. Finalizaste la exploración de combinación de atributos y ajustaste la regularización. No observaste un lanzamiento con más de un 1% de mejora en las métricas clave, en varios trimestres. ¿Ahora qué?&lt;/p&gt;

&lt;p&gt;Es hora de desarrollar la infraestructura para atributos radicalmente diferentes, como el historial de los documentos a los que este usuario accedió en el último día, semana o año, o los datos de una propiedad diferente. Usa entidades de wikidatos o algún recurso interno de tu empresa (como el Gráfico de conocimiento de Google). Usa el aprendizaje profundo. Comienza a ajustar tus expectativas sobre el retorno de la inversión esperado y aumenta tus esfuerzos adecuadamente. Como en cualquier proyecto de ingeniería, debes comparar el beneficio de agregar nuevos atributos con el costo de una mayor complejidad.&lt;/p&gt;

&lt;p&gt;Regla n.º 42: No esperes que la diversidad, la personalización o la relevancia se correlacionen con la popularidad.
La diversidad en un conjunto de contenidos puede significar muchas cosas; la más común es la diversidad de la fuente del contenido. La personalización implica que cada usuario obtiene sus propios resultados. La relevancia implica que los resultados para una consulta específica son más apropiados para esa consulta que para otra. Además, por definición, estas tres propiedades se diferencian de lo común.&lt;/p&gt;

&lt;p&gt;El problema es que lo común tiende a ser difícil de superar.&lt;/p&gt;

&lt;p&gt;Ten en cuenta que tu sistema mide clics, el tiempo dedicado, reproducciones, +1, veces que se comparte el contenido, etc., es decir, la popularidad del contenido. A veces, los equipos intentan aprender un modelo personal con diversidad. Para implementar la personalización, agregan atributos que le permiten al sistema la personalización (algunos atributos que representan el interés del usuario) o la diversificación (atributos que indican si este documento tiene algún atributo en común con otros documentos de los resultados, como el autor o el contenido), y descubren que esos atributos obtienen una ponderación menor (o a veces, un signo diferente) al que esperaban.&lt;/p&gt;

&lt;p&gt;Esto no significa que la diversidad, la personalización o la relevancia no sean valiosas. Como se indicó en la regla anterior, puedes hacer un posprocesamiento para aumentar la diversidad o la relevancia. Si observas que los objetivos a largo plazo aumentan, puedes declarar que la diversidad o la relevancia son valiosas, más allá de la popularidad. Puedes continuar usando el posprocesamiento o directamente modificar el objetivo según la diversidad o la relevancia.&lt;/p&gt;

&lt;p&gt;Regla n.º 43: Tus amigos tienden a ser los mismos en diferentes productos. No así tus intereses.
Varios equipos en Google ganaron mucho terreno al tomar que un modelo que predice la cercanía de una conexión con un producto y lograr que funcione en otro producto. Tus amigos no cambian. Por otro lado, observé a muchos equipos lidiar con atributos de personalización entre productos. Sí, parece que debería funcionar. Por ahora, parece que no. Un método que a veces funciona es usar los datos sin procesar de una propiedad para predecir el comportamiento en otra. Ten en cuenta que también puede ayudar saber que un usuario tiene un historial en otra propiedad. Por ejemplo, la presencia de actividad de usuario en dos propiedades puede ser un indicativo en sí misma.&lt;/p&gt;

&lt;p&gt;Trabajo relacionado
Existen muchos documentos sobre aprendizaje automático en Google y en otras fuentes.&lt;/p&gt;

&lt;p&gt;Curso intensivo de aprendizaje automático: Una introducción al aprendizaje automático aplicado.
Aprendizaje automático: Un enfoque probabilístico de Kevin Murphy, para comprender el campo de aprendizaje automático.
Consejos prácticos para el análisis de conjuntos de datos grandes y complejos: Un enfoque de ciencia de datos sobre los conjuntos de datos.
Aprendizaje profundo de Ian Goodfellow et al, para aprendizaje de modelos no lineales.
Documento de Google sobre la deuda técnica, con muchos consejos generales.
Documentación de Tensorflow.
Agradecimientos
Agradezco a David Westbrook, Peter Brandt, Samuel Ieong, Chenyu Zhao, Li Wei, Michalis Potamias, Evan Rosen, Barry Rosenberg, Christine Robson, James Pine, Tal Shaked, Tushar Chandra, Mustafa Ispir, Jeremiah Harmsen, Konstantinos Katsiapis, Glen Anderson, Dan Duckworth, Shishir Birmiwal, Gal Elidan, Su Lin Wu, Jaihui Liu, Fernando Pereira y Hrishikesh Aradhye por las numerosas correcciones, sugerencias y ejemplos útiles para este documento. Agradezco también a Kristen Lefevre, Suddha Basu y Chris Berg quienes colaboraron en una versión anterior. Cualquier error, omisión u opiniones controversiales es mi responsabilidad.&lt;/p&gt;

&lt;p&gt;Anexo
Existen diferentes referencias a productos de Google en este documento. Para proporcionar más contexto, agregué una descripción breve de los ejemplos más comunes a continuación.&lt;/p&gt;

&lt;p&gt;Descripción de YouTube
YouTube es un servicio de transmisión de videos. Tanto los equipos de YouTube Watch Next y de la página principal de YouTube usan modelos de AA para clasificar recomendaciones de videos. Watch Next recomienda videos para ver después del que se está reproduciendo, la página principal recomienda videos para los usuarios que exploran la página principal.&lt;/p&gt;

&lt;p&gt;Descripción general de Google Play
Google Play tiene muchos modelos para resolver diferentes problemas. Las búsqueda de apps en Play, las recomendaciones personalizadas en la página principal de Play y &amp;ldquo;Otros usuarios también instalaron&amp;rdquo; usan aprendizaje automático.&lt;/p&gt;

&lt;p&gt;Descripción de Google Plus
Google Plus usa aprendizaje automático en diferentes situaciones: clasificar las publicaciones en la sección &amp;ldquo;Novedades&amp;rdquo; que ve el usuario, las publicaciones en la sección &amp;ldquo;Lo más interesante&amp;rdquo; (las publicaciones que son más populares), las personas que conoces, etc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Programa R para Data Science</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/advanced-r/r201-program/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/advanced-r/r201-program/</guid>
      <description>

&lt;h2 id=&#34;introducción&#34;&gt;Introducción&lt;/h2&gt;

&lt;p&gt;El objetivo de la primera parte de este libro es ponerlo al día con las herramientas básicas de __ exploración de datos__ lo más rápido posible. La exploración de datos es el arte de mirar sus datos, generar hipótesis rápidamente, probarlas rápidamente y luego repetir una y otra vez. El objetivo de la exploración de datos es generar muchos clientes potenciales prometedores que luego puede explorar con mayor profundidad.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../data-science-explore.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;En esta parte del libro aprenderá algunas herramientas útiles que tienen una recompensa inmediata:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;La visualización es un excelente lugar para comenzar con la programación R, porque el la recompensa es muy clara: puedes hacer tramas elegantes e informativas que ayudan Entiendes los datos. En [visualización de datos] te sumergirás en la visualización,aprender la estructura básica de un diagrama de ggplot2 y técnicas poderosas para
    convirtiendo datos en tramas.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;La visualización por sí sola no suele ser suficiente, por lo que en [transformación de datos]
    aprenderá los verbos clave que le permiten seleccionar variables importantes,
    filtrar observaciones clave, crear nuevas variables y calcular resúmenes.
  &lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Finalmente, en [análisis exploratorio de datos], combinará la visualización y transformación con tu curiosidad y escepticismo para preguntar y responder Preguntas interesantes sobre los datos.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;El modelado es una parte importante del proceso exploratorio, pero aún no tiene las habilidades para aprenderlo o aplicarlo de manera efectiva. Volveremos a ello en &lt;a href=&#34;# introducción de modelo&#34;&gt;modelado&lt;/a&gt;, una vez que esté mejor equipado con más herramientas de programación y disputas de datos.&lt;/p&gt;

&lt;p&gt;Entre estos tres capítulos que le enseñan las herramientas de exploración hay tres capítulos que se centran en su flujo de trabajo de R. En [workflow: basics], [workflow: scripts] y [workflow: projects] aprenderá buenas prácticas para escribir y organizar su código R. Estos lo prepararán para el éxito a largo plazo, ya que le brindarán las herramientas para mantenerse organizado cuando aborde proyectos reales.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python 101 - Funciones</title>
      <link>https://www.marcusrb.com/en/courses/python/py101/py101-2-funciones/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/python/py101/py101-2-funciones/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Select, From &amp; Where</title>
      <link>https://www.marcusrb.com/en/courses/business-analytics/intro-sql/sql101-1-select/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/business-analytics/intro-sql/sql101-1-select/</guid>
      <description>

&lt;h2 id=&#34;introducción&#34;&gt;Introducción&lt;/h2&gt;

&lt;p&gt;Ahora que sabe cómo acceder y examinar un conjunto de datos, ¡está listo para escribir su primera consulta SQL! Como pronto verá, las consultas SQL lo ayudarán a clasificar un conjunto de datos masivo, para recuperar solo la información que necesita.&lt;/p&gt;

&lt;p&gt;Comenzaremos usando las palabras clave &lt;strong&gt;SELECT, FROM y WHERE&lt;/strong&gt; para obtener datos de columnas específicas según las condiciones que especifique.&lt;/p&gt;

&lt;p&gt;Para mayor claridad, trabajaremos con un pequeño conjunto de datos imaginario &lt;em&gt;pet_records&lt;/em&gt; que contiene solo una tabla, llamada mascotas &lt;em&gt;pets&lt;/em&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ID&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Animal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Dr. Harris&lt;/td&gt;
&lt;td&gt;Rabbit&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Moon&lt;/td&gt;
&lt;td&gt;Dog&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Ripley&lt;/td&gt;
&lt;td&gt;Cat&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Tom&lt;/td&gt;
&lt;td&gt;Cat&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;select-from&#34;&gt;SELECT&amp;hellip;.FROM&lt;/h3&gt;

&lt;p&gt;La consulta SQL más básica selecciona una sola columna de una sola tabla. Para hacer esto,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;especifique la columna que desea después de la palabra &lt;strong&gt;SELECT&lt;/strong&gt;, y luego&lt;/li&gt;
&lt;li&gt;especifique la tabla después de la palabra &lt;strong&gt;FROM&lt;/strong&gt;.
Por ejemplo, para seleccionar la columna &lt;em&gt;Name&lt;/em&gt; (de la tabla de mascotas &lt;em&gt;pets&lt;/em&gt; en la base de datos &lt;em&gt;pet_records&lt;/em&gt; en el proyecto bigquery-public-data), nuestra consulta aparecerá de la siguiente manera:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;../c3GxYRt.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Tenga en cuenta que al escribir una consulta SQL, el argumento que pasamos a FROM no está entre comillas simples o dobles (&amp;lsquo;o &amp;ldquo;). Está en comillas invertidas (`).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;WHERE&lt;/strong&gt; &amp;hellip;
Los conjuntos de datos de BigQuery son grandes, por lo que generalmente querrá devolver solo las filas que cumplan condiciones específicas. Puede hacerlo utilizando la cláusula WHERE.&lt;/p&gt;

&lt;p&gt;La consulta a continuación devuelve las entradas de la columna Nombre &lt;em&gt;Name&lt;/em&gt; que están en filas donde la columna &lt;em&gt;Animal&lt;/em&gt; tiene el texto &lt;em&gt;&amp;lsquo;Cat&amp;rsquo;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../HJOT8Kb.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;: ¿Cuáles son todas las ciudades de EE. UU. En el conjunto de datos OpenAQ?
Ahora que ya tiene lo básico, veamos un ejemplo con un conjunto de datos real. Utilizaremos un conjunto de datos OpenAQ sobre la calidad del aire.&lt;/p&gt;

&lt;p&gt;Primero, configuraremos todo lo que necesitamos para ejecutar consultas y echar un vistazo rápido a las tablas que hay en nuestra base de datos.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.cloud import bigquery

# Create a &amp;quot;Client&amp;quot; object
client = bigquery.Client()

# Construct a reference to the &amp;quot;openaq&amp;quot; dataset
dataset_ref = client.dataset(&amp;quot;openaq&amp;quot;, project=&amp;quot;bigquery-public-data&amp;quot;)

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

# List all the tables in the &amp;quot;openaq&amp;quot; dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there&#39;s only one!)
for table in tables:  
    print(table.table_id)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El conjunto de datos contiene solo una tabla, llamada &lt;em&gt;global_air_quality&lt;/em&gt;. Buscaremos la tabla y echaremos un vistazo a las primeras filas para ver qué tipo de datos contiene.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Construct a reference to the &amp;quot;global_air_quality&amp;quot; table
table_ref = dataset_ref.table(&amp;quot;global_air_quality&amp;quot;)

# API request - fetch the table
table = client.get_table(table_ref)

# Preview the first five lines of the &amp;quot;global_air_quality&amp;quot; table
client.list_rows(table, max_results=5).to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;¡Todo se ve bien! Entonces, hagamos una consulta. Supongamos que queremos seleccionar todos los valores de la columna de la ciudad &lt;em&gt;city&lt;/em&gt; que están en filas donde la columna del país &lt;em&gt;country&lt;/em&gt; es &lt;em&gt;&amp;lsquo;US&amp;rsquo;&lt;/em&gt; (Para &amp;ldquo;Estados Unidos&amp;rdquo;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Query to select all the items from the &amp;quot;city&amp;quot; column where the &amp;quot;country&amp;quot; column is &#39;US&#39;
query = &amp;quot;&amp;quot;&amp;quot;
        SELECT city
        FROM `bigquery-public-data.openaq.global_air_quality`
        WHERE country = &#39;US&#39;
        &amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tómese el tiempo ahora para asegurarse de que esta consulta se alinee con lo que aprendió anteriormente.&lt;/p&gt;

&lt;h3 id=&#34;enviando-la-consulta-al-conjunto-de-datos&#34;&gt;Enviando la consulta al conjunto de datos¶&lt;/h3&gt;

&lt;p&gt;Estamos listos para usar esta consulta para obtener información del conjunto de datos OpenAQ. Como en el tutorial anterior, el primer paso es crear un objeto &lt;em&gt;Client&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a &amp;quot;Client&amp;quot; object
client = bigquery.Client()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Comenzamos configurando la consulta con el método &lt;em&gt;query()&lt;/em&gt;. Ejecutamos el método con los parámetros predeterminados, pero este método también nos permite especificar configuraciones más complicadas sobre las que puede leer en la documentación. Volveremos sobre esto más tarde.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set up the query
query_job = client.query(query)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación, ejecutamos la consulta y convertimos los resultados en un DataFrame de pandas.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# API request - run the query, and return a pandas DataFrame
us_cities = query_job.to_dataframe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora tenemos un DataFrame de pandas llamado &lt;em&gt;us_cities&lt;/em&gt;, que podemos usar como cualquier otro DataFrame.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# What five cities have the most measurements?
us_cities.city.value_counts().head()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Paquetes</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/paquetes/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/paquetes/</guid>
      <description>

&lt;p&gt;R es un programa modular. Hay muchas funciones disponibles en la distribución estándar (consulte Instalación de R), pero se pueden agregar muchas más gracias a paquetes y complementos adicionales.&lt;/p&gt;

&lt;h2 id=&#34;instalación&#34;&gt;Instalación&lt;/h2&gt;

&lt;p&gt;La forma más fácil de instalar paquetes, con una conexión a Internet activa, es a través del menú R: Paquetes → Instalar paquetes.&lt;/p&gt;

&lt;p&gt;A veces, el funcionamiento de un paquete depende de la presencia de otros paquetes (dependencias). Para asegurarse de instalar tanto el paquete como las dependencias, es preferible usar el comando:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;Rcmdr&amp;quot;, dependencies = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;uso&#34;&gt;Uso&lt;/h2&gt;

&lt;p&gt;Los paquetes adicionales no se activan automáticamente cuando se abre R, pero deben &amp;ldquo;llamarse&amp;rdquo; con la biblioteca de comandos (). Por ejemplo, para usar &lt;em&gt;RCommander&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(Rcmdr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Al llamar a un paquete, si faltan las dependencias (cualquier paquete del que dependa), se abrirá un cuadro de diálogo que le permitirá descargarlo e instalarlo automáticamente.&lt;/p&gt;

&lt;p&gt;También se pueden llamar paquetes desde el menú de la consola: Paquetes → Cargar paquete.&lt;/p&gt;

&lt;h2 id=&#34;actualización&#34;&gt;Actualización&lt;/h2&gt;

&lt;p&gt;Periódicamente, es necesario verificar la existencia de versiones más actualizadas de los paquetes R. Es preferible ejecutar este procedimiento desde la ventana R, y haber cerrado RCommander, que a su vez debe actualizarse.&lt;/p&gt;

&lt;p&gt;Para actualizar paquetes en Windows, debe ejecutar R desde los administradores.&lt;/p&gt;

&lt;p&gt;El comando para actualizar los paquetes está en el menú Paquetes (Actualizar paquetes &amp;hellip;), y el procedimiento es completamente automático: solo siga las instrucciones en pantalla.&lt;/p&gt;

&lt;p&gt;También es posible actualizar los paquetes escribiendo el comando&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;update.packages()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;comandos-principales&#34;&gt;Comandos principales&lt;/h2&gt;

&lt;p&gt;Para saber qué paquetes están instalados en su sistema:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para cargar la ayuda del paquete:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;help(package)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para eliminar el paquete del sistema&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remove.packages( &amp;quot;\_nombre_paquete\_&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para averiguar dónde está instalado un archivo de paquete:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;system.file(package = &amp;quot;nombre_paquete&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para aprender más&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://cran.stat.unipd.it/web/packages/&#34; target=&#34;_blank&#34;&gt;Todos los paquetes disponibles&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reglas del aprendizaje automático - Fase II</title>
      <link>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-0-reglas2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-0-reglas2/</guid>
      <description>

&lt;h1 id=&#34;fase-ii-de-aprendizaje-automático-ingeniería-de-atributos&#34;&gt;Fase II de aprendizaje automático: Ingeniería de atributos&lt;/h1&gt;

&lt;p&gt;En la primera fase del ciclo de vida de un sistema de aprendizaje automático, la prioridad es mandar los datos de entrenamiento al sistema de aprendizaje, lograr instrumentar las métricas de interés y crear una infraestructura de publicación. Una vez que cuentas con un sistema integral en funcionamiento con pruebas de unidades y del sistema instrumentadas, comienza la fase II.&lt;/p&gt;

&lt;p&gt;En la segunda fase, hay muchas recompensas a corto plazo. Existe una variedad de atributos obvios que se pueden agregar al sistema. Además, la segunda fase de aprendizaje automático implica agregar tantos atributos como sea posible y combinarlos de formas intuitivas. Durante esta fase, todas las métricas deben continuar subiendo. Habrá muchos lanzamientos, y es una excelente oportunidad para incorporar muchos ingenieros que puedan recopilar todos los datos que necesitas para crear un sistema de aprendizaje verdaderamente sorprendente.&lt;/p&gt;

&lt;p&gt;Regla n.º 16: Planifica el lanzamiento y la iteración.
No esperes que el modelo en el que trabajas ahora sea el último que lanzarás o, incluso, que dejarás de lanzar modelos. Por lo tanto, ten en cuenta que la complejidad de este lanzamiento retrasará los lanzamientos futuros. Muchos equipos han lanzado uno o más modelos por trimestre durante años. Existen tres razones básicas para lanzar modelos nuevos:&lt;/p&gt;

&lt;p&gt;Tienes atributos nuevos.
Deseas ajustar la regularización y combinar atributos antiguos de formas nuevas.
Deseas ajustar el objetivo.
Independientemente de la razón, es recomendable poner atención en el modelo: analizar los datos que se ingresan en el ejemplo te permite encontrar señales nuevas y detectar señales antiguas con problemas. Entonces, a medida que desarrollas el modelo, piensa en lo fácil que es agregar, quitar o recombinar atributos. Piensa en lo fácil que es crear una copia nueva de la canalización y verifica que sea correcta. Piensa en si es posible ejecutar dos o tres copias en paralelo. Por último, no te preocupes si no logras incluir todos los atributos en esta versión de la canalización. Las incluirás el próximo trimestre.&lt;/p&gt;

&lt;p&gt;Regla n.º 17: Comienza con los atributos directamente observados e informados, en lugar de los atributos aprendidos.
Este puede ser un punto controversial, pero evita muchos problemas. Primero, describamos qué es un atributo aprendido. Un atributo aprendido es un atributo generado por un sistema externo (como un sistema de agrupación en clústeres sin supervisar) o el mismo modelo (por ejemplo, mediante un modelo factorizado o aprendizaje profundo). Ambas opciones pueden ser útiles, pero tienen muchos problemas, por lo que no es conveniente incluirlas en el primer modelo.&lt;/p&gt;

&lt;p&gt;Si usas un sistema externo para crear un atributo, recuerda que ese sistema tiene su propio objetivo. Es posible que el objetivo del sistema externo no tenga mucha relación con tu objetivo actual. Si realizas una instantánea del sistema externo, es posible que esté desactualizada. Si actualizas los atributos desde el sistema externo, es posible que los significados cambien. Si usas un sistema externo para proporcionar un atributo, debes tener mucho cuidado con ese enfoque.&lt;/p&gt;

&lt;p&gt;El principal problema de los modelos multiplicados y los modelos profundos es que no son convexos. Por lo tanto, no hay garantía de encontrar una solución óptima (o de aproximarse a esta). Además, el mínimo local de cada iteración puede ser diferente. Esta variación no permite juzgar con exactitud si el impacto de un cambio en tu sistema es relevante o contingente. Si creas un modelo sin atributos profundos, obtienes un sistema de referencia con buen rendimiento. Una vez que alcanzas este punto de referencia, puedes probar enfoques menos ortodoxos.&lt;/p&gt;

&lt;p&gt;Regla n.º 18: Prueba atributos de contenido que generalicen en contextos.
A menudo, un sistema de aprendizaje automático es una parte pequeña de una entidad mucho más grande. Por ejemplo, si imaginas una publicación que se puede usar en Lo Más Interesante, mucha gente hará +1, la compartirá o escribirá un comentario en ella antes de que llegue a mostrarse en Lo más interesante. Si proporcionas esas estadísticas al modelo, este puede promocionar publicaciones nuevas para las que no tiene datos en el contexto que está optimizando. YouTube Watch Next puede usar la cantidad de videos que miraste, o que miraste de forma secuencial (la cantidad de veces que se miró un video después de otro) en la búsqueda de YouTube. También puedes usar clasificaciones de usuarios explícitas. Por último, si usas una acción de usuario como etiqueta, ver esa acción en el documento en un contexto diferente puede ser un excelente atributo. Todos estos atributos te permiten aportar contenido nuevo al contexto. Ten en cuenta que esto no se trata de personalización: primero, descubre si a alguien le gusta el contenido en este contexto; luego, descubre a quién le gusta más o menos.&lt;/p&gt;

&lt;p&gt;Regla n.º 19: Usa atributos muy específicos cuando sea posible.
Gracias a la gran cantidad de datos, es más fácil aprender millones de atributos sencillos que unos pocos atributos complejos. Los identificadores de documentos que se obtienen y las consultas canónicas no brindan mucha generalización, pero alinean las clasificaciones con las etiquetas en las consultas principales. Por lo tanto, no temas agrupar atributos si cada uno se aplica a una fracción muy pequeña de datos, pero la cobertura total es de más del 90%. Puedes usar la regularización para eliminar los atributos que se aplican a pocos ejemplos.&lt;/p&gt;

&lt;p&gt;Regla n.º 20: Combina y modifica los atributos existentes para crear atributos nuevos de una forma legible.
Hay diferentes formas de combinar y modificar atributos. Los sistemas de aprendizaje automático como TensorFlow te permiten preprocesar los datos mediante transformaciones. Los dos enfoques estándar son las &amp;ldquo;discretizaciones&amp;rdquo; y las &amp;ldquo;combinaciones&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;La discretización consiste en tomar un atributo continuo y crear varios atributos discretos. Considera un atributo continuo como la edad. Puedes crear un atributo que es 1 cuando la edad es menor de 18, otro atributo que es 1 cuando la edad es entre 18 y 35, etc. No pienses demasiado en los límites de estos histogramas; los cuantiles básicos serán los más eficaces.&lt;/p&gt;

&lt;p&gt;Las combinaciones unen dos o más columnas de atributos. En la terminología de TensorFlow, una columna de atributos es un conjunto de atributos homogéneos (p. ej., {masculino, femenino}, {EE.UU., Canadá, México}, etc.). Una combinación es una nueva columna de atributos que incluye, p. ej., {masculino, femenino} × {EE.UU., Canadá, México}. Esta nueva columna de atributos contendrá el atributo (masculino, Canadá). Si usas TensorFlow y le indicas que cree esta combinación, este atributo (masculino, Canadá) aparecerá en los ejemplos que representan canadienses masculinos. Ten en cuenta que se necesita una enorme cantidad de datos para aprender modelos con combinaciones de tres, cuatro o más columnas de atributos básicos.&lt;/p&gt;

&lt;p&gt;Las combinaciones que producen columnas de atributos muy grandes pueden producir un sobreajuste. Por ejemplo, imagina que haces algún tipo de búsqueda y tienes una columna de atributos con palabras en la consulta y otra con palabras en el documento. Si unes estas columnas con una combinación, terminarás con muchos atributos (consulta la regla n.º 21).&lt;/p&gt;

&lt;p&gt;Cuando trabajas con texto, existen dos alternativas. La más rigurosa es un producto escalar. En su forma más simple, un producto escalar simplemente cuenta la cantidad de palabras en común entre la consulta y el documento. Por lo tanto, este atributo se puede discretizar. Otro enfoque es una intersección: tenemos un atributo que está presente solo si la palabra &amp;ldquo;poni&amp;rdquo; aparece en el documento y en la consulta, y otro atributo que está presente solo si la palabra &amp;ldquo;el&amp;rdquo; aparece en el documento y en la consulta.&lt;/p&gt;

&lt;p&gt;Regla n.º 21: La cantidad de ponderaciones de atributos que puedes aprender en un modelo lineal es casi proporcional a la cantidad de datos que tienes.
Existen resultados teóricos fascinantes sobre aprendizaje estadístico relacionados con el nivel de complejidad correspondiente de un modelo, pero esta regla es todo lo que necesitas saber. Tuve discusiones con personas que ponían en duda que se pudiera aprender algo con mil ejemplos o que nunca se necesitan más de un millón de ejemplos, porque están empecinados con un determinado método de aprendizaje. La clave es escalar el aprendizaje a la medida del tamaño de los datos:&lt;/p&gt;

&lt;p&gt;Si trabajas con un sistema de ranking de búsquedas, existen millones de palabras diferentes en los documentos y la consulta, y tienes 1000 ejemplos etiquetados, debes usar un producto escalar entre los atributos de consultas y de documentos, TF-IDF y una media docena de otros atributos desarrollados por humanos. 1000 ejemplos, una docena de atributos.
Si tienes un millón de ejemplos, intersecta la columna de atributos de consultas con la de documentos, y aplica regularización y, posiblemente, selección de atributos. Esto te brindará un millón de atributos, pero, con la regularización, tendrás menos. 10 millones de ejemplos, tal vez 100,000 atributos.
Si tienes miles o cientos de miles de millones, puedes combinar las columnas de atributos con tokens de consultas y de documentos mediante la regularización y la selección de atributos. Tendrás miles de millones de ejemplos y 10 millones de atributos. La teoría de aprendizaje estadístico raramente establece límites rígidos, pero ofrece un buen punto de partida.
En última instancia, usa la regla n.º 28 para decir qué atributos usarás.&lt;/p&gt;

&lt;p&gt;Regla n.º 22: Quita los atributos que ya no uses.
Los atributos sin usar crean deuda técnica. Si descubres que no estás usando un atributo y que no sirve combinarlo con otros atributos, quítalo de la infraestructura. Debes mantener limpia tu infraestructura para que puedas probar los atributos más prometedores tan rápido como sea posible. Si es necesario, tu atributo se puede volver a agregar en cualquier momento.&lt;/p&gt;

&lt;p&gt;Ten en cuenta la cobertura cuando analices qué atributos agregarás o conservarás. ¿Cuántos ejemplos cubre el atributo? Por ejemplo, si tienes algunos atributos de personalización, pero solo el 8% de los usuarios tiene atributos de personalización, eso no será muy eficaz.&lt;/p&gt;

&lt;p&gt;Al mismo tiempo, algunos atributos te sorprenden gratamente. Por ejemplo, si tienes un atributo que cubre solo el 1% de los datos, pero el 90% de los ejemplos de este atributo son positivos, es un excelente atributo que agregar.&lt;/p&gt;

&lt;p&gt;Análisis humano del sistema
Antes de avanzar a la tercera fase de aprendizaje automático, es importante hablar de algo que no se enseña en ninguna clase de aprendizaje automático: cómo analizar un modelo existente y mejorarlo. Esto es más un arte que una ciencia. Aun así, existen muchos antipatrones que es conveniente evitar.&lt;/p&gt;

&lt;p&gt;Regla n.º 23: No eres el típico usuario final.
Probablemente, esta es la razón más común por la que un equipo no progresa. Si bien usar un prototipo con tu equipo o usar un prototipo en tu empresa tiene muchos beneficios, los empleados deben analizar si el rendimiento es correcto. Si bien un cambio que sea evidentemente malo no debe usarse, cualquier función que parezca lista para la producción debe probarse aún más, ya sea contratando a gente común para que responda preguntas en una plataforma de participación colectiva o mediante un experimento en vivo con usuarios reales.&lt;/p&gt;

&lt;p&gt;Hay dos razones para ello. Estás demasiado familiarizado con el código. Es posible que busques un aspecto específico de las publicaciones, o que estés muy involucrado (p. ej., sesgo de confirmación). La segunda razón es que tu tiempo es demasiado valioso. Considera el costo de nueve ingenieros en una reunión de una hora de duración y cuántas etiquetas logradas con trabajo humano puedes obtener en una plataforma de participación colectiva.&lt;/p&gt;

&lt;p&gt;Si realmente quieres comentarios de usuarios, implementa metodologías de experiencia de usuario. Crea usuarios persona (puedes encontrar una descripción en el libro Sketching User Experiences [Cómo diseñar experiencias de usuario] de Bill Buxton) al comienzo del proceso y, luego, implementa una prueba de usabilidad (puedes encontrar una descripción en el libro Don’t Make Me Think [No me hagas pensar] de Steve Krug). Los usuarios persona implican crear un usuario hipotético. Por ejemplo, si tu equipo se compone solo por hombres, será beneficioso diseñar un usuario persona femenina de 35 años (completa con atributos de usuario) y observar los resultados que genera, en lugar de los 10 resultados para hombres de 25 a 40 años. También puedes lograr una nueva perspectiva al evaluar cómo reaccionan las personas reales a tu sitio (de forma local o remota) en una prueba de usabilidad.&lt;/p&gt;

&lt;p&gt;Regla n.º 24: Mide el delta entre los modelos.
Una de las formas más sencillas y, a veces, más útiles de realizar mediciones que puedes usar antes de que los usuarios vean tu nuevo modelo es calcular la diferencia entre los nuevos resultados y los obtenidos con el sistema en producción. Por ejemplo, si tienes un problema de ranking, ejecuta ambos modelos con una muestra de consultas en todo el sistema y observa el tamaño de la diferencia simétrica de los resultados (ponderados según la posición en el ranking). Si la diferencia es muy pequeña, entonces puedes deducir que habrá poco cambio, sin necesidad de ejecutar un experimento. Si la diferencia es muy grande, entonces debes asegurarte de que el cambio sea positivo. Analizar las consultas donde la diferencia simétrica es alta te puede ayudar a comprender de forma cualitativa cómo fue el cambio. Sin embargo, asegúrate de que el sistema sea estable. Cuando compares un modelo consigo mismo, asegúrate de que tenga una diferencia simétrica baja (idealmente cero).&lt;/p&gt;

&lt;p&gt;Regla n.º 25: Cuando elijas un modelo, el rendimiento utilitario predomina por sobre el poder de predicción.
Tu modelo puede intentar predecir la tasa de clics. Sin embargo, en última instancia, la pregunta clave es lo que haces con esa predicción. Si la usas para clasificar documentos, entonces la calidad del ranking final importa más que la predicción en sí misma. Si predices la probabilidad de que un documento sea spam y, luego, tienes un punto límite sobre lo que se bloquea, entonces la precisión de lo que se permite tiene más importancia. La mayoría de las veces, estos dos aspectos coinciden; cuando no es así, es probable que haya una pequeña ganancia. Además, si hay algún cambio que mejore la pérdida logística, pero que reduzca el rendimiento del sistema, busca otro atributo. Si esto comienza a suceder con más frecuencia, es hora de volver a evaluar el objetivo del modelo.&lt;/p&gt;

&lt;p&gt;Regla n.º 26: Busca patrones en los errores observados y crea atributos nuevos.
Supongamos que ves un ejemplo de entrenamiento que el modelo &amp;ldquo;no entendió&amp;rdquo;. En una tarea de clasificación, este error puede ser un falso positivo o un falso negativo. En una tarea de clasificación, el error puede ser un par donde un positivo tiene un ranking menor que un negativo. El punto más importante es que sea un ejemplo que el sistema de aprendizaje automático sepa que no lo entendió y que lo corrija si tiene la oportunidad. Si le agregas un atributo al modelo para que pueda corregir el error, el modelo intentará usarlo.&lt;/p&gt;

&lt;p&gt;Por otro lado, si intentas crear un atributo basado en ejemplos que el sistema no considera errores, el atributo se ignorará. Por ejemplo, supongamos que, en la búsqueda de apps de Play, alguien busca &amp;ldquo;juegos gratuitos&amp;rdquo;. Supongamos que uno de los primeros resultados es una app de bromas menos relevante. Entonces, creas un atributo para &amp;ldquo;apps de bromas&amp;rdquo;. Sin embargo, si maximizas la cantidad de instalaciones y las personas instalan una app de bromas cuando buscan juegos gratuitos, el atributo para &amp;ldquo;apps de bromas&amp;rdquo; no tendrá el efecto deseado.&lt;/p&gt;

&lt;p&gt;Una vez que tengas ejemplos que el modelo no haya entendido, busca las tendencias que estén fuera del conjunto de atributos actual. Por ejemplo, si parece que el sistema penaliza las publicaciones más largas, agrega la longitud de la publicación. No seas demasiado específico sobre los atributos que agregues. Si agregas la longitud de la publicación, no intentes adivinar qué significa &amp;ldquo;largo&amp;rdquo;, solo agrega una decena de atributos y permite que el modelo descubra qué hacer con ellos (consulta la regla n.º 21). Esta es la forma más fácil de obtener el resultado deseado.&lt;/p&gt;

&lt;p&gt;Regla n.º 27: Intenta cuantificar el comportamiento no deseado que observes.
Algunos miembros de tu equipo comenzarán a frustrarse con las propiedades del sistema que no les gusten, ya que la función de pérdida existente no las captura. En este punto, deben hacer lo que sea necesario para convertir sus quejas en números sólidos. Por ejemplo, si consideran que se muestran demasiadas &amp;ldquo;apps de bromas&amp;rdquo; en la búsqueda de Play, se pueden contratar a evaluadores humanos para que identifiquen las apps de bromas. (Puedes usar datos etiquetados por humanos en este caso, ya que una fracción relativamente pequeña de consultas representan una gran fracción del tráfico). Si los problemas se pueden medir, puedes comenzar a usarlos como atributos, objetivos o métricas. La regla general es &amp;ldquo;medir primero, optimizar después&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Regla n.º 28: Ten en cuenta que el comportamiento idéntico a corto plazo no implica un comportamiento idéntico a largo plazo.
Imagina que tienes un sistema nuevo que analiza cada doc_id y exact_query, y luego calcula la probabilidad de clic para cada documento y cada consulta. Descubres que este comportamiento es casi idéntico a tu sistema actual en la comparación y la prueba A/B; por lo tanto, dado su simplicidad, lo ejecutas. Sin embargo, notas que no se muestran apps nuevas. ¿Por qué? Bueno, dado que tu sistema solo muestra un documento basado en su propio historial con esa consulta, no hay forma de aprender qué documento nuevo debe mostrar.&lt;/p&gt;

&lt;p&gt;La única forma de entender cómo debe funcionar un sistema a largo plazo es entrenarlo solo con datos adquiridos cuando el modelo está publicado. Esto es muy difícil.&lt;/p&gt;

&lt;p&gt;Desviación entre el entrenamiento y la publicación
La desviación entre el entrenamiento y la publicación es la diferencia entre el rendimiento del entrenamiento y el de la publicación. Existen diferentes razones para esta desviación:&lt;/p&gt;

&lt;p&gt;una discrepancia entre cómo manipulas los datos en las canalizaciones de entrenamiento y del servidor
un cambio en los datos entre el momento del entrenamiento y el del servidor
un ciclo de retroalimentación entre el modelo y el algoritmo
Hemos observado sistemas de aprendizaje automático de producción en Google con una desviación entre el entrenamiento y la publicación que afecta negativamente el rendimiento. La mejor solución es supervisarlo de forma explícita para que los cambios en el sistema y en los datos no generen una desviación inadvertida.&lt;/p&gt;

&lt;p&gt;Regla n.º 29: La mejor manera de asegurarte de que el entrenamiento se asemeja a la publicación es guardar el conjunto de atributos que usas en la publicación y canalizar esos atributos en un registro para luego usarlos en el entrenamiento.
Incluso si no lo puedes hacer para cada ejemplo, hazlo para una fracción pequeña, de forma tal que puedas verificar la coherencia entre la publicación y el entrenamiento (consulta la regla n.º 37). En Google, los equipos que hicieron esta medición se sorprendieron a menudo por los resultados. La página de inicio de YouTube cambió a atributos del registro en el servidor, lo que generó mejoras de calidad importantes y redujo la complejidad del código. Ahora, muchos equipos están cambiando sus infraestructuras.&lt;/p&gt;

&lt;p&gt;Regla n.º 30: ¡Realiza muestras con datos con ponderación por importancia, no los quites!
Cuando tienes muchos datos, es tentador usar los archivos del 1 al 12 e ignorar los archivos del 13 al 99. Esto es un error. Si bien se pueden quitar los datos que nunca se mostraron al usuario, la ponderación por importancia es la mejor opción para el resto. La ponderación por importancia implica que, si decides hacer una muestra con el ejemplo X con una probabilidad del 30%, la ponderación será de &lt;sup&gt;10&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;. Con la ponderación por importancia, se mantienen todas las propiedades de calibración que analizamos en la regla n.º 14.&lt;/p&gt;

&lt;p&gt;Regla n.º 31: Ten en cuenta que, si cruzas datos de una tabla durante el entrenamiento y la publicación, los datos en la tabla pueden cambiar.
Digamos que cruzas ID de documentos a una tabla que contiene atributos para esos documentos (como la cantidad de comentarios o clics). Entre el entrenamiento y el servidor, es posible que cambien los atributos en la tabla. Por lo tanto, puede cambiar predicción del modelo para el mismo documento entre el entrenamiento y la publicación. La forma más sencilla de evitar este tipo de problemas es registrar los atributos durante la publicación (consulta la regla n.º 32). Si la tabla cambia lentamente, puedes tomar una instantánea de la tabla a cada hora o cada día para obtener datos razonablemente cercanos. Ten en cuenta que esto no resuelve completamente el problema.&lt;/p&gt;

&lt;p&gt;Regla n.º 32: Reutiliza el código entre la canalización de entrenamiento y la canalización del servidor, siempre que sea posible.
El procesamiento por lotes es diferente al procesamiento en línea. En el procesamiento en línea, debes responder cada solicitud a medida que llega (p. ej., debes realizar una búsqueda separada para cada consulta). En el procesamiento por lotes, puedes combinar tareas (p. ej., unir funciones). En la publicación, implementas el procesamiento en línea, mientras que el entrenamiento es una tarea de procesamiento por lotes. Sin embargo, hay varias formas de reutilizar el código. Por ejemplo, puedes crear un objeto que sea específico para tu sistema, donde el resultado de cualquier consulta o unión se puede almacenar de una forma legible y donde los errores se pueden probar fácilmente. Luego, una vez que hayas reunido toda la información, durante el entrenamiento o la publicación, ejecutas un método común para conectar el objeto legible específico del sistema y el formato que espera el sistema de aprendizaje automático. Esto elimina una fuente de desviación entre el entrenamiento y la publicación. Como corolario, intenta no usar dos lenguajes de programación diferentes entre el entrenamiento y la publicación. Si haces esto, será casi imposible compartir el código.&lt;/p&gt;

&lt;p&gt;Regla n.º 33: Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero.
En general, mide el rendimiento de un modelo con datos reunidos en forma posterior a aquellos con los que se ha entrenado el modelo, ya que refleja de forma más precisa qué hará el sistema en la producción. Si produces un modelo basado en los datos hasta el 5 de enero, prueba el modelo en los datos a partir del 6 de enero. El rendimiento no debería ser tan bueno en los datos nuevos, pero no debería ser mucho peor. Como puede haber efectos diarios, es posible que no predigas la tasa de clics promedio o la tasa de conversión, pero el área bajo la curva, que representa la posibilidad de darle una puntuación más alta al ejemplo positivo que al ejemplo negativo, debería ser razonablemente parecida.&lt;/p&gt;

&lt;p&gt;Regla n.º 34: En la clasificación binaria para filtrado (como la detección de spam o la identificación de correos electrónicos de interés), realiza pequeños sacrificios a corto plazo en el rendimiento para lograr datos más claros.
En la tarea de filtrado, los ejemplos que se marcan como negativos no se muestran al usuario. Supongamos que tienes un filtro que bloquea el 75% de los ejemplos negativos durante la publicación. Puede surgir la tentación de obtener más datos de entrenamiento a partir de las instancias que se muestran a los usuarios. Por ejemplo, si un usuario marca como spam un correo electrónico que permitió tu filtro, se puede aprender de esta acción.&lt;/p&gt;

&lt;p&gt;Pero este enfoque introduce un sesgo en la muestra. Puedes obtener datos más claros si etiquetas el 1% de todo el tráfico como &amp;ldquo;retenido&amp;rdquo; durante la publicación y envías todos los ejemplos retenidos al usuario. Ahora, el filtro bloqueará al menos el 74% de los ejemplos negativos. Los ejemplos retenidos se convertirán en los datos de entrenamiento.&lt;/p&gt;

&lt;p&gt;Si el filtro bloquea el 95% o más de los ejemplos negativos, este enfoque se hace menos viable. Aun así, si deseas medir el rendimiento en la publicación, puedes hacer una muestra todavía más pequeña (p. ej., 0.1% o 0.001%). Diez mil ejemplos son suficientes para estimar el rendimiento de forma precisa.&lt;/p&gt;

&lt;p&gt;Regla n.º 35: Ten en cuenta la desviación inherente a los problemas de ranking.
Si cambias el algoritmo de clasificación lo suficiente como para ver resultados diferentes, habrás logrado modificar los datos que el algoritmo verá en el futuro. Este tipo de desviación aparecerá, y debes tenerla en cuenta al diseñar el modelo. Existen varias estrategias diferentes que sirven para favorecer los datos que tu modelo ya vio.&lt;/p&gt;

&lt;p&gt;Permite tener una regularización más alta en los atributos que cubren más consultas, a diferencia de esos atributos que solo abarcan una consulta. De esta forma, el modelo favorecerá los atributos que son específicos a una o pocas consultas por sobre los atributos que se generalizan a todas las consultas. Esta estrategia permite evitar que los resultados muy populares acaben en consultas irrelevantes. Este enfoque es contrario a la sugerencia más tradicional de contar con más regularización en columnas de funciones con más valores únicos.
Permite que los atributos solo tengan ponderaciones positivas. Además, cualquier atributo bueno será mejor que uno &amp;ldquo;desconocido&amp;rdquo;.
No uses atributos asociados solo al documentos. Esto es una versión extrema de la regla n.º 1. Por ejemplo, incluso si una app determinada es una descarga popular, más allá de la consulta, no es necesario mostrarla en todos lados. Esto se simplifica al no tener atributos solo de documentos. La razón por la que no deseas mostrar una app popular específica en todos lados está relacionada con la importancia de lograr que las apps que deseas estén disponibles. Por ejemplo, si alguien busca &amp;ldquo;apps para observar pájaros&amp;rdquo;, es posible que descarguen &amp;ldquo;Angry birds&amp;rdquo;, pero de forma claramente accidental. Si se muestra esta app, es posible que mejore la tasa de descarga, pero no se cumplen con las necesidades del usuario.
Regla n.º 36: Evita los ciclos de retroalimentación con atributos posicionales.
La posición del contenido afecta enormemente la probabilidad de que el usuario interactúe con este. Si ubicas una app en la primera posición, se seleccionará con más frecuencia y te dará la impresión de que es más probable que se seleccione. Una forma de lidiar con eso es agregar atributos de posición, es decir, atributos sobre la posición del contenido en la página. Entrena el modelo con atributos de posición para que aprenda a darle una mayor ponderación al atributo &amp;ldquo;primera posición&amp;rdquo;, por ejemplo. Así, el modelo les asigna una ponderación menor a otros factores con ejemplos de &amp;ldquo;primera posición=verdadero&amp;rdquo;. Entonces, en la publicación, no asignas ninguna instancia al atributo de posición (o le asignas el mismo atributo predeterminado), porque calificas candidatos antes de que hayas decidido el orden en el que se mostrarán.&lt;/p&gt;

&lt;p&gt;Ten en cuenta que es importante mantener cualquier atributo de posición separado del resto del modelo, debido a la asimetría entre el entrenamiento y la prueba. Lo ideal es que el modelo sea la suma de una función de los atributos posicionales y una función del resto de atributos. Por ejemplo, no cruces los atributos de posición con cualquier atributo de documentos.&lt;/p&gt;

&lt;p&gt;Regla n.º 37: Mide la desviación entre el entrenamiento y la publicación.
En el sentido más general, existen diversas razones para la desviación. Además, se puede dividir en varias partes:&lt;/p&gt;

&lt;p&gt;La diferencia entre el rendimiento en los datos de entrenamiento y los datos retenidos. En general, esta diferencia siempre existe y no siempre es negativa.
La diferencia entre el rendimiento en los datos retenidos y los datos &amp;ldquo;del día siguiente&amp;rdquo;. De nuevo, esta diferencia siempre existe. Debes ajustar la regularización para maximizar el rendimiento del día siguiente. Sin embargo, caídas notables en el rendimiento entre los datos retenidos y los datos del día siguiente pueden ser un indicador de que algunos atributos dependen del tiempo y posiblemente afecten al rendimiento del modelo de forma negativa.
La diferencia entre el rendimiento en los datos del día siguiente y los datos en vivo. Si aplicas un modelo a un ejemplo en los datos de entrenamiento y el mismo ejemplo en la publicación, deberías obtener el mismo resultado (consulta la regla n.º 5). Por lo tanto, si aparece una discrepancia, probablemente indique un error de ingeniería.
Fase III de aprendizaje automático: Crecimiento reducido, refinamiento de la optimización y modelos complejos
Existen ciertos indicios de que la segunda fase llega a su fin. Primero, las ganancias mensuales comienzan a disminuir. Las métricas comenzarán a emparejarse: en algunos experimentos, verás que algunas aumentan y otras disminuyen. Este es un momento interesante. Dado que las ganancias son más difíciles de obtener, el aprendizaje automático debe sofisticarse aún más. Una advertencia: esta sección contiene más reglas especulativas que las secciones anteriores. Hemos visto a muchos equipos realizar grandes progresos en la Fase I y la Fase II de aprendizaje automático. Una vez que alcanzan la Fase III, los equipos deben buscar su propio camino.&lt;/p&gt;

&lt;p&gt;Regla n.º 38: No pierdas tiempo en nuevos atributos si los objetivos no alineados son un problema.
Cuando las mediciones comienzan a estabilizarse, el equipo comenzará a buscar problemas fuera del alcance de los objetivos de tu sistema de aprendizaje automático actual. Como indicamos anteriormente, si el objetivo algorítmico existente no abarca los propósitos del producto, debes cambiar el objetivo o los propósitos. Por ejemplo, puedes optimizar clics, +1 o descargas, pero toma las decisiones de lanzamiento según los evaluadores humanos.&lt;/p&gt;

&lt;p&gt;Regla n.º 39: Las decisiones de lanzamiento representan los objetivos a largo plazo del producto.
Alice tiene una idea para reducir la pérdida logística de las instalaciones predichas. Agrega un atributo. Se reduce la pérdida logística. Cuando hace un experimento en vivo, observa un aumento en la tasa de instalación. Sin embargo, cuando realiza una reunión para evaluar el lanzamiento, alguien menciona que la cantidad de usuarios activos diarios cayó un 5%. El equipo decide no lanzar el modelo. Alice está decepcionada, pero ahora se da cuenta de que las decisiones de lanzamiento dependen de varios factores y solo algunos de ellos se pueden optimizar directamente con AA.&lt;/p&gt;

&lt;p&gt;La verdad es que el mundo real no es un juego de rol; no hay &amp;ldquo;puntos de ataque&amp;rdquo; que indican el estado de tu producto. El equipo debe usar las estadísticas que reúne para intentar predecir de forma eficaz qué tan bueno será el sistema en el futuro. Deben preocuparse por la participación, el número de usuarios activos en un día (DAU), el número de usuarios activos en 30 días (30 DAU), la ganancia y el retorno de la inversión del anunciante. Estas métricas que se miden en las pruebas A/B representan los objetivos a largo plazo: satisfacer a los usuarios, aumentar la cantidad de usuarios, satisfacer a los socios y obtener ganancias. A su vez, puedes considerar estos propósitos como representantes de otros propósitos: lograr un producto útil y de calidad, y que la empresa prospere de aquí a cinco años.&lt;/p&gt;

&lt;p&gt;Las únicas decisiones de lanzamiento fáciles son cuando todas las métricas mejoran (o al menos no empeoran). Si el equipo puede elegir entre un algoritmo de aprendizaje automático sofisticado o una simple heurística (que funciona mejor en todas las métricas), debe elegir la heurística. Además, no existe una clasificación explícita para todos los valores de métricas posibles. En especial, considera estos dos escenarios siguientes:&lt;/p&gt;

&lt;p&gt;Experimento Usuarios activos por día    Ganancia/día
A   1 millón    $4 millones
B   2 millones  $2 millones
Si el sistema actual es A, entonces es poco probable que el equipo cambie a B. Si el sistema actual es B, entonces es poco probable que el equipo cambie a A. Esto parece contradecir el comportamiento racional; sin embargo, las predicciones de las métricas cambiantes pueden o no cumplirse. Por lo tanto, cada cambio conlleva un riesgo grande. Cada métrica abarca una cierta cantidad de riesgo que preocupa al equipo.&lt;/p&gt;

&lt;p&gt;Además, ninguna métrica representa la preocupación máxima del equipo, &amp;ldquo;¿dónde estará mi producto de aquí a cinco años?&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Por otro lado, las personas tienden a favorecer un objetivo que pueden optimizar directamente. La mayoría de las herramientas de aprendizaje automático favorecen dicho entorno. Un ingeniero agregando atributos nuevos puede lograr un flujo constante de lanzamiento en dicho entorno. Existe un tipo de aprendizaje automático, el aprendizaje de multiobjetivo, que comienza a resolver este problema. Por ejemplo, se puede formular un problema de satisfacción de restricciones con límites inferiores en cada métrica y optimiza alguna combinación lineal de las métricas. Pero, aun así, no todas las métricas se enmarcan fácilmente como objetivos de aprendizaje automático: si el usuario hace clic en un documento o instala una app, se debe a que se mostró el contenido. Pero es mucho más difícil determinar la razón por la que un usuario visita tu sitio. Cómo predecir el éxito futuro de un sitio de forma integral depende IA-completo: es tan difícil como la visión por computadora o el procesamiento de lenguajes naturales.&lt;/p&gt;

&lt;p&gt;Regla n.º 40: Mantén las combinaciones simples.
Los modelos unificados que aceptan atributos sin procesar y clasifican contenido directamente son los modelos más sencillos de depurar y comprender. Sin embargo, una combinación de modelos (un modelo que combina los resultados de otros modelos) puede funcionar mejor. Para mantener las cosas simples, cada modelo debe ser una combinación de modelos que solo acepta como entrada el resultado de otros modelos o un modelo básico que acepta muchos atributos, pero no ambos. Si tienes modelos sobre otros modelos que se entrenan de forma separada y los combinas, se puede generar un comportamiento erróneo.&lt;/p&gt;

&lt;p&gt;Usa un modelo simple como combinación, que solo acepte los resultados de los modelos &amp;ldquo;básicos&amp;rdquo; como entradas. También debes implementar propiedades en esos modelos de conjuntos. Por ejemplo, un aumento en el resultado generado por un modelo básico no debe reducir el resultado del combinado. Además, es mejor que los modelos entrantes se puedan interpretar de forma semántica (p. ej., calibrados), para que los cambios en los modelos subyacentes no confundan al modelo combinado. Asegúrate también que un aumento en la probabilidad predicha de un clasificador subyacente no reduzca la probabilidad predicha del conjunto.&lt;/p&gt;

&lt;p&gt;Regla n.º 41: Cuando el rendimiento se estanque, busca nuevas fuentes de información de forma cualitativa para agregar, en lugar de refinar las señales existentes.
Agregaste cierta información demográfica sobre el usuario. Agregaste cierta información sobre las palabras en el documento. Finalizaste la exploración de combinación de atributos y ajustaste la regularización. No observaste un lanzamiento con más de un 1% de mejora en las métricas clave, en varios trimestres. ¿Ahora qué?&lt;/p&gt;

&lt;p&gt;Es hora de desarrollar la infraestructura para atributos radicalmente diferentes, como el historial de los documentos a los que este usuario accedió en el último día, semana o año, o los datos de una propiedad diferente. Usa entidades de wikidatos o algún recurso interno de tu empresa (como el Gráfico de conocimiento de Google). Usa el aprendizaje profundo. Comienza a ajustar tus expectativas sobre el retorno de la inversión esperado y aumenta tus esfuerzos adecuadamente. Como en cualquier proyecto de ingeniería, debes comparar el beneficio de agregar nuevos atributos con el costo de una mayor complejidad.&lt;/p&gt;

&lt;p&gt;Regla n.º 42: No esperes que la diversidad, la personalización o la relevancia se correlacionen con la popularidad.
La diversidad en un conjunto de contenidos puede significar muchas cosas; la más común es la diversidad de la fuente del contenido. La personalización implica que cada usuario obtiene sus propios resultados. La relevancia implica que los resultados para una consulta específica son más apropiados para esa consulta que para otra. Además, por definición, estas tres propiedades se diferencian de lo común.&lt;/p&gt;

&lt;p&gt;El problema es que lo común tiende a ser difícil de superar.&lt;/p&gt;

&lt;p&gt;Ten en cuenta que tu sistema mide clics, el tiempo dedicado, reproducciones, +1, veces que se comparte el contenido, etc., es decir, la popularidad del contenido. A veces, los equipos intentan aprender un modelo personal con diversidad. Para implementar la personalización, agregan atributos que le permiten al sistema la personalización (algunos atributos que representan el interés del usuario) o la diversificación (atributos que indican si este documento tiene algún atributo en común con otros documentos de los resultados, como el autor o el contenido), y descubren que esos atributos obtienen una ponderación menor (o a veces, un signo diferente) al que esperaban.&lt;/p&gt;

&lt;p&gt;Esto no significa que la diversidad, la personalización o la relevancia no sean valiosas. Como se indicó en la regla anterior, puedes hacer un posprocesamiento para aumentar la diversidad o la relevancia. Si observas que los objetivos a largo plazo aumentan, puedes declarar que la diversidad o la relevancia son valiosas, más allá de la popularidad. Puedes continuar usando el posprocesamiento o directamente modificar el objetivo según la diversidad o la relevancia.&lt;/p&gt;

&lt;p&gt;Regla n.º 43: Tus amigos tienden a ser los mismos en diferentes productos. No así tus intereses.
Varios equipos en Google ganaron mucho terreno al tomar que un modelo que predice la cercanía de una conexión con un producto y lograr que funcione en otro producto. Tus amigos no cambian. Por otro lado, observé a muchos equipos lidiar con atributos de personalización entre productos. Sí, parece que debería funcionar. Por ahora, parece que no. Un método que a veces funciona es usar los datos sin procesar de una propiedad para predecir el comportamiento en otra. Ten en cuenta que también puede ayudar saber que un usuario tiene un historial en otra propiedad. Por ejemplo, la presencia de actividad de usuario en dos propiedades puede ser un indicativo en sí misma.&lt;/p&gt;

&lt;p&gt;Trabajo relacionado
Existen muchos documentos sobre aprendizaje automático en Google y en otras fuentes.&lt;/p&gt;

&lt;p&gt;Curso intensivo de aprendizaje automático: Una introducción al aprendizaje automático aplicado.
Aprendizaje automático: Un enfoque probabilístico de Kevin Murphy, para comprender el campo de aprendizaje automático.
Consejos prácticos para el análisis de conjuntos de datos grandes y complejos: Un enfoque de ciencia de datos sobre los conjuntos de datos.
Aprendizaje profundo de Ian Goodfellow et al, para aprendizaje de modelos no lineales.
Documento de Google sobre la deuda técnica, con muchos consejos generales.
Documentación de Tensorflow.
Agradecimientos
Agradezco a David Westbrook, Peter Brandt, Samuel Ieong, Chenyu Zhao, Li Wei, Michalis Potamias, Evan Rosen, Barry Rosenberg, Christine Robson, James Pine, Tal Shaked, Tushar Chandra, Mustafa Ispir, Jeremiah Harmsen, Konstantinos Katsiapis, Glen Anderson, Dan Duckworth, Shishir Birmiwal, Gal Elidan, Su Lin Wu, Jaihui Liu, Fernando Pereira y Hrishikesh Aradhye por las numerosas correcciones, sugerencias y ejemplos útiles para este documento. Agradezco también a Kristen Lefevre, Suddha Basu y Chris Berg quienes colaboraron en una versión anterior. Cualquier error, omisión u opiniones controversiales es mi responsabilidad.&lt;/p&gt;

&lt;p&gt;Anexo
Existen diferentes referencias a productos de Google en este documento. Para proporcionar más contexto, agregué una descripción breve de los ejemplos más comunes a continuación.&lt;/p&gt;

&lt;p&gt;Descripción de YouTube
YouTube es un servicio de transmisión de videos. Tanto los equipos de YouTube Watch Next y de la página principal de YouTube usan modelos de AA para clasificar recomendaciones de videos. Watch Next recomienda videos para ver después del que se está reproduciendo, la página principal recomienda videos para los usuarios que exploran la página principal.&lt;/p&gt;

&lt;p&gt;Descripción general de Google Play
Google Play tiene muchos modelos para resolver diferentes problemas. Las búsqueda de apps en Play, las recomendaciones personalizadas en la página principal de Play y &amp;ldquo;Otros usuarios también instalaron&amp;rdquo; usan aprendizaje automático.&lt;/p&gt;

&lt;p&gt;Descripción de Google Plus
Google Plus usa aprendizaje automático en diferentes situaciones: clasificar las publicaciones en la sección &amp;ldquo;Novedades&amp;rdquo; que ve el usuario, las publicaciones en la sección &amp;ldquo;Lo más interesante&amp;rdquo; (las publicaciones que son más populares), las personas que conoces, etc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prácticas 2 - Vectores</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-vectores/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-vectores/</guid>
      <description>

&lt;h2 id=&#34;creando-vectores&#34;&gt;Creando vectores&lt;/h2&gt;

&lt;p&gt;Te sientes de suerte?&lt;/p&gt;

&lt;p&gt;Eso espero, porque en este capítulo vamos de viaje a la Ciudad del Pecado, también conocida como el &amp;ldquo;Paraíso del estadístico&amp;rdquo; ;-).&lt;/p&gt;

&lt;p&gt;Gracias a R y a tus nuevas habilidades analíticas, vas a aprender cómo mejorar tus ganancias en el casino y empezar una lucrativa carrera como jugador profesional. En este capítulo veremos cómo puedes fácilmente llevar la cuenta de tus apuestas y como hacer análisis simples de tus jugadas. Próxima parada&amp;hellip; Viva las Vegas!!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Esperamos que recuerdes lo que aprendiste en el capítulo anterior. Asigna el valor &amp;ldquo;Alla vamos!&amp;rdquo; a la variable Vegas&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Define la variable Vegas
Vegas &amp;lt;- &amp;quot;Alla vamos!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creando-vectores-2&#34;&gt;Creando vectores (2)&lt;/h2&gt;

&lt;p&gt;Primero concentrémonos.&lt;/p&gt;

&lt;p&gt;En nuestro camino a para quebrar al casino, haremos uso extensivo de los vectores. Un vector es un arreglo unidimensional que puede contener datos numéricos, caracteres, o valores lógicos. En otras palabras un vector es una herramienta simple para guardar un conjunto de datos del mismo tipo. Por ejemplo podemos llevar la cuenta de las ganancias y pérdidas en los juegos de casino.&lt;/p&gt;

&lt;p&gt;En R, se crea un vector con la función c(). La función se llama c por &amp;ldquo;combinar&amp;rdquo;. Se ponen los valores o elementos del vector dentro de paréntesis, separados por coma. Por ejemplo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vector_numerico &amp;lt;- c(1, 2, 3)
vector_caracter &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;)
vector_logico &amp;lt;- c(TRUE, FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Una vez has creado estos vectores en R puedes usarlos para hacer cálculos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Completa el código de tal manera que el vector_logico contenga tres elementos: TRUE, FALSE y TRUE (en ese orden).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vector_numerico &amp;lt;- c(1, 10, 49)
vector_caracter &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;)
# Completa el código para vector_logico
vector_logico &amp;lt;- c(TRUE, FALSE, TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creando-vectores-3&#34;&gt;Creando vectores (3)&lt;/h2&gt;

&lt;p&gt;Después de una semana en Las Vegas y cero Ferraris en tu garage, decides que es hora de empezar a utilizar tus super-poderes analíticos.&lt;/p&gt;

&lt;p&gt;Antes de hacer un primer análisis, decides llevar un registro de todas tus ganancias y pérdidas de la semana pasada:&lt;/p&gt;

&lt;p&gt;En el poker:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Lunes: ganaste 140&lt;/li&gt;
&lt;li&gt;Martes: perdiste 50&lt;/li&gt;
&lt;li&gt;Miercoles: ganaste 20&lt;/li&gt;
&lt;li&gt;Jueves: perdiste 120&lt;/li&gt;
&lt;li&gt;Viernes: ganaste 240&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;En la ruleta:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Lunes: perdiste 24&lt;/li&gt;
&lt;li&gt;Martes: perdiste 50&lt;/li&gt;
&lt;li&gt;Miercoles: ganaste 100&lt;/li&gt;
&lt;li&gt;Jueves: perdiste 350&lt;/li&gt;
&lt;li&gt;Viernes: ganaste 10&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Solamente has jugado poker y ruleta, porque un grupo de adivinos se ha apoderado de la mesa de dados. Para poder utilizar esos datos en R, decides crear las variables vector_poker y vector_ruleta.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ahora asigna los días de la semana como nombres a los vectores vector_poker y vector_ruleta. Usa los nombres con mayúscula inicial y sin acentos (esto no es clase de ortografía&amp;hellip;): Lunes, Martes, Miercoles, Jueves y Viernes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Imprime los vectores en la consola&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Asigna los nombres a los vectores
names(vector_poker) &amp;lt;- c(&amp;quot;Lunes&amp;quot;,&amp;quot;Martes&amp;quot;,&amp;quot;Miercoles&amp;quot;,&amp;quot;Jueves&amp;quot;,&amp;quot;Viernes&amp;quot;)
names(vector_ruleta) &amp;lt;- c(&amp;quot;Lunes&amp;quot;,&amp;quot;Martes&amp;quot;,&amp;quot;Miercoles&amp;quot;,&amp;quot;Jueves&amp;quot;,&amp;quot;Viernes&amp;quot;)

# Imprime los vectores en la consola
vector_poker
vector_ruleta
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;nombrando-elementos-2&#34;&gt;Nombrando elementos (2)&lt;/h2&gt;

&lt;p&gt;Si quieres ser un buen estadístico, tienes que ser un poco perezoso. (Si ya eres perezoso, quizás hayas nacido con esos excepcionales talentos estadísticos.)&lt;/p&gt;

&lt;p&gt;En los ejercicios anteriores probablemente sentiste que es aburrido escribir una y otra vez información como los días de la semana. Sin embargo, existe una manera mas eficiente de lograr esto, que tal si asignamos el vector que contiene los días de la semana a una variable.&lt;/p&gt;

&lt;p&gt;Así como lo hiciste con las ganancias/pérdidas de la ruleta y el poker, puedes también crear un vector que contenga los días de la semana. Al asignarlo a una variable, éste puede ser reusado.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Crea la variable vector_dias que contenga un vector con los días de la semana, de Lunes a Viernes.&lt;/li&gt;
&lt;li&gt;Usa la variable vector_dias para asignar nombres a los elementos de vector_poker y vector_ruleta.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Crea la variable vector_dias
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;,&amp;quot;Martes&amp;quot;,&amp;quot;Miercoles&amp;quot;,&amp;quot;Jueves&amp;quot;,&amp;quot;Viernes&amp;quot;)

# Asigna los nombres a los elementos de vector_poker y vector_ruleta
names(vector_poker) &amp;lt;-   vector_dias
names(vector_ruleta) &amp;lt;-  vector_dias
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;calculando-las-ganancias-totales&#34;&gt;Calculando las ganancias totales&lt;/h2&gt;

&lt;p&gt;Ahora que tienes los resultados del poker y la ruleta en un vector con elementos apropiadamente nombrados, puedes empezar a hacer análisis.&lt;/p&gt;

&lt;p&gt;Veremos como encontrar la siguiente información:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;La ganancia/pérdida total por cada día de la semana.&lt;/li&gt;
&lt;li&gt;La ganancia/pérdida total en la semana.&lt;/li&gt;
&lt;li&gt;Sabremos si estas ganando o perdiendo dinero en el poker y en la ruleta.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para saber lo que nos proponemos tenemos que hacer cálculos aritméticos con los vectores. Cuando sumamos dos vectores en R, éste hace la suma elemento por elemento, por ejemplo las siguientes líneas de código son completamente equivalentes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;c(1, 2, 3) + c(4, 5, 6)
c(1 + 4, 2 + 5, 3 + 6)
c(5, 7, 9)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Primero experimentemos sumando!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Toma la suma de las variables vector_A y vector_B y asígnala a vector_total.&lt;/li&gt;
&lt;li&gt;Mira el resultado imprimiendo el valor de vector_total en la consola.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vector_A &amp;lt;- c(1, 2, 3)
vector_B &amp;lt;- c(4, 5, 6)

# Asigna la suma de vector_A y vector_B
vector_total &amp;lt;- vector_A + vector_B

# Imprime el vector_total a la consola
vector_total

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;calculando-las-ganancias-totales-2&#34;&gt;Calculando las ganancias totales (2)&lt;/h2&gt;

&lt;p&gt;¿Entendiste como R realiza aritmética con los vectores?&lt;/p&gt;

&lt;p&gt;Ya es hora de tener uno de esos Ferraris en el garage! Primero, hay que saber cuál fue la ganancia/pérdida por cada día. La ganancia neta de cada día es la suma de las ganancias/pérdidas que hiciste en el poker y en la ruleta.&lt;/p&gt;

&lt;p&gt;En R, este cálculo es simplemente la suma de &lt;em&gt;vector_ruleta&lt;/em&gt; y &lt;em&gt;vector_poker&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Asigna a la variable total_diario lo que ganaste/perdiste cada día en total (poker y ruleta combinado).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nombra los elementos de total_diario utilizando vector_dias.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Imprime total_diario en la consola para que puedas ver tus resultados totales en cada día de la semana. Cuáles fueron los días buenos y los días malos?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Calculando las ganancias/pérdidas diarias:
total_diario &amp;lt;- vector_ruleta + vector_poker

# Dando nombres a total_diario
names(total_diario) &amp;lt;- vector_dias

# Imprime total_diario a la consola para ver tus resultados por día
  total_diario

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;calculando-ganancias-totales-3&#34;&gt;Calculando ganancias totales (3)&lt;/h2&gt;

&lt;p&gt;Basado en los análisis previos, parece que tuviste una mezcla de días buenos y malos. Esto no era lo que tu ego esperaba, y te preguntas si quizás hay una (muy pequeña) posibilidad de que hayas perdido dinero en la semana.&lt;/p&gt;

&lt;p&gt;La función que nos ayudará a contestar esta pregunta es sum(). Ésta función calcula la suma de todos los elementos de un vector. Por ejemplo para calcular (y asignar a una variable) el monto total de dinero que has ganado/perdido en el poker en la semana, escribe:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total_poker &amp;lt;- sum(vector_poker)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Calcula el monto total de dinero que has ganado/perdido en la ruleta, asígnalo a la variable _total&lt;em&gt;ruleta&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Ahora que tienes los totales para la ruleta y el poker, puedes fácilmente calcular el valor de _total&lt;em&gt;semana&lt;/em&gt;, que es la suma de todas las ganancias y pérdidas en la semana.&lt;/li&gt;
&lt;li&gt;Imprime el valor de _total&lt;em&gt;semana&lt;/em&gt; a la consola. ¿En total ganaste o perdiste en la semana?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de Poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Ganancias totales en el poker
total_poker &amp;lt;- sum(vector_poker)

# Ahora completa el código:
total_ruleta &amp;lt;- sum(vector_ruleta)
total_semana &amp;lt;- sum(total_poker + total_ruleta)

#Imprime el total que ganaste/perdiste en la semana
total_semana
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;comparando-ganancias-totales&#34;&gt;Comparando ganancias totales&lt;/h2&gt;

&lt;p&gt;Mmmmm&amp;hellip;. parece que estás perdiendo dinero. Que sorpresa! Hora de repensar tu estrategia! Esto va a requerir un análisis más profundo.&lt;/p&gt;

&lt;p&gt;Después de una breve lluvia de ideas en el jacuzzi del hotel, piensas que una posible explicación puede ser que tus habilidades en la ruleta no están tan bien desarrolladas como las de poker. Así que talvez tus ganancias totales en el poker son más grandes (&amp;gt;) que en la ruleta.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Calcula total_poker y total_ruleta como en el ejercicio anterior.&lt;/li&gt;
&lt;li&gt;Fíjate si tus ganancias totales en el poker son más altas que en la ruleta haciendo una comparación. Asigna el resultado de esta comparación a la variable respuesta. ¿Cuál es tu conclusión? ¿Deberías dedicarte a la ruleta o al poker?&lt;/li&gt;
&lt;li&gt;Imprime respuesta a la consola.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Ganancias totales en el poker y en la ruleta
total_poker &amp;lt;- sum(vector_poker)
total_ruleta &amp;lt;- sum(vector_ruleta)

# Probando si las ganancias en el poker son más altas que en la ruleta:
respuesta &amp;lt;- total_poker &amp;gt; total_ruleta

# Imprime respuesta a la consola
respuesta
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;seleccionando-elementos-los-buenos-tiempos&#34;&gt;Seleccionando elementos: los buenos tiempos&lt;/h2&gt;

&lt;p&gt;La corazonada parece que fue cierta. Parece que te va mejor en el poker que en la ruleta.&lt;/p&gt;

&lt;p&gt;Otro posible aspecto a investigar son tus resultados al comienzo de la semana comparados con los últimos días. Talvez te excediste en los tequilas al final de la semana&amp;hellip;&lt;/p&gt;

&lt;p&gt;Para responder esta pregunta, concentrémonos en solo unos elementos del vector_total. En otras palabras, nuestro objetivo es seleccionar elementos específicos de los vectores. Para seleccionar elementos de un vector (y luego matrices, data frames, etc.), puedes utilizar los corchetes. Entre corchetes indicamos los elementos que queremos seleccionar. Por ejemplo, para seleccionar el primer elemento de un vector, escribes vector_poker[1]. Para seleccionar el segundo elemento escribimos vector_poker[2], etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Asigna tu ganancia del Miércoles a la variable _poker&lt;em&gt;miercoles&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Asigna el valor de lo ganado el Miercoles en el poker
poker_miercoles &amp;lt;- vector_poker[3]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;seleccionando-elementos-los-buenos-tiempos-2&#34;&gt;Seleccionando elementos: los buenos tiempos (2)&lt;/h2&gt;

&lt;p&gt;¿Qué tal si analizamos los resultados en los días medios de la semana?&lt;/p&gt;

&lt;p&gt;Para seleccionar múltiples elementos de un vector, puedes hacerlo usando corchetes. En el ejercicio anterior pusimos un número, llamado índice, entre los corchetes y obtuvimos el elemento del vector correspondiente a ese índice. Para seleccionar varios elementos podemos utilizar otro vector cuyos elementos son los índices que deseamos seleccionar. Por ejemplo, para seleccionar el primero y el quinto elemento usamos el vector c(1,5) dentro de los corchetes. El siguiente código selecciona el primero y quinto elementos de vector_poker:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vector_poker[c(1,5)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Asigna los resultados de obtenidos del poker de los días Martes, Miercoles y Jueves a la variable dias_medios_poker.&lt;/li&gt;
&lt;li&gt;Imprime el vector dias_medios_poker a la consola para ver sus elementos.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Define la nueva variable:
dias_medios_poker &amp;lt;- vector_poker[c(2,3,4)]

# Imprime dias_medios_poker
dias_medios_poker
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;seleccionando-elementos-los-buenos-tiempos-3&#34;&gt;Seleccionando elementos: los buenos tiempos (3)&lt;/h2&gt;

&lt;p&gt;Seleccionar varios elementos del vector_poker con c(2,3,4) quizás no sea muy conveniente. Recordemos como estadísticos nuestras raíces perezosas, así que usemos una notación que R utiliza para crear vectores de números consecutivos c(2,3,4) se puede crear con el código 2:4, que genera el vector que contiene los números naturales del 2 al 4 (incluyendo ambos extremos).&lt;/p&gt;

&lt;p&gt;Así que ahora tenemos otra manera de seleccionar los elementos que corresponden a los días medios de la semana: vector_poker[2:4].&lt;/p&gt;

&lt;p&gt;Quizás en este ejemplo no haya mucha diferencia entre c(2,3,4) y 2:4 pero imagina que tuviéramos que extraer los primeros 80 elementos consecutivos de un vector, con la primera notación tendríamos que escribir todos los números del 1 al 80, sin embargo ahora sabemos que podemos hacerlo así de simple: 1:80.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Asigna los resultados del Martes al Viernes del vector_ruleta a la variable martes_a_viernes_ruleta. Utiliza la notación de dos puntos:.&lt;/li&gt;
&lt;li&gt;Imprime martes_a_viernes_ruleta en la consola.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Define la nueva variable siguiendo las instrucciones
martes_a_viernes_ruleta &amp;lt;- vector_ruleta[c(2:5)]

#Imprime martes_a_viernes_ruleta en la consola
martes_a_viernes_ruleta
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;seleccionando-elementos-los-buenos-tiempos-4&#34;&gt;Seleccionando elementos: los buenos tiempos (4)&lt;/h2&gt;

&lt;p&gt;Otra forma de abordar este problema es usar los nombres de los elementos (Lunes, Martes, etc.) en lugar de sus índices. Por ejemplo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vector_poker[&amp;quot;Lunes&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seleccionará el primer elemento de vector_poker porque el primer elemento es el que tiene el nombre &amp;ldquo;Lunes&amp;rdquo;. De la misma manera en que lo hicimos en el ejercicio anterior, puedes utilizar un vector que tenga los nombres de los elementos que deseas extraer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vector_poker[c(&amp;quot;Lunes&amp;quot;,&amp;quot;Martes&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Calcula tus ganancias promedio durante los primeros tres días de la semana seleccionando los elementos por nombre. Asigna este valor a promedio_primeros3_dias. Puedes usar la función mean() para obtener el promedio de los elementos de un vector.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Only poker results
promedio_primeros3_dias &amp;lt;- mean((vector_poker)[c(&amp;quot;Lunes&amp;quot;,&amp;quot;Martes&amp;quot;,&amp;quot;Miercoles&amp;quot;)])

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;selección-por-comparación-paso-1&#34;&gt;Selección por comparación - Paso 1&lt;/h2&gt;

&lt;p&gt;Al utilizar los operadores de comparación, podemos abordar las preguntas anteriores de una manera más interesante.&lt;/p&gt;

&lt;p&gt;Los operadores de comparación en R son los siguientes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;lt; menor que&lt;/li&gt;
&lt;li&gt;&amp;gt; mayor que&lt;/li&gt;
&lt;li&gt;&amp;gt;= mayor o igual que&lt;/li&gt;
&lt;li&gt;== igualdad&lt;/li&gt;
&lt;li&gt;!= no igual&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Como ya hemos visto, 6 &amp;gt; 5 da un valor verdadero: TRUE. Una característica muy buena de R es que puedes utilizar operaciones de comparación también en vectores. por ejemplo c(4,5,6) &amp;gt; 5 resulta en: FALSE, FALSE, TRUE. En otras palabras R hace la comparación por cada elemento y responde TRUE o FALSE dependiendo del resultado de la comparación. Esto es muy interesante y puede ser difícil de entender, practícalo en la consola!&lt;/p&gt;

&lt;p&gt;Internamente, R recicla el valor 5 cuando ejecuta c(4,5,6) &amp;gt; 5. R quiere hacer una comparación elemento por elemento de c(4,5,6) con 5, pero 5 no es un vector de tamaño 3. Para resolver esto R crea el vector c(5,5,5) y luego hace la comparación elemento por elemento.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Obtengamos los días en que obtuvimos valores positivos (&amp;gt; 0) en el poker (vector_poker) y asignémoslo a la variable positivos_poker.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Que días obtiviste ganancias en el poker?
positivos_poker &amp;lt;- vector_poker &amp;gt; 0
positivos_poker
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;selección-por-comparación-paso-2&#34;&gt;Selección por comparación - Paso 2&lt;/h2&gt;

&lt;p&gt;Trabajar con comparaciones hará tu vida analítica más fácil. En lugar de seleccionar &amp;ldquo;a mano&amp;rdquo; un conjunto de días para analizar (como en los primeros ejercicios), puedes decirle a R que te seleccione solo aquellos días en los que tuviste ganancias en el poker.&lt;/p&gt;

&lt;p&gt;En el ejercicio anterior usaste positivos_poker &amp;lt;- vector_poker &amp;gt;0 para encontrar aquellos días en los cuales tuviste ganancias. Ahora, nos gustaría saber no solo los días, sino las cantidades que ganaste en esos días.&lt;/p&gt;

&lt;p&gt;Puedes seleccionar los elementos deseados usando positivos_poker entre corchetes para seleccionar los elementos de vector_poker. Esto funciona, porque R seleccionará solo aquellos elementos en los cuales el vector positivos_poker tiene un valor verdadero (TRUE).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;
- Asigna las ganancias en el poker a la variable ganancias_poker.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Que días obtuviste ganancias en el poker?
selection_vector &amp;lt;- vector_poker &amp;gt; 0

# Selecciona de vector_poker los días con ganancias
ganancias_poker &amp;lt;- vector_poker[selection_vector]

#Imprime
ganancias_poker
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;selección-avanzada&#34;&gt;Selección Avanzada&lt;/h2&gt;

&lt;p&gt;Así como lo hiciste con el poker, también quisieras saber aquellos días en los que tuviste ganancia en la ruleta.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instrucciones&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Asigna los valores de las ganancias que tuviste en la ruleta a la variable ganancias_ruleta. Es decir selecciona solo aquellos valores de vector_ruleta que son positivos y asignalos a la variable ganancias_ruleta.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Script R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resultados en la mesa de poker en la semana
vector_poker &amp;lt;- c(140, -50, 20, -120, 240)

# Resultados en la ruleta en la semana
vector_ruleta &amp;lt;- c(-24, -50, 100, -350, 10)

# Dando nombres a vector_poker y vector_ruleta
vector_dias &amp;lt;- c(&amp;quot;Lunes&amp;quot;, &amp;quot;Martes&amp;quot;, &amp;quot;Miercoles&amp;quot;, &amp;quot;Jueves&amp;quot;, &amp;quot;Viernes&amp;quot;)
names(vector_poker) &amp;lt;- vector_dias
names(vector_ruleta) &amp;lt;- vector_dias

# Que días obtiviste ganancias en la ruleta?
positivos_ruleta &amp;lt;- vector_ruleta &amp;gt; 0

# Selecciona los valores de vector_ruleta que fueron ganancias
ganancias_ruleta &amp;lt;- vector_ruleta[positivos_ruleta]

#Imprime
ganancias_ruleta
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Área de trabajo</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/workspace/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/workspace/</guid>
      <description>

&lt;p&gt;El workspace es el área de trabajo de la sesión R e incluye todos los objetos en uso. En RStudio, los objetos en el espacio de trabajo se pueden explorar en el panel Entorno:&lt;/p&gt;

&lt;p&gt;Panel de entorno RStudio [IMG]&lt;/p&gt;

&lt;p&gt;R no guarda objetos relacionados con una sesión de trabajo individualmente: los conjuntos de datos y las salidas se almacenan en un solo archivo, cuyo nombre predeterminado es .RData. Este archivo es la imagen del área de trabajo activa durante una sesión.&lt;/p&gt;

&lt;h2 id=&#34;guardar-y-cargar-el-espacio-de-trabajo&#34;&gt;Guardar y cargar el espacio de trabajo&lt;/h2&gt;

&lt;p&gt;Al final de una sesión R, puede guardar una imagen del espacio de trabajo, que se volverá a cargar automáticamente en el próximo inicio. Ej.:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;save.image( &amp;quot;C: //.../myfile.RData&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para cargar la imagen del espacio de trabajo, use el comando de carga:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;load( &amp;quot;C: //.../myfile.RData&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El menú Archivo de R
La imagen del espacio de trabajo también se puede guardar o cargar desde el menú R, posiblemente con un nombre elegido por el usuario (por defecto no tiene nombre: .RData).&lt;/p&gt;

&lt;h2 id=&#34;enumerar-los-objetos-presentes&#34;&gt;Enumerar los objetos presentes.&lt;/h2&gt;

&lt;p&gt;Para obtener una lista de los objetos y clases contenidos en el espacio de trabajo, puede usar el comando de menú Varios / Lista de objetos, o escribir el comando&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Enumere los objetos que contienen (por ejemplo: la palabra &lt;em&gt;tab&lt;/em&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls(pattern = &amp;quot;tab&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;## Eliminar los objetos&lt;/p&gt;

&lt;p&gt;Para eliminar los objetos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm(objeto)

# Podemos utilizar también
remove(objeto)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;vaciar-el-área-de-trabajo&#34;&gt;Vaciar el área de trabajo.&lt;/h2&gt;

&lt;p&gt;Para eliminar todos los objetos y limpiar el espacio de trabajo (corresponde a borrar el espacio de trabajo o borrar el entorno en otros programas):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm(list = ls())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Elimine los objetos que contienen (por ejemplo: la palabra &lt;em&gt;tab&lt;/em&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm(list = ls(pattern = &amp;quot;tab&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;objeto = nombre del objeto, sin comillas
El argumento de lista especifica la lista de objetos que se eliminarán.&lt;/p&gt;

&lt;h2 id=&#34;área-de-trabajo-de-r-y-rcommander&#34;&gt;Área de trabajo de R y RCommander&lt;/h2&gt;

&lt;p&gt;RCommander comparte el espacio de trabajo R, y los marcos de datos que forman parte del espacio de trabajo (área de trabajo) se pueden mostrar en la esquina superior izquierda (conjunto de datos o conjunto de datos).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prácticas 3 - Matrices</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-matrices/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-matrices/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cómo funcionan los modelos</title>
      <link>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-1-como-funciona/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-1-como-funciona/</guid>
      <description>

&lt;h2 id=&#34;introducción&#34;&gt;Introducción&lt;/h2&gt;

&lt;p&gt;Comenzaremos con una descripción general de cómo funcionan los modelos de aprendizaje automático y cómo se usan. Esto puede parecer básico si ya ha realizado modelos estadísticos o aprendizaje automático. No se preocupe, progresaremos para construir modelos potentes pronto.&lt;/p&gt;

&lt;p&gt;Este micro curso le permitirá construir modelos a medida que avance en el siguiente escenario:&lt;/p&gt;

&lt;p&gt;Su primo ha ganado millones de dólares especulando con bienes raíces. Se ofreció a convertirse en socio comercial con usted debido a su interés en la ciencia de datos. Él proporcionará el dinero, y usted proporcionará modelos que predicen cuánto valen varias casas.&lt;/p&gt;

&lt;p&gt;Le preguntas a tu primo cómo ha predicho los valores inmobiliarios en el pasado. y dice que es solo intuición. Pero más preguntas revelan que ha identificado patrones de precios de casas que ha visto en el pasado, y usa esos patrones para hacer predicciones para las casas nuevas que está considerando.&lt;/p&gt;

&lt;p&gt;El aprendizaje automático funciona de la misma manera. Comenzaremos con un modelo llamado Árbol de decisión. Hay modelos más elegantes que dan predicciones más precisas. Pero los árboles de decisión son fáciles de entender y son el bloque de construcción básico para algunos de los mejores modelos en ciencia de datos.&lt;/p&gt;

&lt;p&gt;Para simplificar, comenzaremos con el árbol de decisión más simple posible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/7tsb5b1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Divide las casas en solo dos categorías. El precio previsto para cualquier casa en consideración es el precio promedio histórico de las casas en la misma categoría.&lt;/p&gt;

&lt;p&gt;Usamos datos para decidir cómo dividir las casas en dos grupos, y luego nuevamente para determinar el precio previsto en cada grupo. Este paso de capturar patrones de datos se llama ajuste (&lt;strong&gt;fitting)&lt;/strong&gt; o capacitación del modelo (&lt;strong&gt;training&lt;/strong&gt;). Los datos utilizados para ajustarse &lt;strong&gt;fit&lt;/strong&gt; al modelo se denominan datos de &lt;strong&gt;entrenamiento&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Los detalles de cómo se ajusta el modelo (por ejemplo, cómo dividir los datos) son lo suficientemente complejos como para guardarlos para más adelante. Después de que el modelo se haya ajustado, puede aplicarlo a nuevos datos para &lt;strong&gt;predecir&lt;/strong&gt; los precios de viviendas adicionales.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;mejorando-el-árbol-de-decisiones&#34;&gt;Mejorando el árbol de decisiones&lt;/h2&gt;

&lt;p&gt;¿Cuál de los siguientes dos árboles de decisiones es más probable que resulte de ajustar los datos de capacitación de bienes raíces?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/prAjgku.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;El árbol de decisión de la izquierda (Árbol de decisión 1) probablemente tenga más sentido, porque captura la realidad de que las casas con más habitaciones tienden a venderse a precios más altos que las casas con menos habitaciones. El mayor inconveniente de este modelo es que no captura la mayoría de los factores que afectan el precio de la vivienda, como la cantidad de baños, el tamaño del lote, la ubicación, etc.&lt;/p&gt;

&lt;p&gt;Puede capturar más factores utilizando un árbol que tiene más &amp;ldquo;divisiones&amp;rdquo;. Estos se llaman árboles &amp;ldquo;más profundos&amp;rdquo;. Un árbol de decisión que también considera el tamaño total del lote de cada casa podría verse así:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/R3ywQsR.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Usted predice el precio de cualquier casa rastreando a través del árbol de decisión, siempre eligiendo la ruta correspondiente a las características de esa casa. El precio previsto para la casa está en la parte inferior del árbol. El punto en la parte inferior donde hacemos una predicción se llama hoja (&lt;strong&gt;leaf&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;Las divisiones y los valores en las hojas estarán determinados por los datos, por lo que es hora de que revise los datos con los que trabajará.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data Engineer</title>
      <link>https://www.marcusrb.com/big-data-resource/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/big-data-resource/</guid>
      <description>

&lt;p&gt;LOREM IPSUM&lt;/p&gt;

&lt;h2 id=&#34;program-structure&#34;&gt;Program structure&lt;/h2&gt;

&lt;p&gt;La estructura del programa formativo se compone en dos grandes bloques:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[ ] &lt;a href=&#34;https://www.marcusrb.com/databricks-fundamentals&#34;&gt;Databricks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Data visualizacion resourcs</title>
      <link>https://www.marcusrb.com/en/data-visualization-resources/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/data-visualization-resources/</guid>
      <description>

&lt;h2 id=&#34;estructura-recursos&#34;&gt;Estructura recursos&lt;/h2&gt;

&lt;p&gt;Google Data Studio es una herramienta gratuita de visualización e informes de datos basada en la nube que se conecta a muchas fuentes de datos diferentes, y convierte esos datos en paneles informativos e informes que son fáciles de entender y compartir, y son totalmente personalizables.&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; - &lt;a href=&#34;https://www.marcusrb.com/en/power-bi-resources&#34;&gt;Guía de Microsoft Power BI&lt;/a&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; - &lt;a href=&#34;https://www.marcusrb.com/&#34;&gt;Guía de Tableau&lt;/a&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; - &lt;a href=&#34;https://www.marcusrb.com/&#34;&gt;Guía de Google Data Studio&lt;/a&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; - &lt;a href=&#34;https://www.marcusrb.com/&#34;&gt;Guía de Grafana&lt;/a&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Formación fundamentos en Microsoft Power BI</title>
      <link>https://www.marcusrb.com/curso-power-bi-fundamentos/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/curso-power-bi-fundamentos/</guid>
      <description>

&lt;p&gt;Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Power BI te guiaré paso a paso para obtener habilidades de Power Bi. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Power BI con éxito.&lt;/p&gt;

&lt;p&gt;La aplicación de dashboard con Power BI es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.&lt;/p&gt;

&lt;h2 id=&#34;estructura-del-programa-y-cursos&#34;&gt;Estructura del programa y cursos&lt;/h2&gt;

&lt;p&gt;La estructura del programa formativo se compone en dos grandes bloques:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.marcusrb.com/curso-power-bi-fundamentos&#34;&gt;Power BI fundamentos 101&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.marcusrb.com/curso-power-bi-avanzado&#34;&gt;Power BI avanzado 201&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;curso-de-fundamentos-en-power-bi-20-horas&#34;&gt;Curso de fundamentos en Power BI (~20 horas)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Introducción a Analítica de datos&lt;/li&gt;
&lt;li&gt;Business Intelligence vs Big Data&lt;/li&gt;
&lt;li&gt;Fundamentales de Visualización de datos&lt;/li&gt;
&lt;li&gt;Proyectos de Data Discovery y Agilismo&lt;/li&gt;
&lt;li&gt;Introducción a Power BI y conexión a fuentes de datos&lt;/li&gt;
&lt;li&gt;Transformación de datos con Power Query&lt;/li&gt;
&lt;li&gt;Modelado de datos y SQL Analysis Services&lt;/li&gt;
&lt;li&gt;DAX, funciones y mejores prácticas&lt;/li&gt;
&lt;li&gt;Reportes y cuadro de mandos en Power BI&lt;/li&gt;
&lt;li&gt;Power BI App y conexiones on-premise&lt;/li&gt;
&lt;li&gt;Power BI Flow y Data Pipeline&lt;/li&gt;
&lt;li&gt;Labs y Prácticas&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;realización-de-poc-proof-of-concept-con-power-bi&#34;&gt;Realización de PoC (Proof of Concept) con Power BI&lt;/h3&gt;

&lt;p&gt;También realizo pruebas de conceptos en su empresa, con una muestra de datos (también del tipo dummies), me ocupo de transformar su necesidad en un panel de control dinámico y eficiente.&lt;/p&gt;

&lt;p&gt;Se realizará una sesión de 4 horas máximo de &lt;strong&gt;data discovery&lt;/strong&gt; y otras sesiones de detección de KPI e indicadores importantes.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;está-interesado-en-capacitarse-en-power-bi&#34;&gt;¿Está interesado en capacitarse en Power BI ?&lt;/h3&gt;

&lt;p&gt;Puedes adquirir el &lt;a href=&#34;https://www.udemy.com/share/103gxe/&#34; target=&#34;_blank&#34;&gt;curso de Power BI&lt;/a&gt; en la plataforma UDEMY a un precio reducido y totalmente online.&lt;/p&gt;

&lt;p&gt;Si necesitas un curso en remoto, en tu empresa o a nivel profesional puedes llamar hoy o utilizar el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.marcusrb.com/#contact&#34;&gt;Solicita información para Formación de fundamentos en Microsoft PowerBI&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;demo-data-analysis-con-power-bi&#34;&gt;DEMO Data Analysis con Power BI&lt;/h3&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//player.vimeo.com/video/453307595&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Formación avanzada en Microsoft Power BI</title>
      <link>https://www.marcusrb.com/curso-power-bi-avanzado/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/curso-power-bi-avanzado/</guid>
      <description>

&lt;p&gt;Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Power BI te guiaré paso a paso para obtener habilidades de Power Bi. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Power BI con éxito.&lt;/p&gt;

&lt;p&gt;La aplicación de dashboard con Power BI es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.&lt;/p&gt;

&lt;h2 id=&#34;estructura-del-programa-y-cursos&#34;&gt;Estructura del programa y cursos&lt;/h2&gt;

&lt;p&gt;La estructura del programa formativo se compone en dos grandes bloques:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.marcusrb.com/curso-power-bi-fundamentos&#34;&gt;Power BI fundamentos 101&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.marcusrb.com/curso-power-bi-avanzado&#34;&gt;Power BI avanzado 201&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;curso-avanzado-en-dax-y-power-query-de-power-bi-70-horas&#34;&gt;Curso avanzado en DAX y Power Query de Power BI (~70 horas)&lt;/h2&gt;

&lt;p&gt;Incluye el &lt;a href=&#34;https://www.marcusrb.com/curso-power-bi-fundamentos&#34;&gt;curso de fundamentos&lt;/a&gt; 101 más:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Centralización de datos&lt;/strong&gt;
Creación de flujos de datos, configuración de puerta de enlace, actualización de flujos de datos, creación de informes a partir de flujos de datos, centralización de conjuntos de datos, certificación de conjuntos de datos, creación de informes a partir de conjuntos de datos&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;El lenguaje de fórmulas de Power Query&lt;/strong&gt;
Usando La Barra De Fórmula; Usando El Editor Avanzado; Descripción general del lenguaje M; Explorando M usando #shared&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comprensión del código generado automáticamente&lt;/strong&gt;
Excel.Workbook; File.Contents; Table.TransformColumns; Table.TransformColumnTypes; Table.UnpivotColumns; Table.UnpivotOtherColumns&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Crear funciones personalizadas en M&lt;/strong&gt;
Definiendo una función; Definición de parámetros de entrada; El operador de acceso; Definir el cuerpo de la función; Usando parámetros opcionales; Funciones de llamada&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Técnicas de iteración&lt;/strong&gt;
Beneficio de generar listas; Generando listas de números; Generando listas de fechas; Generando listas alfanuméricas; Usando cada función; Aplicar una función a una lista de archivos&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DAX avanzado&lt;/strong&gt;
Usando DAX Studio; Escribir fórmulas complejas; Usando variables; Calcular promedios móviles; Cálculo de totales acumulados; Cálculos de percentiles; Creación de fórmulas avanzadas de inteligencia de tiempo; Usando múltiples tablas de fechas; Trabajando con calendarios no estándar&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Trabajando con tablas calculadas&lt;/strong&gt;
Crear tablas calculadas; Funciones DAX que devuelven tablas; La función CALCULATETABLE; La función ADDCOLUMNS; La función RESUMEN; RESUMEN con ROLLUP; VALORES y funciones DISTINCT; La función CROSSJOIN; La función TOPN; La función ROW; Usar tablas calculadas dentro del modelo de datos&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Usar tablas de parámetros&lt;/strong&gt;
¿Qué es una tabla de parámetros? Cuándo usar tablas de parámetros; Usando la función HASONEVALUE; Usando la función VALUES; Crear rebanadoras personalizadas; Crear múltiples soluciones de tabla de parámetros&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modelado y visualización de datos avanzados&lt;/strong&gt;
Trabajar con múltiples tablas de hechos, Usar relaciones activas e inactivas, Usar la función USERELATIONSHIP, Crear propiedades visuales dinámicas, Usar BLANK () para hacer que la visibilidad sea dinámica&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dashboards avanzados&lt;/strong&gt;
Agregar enlaces personalizados a un tablero de instrumentos; Usando el widget de contenido web; Usando el widget de video; Mosaicos de panel de transmisión en tiempo real, integración de Power Automation y PowerApps&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;programas-personalizados-en-empresas-y-pymes&#34;&gt;Programas personalizados en empresas y pymes&lt;/h3&gt;

&lt;p&gt;Si requieres una formación personalizada tanto sea de fundamentos que conceptos avanzados de inteligencia de negocio, SQL Server Analysis, DAX, modelado de datos y ETL con Power Query, puedo adaptar el contenido de Power BI para un mínimo de 4 personas hasta un máximo de 20.&lt;/p&gt;

&lt;h3 id=&#34;realización-de-poc-proof-of-concept-con-power-bi&#34;&gt;Realización de PoC (Proof of Concept) con Power BI&lt;/h3&gt;

&lt;p&gt;También realizo pruebas de conceptos en su empresa, con una muestra de datos (también del tipo dummies), me ocupo de transformar su necesidad en un panel de control dinámico y eficiente.&lt;/p&gt;

&lt;p&gt;Se realizará una sesión de 4 horas máximo de &lt;strong&gt;data discovery&lt;/strong&gt; y otras sesiones de detección de KPI e indicadores importantes.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;está-interesado-en-capacitarse-en-power-bi&#34;&gt;¿Está interesado en capacitarse en Power BI ?&lt;/h3&gt;

&lt;p&gt;Si necesitas un curso en remoto, en tu empresa o a nivel profesional puedes llamar hoy o utilizar el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.marcusrb.com/#contact&#34;&gt;Solicita información para Formación avanzada en Microsoft PowerBI&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;demo-data-analysis-con-power-bi&#34;&gt;DEMO Data Analysis con Power BI&lt;/h3&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//player.vimeo.com/video/453307595&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Formación Tableau Desktop y Tableau Prep</title>
      <link>https://www.marcusrb.com/curso-tableau/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/curso-tableau/</guid>
      <description>

&lt;p&gt;Con más de 10 años de experiencia en consultoría de datos y formación in-company en las mejores empresas del IBEX35, pymes de Europa y continente americano, he desarrollado un estilo de enseñanza único para ayudar a los aspirantes a aprender a dominar el arte de administrar datos y crear poderosos paneles para tomar decisiones comerciales inteligentes. Cómo instructor del curso de capacitación de Tableau te guiaré paso a paso para obtener habilidades de Tableau. Todos los temas y unidades se desglosan de una manera fácil de aprender, haciendo que el curso sea extremadamente agradable y que todos mis alumnos han logrado utilizar Tableau con éxito.&lt;/p&gt;

&lt;p&gt;La aplicación de dashboard con Tableau es fácil de usar ha sido diseñada para ayudar a los usuarios, de todos los niveles de experiencia, a producir un análisis de datos perspicaz; por lo tanto, nuestro curso de capacitación de BI interactivo y muy atractivo ayuda a los candidatos en todos los departamentos a comunicar datos relacionados con el rendimiento de una manera visualmente comprensible.&lt;/p&gt;

&lt;h2 id=&#34;estructura-del-programa&#34;&gt;Estructura del programa&lt;/h2&gt;

&lt;p&gt;La estructura del programa formativo se compone en dos grandes bloques:
- fundamentos
- avanzado&lt;/p&gt;

&lt;p&gt;ambos programas incluyen sesiones teóricas, laboratorios y casos prácticos. Así como casos reales de empresas con eventos y gestiones en entorno locales, cloud (AWS, GCP, Azure), consultas a base de datos.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;curso-en-tableau-public-desktop-y-prep-30-horas&#34;&gt;Curso en Tableau Public, Desktop y Prep (~30 horas)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Introducción a Analítica de datos&lt;/li&gt;
&lt;li&gt;Business Intelligence vs Big Data&lt;/li&gt;
&lt;li&gt;Fundamentales de Visualización de datos&lt;/li&gt;
&lt;li&gt;Proyectos de Data Discovery y Agilismo&lt;/li&gt;
&lt;li&gt;Introducción a Tableau y conexión a fuentes de datos&lt;/li&gt;
&lt;li&gt;Transformación de datos con Tableau Prep&lt;/li&gt;
&lt;li&gt;creación campos calculados, medidas, parámetros y filtros dinámicos&lt;/li&gt;
&lt;li&gt;Reportes y cuadro de mandos en Tableau&lt;/li&gt;
&lt;li&gt;Tableau Public, Tableau App, Tableau Reader&lt;/li&gt;
&lt;li&gt;Data Pipeline&lt;/li&gt;
&lt;li&gt;Labs y Prácticas&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;programas-personalizados-en-empresas-y-pymes&#34;&gt;Programas personalizados en empresas y pymes&lt;/h3&gt;

&lt;p&gt;Si requieres una formación personalizada tanto sea de fundamentos que conceptos avanzados de inteligencia de negocio, base de datos, modelado de datos y ETL con Tableau Prep, puedo adaptar el contenido de Tableau Desktop para un mínimo de 4 personas hasta un máximo de 20.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;realización-de-poc-proof-of-concept-con-tableau&#34;&gt;Realización de PoC (Proof of Concept) con Tableau&lt;/h3&gt;

&lt;p&gt;También realizo pruebas de conceptos en su empresa, con una muestra de datos (también del tipo dummies), me ocupo de transformar su necesidad en un panel de control dinámico y eficiente.&lt;/p&gt;

&lt;p&gt;Se realizará una sesión de 4 horas máximo de &lt;strong&gt;data discovery&lt;/strong&gt; y otras sesiones de detección de KPI e indicadores importantes.&lt;/p&gt;

&lt;h3 id=&#34;está-interesado-en-capacitarse-en-tableau&#34;&gt;¿Está interesado en capacitarse en Tableau ?&lt;/h3&gt;

&lt;p&gt;Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.marcusrb.com/#contact&#34;&gt;Solicita información para Formación en Tableau&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prácticas 4 - Factores</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-factores/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-factores/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Explorando los datos</title>
      <link>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-2-basic-data-exploration/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-2-basic-data-exploration/</guid>
      <description>

&lt;h1 id=&#34;explorando-los-datos&#34;&gt;Explorando los datos&lt;/h1&gt;

&lt;h2 id=&#34;usando-pandas-para-familiarizarse-con-sus-datos&#34;&gt;Usando pandas para familiarizarse con sus datos&lt;/h2&gt;

&lt;p&gt;El primer paso en cualquier proyecto de aprendizaje automático es familiarizarse con los datos. Usarás la biblioteca Pandas para esto. Pandas es la herramienta principal de datos que los científicos usan para explorar y manipular datos. La mayoría de las personas abrevian pandas en su código como &lt;em&gt;pd&lt;/em&gt;. Hacemos esto con el comando&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La parte más importante de la biblioteca Pandas es el DataFrame. Un DataFrame contiene el tipo de datos que podría considerar como una tabla. Esto es similar a una hoja en Excel, o una tabla en una base de datos SQL.&lt;/p&gt;

&lt;p&gt;Pandas tiene métodos poderosos para la mayoría de las cosas que querrás hacer con este tipo de datos.&lt;/p&gt;

&lt;p&gt;Como ejemplo, veremos datos sobre los precios de la vivienda en Melbourne, Australia. En los ejercicios prácticos, aplicará los mismos procesos a un nuevo conjunto de datos, que tiene los precios de las viviendas en Iowa.&lt;/p&gt;

&lt;p&gt;Los datos de ejemplo (Melbourne) están en la ruta del archivo &lt;strong&gt;../input/melbourne-housing-snapshot/melb_data.csv&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Cargamos y exploramos los datos con los siguientes comandos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# save filepath to variable for easier access
melbourne_file_path = &#39;../input/melbourne-housing-snapshot/melb_data.csv&#39;
# read the data and store data in DataFrame titled melbourne_data
melbourne_data = pd.read_csv(melbourne_file_path)
# print a summary of the data in Melbourne data
melbourne_data.describe()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;interpretación-de-la-descripción-de-datos&#34;&gt;Interpretación de la descripción de datos&lt;/h2&gt;

&lt;p&gt;Los resultados muestran 8 números para cada columna en su conjunto de datos original. El primer número, el recuento, muestra cuántas filas tienen valores no faltantes.&lt;/p&gt;

&lt;p&gt;Los valores perdidos surgen por muchas razones. Por ejemplo, el tamaño de la segunda habitación no se recogería al inspeccionar una casa de 1 habitación. Volveremos al tema de los datos faltantes.&lt;/p&gt;

&lt;p&gt;El segundo valor es la media, que es el promedio. Debajo de eso, std es la desviación estándar, que mide la extensión numérica de los valores.&lt;/p&gt;

&lt;p&gt;Para interpretar los valores mínimo, 25%, 50%, 75% y máximo, imagine ordenar cada columna del valor más bajo al más alto. El primer valor (el más pequeño) es el mínimo. Si recorre un cuarto de camino en la lista, encontrará un número que es mayor que el 25% de los valores y menor que el 75% de los valores. Ese es el valor del 25% (pronunciado &amp;ldquo;percentil 25&amp;rdquo;). Los percentiles 50 y 75 se definen de forma análoga, y el máximo es el número más grande.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Formación en Google Data Studio</title>
      <link>https://www.marcusrb.com/curso-google-data-studio/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/curso-google-data-studio/</guid>
      <description>

&lt;h2 id=&#34;estructura-recursos&#34;&gt;Estructura recursos&lt;/h2&gt;

&lt;p&gt;Google Data Studio es una herramienta gratuita de visualización e informes de datos basada en la nube que se conecta a muchas fuentes de datos diferentes, y convierte esos datos en paneles informativos e informes que son fáciles de entender y compartir, y son totalmente personalizables.&lt;/p&gt;

&lt;h2 id=&#34;características-data-studio&#34;&gt;Características Data Studio&lt;/h2&gt;

&lt;p&gt;Google Data Studio es intuitivo, rápido, flexible y permite una gran cantidad de opciones de diseño y presentación.&lt;/p&gt;

&lt;p&gt;Amplia gama de conectores de datos.
Data Studio tiene 17 conectores de datos internos y alrededor de 108 de terceros para elegir
Funciones fáciles de usar
Data Studio proporciona docenas de funciones matemáticas, de cadena, de fecha y otras para transformar sus datos en valores más útiles.
Variedad de formas, imágenes y texto.
Data Studio le permite agregar formas, imágenes y texto a sus informes y paneles para que sean más fáciles de leer.
Niveles de permisos
Aprovechando la tecnología Google Drive, puede administrar fácilmente a todos sus usuarios y su nivel de acceso
Mezcla de datos, ahora una realidad
Data Studio le permite agregar datos de múltiples fuentes para tener una vista comparativa de ellos a la vez&lt;/p&gt;

&lt;p&gt;Los recursos de este curso están disponibles en:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://bit.ly/31ZpySn&#34; target=&#34;_blank&#34;&gt;Cursos online Google Data Studio&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://support.google.com/datastudio/answer/6390659?utm_source=in-product&amp;amp;utm_medium=feature-panel&amp;amp;utm_campaign=videos&#34; target=&#34;_blank&#34;&gt;Video-Tutoriales Google Data Studio&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://support.google.com/datastudio#topic=6267740&#34; target=&#34;_blank&#34;&gt;Recursos para Google Academy for Ads&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Novedades de Google Data studio&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.marcusrb.com/google-data-studio/example1&#34;&gt;google-data-studio-1&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;demo-data-analysis-con-google-data-studio&#34;&gt;DEMO Data Analysis con Google Data Studio&lt;/h3&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//player.vimeo.com/video/453307594&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Prácticas 5 - Data Frames</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-dataframes/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-dataframes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Caso Práctico 1 - Explorando los datos</title>
      <link>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-3-exercise-1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-3-exercise-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Prácticas 6 - Listas</title>
      <link>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-listas/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/r-studio/intro-r/r101-listas/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seleccionar los datos para el modelo</title>
      <link>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-4-datos-modelo/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/courses/data-science/intro-machine-learning/ml101-4-datos-modelo/</guid>
      <description>

&lt;h2 id=&#34;seleccionar-datos-para-modelar&#34;&gt;Seleccionar datos para modelar&lt;/h2&gt;

&lt;p&gt;Su conjunto de datos tenía demasiadas variables para entenderlo, o incluso para imprimirlo bien. ¿Cómo puede reducir esta cantidad abrumadora de datos a algo que pueda entender?&lt;/p&gt;

&lt;p&gt;Comenzaremos eligiendo algunas variables usando nuestra intuición. Los cursos posteriores le mostrarán técnicas estadísticas para priorizar automáticamente las variables.&lt;/p&gt;

&lt;p&gt;Para elegir variables / columnas, necesitaremos ver una lista de todas las columnas en el conjunto de datos. Eso se hace con la propiedad de columnas del DataFrame (la línea inferior del código a continuación).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

melbourne_file_path = &#39;../input/melbourne-housing-snapshot/melb_data.csv&#39;
melbourne_data = pd.read_csv(melbourne_file_path)
melbourne_data.columns
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# The Melbourne data has some missing values (some houses for which some variables weren&#39;t recorded.)
# We&#39;ll learn to handle missing values in a later tutorial.  
# Your Iowa data doesn&#39;t have missing values in the columns you use.
# So we will take the simplest option for now, and drop houses from our data.
# Don&#39;t worry about this much for now, though the code is:

# dropna drops missing values (think of na as &amp;quot;not available&amp;quot;)
melbourne_data = melbourne_data.dropna(axis=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hay muchas formas de seleccionar un subconjunto de sus datos. El Micro Curso de Pandas los cubre con más profundidad, pero por ahora nos centraremos en dos enfoques.&lt;/p&gt;

&lt;p&gt;Notación de puntos, que usamos para seleccionar el &amp;ldquo;objetivo de predicción&amp;rdquo;
Seleccionando con una lista de columnas, que usamos para seleccionar las &amp;ldquo;características&amp;rdquo;&lt;/p&gt;

&lt;h3 id=&#34;selecting-the-prediction-target&#34;&gt;Selecting The Prediction Target&lt;/h3&gt;

&lt;p&gt;You can pull out a variable with dot-notation. This single column is stored in a Series, which is broadly like a DataFrame with only a single column of data.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll use the dot notation to select the column we want to predict, which is called the prediction target. By convention, the prediction target is called y. So the code we need to save the house prices in the Melbourne data is&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = melbourne_data.Price
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;elegir-características&#34;&gt;Elegir &amp;ldquo;Características&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;Las columnas que se ingresan en nuestro modelo (y luego se usan para hacer predicciones) se denominan &amp;ldquo;características&amp;rdquo;. En nuestro caso, esas serían las columnas utilizadas para determinar el precio de la vivienda. A veces, usará todas las columnas excepto el objetivo como características. Otras veces estará mejor con menos funciones.&lt;/p&gt;

&lt;p&gt;Por ahora, crearemos un modelo con solo algunas características. Más adelante verá cómo iterar y comparar modelos creados con diferentes características.&lt;/p&gt;

&lt;p&gt;Seleccionamos múltiples características al proporcionar una lista de nombres de columnas entre paréntesis. Cada elemento de esa lista debe ser una cadena (con comillas).&lt;/p&gt;

&lt;p&gt;Aquí hay un ejemplo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;melbourne_features = [&#39;Rooms&#39;, &#39;Bathroom&#39;, &#39;Landsize&#39;, &#39;Lattitude&#39;, &#39;Longtitude&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Por conveniencia atribuimos los datos a la variable &lt;strong&gt;X&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = melbourne_data[melbourne_features]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Revisemos rápidamente los datos que usaremos para predecir los precios de la vivienda utilizando el método de descripción y el método de encabezado, que muestra las pocas filas superiores.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X.describe()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X.head()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Tutorial - Enviamos los valores de e-commerce al pixel de Google Ads con Tag Manager.</title>
      <link>https://www.marcusrb.com/en/talk/google-ads-gtm/</link>
      <pubDate>Wed, 16 Sep 2020 18:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/talk/google-ads-gtm/</guid>
      <description>

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;aprende-a-enviar-los-valores-de-e-commerce-al-pixel-de-google-ads-con-tag-manager&#34;&gt;Aprende a enviar los valores de e commerce al pixel de Google Ads con Tag Manager.&lt;/h1&gt;

&lt;p&gt;El paso más tedioso de todos marketeros es registrar correctamente las transacciones desde la web a Google Analytics o cualquier herramienta de analítica. Pero un solo check no es suficiente para poder llevar todas tipos de personalizaciones de comercio electrónico y aprovechar las bondades del dataLayer presente en nuestro site. Un ejemplo es la personalización de los eventos de comercio electrónico mejorado a eventos, o mejor, hacía las plataforams de Advertising, como Google Ads, Facebook, Linkedin, hasta Twitter, Tik Tok y Bing Ads. Es importante tener el dataLayer implementado (bien por plugin o bien via desarrollo), y el resto lo haremos en Google Tag Manager a través de pequeñas funciones en JavaScript. ¿Estás listo para ser el ninja de Google Tag Manager? En este video tutorial te explicaremos como realizar unos pasos más habituales y el resto podrás realizarlos sin dificultades.&lt;/p&gt;

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/5sodoQlfBC8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Tutorial - crear una etiqueta en Google Tag Manager para registrar las llamadas.</title>
      <link>https://www.marcusrb.com/en/talk/llamadas-gtm/</link>
      <pubDate>Wed, 09 Sep 2020 18:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/talk/llamadas-gtm/</guid>
      <description>

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;aprende-a-rear-una-etiqueta-en-google-tag-manager-para-registrar-las-llamadas&#34;&gt;Aprende a rear una etiqueta en Google Tag Manager para registrar las llamadas.&lt;/h1&gt;

&lt;p&gt;¿Necesitas registrar las llamadas efectuadas directamente desde la web? Con la personalización de la etiqueta de seguimiento de eventos de llamadas es posible tanto para Google Ads, como este video tutorial, como el resto de plataformas de publicidad. Aunque la etiqueta está bien escondida dentro del repositorio en Google Tag Manager, veamos como poder realizar con simples pasos su personalización y tener constancia del tráfico entrante de nuestros clientes potenciales y grabar sus interacciones correctamente para la mejor atribución de las conversiones en Google Ads u lo que sea. No hace apoyo técnico, solo una cuenta de Google Ads, Google Tag Manager y seguir los pasos del vídeo. En caso de tener otro tipo de evento, como slider o eventos tipo Ajax, entonces se requiere una personalización avanzada con la llamada a la librería de Ajax o jQuery.&lt;/p&gt;

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/D369iin5yJI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Tutorial - Aprende cómo personalizar el pixel de Bing Ads con Google Tag Manager.</title>
      <link>https://www.marcusrb.com/en/talk/bing_ads_gtm/</link>
      <pubDate>Tue, 01 Sep 2020 18:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/talk/bing_ads_gtm/</guid>
      <description>

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;aprende-cómo-personalizar-el-pixel-de-bing-ads-con-google-tag-manager&#34;&gt;Aprende cómo personalizar el pixel de Bing Ads con Google Tag Manager.&lt;/h1&gt;

&lt;p&gt;En este tutorial vemos como personalizar el pixel de seguimiento del segundo buscador más importante, Bing.&lt;/p&gt;

&lt;p&gt;Aunque no sea muy extendido su uso, Bing Ads cada vez más está ganando posiciones de mercado, como en muchos paises europeos es tan importante más que Google (por la cuestión privacidad), siendo el canal habitual en Francia, Alemania y UK. Gracias al gesto de etiquetas Google Tag Manager, podemos sin intervención de un desarrollador implementar el pixel o los diferentes de eventos, donde necesitamos recoger las interacciones de los usuarios, eventos de usabilidad y todas aquellos relacionados con el comercio electrónico. Gracias a la etiqueta nativa presente en Google Tag Manager, podemos en pocos clics crear diferentes pixel, el principal como base, y el resto para las interacciones. Es importante contar también con el dataLayer de comercio electrónico (clásico o mejorado), para recolectar el resto de eventos específicos delos pasos del funnel, al igual que hacemos con Google Ads y Google Analytics..&lt;/p&gt;

&lt;h2 id=&#34;quién-es-el-ponente&#34;&gt;¿Quién es el ponente?&lt;/h2&gt;

&lt;p&gt;Marco Russo&lt;/p&gt;

&lt;p&gt;Consultor y Especialista en Data &amp;amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.&lt;/p&gt;

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/90OMn3m3Cxk&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Consultor freelance de analítica digital, Google Analytics, GA4 y GTM</title>
      <link>https://www.marcusrb.com/servicios-freelance-google-analytics/</link>
      <pubDate>Wed, 10 Jun 2020 17:52:54 +0100</pubDate>
      <guid>https://www.marcusrb.com/servicios-freelance-google-analytics/</guid>
      <description>

&lt;h2 id=&#34;como-realizar-una-consultoría-o-auditoría-de-google-analytics&#34;&gt;Como realizar una consultoría o auditoría de Google Analytics&lt;/h2&gt;

&lt;p&gt;¿Necesita ayuda con Google Analytics? Permita que le muestre cómo utilizar los datos analíticos para cuantificar sus esfuerzos de SEO, aumentar el tráfico web, convertir a más visitantes en clientes potenciales y mejorar de forma mensurable su desempeño de marketing.&lt;/p&gt;

&lt;h3 id=&#34;qué-es-google-analytics&#34;&gt;¿Qué es Google Analytics?&lt;/h3&gt;

&lt;p&gt;Google Analytics es una herramienta de análisis web que es extremadamente poderosa, además de ser gratuita. Los propietarios de negocios, ejecutivos de marketing y webmasters pueden usarlo para: medir y rastrear la actividad del sitio web, optimizar el rendimiento del sitio web, mejorar las tasas de conversión (por ejemplo, visitante a líder o visitante a venta) e incluso mejorar el rendimiento del marketing offline. Google Analytics está &amp;ldquo;basado en cookies&amp;rdquo; (a diferencia de una herramienta de análisis web que analiza los archivos de registro o logs).&lt;/p&gt;

&lt;p&gt;Para usar Google Analytics, simplemente hay que implementar una pequeña porción del código de JavaScript en cada página de su sitio web que desea rastrear. Relativamente hablando, esto hace que Google Analytics sea extremadamente fácil de usar y algunos sitios web se pueden configurar en menos de 30 minutos. Google Analytics se puede personalizar de varias maneras, por lo que es casi tan poderoso como muchas soluciones analíticas de sitios web pagados.&lt;/p&gt;

&lt;h3 id=&#34;necesitas-ayuda-con-google-analytics-y-la-nueva-versión-ga4&#34;&gt;¿Necesitas ayuda con Google Analytics y la nueva versión GA4?&lt;/h3&gt;

&lt;p&gt;Si eres como la mayoría de los dueños de negocios, pones Google Analytics en tu sitio web, lo miras por unos meses y luego te ocupas de otras cosas. ¿Quién tiene tiempo para ver toda esta información? ¿Suena familiar? Únete a la multitud. Una de las quejas más comunes que tienen los propietarios de negocios locales sobre los datos de análisis de marketing y Google Analytics en particular es que hay una tonelada de datos: demasiados datos y no suficientes conocimientos procesables. ¡No necesita otro informe para analizar ni una hoja de cálculo para interpretar!&lt;/p&gt;

&lt;p&gt;Te ofrezco una solución de &amp;ldquo;hágalo por mí&amp;rdquo; que ayuda a los propietarios de negocios pequeños (y no tan pequeños) a personalizar Google Analytics para satisfacer mejor sus necesidades, examinar los informes interminables para identificar los más relevantes para el cliente y convertir los datos en resultados accionables! Actualizado también a la nueva versión Google Analytics 4 o GA4, incluido Firebase Analytics.&lt;/p&gt;

&lt;h3 id=&#34;está-interesado-en-consultar-o-capacitarse-en-google-analytics-o-análisis-digital&#34;&gt;¿Está interesado en consultar o capacitarse en Google Analytics o análisis digital?&lt;/h3&gt;

&lt;p&gt;Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../#contact&#34;&gt;Solicita información para Google Analytics - GA4&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;realizar-una-consultoría-y-o-auditoría-de-google-tag-manager&#34;&gt;Realizar una consultoría y/o auditoría de Google Tag Manager&lt;/h2&gt;

&lt;p&gt;Google Tag Manager, la herramienta más utilizada para etiquetar eventos e interacciones, será el rol más importante para detectar nuevos insight en tu negocio, tener claro desde principio que medir y como medirlo,&lt;/p&gt;

&lt;p&gt;El Administrador de etiquetas de Google es una gran herramienta para que los dueños de negocios o los equipos de marketing vean cómo funciona su sitio web: qué áreas están funcionando, qué rendimiento tiene bajo rendimiento, qué funciones no se están usando como creía, etc. Lo vemos como una excelente forma de recopilar datos de experiencia de usuario UX &lt;b&gt;&amp;ldquo;User Experience&amp;rdquo;&lt;/b&gt;, y mejorar el ratio de conversión, CRO &lt;b&gt;&amp;ldquo;Conversion Rate Optimization&amp;rdquo;&lt;/b&gt; en vivo en su sitio de clientes reales.&lt;/p&gt;

&lt;p&gt;Google proporciona una guía de desarrollador para usar la herramienta de análisis, tengo experiencia de casi 5 años en la herramienta de analísis para grandes clientes y cuentas, además de tener un background de programad web, especializado en la configuración de Google Tag Manager, así como en el análisis de los datos que proporciona (porque, sin información accionable, &lt;b&gt;¿qué valor tiene?&lt;/b&gt; datos proporcionan en sí mismo para obtener más de su sitio web?).&lt;/p&gt;

&lt;h3 id=&#34;que-servicio-incluye-el-servicio-de-consultoría-de-tag-manager&#34;&gt;Que servicio incluye el servicio de consultoría de Tag Manager?&lt;/h3&gt;

&lt;p&gt;Como servicio de consultoría de Tag Manager, primero tendré que ver su objetivo empresarial y qué quieres que se analice, puedo analizar todo un sitio web, pero la pregunta sería: ¿Todos los datos valen lo mismo para su empresa? Así que ofrezco:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Auditoría de sitios web y mapeo de etiquetas&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Configuración y configuración&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Aplicación y monitoreo&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Formación&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;plan-de-implementación-de-tag-manager&#34;&gt;Plan de implementación de Tag Manager&lt;/h3&gt;

&lt;p&gt;Google Tag Manager permite el etiquetado inmediato y la inserción de fragmentos de código a su sitio web, pero sin un experto en la mejor forma de configurarlo o vincularlo con su seguimiento actual de Google Analytics, puede parecer perdido. Realizaré la configuración y la migración de datos del Administrador de etiquetas de Google, lo que permite un seguimiento y generación de informes analíticos eficientes. En caso de ayuda, podré necesitaré apoyo de vustro departamento IT.&lt;/p&gt;

&lt;p&gt;Os asesoraré para que el Administrador de etiquetas de Google será realizado de la mejor configuración junto con el mapeo de las etiquetas en función del objetivo de su sitio (lo que le permite realizar un seguimiento de todo, desde diferentes formularios de captura de clientes hasta clics de enlaces específicos).&lt;/p&gt;

&lt;h3 id=&#34;auditoría-de-google-tag-manager&#34;&gt;Auditoría de Google Tag Manager&lt;/h3&gt;

&lt;p&gt;Teniendo en cuenta que además de un servicio de consultoría e implementación, también podré asesorarle sobre los puntos claves para su correcta instalación, en el caso ya esté realizado el mismo por vosotros o terceros. Así que me dedicaré con este servicio a la búsqueda optima de su correcta implementación, auditando todo el proceso de recogida de los datos, hasta la correcta visualización en las etiquetas, sean de Google Analytics, Google Ads, Facebook Ads u otras etiquetas o pixel de conversiones, además de todos los puntos de contacto llamados &lt;b&gt;micro-conversiones&lt;/b&gt;, pasos previo a su conversión.&lt;/p&gt;

&lt;h3 id=&#34;formación-in-company-de-google-tag-manager&#34;&gt;Formación in-company de Google Tag Manager&lt;/h3&gt;

&lt;p&gt;Si tiene un equipo de marketing familiarizado con Google Analytics y sus productos relacionados y solo está buscando ayuda para comenzar a implementar y usar el Administrador de etiquetas de Google, también ofrezco sesiones de capacitación personalizadas. Para estas sesiones, trabajaré con usted personalmente o en equipo para asegurarnos de que confía en su configuración y en el uso del Administrador de etiquetas de Google en su sitio. Incluso ofrezco controles de seguimiento y monitoreo después de mis capacitaciones para garantizar que se sienta cómodo con la herramienta y cómo puede desarrollarse para que coincida con el crecimiento y los cambios de su sitio web.&lt;/p&gt;

&lt;p&gt;Mis servicios de formación in-company serán presenciales en vuestras oficinas en un máximo de 8 - 12 horas, siendo posible más horas a distancia via streaming, u online a través de la plataforma Moodle de propiedad de una escuela digital.&lt;/p&gt;

&lt;h3 id=&#34;está-interesado-en-consultar-o-capacitarse-en-google-tag-manager&#34;&gt;¿Está interesado en consultar o capacitarse en Google Tag Manager?&lt;/h3&gt;

&lt;p&gt;Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.marcusrb.com/#contact&#34;&gt;Solicita información para Google Tag Manager&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Formación in-company &amp; docencia en escuelas de negocios</title>
      <link>https://www.marcusrb.com/formacion-in-company-docencia-escuelas-negocio/</link>
      <pubDate>Mon, 01 Jun 2020 18:31:41 +0000</pubDate>
      <guid>https://www.marcusrb.com/formacion-in-company-docencia-escuelas-negocio/</guid>
      <description>

&lt;p&gt;En los últimos 10 años, he participado en los programas formativos de muchas escuelas de negocio, universidades y centro de formación de la administración pública. He realizado servicio de tutoría, docencia de asignaturas, formación a grandes empresas, pymes - autónomos, particulares.&lt;/p&gt;

&lt;h2 id=&#34;docencia-para-programas-de-posgrados-máster-cursos-y-webinar&#34;&gt;Docencia para programas de posgrados, máster, cursos y webinar.&lt;/h2&gt;

&lt;p&gt;Con más de 10 años de expieriencia en sector formativo, colaboro con diferentes escuelas de negocio y universidades, así como administración públicas para realizar cursos, seminarios o participar en congresos en materia de analítica digital, analítica de datos y visualización.&lt;/p&gt;

&lt;p&gt;He preparado varios programas, módulos y cursos personalizados de fundamentos hasta niveles más avanzados con duración máxima de 120 horas de formación a un público de casi 100 alumnos en algunos casos. Las formaciones pueden ser presenciales, a distancia, grabadas o en formato blended.&lt;/p&gt;

&lt;p&gt;Gestiono y preparo:
- el cuadro formativo
- tutoría y asesoramiento
- los módulos en formato presentación
- los casos prácticos
- los laboratorios durante las horas lectivas
- las prácticas y soluciones
- grabaciones&lt;/p&gt;

&lt;h2 id=&#34;formación-in-company-en-analítica-de-negocio&#34;&gt;Formación in-company en Analítica de negocio&lt;/h2&gt;

&lt;p&gt;Tanto si tiene un equipo de marketing familiarizado con herramientas de análisis digitales como &lt;strong&gt;Google Analytics&lt;/strong&gt; y sus productos relacionados y solo está buscando ayuda para comenzar a implementar y usar el Administrador de etiquetas de &lt;strong&gt;Tag Manager&lt;/strong&gt;, también ofrezco sesiones de capacitación personalizadas. Para estas sesiones, trabajaré con usted personalmente o en equipo para asegurarnos de que confía en su configuración y en el uso del Administrador de etiquetas de Google en su sitio. Incluso ofrezco controles de seguimiento y monitoreo después de mis capacitaciones para garantizar que se sienta cómodo con la herramienta y cómo puede desarrollarse para que coincida con el crecimiento y los cambios de su sitio web. Tengo amplia experiencia también en otras herramientas de publicidad &lt;strong&gt;Google Ads&lt;/strong&gt; y &lt;strong&gt;FacebookAds&lt;/strong&gt; , me limito a formación y sesiones de auditoría.&lt;/p&gt;

&lt;p&gt;Para el resto de formaciones a medidas con un &lt;strong&gt;mínimo de 10 horas&lt;/strong&gt;, relacionadas con el resto de disciplinas de Data, tengo años de experiencia en profesorado y formaciones en:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sesiones de Data Discovery&lt;/li&gt;
&lt;li&gt;Preparación de indicadores y fuentes de datos&lt;/li&gt;
&lt;li&gt;SQL y bases de datos&lt;/li&gt;
&lt;li&gt;Modelado de datos&lt;/li&gt;
&lt;li&gt;ETL, preparación y transformación de datos&lt;/li&gt;
&lt;li&gt;Visualización con Power BI, Tableau, Data Studio, AWS QuickSight&lt;/li&gt;
&lt;li&gt;Minería de datos y Machine Learning&lt;/li&gt;
&lt;li&gt;Deep Learning&lt;/li&gt;
&lt;li&gt;Python para data science y analísis de datos, Pandas - Numpy - Scipy - Scikit-learn, visualización Matplotlib / Seaborn&lt;/li&gt;
&lt;li&gt;R Studio&lt;/li&gt;
&lt;li&gt;Cloud Engineer en AWS, Google Cloud Platform, Microsoft Azure y DataBricks&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Mis servicios de formación in-company serán presenciales en vuestras oficinas, siendo posible a distancia via streaming u online a través de las plataformas habituales (Zoom, Google Meet, Teams, etc.)&lt;/p&gt;

&lt;h3 id=&#34;está-interesado-en-capacitarse-en-una-de-estas-disciplinas-o-quieres-contarme-algo-más&#34;&gt;¿Está interesado en capacitarse en una de estas disciplinas o quieres contarme algo más?&lt;/h3&gt;

&lt;p&gt;Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../#contact&#34;&gt;Solicita información para Formación en Datos&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kaggle nivel 1</title>
      <link>https://www.marcusrb.com/en/talk/kaggle_1/</link>
      <pubDate>Thu, 16 Apr 2020 12:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/talk/kaggle_1/</guid>
      <description>

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;cómo-participar-en-retos-kaggle-dedata-science&#34;&gt;Cómo participar en retos Kaggle deData Science.&lt;/h1&gt;

&lt;p&gt;Nivel 1.&lt;/p&gt;

&lt;p&gt;16 de Abril, 12-1h&lt;/p&gt;

&lt;h2 id=&#34;cómo-participar-en-retos-kaggle-de-data-science-nivel-1&#34;&gt;Cómo participar en retos Kaggle de Data Science. Nivel 1.&lt;/h2&gt;

&lt;p&gt;Tanto si ya tienes algo de experiencia en Data Analytics como si no, no puedes perderte este evento de Data Science en el que trabajaremos directamente en la plataforma Kaggle con Python. Haremos el reto más &amp;ldquo;famoso&amp;rdquo;, pero también descubriremos métodos y técnicas útiles para otros retos. ¡Por cierto! habrá un premio para el primero.&lt;/p&gt;

&lt;p&gt;Se requiere la instalación previa en vuestro ordenador del programa Anaconda, Docker con Jupyter y Visual Studio. No dudes en contactar con nosotros antes del Webinar por si necesitas instalar alguna herramienta adicional.&lt;/p&gt;

&lt;p&gt;Qué veremos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Organización del entorno de trabajo&lt;/li&gt;
&lt;li&gt;Aplicación de Metodologías para la exploración de datos&lt;/li&gt;
&lt;li&gt;Entrenamiento de tu primer modelo de aprendizaje automático&lt;/li&gt;
&lt;li&gt;Enfrentarse a las competiciones de &amp;ldquo;Primeros pasos&amp;hellip;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Competir para maximizar los aprendizajes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para apuntaros al webinar, tenéis que acceder al siguiente formulario (&lt;a href=&#34;https://eventos.paradigmadigital.com/kaggle-data-science&#34; target=&#34;_blank&#34;&gt;https://eventos.paradigmadigital.com/kaggle-data-science&lt;/a&gt;) e introducir vuestros datos. El día del evento, recibiréis por email la url del webinar para que podáis conectaros y participar en el mismo.&lt;/p&gt;

&lt;h2 id=&#34;quién-es-el-ponente&#34;&gt;¿Quién es el ponente?&lt;/h2&gt;

&lt;p&gt;Marco Russo&lt;/p&gt;

&lt;p&gt;Consultor y Especialista en Data &amp;amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.&lt;/p&gt;

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/UzwRO4hj8c8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Proyectos realizados en R Studio</title>
      <link>https://www.marcusrb.com/en/projects/r-studio/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0200</pubDate>
      <guid>https://www.marcusrb.com/en/projects/r-studio/</guid>
      <description>

&lt;h2 id=&#34;se-creará-una-sección-especial-con-varios-ejemplos-del-lenguaje-r-para-aprender-la-análisis-de-datos&#34;&gt;Se creará una sección especial con varios ejemplos del lenguaje R, para aprender la análisis de datos.&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt; es un lenguaje de programación para la gestión y la análisis de datos, además de visualización de gráficos. Es un software libre y disponible en diferentes entornos (Unix, Linux, MacOS, Windows).&lt;/p&gt;

&lt;p&gt;Esta primera sección se especificará como instalar y como utilizarlas. Además de contribuir a añadir varios ejemplos de script para la exploración de datos, limpieza, uso de funciones matemáticas y estadísticas, aprendizaje automático y casos de uso como solución de negocio.&lt;/p&gt;

&lt;p&gt;Unos de los primeros proyectos realizados será la exploración de los datos, o EDA (Explorationa Data Analysis), pero con datos de &lt;strong&gt;Google Analytics&lt;/strong&gt;, es decir exploraremos los datos de un sitio web y que conclusiones podemos sacar con esto.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../r-studio/cheatsheets&#34;&gt;Cheatsheets&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Consultor freelance Machine Learning &amp; AI</title>
      <link>https://www.marcusrb.com/consultoria-freelance-machine-learning/</link>
      <pubDate>Sun, 01 Sep 2019 18:43:20 +0000</pubDate>
      <guid>https://www.marcusrb.com/consultoria-freelance-machine-learning/</guid>
      <description>

&lt;h2 id=&#34;machine-learning-and-deep-learning-consulting&#34;&gt;Machine Learning and Deep Learning Consulting&lt;/h2&gt;

&lt;p&gt;¿Cuál es la diferencia entre ML y consultoría de IA?&lt;/p&gt;

&lt;p&gt;Aunque el aprendizaje automático (ML) es el subcampo de la IA con la mayoría de las aplicaciones comerciales, es mejor distinguirlas.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IA: incluye todas las aplicaciones en las que la computadora imita la inteligencia humana
ML: aplicaciones que utilizan datos conocidos para crear modelos que se pueden utilizar para clasificar / procesar nuevos datos
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;¿ML consulting = consultoría de aprendizaje profundo?&lt;/p&gt;

&lt;p&gt;No exactamente. El aprendizaje profundo es un subconjunto del aprendizaje automático. Sin embargo, el aprendizaje profundo es la técnica de aprendizaje automático más exitosa en términos de precisión a partir de 2019 en la mayoría de las áreas.&lt;/p&gt;

&lt;p&gt;No es raro ver que en la industria se implementen técnicas alternativas como los bosques de decisiones en lugar del aprendizaje profundo. Esto se debe a que la falta de explicación de los resultados es un desafío para los modelos de aprendizaje profundo. Hay casos en los que los modelos de aprendizaje profundo no se implementan en producción.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ya que los gerentes no se sienten cómodos con modelos que no comprenden y que no brindan una explicación de los resultados.
en los casos en que se requiera auditabilidad. Por ejemplo, la legislación laboral prohíbe la discriminación. Cualquier algoritmo que utilice criterios que se hayan utilizado anteriormente para la discriminación (es decir, género o raza) no puede tomar decisiones de recursos humanos legalmente sin proporcionar una justificación que involucre razones distintas a esos criterios. Lamentablemente, excluir del modelo criterios potencialmente discriminatorios no resuelve el problema. Por ejemplo, el nombre, los patrones en PTO, la brecha salarial y muchos otros puntos de datos podrían usarse para incluir indirectamente el género en la toma de decisiones. Los modelos de caja negra, sin importar cuán precisos o útiles sean sus resultados, no se pueden implementar en tales situaciones.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Explicar el aprendizaje profundo es un área activa de investigación llamada XAI (IA explicable).
¿Cuáles son las barreras para la adopción del AA?&lt;/p&gt;

&lt;p&gt;Como destaca Deloitte, estas son las barreras mencionadas con mayor frecuencia según los profesionales:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Escasez de talento: a agosto de 2018, había 150.000 puestos de trabajo de ciencia de datos sin completar en los EE. UU. Que Linkedin describió como una escasez aguda en las grandes ciudades de EE. UU.
Inmadurez de la infraestructura y los procesos de ML: ML es un nuevo paradigma de programación, que deriva reglas de los datos en lugar de la entrada del programador. Nos tomó decenas de años crear Scrum, el enfoque de programación ágil, que la mayoría de los equipos utilizan actualmente. Del mismo modo, se necesitará tiempo para que los procesos y marcos de ML alcancen la madurez. TensorFlow, uno de los marcos de aprendizaje automático más utilizados, se publicó a finales de 2015.
La mayoría de las técnicas de aprendizaje automático consumen mucha información: los datos de entrenamiento etiquetados con precisión requieren mucho tiempo y son costosos de generar. Los profesionales del aprendizaje automático deben ser creativos al aprovechar los datos públicos o etiquetar los datos necesarios. Es por eso que se fundaron numerosas empresas de etiquetado de datos desde la década de 2010. Otra solución a esto es el aprendizaje de una sola vez y otros enfoques que requieren menos datos; sin embargo, esta es un área de investigación en curso.
El aprendizaje profundo no se puede explicar. Como se discutió, esto está obstaculizando el progreso y XAI intenta abordarlo.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;¿Cuál es el futuro de la consultoría de aprendizaje automático?&lt;/p&gt;

&lt;p&gt;La consultoría de ML crecerá al abordar los problemas identificados:&lt;/p&gt;

&lt;p&gt;Expansión del grupo de talentos: la mayoría de las consultorías están analizando su fuerza laboral en detalle para identificar a aquellos que son capaces de la ciencia de datos. Una formación en programación, estadística o matemáticas tiende a ser suficiente para que las personas trabajen como científicos de datos después de una formación relativamente rápida.&lt;/p&gt;

&lt;p&gt;Mejorar la infraestructura y los procesos de ML: a medida que ML madura como paradigma de programación, mejores procesos, mejores recursos informáticos (es decir, GPU y chips de IA) y más automatización harán que ML sea más rápido y más fácil.&lt;/p&gt;

&lt;p&gt;Ser creativo con los datos: los avances en el procesamiento del lenguaje natural (NLP) se debieron a la amplia disponibilidad de documentos gubernamentales traducidos en Canadá y Europa. Si bien la búsqueda de datos es una solución relativamente sencilla, las áreas de investigación de la inteligencia artificial, como el aprendizaje por transferencia o la síntesis de datos, podrían ser soluciones más técnicas.&lt;/p&gt;

&lt;p&gt;También se esperan avances en la IA explicable que aumentarían la confianza en los sistemas ML y permitirían su adopción más generalizada.&lt;/p&gt;

&lt;p&gt;Por último, es probable que las aplicaciones locales de aprendizaje automático hagan que las aplicaciones de IoT sean más inteligentes y rápidas al llevar la toma de decisiones a los dispositivos periféricos.
¿Cuáles son las actividades típicas de consultoría de ML?
Comprender las necesidades comerciales&lt;/p&gt;

&lt;p&gt;Como en toda consultoría, todo comienza con la necesidad empresarial. Ya sea que se trate de predecir dónde instalar estaciones base de telecomunicaciones o a quién mostrar anuncios, malinterpretar los requisitos comerciales sigue siendo una de las principales razones de la falta de éxito de los proyectos de consultoría y software. La consultoría ML, en la intersección de la consultoría y el software, es especialmente propensa a este problema.
Configurar el equipo y el proceso&lt;/p&gt;

&lt;p&gt;No todos los problemas necesitan aprendizaje automático. El aprendizaje automático y otros enfoques heurísticos tienen sentido en problemas que no pueden reducirse a un conjunto de reglas. Si las reglas son bien conocidas y simples, los sistemas basados ​​en reglas superan al aprendizaje automático y son más simples de mantener.&lt;/p&gt;

&lt;p&gt;Si ML es un buen ajuste fo es necesario delinear un problema, el equipo del proyecto, las partes interesadas y los objetivos de alto nivel.
Recolección y exploración de datos&lt;/p&gt;

&lt;p&gt;Si la empresa tiene los datos, este es un paso relativamente sencillo. Los consultores deben trabajar con las empresas para validar que los datos estén correctamente etiquetados y no sean contradictorios.&lt;/p&gt;

&lt;p&gt;Si los datos no están disponibles, se deben considerar las técnicas descritas anteriormente, como aprovechar los datos en línea, pagar por el etiquetado de datos y enfoques novedosos de ML, como el aprendizaje de una sola vez.
Modelo de desarrollo&lt;/p&gt;

&lt;p&gt;Se necesitan miles de experimentos para desarrollar un modelo de aprendizaje automático de alto rendimiento. Este es un proceso iterativo que tiene en cuenta las últimas investigaciones, comprende la dinámica empresarial y la exploración de datos.&lt;/p&gt;

&lt;p&gt;En última instancia, todos los modelos se evalúan con el mismo conjunto de datos de prueba para evaluar su precisión.
Desarrollo de aplicaciones de pila completa&lt;/p&gt;

&lt;p&gt;Llevar un modelo a producción requiere trabajo adicional de desarrollo e integración de software.&lt;/p&gt;

&lt;p&gt;La mayoría de las veces, los modelos ML están encapsulados en API que son fáciles de integrar con cualquier aplicación. El desarrollo de la aplicación que operacionalizará el modelo ML y lo hará parte del proceso de toma de decisiones puede ser más difícil que construir el modelo. El desarrollo de aplicaciones puede requerir la integración a los sistemas empresariales existentes, lo que requiere trabajar con desarrolladores externos.&lt;/p&gt;

&lt;p&gt;Los problemas de escalabilidad y seguridad de los datos también deben abordarse como parte de la puesta en funcionamiento del modelo.&lt;/p&gt;

&lt;h4 id=&#34;está-interesado-en-consultar-o-capacitarse-en-aprendizaje-automático&#34;&gt;¿Está interesado en consultar o capacitarse en Aprendizaje automático?&lt;/h4&gt;

&lt;p&gt;Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../#contact&#34;&gt;Solicita información en Machine Learning&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Consultoría freelance reportes Google Data Studio</title>
      <link>https://www.marcusrb.com/servicios-freelance-google-data-studio/</link>
      <pubDate>Sun, 01 Sep 2019 18:43:20 +0000</pubDate>
      <guid>https://www.marcusrb.com/servicios-freelance-google-data-studio/</guid>
      <description>

&lt;h2 id=&#34;servicios-freelance-de-google-data-studio&#34;&gt;Servicios freelance de Google Data Studio&lt;/h2&gt;

&lt;p&gt;Google Data Studio es una herramienta gratuita de visualización e informes de datos basada en la nube que se conecta a muchas fuentes de datos diferentes y convierte esos datos en paneles informativos e informes que son fáciles de entender y compartir, y son totalmente personalizables.&lt;/p&gt;

&lt;p&gt;Vea algunos de nuestros paneles en vivo, por ejemplo, informes de paneles de datos de Google Data Studio accionables&lt;/p&gt;

&lt;p&gt;Características clave
Google Data Studio es intuitivo, rápido, flexible y permite una gran cantidad de opciones de diseño y presentación.&lt;/p&gt;

&lt;p&gt;Amplia gama de conectores de datos.
Data Studio tiene 17 conectores de datos internos y alrededor de 108 de terceros para elegir&lt;/p&gt;

&lt;p&gt;Funciones fáciles de usar
Data Studio proporciona docenas de funciones matemáticas, de cadena, de fecha y otras para transformar sus datos en valores más útiles.&lt;/p&gt;

&lt;p&gt;Variedad de formas, imágenes y texto.
Data Studio le permite agregar formas, imágenes y texto a sus informes y paneles para que sean más fáciles de leer.&lt;/p&gt;

&lt;p&gt;Niveles de permisos
Aprovechando la tecnología Google Drive, puede administrar fácilmente a todos sus usuarios y su nivel de acceso&lt;/p&gt;

&lt;p&gt;Mezcla de datos, ahora una realidad
Data Studio le permite agregar datos de múltiples fuentes para tener una vista comparativa de ellos a la vez&lt;/p&gt;

&lt;h3 id=&#34;está-interesado-en-servicios-o-capacitarse-en-visualización-en-google-data-studio&#34;&gt;¿Está interesado en servicios o capacitarse en visualización en Google Data Studio ?&lt;/h3&gt;

&lt;p&gt;Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../#contact&#34;&gt;Solicita información para la consultoría freelance en Data Studio&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Consultoría optimización y análisis en Microsoft Power BI</title>
      <link>https://www.marcusrb.com/servicios-freelance-analisis-dax-power-bi/</link>
      <pubDate>Sun, 01 Sep 2019 18:43:20 +0000</pubDate>
      <guid>https://www.marcusrb.com/servicios-freelance-analisis-dax-power-bi/</guid>
      <description>

&lt;h2 id=&#34;qué-es-microsoft-power-bi&#34;&gt;¿Qué es Microsoft Power BI?&lt;/h2&gt;

&lt;p&gt;Ha pasado algún tiempo desde que Microsoft lanzó Power BI y la forma en que las cosas habían progresado para esta herramienta excepcional de Business Intelligence and Analytics, parecía que solo sería cuestión de tiempo antes de que se convirtiera en la plataforma preferida para BI y análisis con el mayoría de las empresas con visión de futuro. Power BI es una herramienta poderosa en manos de las empresas que desean extraer y convertir datos de múltiples fuentes dispares para obtener información significativa. Ofrece una experiencia de usuario sin precedentes con oportunidades de visualización interactiva junto con verdaderas capacidades de análisis de autoservicio. Todo esto ayuda a ver los mismos datos desde una variedad de ángulos, sin mencionar que los informes y paneles pueden ser creados por cualquier persona en la organización sin la ayuda de los administradores de TI. Algunos de los beneficios únicos de Power BI son:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;* Potentes gráficos y visualizaciones del tablero que se actualizan continuamente.
* Función de análisis en memoria y base de datos en columna que admite datos tabulares.
* Lo mejor de ambos mundos cuando se trata de facilidad de uso y rendimiento en una sola herramienta de BI.
* Geo-mapping interactivo con Bing Maps.
* Secuencias de comandos de expresiones de análisis de datos (DAX) para crear medidas y columnas.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vea algunos de nuestros paneles en vivo, por ejemplo, informes de panel de control de Power BI procesables&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ideepeners.com/wp-content/uploads/2019/12/BI_1.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;por-qué-necesita-power-bi&#34;&gt;¿Por qué necesita Power BI?&lt;/h2&gt;

&lt;p&gt;Permita que sus empleados y tomadores de decisiones analicen los datos más rápido con una mejor eficiencia y comprensión utilizando el servicio de análisis basado en la nube Power BI proporciona a los usuarios una amplia gama de información a través de paneles simplificados pero efectivos, informes precisos y visualizaciones atractivas, convirtiéndose así en una herramienta perfecta para dar vida a los datos.&lt;/p&gt;

&lt;p&gt;Power BI se conecta a cualquier fuente de datos y ofrece información empresarial convincente a un costo muy bajo, brindando análisis en tiempo real utilizando un tablero efectivo en varios dispositivos como computadoras de escritorio, dispositivos móviles, tabletas, etc. Microsoft Power BI está diseñado de una manera que no requiere finalización que los usuarios tengan habilidades de programación para explorar, analizar y procesar los datos para tomar decisiones comerciales mejor informadas.&lt;/p&gt;

&lt;h3 id=&#34;está-interesado-en-consultoría-o-capacitarse-en-visualización-en-power-bi&#34;&gt;¿Está interesado en consultoría o capacitarse en visualización en Power BI ?&lt;/h3&gt;

&lt;p&gt;Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../#contact&#34;&gt;Solicita información para la consultoría en Power BI&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Freelance Data Analytics and Business Intelligence</title>
      <link>https://www.marcusrb.com/en/freelance-data-analytics-business-intelligence-service/</link>
      <pubDate>Sun, 01 Sep 2019 18:43:20 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/freelance-data-analytics-business-intelligence-service/</guid>
      <description>

&lt;h2 id=&#34;data-analytics-business-intelligence-freelance&#34;&gt;Data Analytics &amp;amp; Business Intelligence freelance&lt;/h2&gt;

&lt;p&gt;It is a sum total concept that includes data analysis, for companies and to tell you that these are the absolute points that are missing in the growth of your company and with the help of these we can record enormous growth, I will provide you with a general summary of your data and we will generate an anonymous number of reports that will be a key aspect in the growth of our clients.&lt;/p&gt;

&lt;h3 id=&#34;are-you-interested-in-freelance-services-or-training-in-data-analytics-and-business-intelligence&#34;&gt;Are you interested in freelance services or training in Data Analytics and Business Intelligence?&lt;/h3&gt;

&lt;p&gt;Call me today or use the online form below. Thanks a lot!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../#contact&#34;&gt;Request information in Data analytics, Business Intelligence&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Freelance en gestión y mantenimiento dashboard en Tableau</title>
      <link>https://www.marcusrb.com/servicios-freelance-tableau/</link>
      <pubDate>Sun, 01 Sep 2019 18:43:20 +0000</pubDate>
      <guid>https://www.marcusrb.com/servicios-freelance-tableau/</guid>
      <description>

&lt;h2 id=&#34;servicio-de-consultoría-freelance-en-tableau&#34;&gt;Servicio de consultoría freelance en Tableau&lt;/h2&gt;

&lt;p&gt;Creemos en el futuro del análisis visual del mundo empresarial. Si aterrizaste aquí, probablemente estés en algún lugar del mismo camino.&lt;/p&gt;

&lt;p&gt;Para ayudarlo en su viaje, ofrecemos ayuda y consultoría de Tableau.&lt;/p&gt;

&lt;p&gt;Somos un socio de Tableau que trabajamos para ayudar a las personas a aprovechar al máximo el software y responder las preguntas correctas con datos.&lt;/p&gt;

&lt;h2 id=&#34;por-qué-necesita-consultar-tableau&#34;&gt;¿Por qué necesita consultar Tableau?&lt;/h2&gt;

&lt;p&gt;¡Aceptar asistencia no es una debilidad! Realmente creemos que un buen líder sabe cuándo pedir ayuda. Probablemente todos hemos aprendido que en algún momento podríamos usar algo de soporte.&lt;/p&gt;

&lt;p&gt;¡La velocidad es vital en el análisis de autoservicio! Permítanme repetirlo: ¡la velocidad es el juego!&lt;/p&gt;

&lt;p&gt;Si tiene la oportunidad de encontrar un consultor de Tableau que obtenga sus puntos débiles, es posible que pueda acelerar su progreso de una manera que no creía posible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Entonces, ¿cómo podemos ayudar?&lt;/strong&gt;
Un enfoque creativo para la resolución de problemas. Ayuda diligente de Tableau, comunicación clara, hacer las cosas de la manera correcta.&lt;/p&gt;

&lt;p&gt;¡Simple como eso!&lt;/p&gt;

&lt;h3 id=&#34;una-pasión-por-la-visualización-de-datos&#34;&gt;Una pasión por la visualización de datos&lt;/h3&gt;

&lt;p&gt;Estamos trabajando con Tableau todos los días. ¡Alimentamos incansablemente a la bestia que es nuestro apetito por responder preguntas con datos!
Nuestra experiencia profesional en análisis web nos brinda una vanguardia. Sobre todo entendemos la importancia de la corrección y la calidad de los datos.&lt;/p&gt;

&lt;p&gt;Creemos que cuando te apasiona algo, el trabajo se vuelve mucho más fácil. Para nosotros, ¡ni siquiera parece trabajo!
El amor por trabajar con datos es lo que nos impulsa. Tenacidad, dedicación y creatividad son lo que obtendrá de nosotros.&lt;/p&gt;

&lt;h3 id=&#34;business-analytics-vs-business-intelligence&#34;&gt;Business Analytics vs. Business Intelligence&lt;/h3&gt;

&lt;p&gt;Creemos que puede beneficiarse de nuestro trabajo en ambas áreas. Al combinar la eficiencia (BI) con la creatividad (BA), estamos tratando de ayudar a las empresas a responder preguntas urgentes y formular otras nuevas.&lt;/p&gt;

&lt;p&gt;Hemos estado utilizando un amplio espectro de fuentes de datos: desde Big Data como Exasol o Vertica hasta bases de datos clásicas como MySQL o Postgres, y otras menos tradicionales como Google Analytics y conectores de datos web.&lt;/p&gt;

&lt;h3 id=&#34;diseño-e-implementación-del-tablero-de-instrumentos&#34;&gt;Diseño e implementación del tablero de instrumentos&lt;/h3&gt;

&lt;p&gt;Para nosotros, un Tableau Dashboard es solo la punta del iceberg. La regla de pasar el 80% del tiempo trabajando con los datos y el 20% en la visualización en sí se aplica a nuestro caso.&lt;/p&gt;

&lt;p&gt;No solo eso, sino que el principio de Pareto también es relevante para la cantidad de hojas que creamos versus cuántas guardamos en la versión final del tablero.&lt;/p&gt;

&lt;p&gt;¡La velocidad de probar diferentes enfoques en Tableau es fantástica! Sería una pena si no lo usáramos.&lt;/p&gt;

&lt;p&gt;Por lo general, comenzamos construyendo muchos gráficos diferentes (a veces cientos), y conservamos solo los que consideramos relevantes para la historia.&lt;/p&gt;

&lt;p&gt;Puede encontrar algunos ejemplos de visualizaciones de datos que hemos creado al final de esta página. Y algunos paneles más creativos en el perfil de Tableau Public de Dorian (nuestro jefe de Tableau).&lt;/p&gt;

&lt;h3 id=&#34;optimización-del-rendimiento-de-tableau&#34;&gt;Optimización del rendimiento de Tableau&lt;/h3&gt;

&lt;p&gt;Tenemos un estudio de caso sobre el ajuste del rendimiento de Tableau, que debe leer si tiene ~ 15 minutos disponibles.&lt;/p&gt;

&lt;p&gt;La idea principal es que no nos centremos únicamente en la rapidez con la que podemos hacer un tablero, sino también en lo rápido que puede responder a sus preguntas.&lt;/p&gt;

&lt;p&gt;Además, trabajar con grandes cantidades de datos es complicado. Sin embargo, estamos aquí para romper esos muros.&lt;/p&gt;

&lt;h4 id=&#34;licencias-y-soporte&#34;&gt;Licencias y soporte&lt;/h4&gt;

&lt;p&gt;¿Desea tomar la decisión correcta con respecto a la configuración de Tableau dentro de su organización?&lt;/p&gt;

&lt;p&gt;Podemos ayudarlo a descubrir cuál es la combinación correcta de:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Productos de Tableau: escritorio, preparación, servidor, en línea&lt;/li&gt;
&lt;li&gt;Roles de usuario: (Creador, Explorador, Visor)&lt;/li&gt;
&lt;li&gt;Licencias: licencias básicas basadas en el usuario, análisis integrados, gestión de datos&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;También estamos disponibles para ofrecer soporte técnico para su servidor si lo necesita.&lt;/p&gt;

&lt;p&gt;Uno de nuestros objetivos es ayudar a los clientes a dormir mejor. Esta parte cae directamente en esa área.&lt;/p&gt;

&lt;p&gt;Experiencia complementaria con R y Python
Tecnologías como R y Python nos permiten trabajar con los datos en el nivel más cercano.&lt;/p&gt;

&lt;p&gt;Por nombrar algunas cosas para las que hemos utilizado modelos estadísticos creados con R junto con Tableau para duplicar el análisis:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Modelos de atribución de Markov: atribución de canal de marketing más inteligente que el promedio para tiendas de comercio electrónico&lt;/li&gt;
&lt;li&gt;Pronosticar usando algoritmos que son un poco más avanzados (por ejemplo, el Profeta de Facebook) que los básicos (como el que usa Tableau)&lt;/li&gt;
&lt;li&gt;Cálculos bayesianos: los desarrollamos para cortar y cortar los resultados de las pruebas A / B dentro de Tableau&lt;/li&gt;
&lt;li&gt;Comprobaciones de impacto causal: estamos trabajando con este algoritmo para evaluar si los crecimientos o las caídas son el resultado de algo que hicimos o simplemente ruido aleatorio en los datos&lt;/li&gt;
&lt;li&gt;Flujos de trabajo de Business Intelligence: programación de informes usando Python junto con tabcmd (la interfaz de línea de comando para Tableau Server)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;está-interesado-en-consultoría-o-capacitarse-en-visualización-en-tableau&#34;&gt;¿Está interesado en consultoría o capacitarse en visualización en Tableau ?&lt;/h3&gt;

&lt;p&gt;Llámame hoy o utiliza el formulario en línea abajo indicado. Muchas gracias!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../#contact&#34;&gt;Solicita información para la consultoría en Tableau&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing technical content in Academic</title>
      <link>https://www.marcusrb.com/en/post/writing-technical-content/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/post/writing-technical-content/</guid>
      <description>&lt;p&gt;Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Highlight your code snippets, take notes on math classes, and draw diagrams from textual representation.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;On this page, you&#39;ll find some examples of the types of technical content that can be rendered with Academic.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;

&lt;p&gt;Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the &lt;code&gt;highlight&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```python
import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;renders as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;math&#34;&gt;Math&lt;/h3&gt;

&lt;p&gt;Academic supports a Markdown extension for $\LaTeX$ math. You can enable this feature by toggling the &lt;code&gt;math&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file and adding &lt;code&gt;markup: mmark&lt;/code&gt; to your page front matter.&lt;/p&gt;

&lt;p&gt;To render &lt;em&gt;inline&lt;/em&gt; or &lt;em&gt;block&lt;/em&gt; math, wrap your LaTeX math with &lt;code&gt;$$...$$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Example &lt;strong&gt;math block&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;$$\gamma_{n} = \frac{ 
\left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T 
\left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}
{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;renders as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Example &lt;strong&gt;inline math&lt;/strong&gt; &lt;code&gt;$$\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2$$&lt;/code&gt; renders as &lt;span  class=&#34;math&#34;&gt;\(\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2\)&lt;/span&gt; .&lt;/p&gt;

&lt;p&gt;Example &lt;strong&gt;multi-line math&lt;/strong&gt; using the &lt;code&gt;\\&lt;/code&gt; math linebreak:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;$$f(k;p_0^*) = \begin{cases} p_0^* &amp;amp; \text{if }k=1, \\
1-p_0^* &amp;amp; \text {if }k=0.\end{cases}$$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;renders as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[f(k;p_0^*) = \begin{cases} p_0^* &amp; \text{if }k=1, \\
1-p_0^* &amp; \text {if }k=0.\end{cases}\]&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&#34;diagrams&#34;&gt;Diagrams&lt;/h3&gt;

&lt;p&gt;Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the &lt;code&gt;diagram&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file or by adding &lt;code&gt;diagram: true&lt;/code&gt; to your page front matter.&lt;/p&gt;

&lt;p&gt;An example &lt;strong&gt;flowchart&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```mermaid
graph TD;
  A--&amp;gt;B;
  A--&amp;gt;C;
  B--&amp;gt;D;
  C--&amp;gt;D;
```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;renders as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD;
  A--&amp;gt;B;
  A--&amp;gt;C;
  B--&amp;gt;D;
  C--&amp;gt;D;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An example &lt;strong&gt;sequence diagram&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```mermaid
sequenceDiagram
  participant Alice
  participant Bob
  Alice-&amp;gt;John: Hello John, how are you?
  loop Healthcheck
      John-&amp;gt;John: Fight against hypochondria
  end
  Note right of John: Rational thoughts &amp;lt;br/&amp;gt;prevail...
  John--&amp;gt;Alice: Great!
  John-&amp;gt;Bob: How about you?
  Bob--&amp;gt;John: Jolly good!
```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;renders as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;sequenceDiagram
  participant Alice
  participant Bob
  Alice-&amp;gt;John: Hello John, how are you?
  loop Healthcheck
      John-&amp;gt;John: Fight against hypochondria
  end
  Note right of John: Rational thoughts &amp;lt;br/&amp;gt;prevail...
  John--&amp;gt;Alice: Great!
  John-&amp;gt;Bob: How about you?
  Bob--&amp;gt;John: Jolly good!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An example &lt;strong&gt;Gantt diagram&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```mermaid
gantt
  dateFormat  YYYY-MM-DD
  section Section
  A task           :a1, 2014-01-01, 30d
  Another task     :after a1  , 20d
  section Another
  Task in sec      :2014-01-12  , 12d
  another task      : 24d
```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;renders as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;gantt
  dateFormat  YYYY-MM-DD
  section Section
  A task           :a1, 2014-01-01, 30d
  Another task     :after a1  , 20d
  section Another
  Task in sec      :2014-01-12  , 12d
  another task      : 24d
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h3&gt;

&lt;p&gt;You can even write your todo lists in Academic too:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;- [x] Write math example
- [x] Write diagram example
- [ ] Do something else
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;renders as&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked=&#34;&#34; disabled=&#34;&#34;&gt; Write math example&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked=&#34;&#34; disabled=&#34;&#34;&gt; Write diagram example&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;&gt; Do something else&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;tables&#34;&gt;Tables&lt;/h3&gt;

&lt;p&gt;Represent your data in tables:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;| First Header  | Second Header |
| ------------- | ------------- |
| Content Cell  | Content Cell  |
| Content Cell  | Content Cell  |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;renders as&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;First Header&lt;/th&gt;
&lt;th&gt;Second Header&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;asides&#34;&gt;Asides&lt;/h3&gt;

&lt;p&gt;Academic supports a Markdown extension for asides, also referred to as &lt;em&gt;notices&lt;/em&gt; or &lt;em&gt;hints&lt;/em&gt;. By prefixing a paragraph with &lt;code&gt;A&amp;gt;&lt;/code&gt;, it will render as an aside. You can enable this feature by adding &lt;code&gt;markup: mmark&lt;/code&gt; to your page front matter, or alternatively using the &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#alerts&#34;&gt;&lt;em&gt;Alert&lt;/em&gt; shortcode&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;A&amp;gt; A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;renders as&lt;/p&gt;

&lt;aside&gt;
&lt;p&gt;A Markdown aside is useful for displaying notices, hints, or definitions to your readers.&lt;/p&gt;
&lt;/aside&gt;

&lt;h3 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Meetup - Escalando de nivel con Google Tag Manager</title>
      <link>https://www.marcusrb.com/en/talk/google_tag_manager_2/</link>
      <pubDate>Tue, 25 Jun 2019 18:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/talk/google_tag_manager_2/</guid>
      <description>

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;destripando-google-tag-manager&#34;&gt;Destripando Google Tag Manager&lt;/h1&gt;

&lt;p&gt;Nivel 1.&lt;/p&gt;

&lt;p&gt;30 octubre 2018&lt;/p&gt;

&lt;h2 id=&#34;destripando-google-tag-manager-1&#34;&gt;Destripando Google Tag Manager&lt;/h2&gt;

&lt;p&gt;Después del primer Meetup sobre los fundamentos de Google Tag Manager, que está disponible en nuestro canal de YouTube, en este webinar vamos a realizar todo tipo de prácticas que nos permitirán conectar la interfaz más utilizada con el ecosistema de Google: Analytics, Marketing Platform como también Facebook Ads y herramientas de CRO. Pasaremos sucesivamente a las configuraciones avanzadas que nos permitirán subir de nivel y poder disponer de las métricas esenciales sin miedo de perder datos a la hora de analizar tu negocio.&lt;/p&gt;

&lt;h2 id=&#34;quién-es-el-ponente&#34;&gt;¿Quién es el ponente?&lt;/h2&gt;

&lt;p&gt;Marco Russo&lt;/p&gt;

&lt;p&gt;Consultor y Especialista en Data &amp;amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.&lt;/p&gt;

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/mWEzBtHtbmM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>https://www.marcusrb.com/en/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Análisis de datos con R</title>
      <link>https://www.marcusrb.com/en/projects/analisis-datos-con-r/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0100</pubDate>
      <guid>https://www.marcusrb.com/en/projects/analisis-datos-con-r/</guid>
      <description>

&lt;h2 id=&#34;se-creará-una-sección-especial-con-varios-ejemplos-del-lenguaje-r-para-aprender-la-análisis-de-datos&#34;&gt;Se creará una sección especial con varios ejemplos del lenguaje R, para aprender la análisis de datos.&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt; es un lenguaje de programación para la gestión y la análisis de datos, además de visualización de gráficos. Es un software libre y disponible en diferentes entornos (Unix, Linux, MacOS, Windows).&lt;/p&gt;

&lt;p&gt;Esta primera sección se especificará como instalar y como utilizarlas. Además de contribuir a añadir varios ejemplos de script para la exploración de datos, limpieza, uso de funciones matemáticas y estadísticas, aprendizaje automático y casos de uso como solución de negocio.&lt;/p&gt;

&lt;p&gt;Unos de los primeros proyectos realizados será la exploración de los datos, o EDA (Explorationa Data Analysis), pero con datos de &lt;strong&gt;Google Analytics&lt;/strong&gt;, es decir exploraremos los datos de un sitio web y que conclusiones podemos sacar con esto.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Meetup - Destripando Google Tag Manager</title>
      <link>https://www.marcusrb.com/en/talk/google_tag_manager_1/</link>
      <pubDate>Tue, 30 Oct 2018 18:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/talk/google_tag_manager_1/</guid>
      <description>

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;destripando-google-tag-manager&#34;&gt;Destripando Google Tag Manager&lt;/h1&gt;

&lt;p&gt;Nivel 1.&lt;/p&gt;

&lt;p&gt;30 octubre 2018&lt;/p&gt;

&lt;h2 id=&#34;destripando-google-tag-manager-1&#34;&gt;Destripando Google Tag Manager&lt;/h2&gt;

&lt;p&gt;Hemos puesto este título un poco en honor a la fiesta de Halloween, pero realmente hay cierto &amp;ldquo;miedo&amp;rdquo; al utilizar la herramienta de medición más utilizada del mundo por parte de muchos usuarios.&lt;/p&gt;

&lt;p&gt;En esta ocasión le toca el turno a &amp;ldquo;Google Tag Manager&amp;rdquo;, vamos a conocer los casos típicos de uso, y no solo de la forma sencilla, gracias a nuestro experto Marco Russo.&lt;/p&gt;

&lt;p&gt;Aunque generalmente es utilizada por profesionales de digital marketing y analítica digital, es una herramienta todo-terreno, válida también para realizar pruebas de performance, testing front y back-end, interactuando con el DOM, jugar con script de Google AppScript y Google Spreadsheet, etc.&lt;/p&gt;

&lt;p&gt;Pretendemos dar una visión general sobre las variadas opciones de GTM, y de cómo explotarlas al máximo mostrando ideas y casos prácticos.&lt;/p&gt;

&lt;p&gt;Marco Russo es consultor y especialista en Digital Data Analytics a nivel internacional trabajando para diferentes sectores industriales. Además, desde hace 6 años compagina su trabajo con la formación in-company y en diferentes escuelas de negocios y Cámara de Comercio, realizando módulos y cursos de Analytics, DataViz, CRO y Google Tag Manager. Cuando no piensa en los datos, además de compaginar su tiempo con la familia, puedes encontrarlo escalando montañas en la sierra de Madrid con su bici de carretera o en una cancha de basket.&lt;/p&gt;

&lt;h2 id=&#34;quién-es-el-ponente&#34;&gt;¿Quién es el ponente?&lt;/h2&gt;

&lt;p&gt;Marco Russo&lt;/p&gt;

&lt;p&gt;Consultor y Especialista en Data &amp;amp; Machine Learning, Business Analytics y Visualización de datos en Paradigma Digital, con más de 7 años de experiencias en diferentes sectores y clientes, además profesor para importantes escuelas de negocios y colaborador en la Universitat Oberta de Catalunya. Especializado en data mining, optimización de modelos y machine learning en área del Marketing, Retail y Banca-Finanzas entre otras. Cuando no estoy jugando con IoT, datos y robótica, dedico el tiempo con mi familia y a mi deporte favorito, bici de carretera.&lt;/p&gt;

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/xYc69otQHbg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://www.marcusrb.com/en/projects/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/projects/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://www.marcusrb.com/en/projects/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/projects/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>https://www.marcusrb.com/en/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://www.marcusrb.com/en/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://www.marcusrb.com/en/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
